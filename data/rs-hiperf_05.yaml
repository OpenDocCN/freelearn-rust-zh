- en: Profiling Your Rust Application
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析你的 Rust 应用程序
- en: One of the elementary steps to understand why your application is going slower
    than expected is to check what your application is doing at a low level. In this
    chapter, you will understand the importance of low-level optimization, and learn
    about some tools that will help you find where your bottlenecks are.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 理解为什么你的应用程序运行速度比预期慢的一个基本步骤是检查你的应用程序在低层次上正在做什么。在本章中，你将了解低级优化的重要性，并了解一些可以帮助你找到瓶颈的工具。
- en: 'In this chapter, you will learn about the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解以下主题：
- en: How the processor works at low level
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理器在低层次上的工作原理
- en: CPU caches
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU 缓存
- en: Branch prediction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分支预测
- en: How to fix some of the most common bottlenecks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何修复一些最常见的瓶颈
- en: How to use Callgrind to find your most-used code
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 Callgrind 找到你最常用的代码
- en: How to use Cachegrind to see where your code might be performing poorly in the
    cache
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 Cachegrind 查看你的代码在缓存中可能表现不佳的地方
- en: Learn how to use OProfile to know where your program spends most of its execution
    time
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用 OProfile 了解你的程序在执行时间上花费最多的地方
- en: Understanding the hardware
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解硬件
- en: To understand what our software is doing, we should first understand how the
    compiled code is running in our system. We will, therefore, start with how the
    **Central Processing Unit** (**CPU**) works.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解我们的软件正在做什么，我们首先应该了解编译后的代码是如何在我们的系统中运行的。因此，我们将从中央处理单元（**CPU**）的工作原理开始。
- en: Understanding how the CPU works
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 CPU 的工作原理
- en: The CPU is in charge of running the central logic of your application. Even
    if your application is a graphics application running most of its workload in
    the GPU, the CPU is still going to be governing all that process. There are many
    different CPUs, some faster than others for certain things, others that are more
    efficient and consume less power, sacrificing their computing power. In any case,
    Rust can compile for most CPUs, since it knows how they work.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 负责运行你应用程序的核心逻辑。即使你的应用程序是一个主要在 GPU 上运行大部分工作负载的图形应用程序，CPU 仍然会管理所有这些过程。有各种各样的
    CPU，有些在特定方面比其他 CPU 更快，有些则更高效且功耗更低，牺牲了一些计算能力。无论如何，Rust 可以编译为大多数 CPU，因为它知道它们是如何工作的。
- en: But our job here is to figure out how they work by ourselves, since sometimes
    the compiler won't be as efficient at improving our machine code as we are. So,
    let's get to the center of the processing, where things get done.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们的任务是自己弄清楚它们是如何工作的，因为有时编译器在改进我们的机器代码方面可能不如我们高效。所以，让我们深入到处理的核心，那里是事情发生的地方。
- en: 'The processor has a set of instructions it knows how to execute. We can ask
    it to perform any kind of instruction in that set, but we have a limitation: in
    most cases, it can only work with what''s called a **register**. A register is
    a small location near the **arithmetic and logical unit** (**ALU**) inside the
    CPU. It can contain one variable, as big as the processor word size; nowadays,
    that is 64 bits most of the time, but it can be 32, 16, or even 8 in some embedded
    processors.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器有一组它知道如何执行的指令。我们可以要求它执行该组中的任何类型的指令，但我们有一个限制：在大多数情况下，它只能与所谓的 **寄存器** 一起工作。寄存器是
    CPU 内部靠近 **算术逻辑单元**（**ALU**）的一个小位置。它可以包含一个变量，大小与处理器字大小相同；如今，这通常是 64 位，但在某些嵌入式处理器中可能是
    32 位、16 位甚至 8 位。
- en: Those registers go as fast as the processor itself, so information can be modified
    in them without having to wait for anything (well, just for the actual instructions
    to execute). This is great; in fact, you might be wondering, why do we even need
    RAM if we have registers?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些寄存器的速度与处理器本身一样快，因此可以在其中修改信息，而无需等待任何事情（好吧，只是实际指令执行）。这很棒；事实上，你可能想知道，如果我们已经有了寄存器，为什么还需要
    RAM？
- en: 'Well, the answer is simple: price. Having more registers in a CPU is very costly.
    You cannot have much more than a couple of dozen registers (in the best case scenario),
    since the processor wiring would get very complicated. Taking into account that
    for each instruction and register you will have dedicated data lines and control
    lines, this is very costly.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，答案很简单：价格。在 CPU 中拥有更多的寄存器是非常昂贵的。在最佳情况下，你不可能拥有超过几十个寄存器，因为处理器布线会变得非常复杂。考虑到每个指令和寄存器你都会有专用数据线和控制线，这是非常昂贵的。
- en: So, an external memory is required, one that can be freely accessed without
    requiring you to read all the memory sequentially, and still being very fast.
    **Random Access Memory** (**RAM**) is there for you. Your program will be loaded
    from RAM and will use RAM to store data that will need to be manipulated during
    the software execution. Of course, that software in RAM has to be loaded from
    the hard or solid state disk to RAM before being usable.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，需要一个外部内存，一个可以自由访问而不需要你按顺序读取所有内存的内存，同时仍然非常快。**随机存取存储器**（**RAM**）就在那里。你的程序将从RAM加载，并将使用RAM来存储在软件执行过程中需要被操作的数据。当然，RAM中的软件在可用之前必须从硬盘或固态硬盘加载到RAM中。
- en: The main issue with RAM is that even if it's much, much faster than even the
    fastest SSD out there, it's still not as fast as the processor. The processor
    can execute tens of instructions while waiting for the RAM to get some data into
    one of the processor's registers to be able to operate with them. So, to avoid
    having the processor waiting every time it needs to load or store something in
    RAM, we have caches.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: RAM的主要问题是即使它比最快的SSD（固态硬盘）快得多，它仍然不如处理器快。处理器可以在等待RAM将一些数据放入处理器的一个寄存器以便操作它们时执行数十条指令。因此，为了避免处理器每次需要从RAM中加载或存储数据时都要等待，我们有了缓存。
- en: Speeding up memory access with the cache
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用缓存加速内存访问
- en: 'The first level of cache, also known as the L1 cache, is a cache located almost
    as close as the registers to the ALU. The L1 cache is almost as fast as a register
    (about three times slower), and it has a very interesting property. Its internal
    structure can be represented as a lookup table. It will have two columns: the
    first will contain a memory address in the RAM, while the second will contain
    the contents of that memory address. When we need to load something from the RAM
    to a register, if it''s already in the L1 cache, it will be almost immediate.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第一级缓存，也称为L1缓存，是一个几乎与ALU（算术逻辑单元）一样靠近的缓存。L1缓存的速度几乎与寄存器一样快（大约慢三倍），并且它有一个非常有趣的特性。其内部结构可以表示为一个查找表。它将有两列：第一列将包含RAM中的内存地址，而第二列将包含该内存地址的内容。当我们需要从RAM中加载某物到寄存器时，如果它已经在L1缓存中，这将几乎是瞬间的。
- en: 'And not only that, this cache is usually divided in two very differentiated
    areas: the data cache and the instructions cache. The first will contain information
    about variables that the program is using, while the second will contain the instructions
    the program will execute. This way, since we know how the program is being executed,
    we can preload the next instructions in the instructions cache and execute them
    without having to wait for the RAM. In the same way, since we know what variables
    will be required in the execution of the program, we can preload them in the cache.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅如此，这个缓存通常被分为两个非常不同的区域：数据缓存和指令缓存。第一个将包含程序正在使用的变量的信息，而第二个将包含程序将要执行的指令。这样，因为我们知道程序是如何被执行的，我们可以在指令缓存中预加载下一条指令并执行它们，而无需等待RAM。同样，因为我们知道在程序执行过程中需要哪些变量，我们可以在缓存中预加载它们。
- en: But this cache has some problems too. Even if it's three times slower than the
    processor, it is still too expensive. Also, there is not much physical space so
    close to the processor, so its size is limited usually to about 32 KiB. Since
    most software requires more than that size, and we want it to execute fast without
    having to wait for the RAM, we usually have a second level of cache, called the
    L2 cache, that also runs at the processor's clock speed, but being farther away
    from the L1 cache makes its signal arrives with a higher latency.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这个缓存也有一些问题。即使它比处理器慢三倍，它仍然太昂贵。此外，在处理器附近没有太多的物理空间，所以其大小通常限制在大约32 KiB。由于大多数软件需要比这更大的空间，而我们希望它执行得快，无需等待RAM，我们通常有一个二级缓存，称为L2缓存，它也以处理器的时钟速度运行，但由于离L1缓存较远，其信号到达的延迟更高。
- en: The L2 cache is thus almost as fast as the L1 cache and usually has up to 4
    MiB of space. Instructions and data are combined. This is still not enough for
    many operations your software might do. Remember you will require all the data
    you are using, and in image processing, that might be millions of pixels with
    their value. So, for that, some high-end processors have an L3 cache in this case,
    farther away and with a slower clock, but still much faster than the RAM. Sometimes,
    this cache can be up to 32 MiB at the time of writing.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: L2 缓存因此几乎和 L1 缓存一样快，通常有高达 4 MiB 的空间。指令和数据是合并的。但这对于许多软件可能进行的操作来说仍然不够。记住，你需要使用你所有的数据，在图像处理中，那可能就是数百万像素及其值。因此，为此，一些高端处理器在这种情况下有一个
    L3 缓存，距离更远，时钟速度更慢，但仍然比 RAM 快得多。有时，这个缓存在写作时可以达到 32 MiB。
- en: But still, we know that even in processors such as the Ryzen processors, with
    more than 40 MiB of combined caches, we will need more space. We have the RAM,
    of course, since in the end, the cache is just a copy of the RAM we have close
    to the processor for faster use. For every new piece of information, we will need
    to load it from RAM to the L3 cache, then the L2, then the L1, and finally a register,
    making the process slower.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但即便如此，我们知道即使在像 Ryzen 处理器这样的处理器中，拥有超过 40 MiB 的组合缓存，我们仍然需要更多的空间。我们当然有 RAM，因为最终，缓存只是我们放在处理器附近以供更快使用
    RAM 的副本。对于每一条新的信息，我们都需要将其从 RAM 加载到 L3 缓存，然后是 L2，然后是 L1，最后是寄存器，这使得整个过程变慢。
- en: For this, processors have highly complex algorithms programmed in pure silicon,
    in hardware, that are able to predict what memory locations are going to be accessed
    next, and preload the locations in bursts. This means that if the processor knows
    you will access variables stored from address 1,000 to 2,000 in the RAM, it will
    request the RAM to load the whole batch of the memory in the L3 cache, and when
    the time to use them approaches, the L2 cache will copy the data from the L3,
    and so will the L1 from the L2\. When your program asks for that memory location's
    value, it will magically be in the L1 cache already, and be extremely fast to
    retrieve.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个，处理器有高度复杂的算法，这些算法是用纯硅编写的，在硬件中，能够预测将要访问的内存位置，并批量预加载这些位置。这意味着如果处理器知道你将访问 RAM
    中地址 1,000 到 2,000 的变量，它将请求 RAM 加载整个内存批次到 L3 缓存，当使用它们的时间接近时，L2 缓存将从 L3 复制数据，L1
    也从 L2 复制。当你的程序请求该内存位置的值时，它将神奇地已经在 L1 缓存中，并且检索速度极快。
- en: Cache misses
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存未命中
- en: But how well does this work? Is it possible to have 100% efficiency? Well, no.
    Sometimes, your software will do something the processor wasn't built to predict,
    and the data will not be in the L1 cache. This means that the processor will ask
    the L1 cache for the data, and this L1 cache will see that it doesn't have it,
    losing time. It will then ask the L2, the L2 will ask the L3, and so on. If you
    are lucky, it will be on the second level of cache, but it might not even be on
    the L3 and thus your program will need to wait for the RAM, after waiting for
    the three caches.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但这效果如何？是否可能达到 100% 的效率？好吧，不。有时，你的软件会做一些处理器没有设计去预测的事情，数据就不会在 L1 缓存中。这意味着处理器会向
    L1 缓存请求数据，而这个 L1 缓存会发现它没有，浪费了时间。然后它会询问 L2，L2 会询问 L3，以此类推。如果你很幸运，它会在第二级缓存中，但可能甚至不在
    L3 中，因此你的程序需要等待 RAM，在等待了三个缓存之后。
- en: This is what is called a cache miss. How often does this happen? Depending on
    how optimized your code and the CPU are, it might be between 2% and 5% of the
    time. It seems low, but when it happens, the whole processor stops to wait, which
    means that even if it doesn't happen many times, it has a huge performance impact,
    making things much slower. And as we saw, it's not only the loss of time of having
    to wait for the slower storage, it's also the lookup time in the previous storage
    that is lost, so in cache misses, it would have been faster to just ask the RAM
    directly (if the value wasn't in any cache).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所谓的缓存未命中。这种情况发生得多频繁？根据你的代码和 CPU 的优化程度，它可能在 2% 到 5% 之间。这似乎很低，但一旦发生，整个处理器都会停下来等待，这意味着即使它不会经常发生，它也会对性能产生巨大的影响，使事物变得非常慢。而且正如我们所见，这不仅仅是等待较慢存储的时间损失，还包括在先前存储中的查找时间损失，所以在缓存未命中中，直接询问
    RAM（如果值不在任何缓存中）会更快。
- en: How can you fix it?
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你该如何解决这个问题？
- en: It is not easy to fix this situation. Cache misses happen sometimes because
    you use big arrays only once, for example, in scenarios such as video buffers.
    If you are streaming some data, it might happen that the data is not yet in the
    cache and that it will no longer be used after using it once. This creates two
    problems. First, the time you need it, it's still in some area of the RAM, creating
    a cache miss. And second, once you load it into the cache, it will occupy most
    of the cache, forgetting about other variables you might require and creating
    more cache misses. This last effect is called **cache pollution**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 修复这种情况并不容易。缓存未命中有时发生是因为你只使用了一次大数组，例如在视频缓冲区等场景中。如果你正在流式传输一些数据，可能会发生数据尚未在缓存中，并且使用一次后就不会再使用。这会带来两个问题。首先，当你需要它时，它仍然在
    RAM 的某个区域，造成缓存未命中。其次，一旦你将其加载到缓存中，它将占用大部分缓存，忘记你可能需要的其他变量，并造成更多的缓存未命中。这种最后的效果被称为**缓存污染**。
- en: The best way to avoid this kind of behavior is to use smaller buffers, but this
    creates other problems, such as the data requiring constant buffering, so you
    will need to see what is best for your particular situation. If it's not caused
    by buffers, it might be that you are creating too many variables and only use
    them once. Try to find out whether you can reuse information, or whether you can
    change some executions for loops. But be careful, since some loops can affect
    branch prediction, as we will see later.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这种行为最好的方法是用更小的缓冲区，但这会带来其他问题，比如数据需要持续的缓冲，因此你需要看看对你特定情况来说什么才是最好的。如果不是由缓冲区引起的，可能是因为你创建了太多的变量，但只使用了一次。尝试找出你是否可以重用信息，或者是否可以改变一些循环的执行。但要注意，因为一些循环可能会影响分支预测，正如我们稍后将要看到的。
- en: Cache invalidation
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存失效
- en: There is another big issue with caches, called **cache invalidation**. Since,
    usually, in new processors, you use multithreaded applications, it sometimes happens
    that one thread changes some information in memory and that the other threads
    need to check it. As you might know, or as you will see in [Chapter 10](03028198-9025-4bfd-8677-215147e9400d.xhtml), *Multithreading*,
    Rust makes this perfectly safe at compile time. Data races won't happen, but it
    won't prevent cache invalidation performance issues.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存还有一个大问题，称为**缓存失效**。由于通常在新处理器中，你使用多线程应用程序，有时会发生一个线程更改内存中的某些信息，而其他线程需要检查它。正如你可能知道的那样，或者正如你将在第
    10 章[多线程](03028198-9025-4bfd-8677-215147e9400d.xhtml)中看到的，Rust 在编译时使这一点完全安全。数据竞争不会发生，但这不会防止缓存失效的性能问题。
- en: A cache invalidation happens in the case that some information in the RAM gets
    changed by another CPU or thread. This means that if that memory location was
    cached in any L1 to L3 caches, somehow it will need to be removed from there,
    since it will have old values. This is usually done by the storage mechanism.
    Whenever a memory address gets changed, any cache pointing to that memory address
    gets invalidated. This way, the next instruction trying to read the data from
    that address will create a cache miss, thus making the cache refresh and get data
    from the RAM.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当 RAM 中的某些信息被另一个 CPU 或线程更改时，就会发生缓存失效。这意味着如果该内存位置被任何 L1 到 L3 缓存缓存，它将需要以某种方式从那里移除，因为它将包含旧值。这通常是通过存储机制完成的。每当内存地址更改时，任何指向该内存地址的缓存都会被失效。这样，下一个尝试从该地址读取数据的指令将创建一个缓存未命中，从而使得缓存刷新并从
    RAM 获取数据。
- en: This is pretty inefficient, in any case, since every time you change a shared
    variable, that variable will require a cache refresh in the rest of the threads
    it gets used. In Rust, for that, you will be using an `Arc`. To try to avoid this
    kind of performance pitfall, you should try to share as little as possible between
    threads, and if messages have to be delivered to them, it might sometimes make
    sense to use structures in the `std::sync::mpsc` module, as we will see in [Chapter
    10](03028198-9025-4bfd-8677-215147e9400d.xhtml), *Multithreading*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，这都很低效，因为每次你更改一个共享变量时，该变量将需要在其他使用它的线程中刷新缓存。在 Rust 中，为此，你将使用一个 `Arc`。为了尝试避免这种性能陷阱，你应该尽量在线程之间共享尽可能少的内容，如果必须将消息传递给它们，有时使用
    `std::sync::mpsc` 模块中的结构体可能是有意义的，正如我们将在第 10 章[多线程](03028198-9025-4bfd-8677-215147e9400d.xhtml)中看到的。
- en: CPU pipeline
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPU流水线
- en: Instruction sets get more and more complex, and processors have faster and faster
    clock speeds, which sometimes makes most CPU instructions require more than one
    clock tick to execute. This is usually because the CPU needs to first understand
    what instruction is being executed, understand its operands, produce the meaningful
    signals to get those operands, perform the operations, and then save those operations.
    And no more than one step can be done per clock tick.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 指令集变得越来越复杂，处理器的时钟速度也越来越快，这有时使得大多数CPU指令需要多个时钟周期来执行。这通常是因为CPU需要首先理解正在执行的指令，理解其操作数，产生获取这些操作数的有效信号，执行操作，然后保存这些操作。并且每个时钟周期不能完成超过一个步骤。
- en: This is usually solved in processors by creating a CPU pipeline. This means
    that when a new instruction comes in, while that instruction gets analyzed and
    executed, the next instruction comes to the CPU to get analyzed. This has some
    complications, as you might imagine.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常在处理器中通过创建CPU流水线来解决。这意味着当一条新指令进来时，在指令被分析和执行的同时，下一条指令来到CPU以进行分析。这可能会带来一些复杂性，正如你可能想象的那样。
- en: First, if an instruction requires the output of a previous instruction, it might
    need to sometimes wait for the result of another instruction. It might also sometimes
    happen that the instruction being executed is a jump to another place in the memory
    so that new instructions need to be fetched from the RAM and the pipeline needs
    to be removed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果一条指令需要前一条指令的输出，它有时可能需要等待另一条指令的结果。也可能有时正在执行的指令是跳转到内存中的另一个位置，因此需要从RAM中获取新的指令，并移除流水线。
- en: 'Overall, what this technique achieves is to be able to execute one instruction
    per clock cycle in ideal conditions (once the pipeline is full). Most new processors
    do this, since it enables much faster execution without requiring a clock speed
    improvement, as we can see in this diagram:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这项技术实现的是在理想条件下（一旦流水线充满）每个时钟周期执行一条指令。大多数新型处理器都这样做，因为它可以在不提高时钟速度的情况下实现更快的执行，正如我们在这张图中可以看到的：
- en: '![](img/a2973ad4-d32a-45a5-b3e0-9fe358416125.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a2973ad4-d32a-45a5-b3e0-9fe358416125.png)'
- en: Another extra benefit from this approach is that dividing the instruction processing
    makes each step easier to implement, and not only that. Since each section will
    be physically smaller, electrons at light speed will be able to synchronize the
    whole step circuit in less time, making it possible for the clock to run faster.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的另一个额外好处是，将指令处理分解成更多步骤使得每一步更容易实现。不仅如此，由于每个部分在物理上会更小，光速下的电子能够在更短的时间内同步整个步骤电路，使得时钟可以运行得更快。
- en: In any case, though, dividing each instruction execution into more steps increases
    the complexity of the CPU wiring, since it has to fix potential concurrency issues.
    In the event that an instruction requires the output of the previous one to work,
    four different things can happen. As a first, and bad, option, it could happen
    that the behavior gets undefined. This is not what we want, and fixing this complicates
    the wiring of the processor.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无论如何，将每个指令执行分解成更多步骤会增加CPU布线的复杂性，因为它必须解决潜在的并发问题。如果一条指令需要前一条指令的输出才能工作，可能会发生四种不同的情况。首先，也是最糟糕的情况，行为可能会变得未定义。这不是我们想要的，解决这个问题会复杂化处理器的布线。
- en: The most important wiring piece to fix this is to first detect it. This on its
    own will make the wiring more complex. Once the CPU can detect the potential issue,
    the easiest fix is to simply wait for the output without advancing the pipeline.
    This is called **stalling**, and will hurt the performance of the CPU, but it
    will work properly.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的最重要的布线部分是首先检测它。仅此一项就会使布线更加复杂。一旦CPU能够检测到潜在的问题，最简单的修复方法就是简单地等待输出而不推进流水线。这被称为**停顿**，这会损害CPU的性能，但它会正常工作。
- en: Some processors will handle this by adding some extra input paths that will
    contain previous results, in case they need to be used, but this will greatly increase
    the complexity of the pipeline. Another option would be to detect some safety
    instructions and make them run before the instruction that requires the output
    of the previous one. This last option is called **out of order execution** and
    will also increase the complexity of the CPU.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一些处理器将通过添加一些额外的输入路径来处理这个问题，这些路径将包含之前的结果，以防需要使用它们，但这将大大增加流水线的复杂性。另一个选择是检测一些安全指令，并在需要前一个指令输出的指令之前运行它们。这个最后的选项被称为**乱序执行**，它也会增加CPU的复杂性。
- en: So, in conclusion, to improve the speed of a CPU, apart from making its clock
    run faster, we have the option to create a pipeline of instructions. This will
    make it possible to run one instruction per clock tick (ideally) and sometimes
    even increase the clock speed. It will increase the complexity of the CPU, though,
    making it much more expensive.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结来说，为了提高CPU的速度，除了让时钟运行得更快之外，我们还有创建指令流水线的选项。这将使得每时钟周期运行一条指令（理想情况下）成为可能，有时甚至可以提高时钟速度。然而，这也会增加CPU的复杂性，使其成本大大提高。
- en: And what are the pipelines of current processors like, you might ask? Well,
    they come in different lengths and behaviors, but in the case of some high-end
    Intel chips, pipelines can be larger than 30 steps. This will make them run really
    fast, but greatly increase their complexity and price.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你可能会问，当前处理器的流水线是什么样的呢？嗯，它们的长度和行为各不相同，但在一些高端英特尔芯片的情况下，流水线的长度可以超过30步。这将使它们运行得非常快，但会极大地增加它们的复杂性和价格。
- en: When you develop applications, a way to avoid slowing down the pipeline will
    be to try to perform operations that do not require previous results first, and
    then use the generated results, even though this, in practice, is very difficult
    to do, and some compilers will actually do it for you.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开发应用程序时，避免流水线变慢的一种方法是将不需要先前结果的操作先执行，然后使用生成的结果，尽管在实践中这非常困难，一些编译器实际上会为你这样做。
- en: Branch prediction
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分支预测
- en: There is one situation that we didn't see how to solve, though. When our processor
    receives a conditional jump instruction, depending on the current state of the
    processor flags, the next instruction to execute will be one or another. This
    means that we cannot anticipate some instructions and load them into the pipeline.
    Or can we?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种情况是我们没有看到如何解决，那就是当我们的处理器接收到一个条件跳转指令时，根据处理器标志位的当前状态，下一个要执行的指令将是其中一个。这意味着我们无法预测某些指令并将它们加载到流水线中。或者我们可以吗？
- en: There are multiple ways of somehow predicting what code will be run next without
    doing the computation of the last instructions. The simplest way, and one used
    in old processors, was to statically decide which branches will load the next
    instructions and which will load instructions at the jump address.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以预测接下来将要运行的代码，而无需计算最后几条指令。最简单的方法，也是老处理器中使用的方法，是静态地决定哪些分支将加载下一个指令，哪些将在跳转地址加载指令。
- en: Some processors would do that by deciding that some type of instructions were
    more likely to jump than others. Other processors would look into the jump address.
    If the address was lower, they would load the instructions at the target address
    into the pipeline, if not, they would load the ones at the next address. For example,
    in loops, it's much more likely to loop back to the beginning of the loop more
    times than continuing the flow of the program.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一些处理器会通过决定某些类型的指令比其他指令更有可能跳转来做到这一点。其他处理器会查看跳转地址。如果地址较低，它们会将目标地址的指令加载到流水线中，如果不低，它们会将下一个地址的指令加载到流水线中。例如，在循环中，返回循环开始的可能性比继续程序流程的可能性要大得多。
- en: In both preceding cases, the decision was made statically, when developing the
    processor, and the actual program execution wouldn't change the way pipeline loading
    would work. A much-improved approach, a dynamic one, was to count how many times
    the processor would jump for a given conditional jump.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述两种情况下，决策是在处理器开发时静态做出的，实际的程序执行不会改变流水线加载的工作方式。一个改进的方法，即动态方法，是计算处理器在给定条件跳转时跳转的次数。
- en: The first time the processor gets to the branch, it won't know whether the code
    will jump or not, so it will probably load the next instruction to the pipeline.
    If it jumps, the next instruction will be canceled and new instructions will be
    loaded in the pipeline. This will make the processor wait for as many cycles as
    the pipeline has stages.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器第一次到达分支时，它不知道代码是否会跳转，所以它可能会将下一个指令加载到流水线中。如果它跳转，下一个指令将被取消，并将新的指令加载到流水线中。这将使处理器等待流水线阶段数那么多个周期。
- en: In this old method, we would have put the counter of jumps for those instructions
    to `1`, and the counter of no jumps to `0`. The next time the program gets to
    the same jump, seeing the counter, the processor will start loading the instructions
    that come from the jump into the pipeline, instead of the next instructions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个旧方法中，我们会将这些指令的跳转计数器设置为`1`，没有跳转的计数器设置为`0`。下次程序到达相同的跳转时，看到计数器，处理器将开始将来自跳转的指令加载到流水线中，而不是加载下一个指令。
- en: Once the calculation has been done, if the processor actually needs to jump,
    it already has the instructions in the pipeline. If not, it will need to load
    the next instructions into it. In both cases, the respective counter would go
    up by 1.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成计算，如果处理器实际上需要跳转，它已经在流水线中有指令了。如果没有，它需要将其加载到下一个指令中。在这两种情况下，相应的计数器都会增加1。
- en: This means that, for example, in a long `for` loop, the jump counter will increase
    to a high number, since it will, most of the time, have to jump back to the beginning
    of the loop, and only after the last iteration will it continue the flow of the
    application. This means that for all except the first and the last iteration,
    there will be no empty pipeline and the branch will be predicted properly.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，例如，在一个长的`for`循环中，跳转计数器会增加到很高的数字，因为它大多数时候必须跳回到循环的开始，只有在最后一次迭代之后才会继续应用程序的流程。这意味着除了第一次和最后一次迭代之外，将不会有空流水线，分支将被正确预测。
- en: These counters are actually a bit more complex since they would saturate at
    1 or 2 bits, meaning that the counter could indicate whether the last time the
    branch was taken or not, or how sure the processor was that the next time the
    branch would be taken. The counter could be `0`, if it usually never takes the
    branch, 1, if it might take it, 2, if it many times takes the branch or 3 if it
    takes it almost always. This means that a branch that gets taken only some of
    the time will have a better prediction. Some benchmarks have shown that the accuracy
    can be as high as 93.5%.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计数器实际上要复杂一些，因为它们会在1或2位饱和，这意味着计数器可以指示上一次分支是否被采取，或者处理器下一次分支被采取的确定性如何。计数器可以是`0`，如果它通常从不采取分支，`1`，如果它可能采取，`2`，如果它多次采取分支，或者`3`，如果它几乎总是采取。这意味着只有部分时间采取的分支将会有更好的预测。一些基准测试表明，准确性可以高达93.5%。
- en: It's amazing how a simple counter will make branch prediction much more efficient,
    right? Well, of course, this has big limitations. In code, that branching depends
    on some condition, but where patterns can be seen (an `if` condition that returns
    true on almost every second call, for example), counters will fail enormously,
    since they will have no clue of the pattern.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的计数器如何使分支预测变得更加高效，真是令人惊讶，对吧？当然，这有很大的局限性。在代码中，这种分支依赖于某些条件，但可以在其中看到模式的地方（例如，几乎每次调用都返回真的`if`条件），计数器会完全失败，因为它们对模式一无所知。
- en: For this kind of behavior, complex adaptive prediction tables get used. It will
    store the last *n* occurrences of the `jump` instruction in a table, and see whether
    there is a pattern. If there is, it will group the outcomes in groups of the number
    of elements in the pattern, and better predict this kind of behavior. This increases
    the accuracy up to 97% in some cases.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种行为，会使用复杂的自适应预测表。它将在表中存储最后*n*次`跳转`指令的发生，并查看是否存在模式。如果有，它将把结果分组为模式中元素数量的组，并更好地预测这种行为。在某些情况下，这可以提高准确性到97%。
- en: There are many different branch prediction techniques, and depending on the
    pipeline size, it will make more sense to use more complex predictors or simpler
    ones. If the processor has a 30-stage pipeline, failing to predict a branch will
    end up in a 30-cycle delay for the next instruction. If it has 2 stages, it will
    only lose 2 cycles. This means that more complex and expensive pipelines will
    also require more complex and expensive branch predictors.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的分支预测技术，并且根据流水线的大小，使用更复杂的预测器或更简单的预测器更有意义。如果处理器有一个30阶段的流水线，未能预测分支将导致下一个指令的30个周期延迟。如果它有2个阶段，它将只损失2个周期。这意味着更复杂和昂贵的流水线也需要更复杂和昂贵的分支预测器。
- en: The relevance of branch prediction for our code
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分支预测对我们代码的相关性
- en: This book is not about creating processors, so you might think that all this
    branch prediction theory does not make sense for our use case of improving the
    efficiency of our Rust code. But the reality is that this theory can make us develop
    more efficient applications.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书不是关于创建处理器的，所以你可能认为所有这些分支预测理论对我们提高Rust代码效率的使用案例来说都没有意义。但现实是，这个理论可以让我们开发出更高效的应用程序。
- en: First, knowing that patterns in conditional execution will probably be detected
    after two passes by new and expensive processors will make us try to use those
    patterns if we know that our code will be mainly used by newer processors. On
    the other hand, if we know that our code will run in a cheaper or older processor,
    we might optimize its execution by maybe writing the result of the pattern condition
    sequentially if possible (in a loop) or by trying to group conditions in other
    ways.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，知道新的昂贵处理器将在两次遍历后检测到条件执行中的模式，这会让我们尝试使用这些模式，如果我们知道我们的代码将主要被较新的处理器使用。另一方面，如果我们知道我们的代码将在较便宜或较旧的处理器上运行，我们可能可以通过可能地（在循环中）按顺序编写模式条件的计算结果或通过尝试以其他方式组合条件来优化其执行。
- en: Also, we have to take into account compiler optimizations. Rust will often optimize
    loops to the point of copying some code 12 times if it knows it will always be
    executed that number of times, to avoid branching. It will also lose some optimization
    prediction if we have many branches in the same code generation unit (a function,
    for example).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要考虑编译器优化。Rust通常会优化循环，如果它知道它将始终执行相同次数的代码，就会复制一些代码12次，以避免分支。如果我们在同一个代码生成单元（例如，一个函数）中有许多分支，它也会丢失一些优化预测。
- en: This is where Clippy lints such as **cyclomatic complexity** enter into play.
    They will show as functions where we are adding too many branches. This can be
    fixed by dividing such functions into smaller ones. The Rust compiler will better
    optimize the given function, and if we have link-time optimizations enabled, it
    might even end up in the same function, in the end, making the processor branchless.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是像**循环复杂度**这样的Clippy检查开始发挥作用的地方。它们会显示在添加太多分支的功能中。可以通过将这些函数分解成更小的函数来修复这个问题。Rust编译器将更好地优化给定的函数，如果我们启用了链接时间优化，最终甚至可能是在同一个函数中完成，从而使得处理器无分支。
- en: We shouldn't completely rely on hardware branch prediction, especially if our
    code is performance-critical, and we should develop taking into account how the
    processor will optimize it too. If we know for sure which processor will be running
    our code, we might even decide to learn the branch prediction techniques of the
    processor from the developer manual and write our code accordingly.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应该完全依赖硬件分支预测，尤其是如果我们的代码是性能关键的话，我们应该在开发时考虑到处理器也会对其进行优化。如果我们确定我们的代码将在哪个处理器上运行，我们甚至可以决定从开发手册中学习该处理器的分支预测技术，并据此编写我们的代码。
- en: Profiling tools
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能分析工具
- en: You might be wondering how we will detect bottlenecks such as these ones in
    our application. We all know that not all developers will take such low-level
    details into account, and even if they do, they might forget to do it in some
    critical code that the program needs to run many times in a row. We cannot check
    the whole code base manually but, fortunately, there are some profiling tools
    that will give us information about our software.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道我们如何在应用程序中检测到这些瓶颈。我们都知道并不是所有的开发者都会考虑到这样的低级细节，即使他们考虑到了，他们也可能忘记在程序需要连续多次运行的关键代码中执行它。我们无法手动检查整个代码库，但幸运的是，有一些性能分析工具会给我们提供关于我们软件的信息。
- en: Valgrind
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Valgrind
- en: Let's first start with a tool that will help you find where your software spends
    more time. **Valgrind** is a tool that helps to find bottlenecks. Two main tools
    inside Valgrind will give us the statistics we need to find out where to improve
    our code. It's included in most Linux distributions. There are Windows alternatives,
    but if you have access to a Linux machine (even if it's a virtual one), Valgrind
    will really make the difference when getting results.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从一个可以帮助你找到软件花费更多时间的工具开始。**Valgrind**是一个帮助找到瓶颈的工具。Valgrind内的两个主要工具将为我们提供所需的统计数据，以找出我们需要改进代码的地方。它包含在大多数Linux发行版中。有Windows的替代品，但如果你可以访问一台Linux机器（即使是一个虚拟机），Valgrind在获取结果时真的会带来很大的差异。
- en: 'The easiest way to use it is to use `cargo-profiler`. This tool is in `crates.io`,
    but it''s no longer updated, and the version in GitHub has some much-needed fixes.
    You can install it by running the following command:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用它的最简单方法是使用 `cargo-profiler`。这个工具在 `crates.io` 上，但它已经不再更新，GitHub 上的版本有一些急需的修复。你可以通过运行以下命令来安装它：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once installed, you can use it by running `cargo profiler callgrind` or `cargo
    profiler cachegrind`, depending on the Valgrind tool you want to use. Nevertheless,
    `cargo-profiler` does not compile with source annotations by default, so it might
    make sense to use the `cargo rustc` command to compile with a `-g` flag, and then
    run Valgrind directly in those binaries:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，你可以通过运行 `cargo profiler callgrind` 或 `cargo profiler cachegrind` 来使用它，具体取决于你想要使用的
    Valgrind 工具。不过，`cargo-profiler` 默认不编译带有源注释的代码，所以使用 `cargo rustc` 命令并带上 `-g` 标志来编译，然后在那些二进制文件上直接运行
    Valgrind 可能是有意义的：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Callgrind
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Callgrind
- en: Callgrind will show statistics about the most-used functions in your program.
    To run it, you will need to run `cargo profiler callgrind {args}`, where `args`
    are the arguments to your executable. There is an interesting issue here though.
    Rust uses `jemalloc` as the default allocator, but Valgrind will try to use its
    own allocator to detect calls. There is a way to use Valgrind's allocator, but
    it will only work in nightly Rust.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Callgrind 将显示程序中最常用的函数的统计数据。要运行它，你需要运行 `cargo profiler callgrind {args}`，其中
    `args` 是你的可执行文件的参数。不过这里有一个有趣的问题。Rust 使用 `jemalloc` 作为默认分配器，但 Valgrind 会尝试使用它自己的分配器来检测调用。有一种方法可以使用
    Valgrind 的分配器，但这只能在 nightly Rust 中工作。
- en: 'You will need to add the following lines to your `main.rs` file:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要将以下行添加到你的 `main.rs` 文件中：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will force Rust to use the system allocator. You might need to add `#![allow(unused_extern_crates)]`
    to the file so that it doesn't alert you for an unused crate. An interesting flag
    for Valgrind is `-n {num}`.  This will limit the results to the `num` most relevant
    ones. In the case of Callgrind, it will only show the most-used functions. An
    optional `--release` flag will tell `cargo profiler` if you want to profile the
    application in release mode instead of doing it in debug mode.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将强制 Rust 使用系统分配器。你可能需要向文件中添加 `#![allow(unused_extern_crates)]`，这样它就不会提醒你未使用的包。对于
    Valgrind 来说，一个有趣的标志是 `-n {num}`。这将限制结果只显示最相关的 `num` 个。在 Callgrind 的情况下，它只会显示最常用的函数。一个可选的
    `--release` 标志会告诉 `cargo profiler` 你是否想要在发布模式下而不是在调试模式下进行性能分析。
- en: 'Let''s see an output of the Callgrind tool:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Callgrind 工具的输出：
- en: '![](img/73d0a237-8dfa-4030-aa2d-d9261b106d2d.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/73d0a237-8dfa-4030-aa2d-d9261b106d2d.png)'
- en: Let's analyze what this means. I only selected the top functions, but we can
    already see lots of information. The most-used function is a system call for a
    memory copy. This means that this program is copying lots of data in memory between
    one position and another. Fortunately, at least it uses an efficient system call,
    but maybe we should check whether we need so much copying for its job.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下这代表什么。我只选择了最常用的函数，但我们已经可以看到很多信息。最常用的函数是一个内存复制的系统调用。这意味着这个程序在内存中的不同位置之间复制了大量的数据。幸运的是，至少它使用了一个高效的系统调用，但也许我们应该检查我们是否真的需要为这项工作做这么多复制。
- en: The second most-used function is something called `Executor` in the `abxml`
    module/crate. You might think the `regex` reference is more used because it's
    in the second position, but the `Executor` seems to be divided since it seems
    that some of the references lost their initial `lib.rs` (third and fifth elements
    seem the same). It seems that we use that function a lot, or that at least it's
    taking most of our CPU time.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次最常用的函数是在 `abxml` 模块/包中称为 `Executor` 的东西。你可能会认为 `regex` 引用更常用，因为它排在第二位，但 `Executor`
    似乎被分割了，因为似乎有些引用丢失了最初的 `lib.rs`（第三和第五个元素看起来相同）。看起来我们使用那个函数很多，或者至少它占用了我们大部分的 CPU
    时间。
- en: We should ask ourselves if this is normal. Should it spend so much time on that
    function? In the case of this program, the SUPER Android Analyzer ([http://superanalyzer.rocks/](http://superanalyzer.rocks/)),
    it uses that function to get the resources of the application. It makes sense
    that most of the time it would be actually analyzing the application instead of
    decompressing it (and in fact, we are not being shown the use of a Java dependency
    that takes 80% of the time). But it seems that the decompression of the resources
    of the `apk` file takes a lot of time.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该问问自己这是否正常。它是否应该在该函数上花费这么多时间？在这个程序的情况下，SUPER Android Analyzer ([http://superanalyzer.rocks/](http://superanalyzer.rocks/))，它使用这个函数来获取应用程序的资源。大多数时间实际上分析应用程序而不是解压缩它（实际上，我们没有看到占用80%时间的Java依赖项的使用）。但看起来`apk`文件的资源解压缩花费了很长时间。
- en: We could check if there is something that could be optimized in that function
    or in descendant functions. It makes sense that if we could optimize 10% of that
    function, we would gain a lot of speed in the application.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查在该函数或其子函数中是否有可以优化的地方。如果我们能优化该函数的10%，那么在应用程序中我们会获得很多速度提升。
- en: Another option would be to check our regular expression usage, as we can see
    that many instructions are used to check regular expressions and compile them.
    An improvement in the regular expression engine would also make a difference.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是检查我们的正则表达式使用情况，因为我们可以看到许多指令被用来检查和编译正则表达式。对正则表达式引擎的改进也会有所影响。
- en: We finally see that the SHA-1 and SHA-256 algorithm execution take a lot of
    time too. Do we need them? Could they be optimized? Maybe by using the native
    algorithm implementations often found in newer processors, we could speed up the
    execution. It might make sense to create a pull request in the upstream crate.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终看到SHA-1和SHA-256算法的执行也花费了很长时间。我们需要它们吗？它们可以被优化吗？也许通过使用在新处理器中经常发现的本地算法实现，我们可以加快执行速度。在主crate中创建一个pull
    request可能是有意义的。
- en: As you can see, Callgrind gives us tons of valuable information about the execution
    of our program, and we can at least see where it makes sense to spend time trying
    to optimize the code. In this particular case, hashing algorithms, regular expressions,
    and resource decompression take most of the time; we should try to optimize those
    functions. On the other hand, for example, one of the lesser-used functions is
    the XML emitter. So even if we find out how to optimize that function by 90%,
    it won't really make a difference. If it's an easy optimization, we can do it
    (better something than nothing), but if it will take us a long time to implement,
    it probably makes no sense to do it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Callgrind为我们提供了大量关于程序执行的有价值信息，我们至少可以看到在哪里花费时间尝试优化代码是有意义的。在这个特定案例中，散列算法、正则表达式和资源解压缩占据了大部分时间；我们应该尝试优化这些函数。另一方面，例如，一个较少使用的函数是XML发射器。所以即使我们找到了如何将这个函数优化90%的方法，实际上也不会有多大区别。如果这是一个简单的优化，我们可以做（总比什么都不做好），但如果它需要我们花费很长时间来实现，那么可能没有做的必要。
- en: Cachegrind
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cachegrind
- en: 'We have talked at length about caches in this chapter, but is there a way to
    see how our application is performing in this sense? There actually is. It''s
    called **Cachegrind**, and it''s part of Valgrind. It''s used in the same way
    as Callgrind, with `cargo profiler`. In the case of the same preceding application,
    the `cargo profiler` failed to parse Cachegrind''s response, so I had to run Valgrind
    directly, as you can see in the following screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中详细讨论了缓存，但有没有办法看到我们的应用程序在这方面的表现呢？实际上是有方法的。它被称为**Cachegrind**，它是Valgrind的一部分。它和Callgrind一样使用，与`cargo
    profiler`一起使用。在相同的前置应用程序中，`cargo profiler`未能解析Cachegrind的响应，所以我不得不直接运行Valgrind，如下面的截图所示：
- en: '![](img/368b2b62-6548-4978-bd60-c97ac8f8d4ac.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/368b2b62-6548-4978-bd60-c97ac8f8d4ac.png)'
- en: This was my second or third run, so some information might have already been
    cached. But still, as you can see, 2.1% of the first-level data cache missed,
    which is not so bad, given that the second-level cache had that data most of the
    time (it only missed 0.1% of the time).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我第二次或第三次运行，所以一些信息可能已经被缓存。但仍然，如您所见，一级数据缓存未命中率为2.1%，考虑到二级缓存大部分时间都有这些数据（它只有0.1%的时间未命中），这并不算太坏。
- en: Instruction data was fetched properly at the level 1 cache almost all the time,
    except for 0.04% of the time. There was almost no miss at level 2\. Cachegrind
    can also give us more valuable information with some flags though.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 指令数据几乎总是从一级缓存中正确获取，除了0.04%的时间。在二级缓存中几乎没有缺失。Cachegrind也可以通过一些标志提供更多有价值的信息。
- en: 'Using `--branch-sim=yes` as an argument, we can see how the branch prediction
    worked:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`--branch-sim=yes`作为参数，我们可以看到分支预测是如何工作的：
- en: '![](img/01bda42e-d40f-4878-85df-42c635d27442.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/01bda42e-d40f-4878-85df-42c635d27442.png)'
- en: As we can see, 3.3% of the branches were not predicted properly. This means
    that an interesting improvement could be done if branches were more predictable,
    or if some loops were unrolled, as we saw previously.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，3.3%的分支没有被正确预测。这意味着如果分支能被更好地预测，或者如果某些循环被展开，就像我们之前看到的那样，我们可以进行一些有趣的改进。
- en: This, by itself, tells us nothing about where we could improve our code. But
    using the tool creates a `cachegrind.out` file in the current directory. This
    file can be used by another tool, `cg_anotate`, that will show improved stats.
    Running it, you will see the various stats on a per-function basis, where you
    can see which functions are giving the cache more trouble, and go there to try
    to fix them.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 仅凭这一点，并不能告诉我们如何改进我们的代码。但使用这个工具会在当前目录下创建一个`cachegrind.out`文件。这个文件可以被另一个工具`cg_anotate`使用，它会显示改进的统计数据。运行它，你将看到基于函数的各种统计数据，你可以看到哪些函数给缓存带来了更多麻烦，然后去那里尝试修复它们。
- en: In the case of SUPER, it seems that the resource decompress is giving more cache
    misses. It might make sense, though, since it's reading new data from a file and
    reading that data almost all the time from memory for the first time. But maybe
    we can check those functions and try to improve the fetching, by using buffers,
    for example, if they are not being used.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在SUPER的情况下，资源解压缩似乎导致了更多的缓存缺失。尽管如此，这可能是合理的，因为它正在从文件中读取新数据，并且第一次几乎总是从内存中读取这些数据。但也许我们可以检查这些函数，并尝试通过使用缓冲区等方法来改进获取。
- en: OProfile
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OProfile
- en: OProfile is another great tool that can give us interesting information about
    our program. It's also only available for Linux, but you will find a similar tool
    for Windows too. Once again, if you can get a Linux partition to check this, your
    results will probably be more in line with what you will read next. To install
    it, install the `oprofile` package of your distribution. You might also need to
    install the generic Linux tools (`linux-tools-generic` in Ubuntu).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: OProfile是另一个可以给我们提供关于程序有趣信息的优秀工具。它也仅适用于Linux，但你会发现Windows也有类似的工具。再次强调，如果你能获得一个Linux分区来检查这个，你的结果可能会更接近你接下来要读到的。要安装它，安装你发行版的`oprofile`包。你可能还需要安装通用的Linux工具（在Ubuntu中为`linux-tools-generic`）。
- en: 'OProfile is not of much help without source annotations, so you should first
    compile your binary with them by using the following command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 没有源代码注释，OProfile帮助不大，所以你应该首先使用以下命令编译带有它们的二进制文件：
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will need to be root to do the profiling, as it will directly get the kernel
    counters for it. Don''t worry; once you profile the application, you can stop
    being root. To profile the application, simply run `operf` with the binary and
    the arguments to profile:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要root权限来进行性能分析，因为它将直接获取内核计数器。别担心；一旦你分析了应用程序，你就可以停止使用root。要分析应用程序，只需运行带有二进制文件和要分析参数的`operf`：
- en: '![](img/2db4a3e0-5be1-4dab-a13e-e91811cb6588.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2db4a3e0-5be1-4dab-a13e-e91811cb6588.png)'
- en: 'This will create an `oprofile_data` directory in the current path. To make
    some sense from it, you can use the `opannotate` command. This will show a bunch
    of stats, with some of the source code present, with how much time the CPU spends
    in each place. In the case of our Android analyzer, we can see that the rule processing
    takes quite a lot of time:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在当前路径下创建一个`oprofile_data`目录。为了从中获得一些意义，你可以使用`opannotate`命令。这将显示一些统计数据，其中包含一些源代码，以及CPU在每个地方花费的时间。在我们的Android分析器的情况下，我们可以看到规则处理花费了相当多的时间：
- en: '![](img/78b6a992-02a9-4c3d-a429-4554900a0f0b.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/78b6a992-02a9-4c3d-a429-4554900a0f0b.png)'
- en: In this case, this is probably reasonable. It makes sense that a piece of software
    that is supposed to analyze a file with rules would spend lots of time analyzing
    that file with those rules. But, nevertheless, it also means that in that code,
    we could find some optimization that could make a difference.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这可能是合理的。一个旨在用规则分析文件的软件花费大量时间用这些规则分析文件是有道理的。但，尽管如此，这也意味着在那段代码中，我们可能找到一些优化，这可能会带来差异。
- en: With OProfile, we could find some areas where the program is spending more time
    than it should. Maybe we would find a bottleneck in an unexpected area. That is
    why it's important to use these kinds of tools.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OProfile，我们可以找到程序花费时间过多的区域。也许我们会在一个意想不到的领域发现瓶颈。这就是为什么使用这些工具很重要的原因。
- en: Summary
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how the processor really works. You understood
    the multiple hacks we have in the hardware so that everything runs much, much
    faster than it would if the CPU was always waiting for the RAM. You also got a
    grasp of the most common performance issues and some information on how to fix
    them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了处理器是如何真正工作的。你理解了我们在硬件中实施的多个技巧，这样一切运行得比CPU总是等待RAM时快得多。你还掌握了最常见的性能问题以及一些关于如何修复它们的信息。
- en: Finally, you learned about the Callgrind, Cachegrind, and OProfile tools, which
    will help you find those bottlenecks so that you can fix them easily. They will
    even show where in your source code you can find the slowdowns.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你学习了关于Callgrind、Cachegrind和OProfile工具的内容，这些工具将帮助你找到那些瓶颈，以便你可以轻松地修复它们。它们甚至还会显示在你的源代码中可以找到哪些减速点。
- en: In [Chapter 6](4a4cf14d-1c2e-4a33-9856-6ef520591a44.xhtml), *Benchmarking*,
    you will learn how to benchmark your application. It is especially interesting
    to compare it to other applications or to a previous version of your own application.
    You will learn how to spot changes that make your application slower.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](4a4cf14d-1c2e-4a33-9856-6ef520591a44.xhtml)“基准测试”中，你将学习如何对你的应用程序进行基准测试。将其与其他应用程序或你自己的应用程序的早期版本进行比较特别有趣。你将学会如何发现使你的应用程序变慢的变化。

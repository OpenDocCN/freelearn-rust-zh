- en: Locks – Mutex, Condvar, Barriers and RWLock
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 锁定机制 – Mutex、Condvar、屏障和RWLock
- en: In this chapter, we're going to do a deep-dive on hopper, the grown-up version
    of Ring from [Chapter 4](5a332d94-37e4-4748-8920-1679b07e2880.xhtml), *Sync and
    Send – the Foundation of Rust Concurren**cy*. Hopper's approach to back-pressure—the
    weakness we identified in telem*—*is to block when filled to capacity, as `SyncSender`
    does. Hopper's special trick is that it pages out to disk. The hopper user defines
    how many bytes of in-memory space hopper is allowed to consume, like `SyncSender`,
    except in terms of bytes rather than total elements of `T`. Furthermore, the user
    is able to configure the number of on-disk bytes that are consumed when hopper's
    in-memory capacity is filled and it has to page out to disk. The other properties
    of MSPC are held, in-order delivery, retention of data once stored, and so on.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入研究hopper，它是[第4章](5a332d94-37e4-4748-8920-1679b07e2880.xhtml)，*Sync和Send – Rust并发基础*中Ring的成熟版本。hopper处理背压——我们在telem*中识别出的弱点——是在填满容量时阻塞，就像`SyncSender`一样。hopper的特殊技巧是将其页面输出到磁盘。hopper用户定义了hopper允许消耗多少内存空间，就像`SyncSender`一样，只是以字节为单位而不是`T`的总元素数。此外，当hopper的内存容量填满并需要页面输出到磁盘时，用户还可以配置消耗的磁盘字节数。MSPC的其他属性保持不变，如有序交付、存储后保留数据等。
- en: Before we can dig through hopper, however, we need to introduce more of Rust's
    concurrency primitives. We'll work on some puzzles from *The Little Book of Semaphores*
    to explain them, which will get a touch hairy in some places on account of how
    Rust does not have a semaphore available.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们深入挖掘之前，我们需要介绍更多Rust的并发原语。我们将通过解决来自*《信号量小书》*的一些谜题来解释它们，由于Rust没有提供信号量，因此在某些地方可能会有些棘手。
- en: 'By the close of this chapter, we will have:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将：
- en: Discussed the purpose and use of Mutex, Condvar, Barriers, and RWLock
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论了Mutex、Condvar、屏障和RWLock的目的和使用方法
- en: Investigated a disk-backed specialization of the standard library's MPSC called
    hopper
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调查了标准库中MPSC的磁盘后端特殊化版本hopper
- en: Seen how to apply QuickCheck, AFL, and comprehensive benchmarks in a production
    setting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看到了如何在生产环境中应用QuickCheck、AFL和全面基准测试
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires a working Rust installation. The details of verifying
    your installation are covered in [Chapter 1](5f3aec9d-fd53-48ff-9ba8-43ce13e91cff.xhtml),
    *Preliminaries – Machine Architecture and Getting Started with Rust*. No additional
    software tools are required.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要安装一个有效的Rust环境。验证安装的详细信息在[第1章](5f3aec9d-fd53-48ff-9ba8-43ce13e91cff.xhtml)，*预备知识 – 机器架构和Rust入门*中有所介绍。不需要额外的软件工具。
- en: 'You can find the source code for this book''s projects on GitHub at: [https://github.com/PacktPublishing/Rust-Concurrency/](https://github.com/PacktPublishing/Rust-Concurrency/).
    This chapter has its source code under `Chapter05`.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到本书项目的源代码：[https://github.com/PacktPublishing/Rust-Concurrency/](https://github.com/PacktPublishing/Rust-Concurrency/)。本章的源代码位于`Chapter05`。
- en: Read many, write exclusive locks – RwLock
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取多个，写入独占锁——RwLock
- en: 'Consider a situation where you have a resource that must be manipulated only
    a single thread at a time, but is safe to be queried by many—that is, you have
    many readers and only one writer. While we could protect this resource with a
    `Mutex`, the trouble is that the mutex makes no distinction between its lockers;
    every thread will be forced to wait, no matter what their intentions. `RwLock<T>`
    is an alternative to the mutex concept, allowing for two kinds of locks—read and
    write. Analogously to Rust''s references, there can only be one write lock taken
    at a time but multiple reader locks, exclusive of a write lock. Let''s look at
    an example:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这样一种情况，你有一个资源，一次只能由一个线程进行操作，但可以被多个线程安全地查询——也就是说，你有多个读者和一个写者。虽然我们可以用`Mutex`来保护这个资源，但问题是互斥锁对它的锁定者没有区分；无论它们的意图如何，每个线程都将被迫等待。`RwLock<T>`是互斥锁概念的替代品，允许两种类型的锁——读锁和写锁。类似于Rust的引用，一次只能有一个写锁，但可以有多个读锁，且不包括写锁。让我们看一个例子：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The idea here is that we''ll have one writer thread spinning and incrementing,
    in a wrapping fashion, a shared resource—a `u16.` Once the `u16` has been wrapped
    100 times, the writer thread will exit. Meanwhile, a `total_readers` number of
    read threads will attempt to take a read lock on the shared resource—a `u16—until`
    it hits zero `100` times. We''re gambling here, essentially, on thread ordering.
    Quite often, the program will exit with this result:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是，我们将有一个写线程在循环中旋转并递增一个共享资源——一个 `u16`。一旦 `u16` 被包装了 100 次，写线程将退出。同时，一定数量的读线程（`total_readers`）将尝试获取共享资源——一个
    `u16` 的读锁，直到它达到零 `100` 次。本质上，我们在这里是在赌线程的顺序。程序通常会以以下结果退出：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This means that each reader thread never failed to get its read lock—there
    were no write locks present. That is, the reader threads were scheduled before
    the writer. Our main function only joins on reader handlers and so the writer
    is left writing as we exit. Sometimes, we''ll hit just the right scheduling order,
    and get the following result:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着每个读线程从未失败地获取其读锁——没有写锁存在。也就是说，读线程在写线程之前被调度。我们的主函数只连接到读处理程序，所以写线程在我们退出时仍在写。有时，我们会遇到正确的调度顺序，并得到以下结果：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this particular instance, the second and final reader threads were scheduled
    just after the writer and managed to catch a time when the guard was not zero.
    Recall that the first element of the pair is the total number of times the reader
    thread was not able to get a read lock and was forced to retry. The second is
    the number of times that the lock was acquired. In total, the writer thread did
    `(2^18 * 100) ~= 2^24` writes, whereas the second reader thread did `log_2 2630308
    ~= 2^21` reads. That's a lot of lost writes, which, maybe, is okay. Of more concern,
    that's a lot of useless loops, approximately `2^26`. Ocean levels are rising and
    we're here burning up electricity like nobody had to die for it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的例子中，第二个和最后一个读线程在写线程之后被调度，并设法捕捉到守卫不为零的时刻。回想一下，这对中的第一个元素是读线程无法获取读锁并被迫重试的总次数。第二个是获取锁的次数。总的来说，写线程执行了
    `(2^18 * 100) ~= 2^24` 次写操作，而第二个读线程执行了 `log_2 2630308 ~= 2^21` 次读操作。这丢失了很多写操作，也许是可以接受的。但更令人担忧的是，这导致了大约
    `2^26` 次无用的循环。海平面正在上升，而我们像没有人需要为此而死一样燃烧着电力。
- en: 'How do we avoid all this wasted effort? Well, like most things, it depends
    on what we''re trying to do. If we need every reader to get every write, then
    an MPSC is a reasonable choice. It would look like this:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何避免所有这些浪费的努力？嗯，像大多数事情一样，这取决于我们想要做什么。如果我们需要每个读者都能获取到每个写操作，那么 MPSC 是一个合理的选择。它看起来会是这样：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It will run—for a while—and print out the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它将运行——一段时间——并打印出以下内容：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: But what if every reader does not need to see every write, meaning that it's
    acceptable for a reader to miss writes so long as it does not miss all of the
    writes? We have options. Let's look at one.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果每个读者不需要看到每个写操作，也就是说，只要读者没有错过所有写操作，那么错过写操作是可以接受的，我们就有选择了。让我们看看其中一个。
- en: Blocking until conditions change – condvar
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阻塞直到条件改变 —— 条件变量
- en: One option is a condvar, or CONDition VARiable. Condvars are a nifty way to
    block a thread, pending a change in some Boolean condition. One difficulty is
    that condvars are associated exclusively with mutexes, but in this example, we
    don't mind all that much.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个选项是条件变量，或称 CONDition VARiable。条件变量是一种巧妙的方式来阻塞一个线程，直到某个布尔条件发生变化。一个困难是条件变量仅与互斥锁相关联，但在这个例子中，我们并不太在意这一点。
- en: The way a condvar works is that, after taking a lock on a mutex, you pass the
    `MutexGuard` into `Condvar::wait`, which blocks the thread. Other threads may
    go through this same process, blocking on the same condition. Some other thread
    will take the same exclusive lock and eventually call either `notify_one` or `notify_all`
    on the condvar. The first wakes up a single thread, the second wakes up *all*
    threads. Condvars are subject to spurious wakeup, meaning the thread may leave
    its block without a notification being sent to it. For this reason condvars check
    their conditions in a loop. But, once the condvar wakes, you *are* guaranteed
    to hold the mutex, which prevents deadlocks on spurious wakeup.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 条件变量工作的方式是，在获取互斥锁之后，你将 `MutexGuard` 传递给 `Condvar::wait`，这将阻塞线程。其他线程可能通过这个过程，阻塞在相同的条件下。某个其他线程将获取相同的独占锁，并最终在条件变量上调用
    `notify_one` 或 `notify_all`。第一个唤醒单个线程，第二个唤醒 *所有* 线程。条件变量可能会发生虚假唤醒，这意味着线程可能在没有收到通知的情况下离开其阻塞状态。因此，条件变量会在循环中检查其条件。但是，一旦条件变量唤醒，你
    *确实* 保证持有互斥锁，这防止了虚假唤醒时的死锁。
- en: 'Let''s adapt our example to use a condvar. There''s actually a fair bit going
    on in this example, so we''ll break it down into pieces:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将我们的例子修改为使用`condvar`。实际上，这个例子中有很多内容，所以我们将它分解成几个部分：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our preamble is similar to the previous examples, but the setup is already
    quite strange. We''re synchronizing threads on `mutcond`, which is an `Arc<(Mutex<(bool,
    u16)>, Condvar)>`. Rust''s condvar is a touch awkward. It''s undefined behavior
    to associate a condvar with more than one mutex, but there''s really nothing in
    the type of `Condvar` that makes that an invariant. We just have to remember to
    keep them associated. To that end, it''s not uncommon to see a `Mutex` and `Condvar`
    paired up in a tuple, as here. Now, why `Mutex<(bool, u16)>`? The second element
    of the tuple is our *resource*, which is common to other examples. The first element
    is a Boolean flag, which we use as a signal to mean that there are writes available.
    Here are our reader threads:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的序言与前面的例子相似，但设置已经相当奇怪。我们正在同步`mutcond`上的线程，它是一个`Arc<(Mutex<(bool, u16)>, Condvar)>`。Rust的`Condvar`有点尴尬。将`Condvar`与多个互斥量关联是未定义的行为，但`Condvar`的类型中并没有使这成为不变量的东西。我们只需记住保持它们关联。为此，在元组中将`Mutex`和`Condvar`配对是很常见的，就像这里一样。现在，为什么是`Mutex<(bool,
    u16)>`？元组的第二个元素是我们的*资源*，这在其他例子中是常见的。第一个元素是一个布尔标志，我们用它作为信号表示有可用的写入。以下是我们的读取线程：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Until `total_zeros` hits 100, the reader thread locks the mutex, checks the
    guard inside the mutex for write availability, and, if there are no writes, does
    a wait on the condvar, which gives up the lock. The reader thread is then blocked
    until a `notify_all` is called—as we''ll see shortly. Every reader thread then
    races to be the first to reacquire the lock. The lucky winner notes that there
    are no more writes to be read and then does the normal flow we''ve seen in previous
    examples. It bears repeating that every thread that wakes up from a condition
    wait is racing to be the first to reacquire the mutex. Our reader is uncooperative
    in that it immediately prevents the chance of any other reader threads finding
    a resource available. However, they will still wake up spuriously and be forced
    to wait again. Maybe. The reader threads are also competing with the writer thread
    to acquire the lock. Let''s look at the writer thread:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 直到`total_zeros`达到100，读取线程会锁定互斥量，检查互斥量内的守卫以检查写入可用性，如果没有写入，就会在`condvar`上等待，放弃锁。然后读取线程会阻塞，直到调用`notify_all`——我们很快就会看到。每个读取线程都会争先恐后地重新获取锁。幸运的获胜者会注意到没有更多的写入要读取，然后执行我们在前面的例子中看到的正常流程。需要重复的是，每个从条件等待中唤醒的线程都在争先恐后地第一个重新获取互斥量。我们的读取者不合作，它会立即阻止其他读取线程发现资源可用。然而，它们仍然会意外地醒来并被迫再次等待。也许。读取线程还在与写入线程竞争获取锁。让我们看看写入线程：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The writer thread is an infinite loop, which we orphan in an unjoined thread.
    Now, it''s entirely possible that the writer thread will acquire the lock, bump
    the resource, notify waiting reader threads, give up the lock, and then immediately
    re-acquire the lock to begin the while process before any reader threads can get
    scheduled in. This means it''s entirely possible that the resource being zero
    will happen several times before a reader thread is lucky enough to notice. Let''s
    close out this program:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 写入线程是一个无限循环，我们在一个未连接的线程中将其孤儿化。现在，完全有可能写入线程会获取锁，增加资源，通知等待的读取线程，放弃锁，然后在任何读取线程被调度之前立即重新获取锁以开始while过程。这意味着在读取线程足够幸运地注意到之前，资源为零的情况完全可能发生多次。让我们结束这个程序：
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Ideally, what we''d like is some manner of bi-directionality—we''d like the
    writer to signal that there are reads and the reader to signal that there is capacity.
    This is suspiciously like how Ring in the previous chapter worked through its
    size variable, when we were careful to not race on that variable, that is. We
    might, for instance, layer another condition variable into the mix, this one for
    the writer, but that''s not what we have here and the program suffers for it.
    Here''s one run:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望有一种双向性——我们希望写入者发出有读取的信号，而读取者发出有容量的信号。这很可疑地类似于上一章中的Ring通过其大小变量工作，当我们小心不要在该变量上竞争时。例如，我们可以在混合中添加另一个条件变量，这个变量是给写入者的，但这里并不是这样，程序因此受到影响。以下是其中一个运行实例：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Phew! That's significantly more loops than previous examples. None of this is
    to say that condition variables are hard to use—they're not—it's just that they
    need to be used in conjunction with other primitives. We'll see an excellent example
    of this later in the chapter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 呼吁！这比之前的例子中的循环要多得多。这并不是说条件变量很难使用——它们并不难——只是它们需要与其他原语一起使用。我们将在本章后面看到一个很好的例子。
- en: Blocking until the gang's all here - barrier
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阻塞直到全员到齐——障碍
- en: 'A barrier is a synchronization device that blocks threads until such time that
    a predefined number of threads have waited on the same barrier.  When a barrier''s
    waiting threads wake up, one is declared leader—discoverable by inspecting the
    `BarrierWaitResult`—but this confers no scheduling advantage. A barrier becomes
    useful when you wish to delay threads behind an unsafe initialization of some
    resource—say a C library''s internals that have no thread-safety at startup, or
    have a need to force participating threads to start a critical section at roughly
    the same time. The latter is the broader category, in your author''s experience.
    When programming with atomic variables, you''ll run into situations where a barrier
    will be useful. Also, consider for a second writings multi-threaded code for low-power
    devices. There are two strategies possible these days for power management: scaling
    the CPU to meet requirements, adjusting the runtime of your program live, or burning
    through a section of your program as quickly as possible and then shutting down
    the chip. In the latter approach, a barrier is just the primitive you need.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 障碍是一种同步设备，它会阻塞线程，直到预定义数量的线程在同一个障碍上等待。当障碍等待的线程醒来时，会宣布一个领导者——可以通过检查`BarrierWaitResult`来发现——但这并不提供调度优势。当您希望延迟线程在某个资源的不安全初始化之后时，障碍变得有用——比如一个在启动时没有线程安全性的C库的内部结构，或者需要强制参与线程在大约相同的时间开始临界区。后者是更广泛的类别，根据作者的实践经验。当使用原子变量编程时，您会遇到障碍有用的场景。此外，考虑一下为低功耗设备编写多线程代码。如今，在电源管理方面有两种可能的策略：将CPU扩展以满足要求，实时调整程序的运行时间，或者尽可能快地烧毁程序的一部分，然后关闭芯片。在后一种方法中，障碍正是您需要的原语。
- en: More mutexes, condvars, and friends in action
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多互斥锁、条件变量和类似功能正在行动
- en: Admittedly, the examples in the preceding sections got a little convoluted.
    There's a reason to that madness, I promise, but before we move on I'd like to
    show you some working examples from the charming *The Little Book of Semaphores*.
    This book, in case you skipped previous bibliographic notes, is a collection of
    concurrency puzzles suitable for self-learning, on account of the puzzles being
    amusing and coming with good hints. As the title implies, the book does make use
    of the semaphore primitive, which Rust does not have. Though, as mentioned in
    the previous chapter, we will build a semaphore in the next chapter.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 承认，前几节的例子有点复杂。这种疯狂的背后有原因，我保证，但在我们继续之前，我想给你展示一些来自迷人的《信号量小书》的工作示例。如果你跳过了之前的文献注释，这本书是适合自学的一组并发谜题，因为这些谜题很有趣，并且附带很好的提示。正如标题所暗示的，这本书确实使用了信号量原语，而Rust没有。尽管如此，正如前一章提到的，我们将在下一章构建一个信号量。
- en: The rocket preparation problem
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 火箭准备问题
- en: This puzzle does not actually appear in *The Little Book of Semaphores* but
    it is based on one of the puzzles there - the cigarette smoker's problem from
    section 4.5\. Personally, I think cigarettes are gross so we're going to reword
    things a touch. The idea is the same.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个谜题实际上并没有出现在《信号量小书》中，但它基于那里的一则谜题——4.5节中的吸烟者问题。我个人认为香烟很恶心，所以我们稍微改一下措辞。想法是相同的。
- en: We have four threads in total. One thread, the `producer`, randomly publishes
    one of `fuel`, `oxidizer`, or `astronauts`. The remaining three threads are the
    consumers, or *rockets*, which must take their resources in the order listed previously.
    If a rocket doesn't get its resources in that order, it's not safe to prepare
    the rocket, and if it doesn't have all three, the rocket can't lift-off. Moreover,
    once all the rockets are prepped, we want to start a 10 second count-down, only
    after which may the rockets lift-off.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总共有四个线程。一个线程是“生产者”，随机发布“燃料”、“氧化剂”或“宇航员”中的一个。剩下的三个线程是消费者，或者说是“火箭”，它们必须按照之前列出的顺序获取资源。如果火箭没有按照这个顺序获取资源，那么准备火箭是不安全的，而且如果没有全部三个资源，火箭就不能起飞。此外，一旦所有火箭都准备好了，我们想要开始10秒倒计时，只有在这之后火箭才能起飞。
- en: 'The preamble to our solution is a little longer than usual, in the interest
    of keeping the solution as a standalone:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解决方案的序言比平常要长一些，目的是为了将解决方案作为一个独立的单元：
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We don''t really need excellent randomness for this solution—the OS scheduler
    injects enough already—but just something small-ish. `XorShift` fits the bill.
    Now, for our resources:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个解决方案，我们并不真的需要出色的随机性——操作系统调度器已经注入了足够的随机性——但只需要一点小小的随机性。`XorShift` 就能满足这个要求。现在，对于我们的资源：
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The struct is protected by a single `Mutex<(bool, bool, bool)>`, the Boolean
    being a flag to indicate that there''s a resource available. We hold the first
    flag to mean `fuel`, the second `oxidizer`, and the third `astronauts`. The remainder
    of the struct are condvars to match each of these resource concerns. The `producer`
    is a straightforward infinite loop:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结构体被一个 `Mutex<(bool, bool, bool)>` 保护，布尔值是一个标志，用来指示是否有资源可用。我们持有第一个标志表示 `燃料`，第二个
    `氧化剂`，第三个 `宇航员`。结构体的其余部分是匹配每个这些资源关注的条件变量。`生产者` 是一个简单的无限循环：
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'On each iteration, the producer chooses a new resource—`rng.next_u32() % 3`—and
    sets the Boolean flag for that resource before notifying all threads waiting on
    the `fuel` condvar. Meanwhile, the compiler and CPU are free to re-order instructions
    and the memory `notify_all` acts like a causality gate; everything before in the
    code is before in causality, and likewise, afterwards. If the resource bool flip
    was after the notification, the wake-up would be spurious from the point of view
    of the waiting threads and lost from the point of view of the producer. The `rocket`
    is straightforward:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，生产者选择一个新的资源——`rng.next_u32() % 3`——并为该资源设置布尔标志，然后通知所有等待 `燃料` 条件变量的线程。同时，编译器和
    CPU 可以自由地重新排序指令，而内存 `notify_all` 则像是一个因果门；代码中的所有内容在因果上都是先于之后的，同样，之后的也是如此。如果资源布尔翻转在通知之后，那么从等待线程的角度来看，唤醒将是虚假的，并且从生产者的角度来看，它将丢失。`火箭`
    是简单的：
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Each thread, regarding the resource requirements, waits on the condvar for
    the producer to make it available. A race occurs to re-acquire the mutex, as discussed,
    and only a single thread gets the resource. Finally, once all the resources are
    acquired, the `all_go` barrier is hit to delay any threads ahead of the count-down.
    Here we need the `main` function:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程，就资源需求而言，都会等待生产者使其可用。正如讨论的那样，发生了一场争夺重新获取互斥锁的竞争，只有一个线程能够获得资源。最后，一旦所有资源都获得，就会遇到
    `all_go` 障碍，以延迟任何在倒计时之前的线程。这里我们需要 `main` 函数：
- en: '[PRE14]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note that, roughly, the first half of the function is made up of the barriers
    and resources or rocket threads. `all_go.wait()` is where it gets interesting.
    This main thread has spawned all the children and is now blocked on the all-go
    signal from the rocket threads, meaning they''ve collected their resources and
    are also blocked on the same barrier. That done, the count-down happens, to add
    a little panache to the solution; meanwhile, the rocket threads have started to
    wait on the `lift_off` barrier. It is, incidentally, worth noting that the producer
    is still producing, drawing CPU and power. Once the count-down is complete, the
    rocket threads are released, the main thread joins on them to allow them to print
    their goodbyes, and the program is finished. Outputs will vary, but here''s one
    representative example:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，大致来说，函数的前半部分由障碍和资源或火箭线程组成。`all_go.wait()` 是有趣的地方。这个主线程已经生成了所有子线程，现在正阻塞在来自火箭线程的
    all-go 信号上，这意味着它们已经收集了资源，并且也阻塞在同一个障碍上。完成这些后，发生倒计时，为解决方案增添一点风采；同时，火箭线程已经开始在 `lift_off`
    障碍上等待。顺便提一下，值得注意的是，生产者仍在生产，消耗 CPU 和电力。一旦倒计时完成，火箭线程被释放，主线程与它们连接，允许它们打印告别信息，程序结束。输出将会有所不同，但这里有一个代表性的例子：
- en: '[PRE15]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The rope bridge problem
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绳索桥问题
- en: 'This puzzle appears in *The Little Book of Semaphores* as the *Baboon crossing
    problem*, section 6.3\. Downey notes that it is adapted from Tanenbaum''s *Operating
    Systems: Design and Implementation*, so you''re getting it third-hand here. The
    problem description is thus:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个谜题出现在 *《信号量小书》* 中的 *黑猩猩过河问题*，第 6.3 节。Downey 指出，它是从 Tanenbaum 的 *操作系统：设计与实现*
    中改编的，所以你在这里得到的是第三手资料。问题描述如下：
- en: '"There is a deep canyon somewhere in Kruger National Park, South Africa, and
    a single rope that spans the canyon. Baboons can cross the canyon by swinging
    hand-over-hand on the rope, but if two baboons going in opposite directions meet
    in the middle, they will fight and drop to their deaths. Furthermore, the rope
    is only strong enough to hold 5 baboons. If there are more baboons on the rope
    at the same time, it will break.Assuming that we can teach the baboons to use
    semaphores, we would like to design a synchronization scheme with the following
    properties:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在南非克鲁格国家公园的某个地方有一个深峡谷，峡谷上只有一根横跨的绳子。狒狒可以通过手拉手的方式在绳子上荡过峡谷，但如果两只朝相反方向行进的狒狒在中间相遇，它们将会打斗并掉入峡谷死亡。此外，这根绳子只能承受5只狒狒的重量。如果有更多的狒狒同时站在绳子上，它将会断裂。假设我们可以教会狒狒使用信号量，我们希望设计一个具有以下特性的同步方案：
- en: '*Once a baboon has begun to cross, it is guaranteed to get to the other side
    without running into a baboon going the other way.*'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一旦狒狒开始过桥，就保证它能到达对岸而不会遇到朝相反方向行进的狒狒。*'
- en: '*There are never more than 5 baboons on the rope.*"'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*绳子上永远不会超过5只狒狒。*"'
- en: 'Our solution does not use semaphores. Instead, we lean on the type system to
    provide guarantees, leaning on it to ensure that no left-traveler will ever meet
    a right-traveler. Let''s dig in:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解决方案不使用信号量。相反，我们依赖类型系统来提供保证，依赖它来确保左行狒狒永远不会遇到右行狒狒。让我们深入探讨：
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We see here the usual preamble, and then a type, `Bridge`. `Bridge` is the
    model of the `rope` bridge of the problem statement and is either empty, has left-traveling
    baboons on it, or has right-traveling baboons on it; there''s no reason to fiddle
    with flags and infer state when we can just encode it into a type. In fact, leaning
    on the type system, our synchronization is very simple:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到了通常的序言，然后是一个类型，`Bridge`。`Bridge`是问题陈述中绳桥的模型，可以是空的，也可以有左行狒狒或右行狒狒在上面；我们不需要通过标志和推断状态来调整，因为我们可以直接将其编码到类型中。事实上，依赖类型系统，我们的同步非常简单：
- en: '[PRE17]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Just a single mutex. We represent either side of the bridge as a thread, each
    side trying to get its own baboons across but co-operatively allowing baboons
    to reach its side. Here''s the left side of the bridge:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 只需要一个互斥锁。我们将桥的每一侧表示为一个线程，每一侧都试图让自己的狒狒过桥，但合作地允许狒狒到达其一侧。这里是桥的左侧：
- en: '[PRE18]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'When the left side of the bridge finds the bridge itself empty, one right-traveling
    baboon is sent on its way. Further, when the left side finds that there are already
    right-traveling baboons on the bridge and the rope''s capacity of five has not
    been reached, another baboon is sent on its way. Left-traveling baboons are received
    from the rope, decrementing the total baboons on the rope. The special case here
    is the clause for `Bridge::Left(0)`. While there are no baboons on the bridge
    still, technically, if the right side of the bridge were to be scheduled before
    the left side, it would send a baboon on its way, as we''ll see shortly. We could
    make the removal of a baboon more aggressive and set the bridge to `Bridge::Empty`
    as soon as there are no travelers, of course. Here''s the right side of the bridge,
    which is similar to the left:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当桥的左侧发现桥本身是空的，就会派一只右行狒狒上路。进一步地，当左侧发现桥上已经有右行狒狒，且绳子的容量五只尚未达到，就会再派一只狒狒上路。左行狒狒从绳子上被接走，绳子上狒狒的总数减少。这里的特殊情况是`Bridge::Left(0)`的条款。尽管桥上还没有狒狒，但从技术上讲，如果桥的右侧在左侧之前被调度，它就会派一只狒狒上路，正如我们很快就会看到的。当然，我们可以使移除狒狒的操作更加积极，并在没有旅行者时立即将桥设置为`Bridge::Empty`。以下是桥的右侧，与左侧类似：
- en: '[PRE19]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now, is this fair? It depends on your system mutex. Some systems provide mutexes
    that are fair in that, if one thread has acquired the lock repeatedly and starved
    other threads attempting to lock the same primitive, the greedy thread will be
    de-prioritized under the others. Such fairness may or may not incur additional
    overheads when locking, depending on the implementation. Whether you want fairness,
    in fact, depends strongly on the problem you're trying to solve. If we were only
    interested in shuffling baboons across the rope bridge as quickly as possible—maximizing
    throughput—then it doesn't truly matter which direction they're coming from. The
    original problem statement makes no mention of fairness, which it kind of shuffles
    around by allowing the stream of baboons to be infinite. Consider what would happen
    if the baboons were finite on either side of the bridge and we wanted to reduce
    the time it takes for any individual baboon to cross to the other side, to minimize
    latency. Our present solution, adapted to cope with finite streams, is pretty
    poor, then, in that regard. The situation could be improved by occasional yielding,
    layering in more aggressive rope packing, intentional back off, or a host of other
    strategies. Some platforms allow you to dynamically shift the priority of threads
    with regard to locks, but Rust does not offer that in the standard library.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这是公平的吗？这取决于你的系统互斥锁。一些系统提供了公平的互斥锁，即如果一个线程反复获取锁并饿死尝试锁定相同原语的其他线程，那么贪婪的线程将在其他线程之下被降级。这种公平性在锁定时可能会或可能不会产生额外的开销，具体取决于实现。实际上，你是否想要公平性，很大程度上取决于你试图解决的问题。如果我们只对尽可能快地将狒狒从绳索桥上运过去感兴趣——最大化吞吐量——那么它们来自哪个方向实际上并不重要。原始问题陈述没有提到公平性，它通过允许狒狒流是无限的来某种程度上打乱了这一点。考虑如果桥的两侧的狒狒是有限的，而我们想减少任何单个狒狒过桥到另一侧所需的时间，以最小化延迟会发生什么。我们目前的解决方案，适应有限流，在这方面相当差。通过偶尔让步、分层更积极的绳索打包、有意后退或其他众多策略，可以改善这种情况。一些平台允许你动态地改变线程相对于锁的优先级，但Rust的标准库并不提供这一点。
- en: Or, you know, we could use two bounded MPSC channels. That's an option. It all
    depends on what you're trying to get done.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你知道的，我们可以使用两个有界MPSC通道。这是一个选择。一切取决于你试图完成的事情。
- en: Ultimately, the tools in safe Rust are very useful for performing computations
    on existing data structures without having to dip into any funny business. If
    that's your game, you're very unlikely to need to head much past `Mutex` and `Condvar`,
    and possibly into `RwLock` and `Barrier`. But, if you're building structures that
    are made of pointers, you'll have to dip into some of the funny business we saw
    in Ring, with all the dangers that brings.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在安全的Rust中使用的工具对于在现有数据结构上执行计算非常有用，而无需涉及任何奇怪的业务。如果你是这样的游戏，你很可能不需要深入到`Mutex`和`Condvar`之外，可能还需要到`RwLock`和`Barrier`。但是，如果你正在构建由指针组成的结构，你将不得不深入研究我们在Ring中看到的那些奇怪的业务，以及它带来的所有危险。
- en: Hopper—an MPSC specialization
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hopper——一个MPSC专业化
- en: As mentioned at the tail end of the last chapter, you'd need a fairly specialized
    use-case to consider not using stdlib's MPSC. In the rest of this chapter, we'll
    discuss such a use-case and the implementation of a library meant to fill it.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如上章末所述，你需要一个相当专业的用例来考虑不使用stdlib的MPSC。在本章的其余部分，我们将讨论这样一个用例以及旨在填补这一空白的库的实现。
- en: The problem
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Recall back to the last chapter, where the role-threads in telem communicated
    with one another over MPSC channels. Recall also that telem was a quick version
    of the cernan ([https://crates.io/crates/cernan](https://crates.io/crates/cernan))
    project, which fulfills basically the same role but over many more ingress protocols,
    egress protocols, and with the sharp edges worn down. One of the key design goals
    of cernan is that if it receives your data, it will deliver it downstream at least
    once. This implies that, for supporting ingress protocols, cernan must know, along
    the full length of the configured routing topology, that there is sufficient space
    to accept a new event, whether it's a piece of telemetry, a raw byte buffer, or
    a log line. Now, that's possible by using `SyncSender`, as we've seen. The trick
    comes in combination with a second design goal of cernan—very low, configurable
    resource consumption. Your author has seen cernan deployed to high-availability
    clusters with a single machine dedicated to running a cernan, as well as cernan
    running shotgun with an application server or as a part of a daemonset in a k8s
    cluster. In the first case, cernan can be configured to consume all of the machine's
    resources. In the later two, some thought has to be taken to giving cernan just
    enough resources to do its duties, relative to the expected input of the telemetered
    systems.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下上一章，其中telem中的角色线程通过MPSC通道相互通信。还要记住，telem是cernan ([https://crates.io/crates/cernan](https://crates.io/crates/cernan))项目的快速版本，它基本上扮演了相同的角色，但覆盖了更多的入口协议、出口协议，并且边缘更加锋利。cernan的一个关键设计目标是，如果它接收了你的数据，它至少会将它传递到下游一次。这意味着，为了支持入口协议，cernan必须在其配置的路由拓扑的全长中知道，有足够的空间来接受一个新事件，无论它是一段遥测数据、原始字节数据缓冲区还是日志行。现在，这是可能的，正如我们所看到的，通过使用`SyncSender`。技巧在于与cernan的第二个设计目标相结合——非常低、可配置的资源消耗。您的作者看到cernan被部署到高可用性集群中，一个机器专门用于运行cernan，以及cernan与应用程序服务器一起运行或作为k8s集群中daemonset的一部分。在前一种情况下，cernan可以被配置为消耗机器的所有资源。在后两种情况下，需要考虑给cernan提供足够多的资源来完成其任务，相对于遥测系统的预期输入。
- en: On modern systems, there's often an abundance of disk space and limited—relatively
    speaking—RAM. The use of `SyncSender` would require either a relatively low number
    of ingestible events or a high possible consumption of memory by cernan. These
    cases usually hit when an egress protocol is failing, possibly because the far-system
    is down. If cernan were to exacerbate a partial system failure by crowding out
    an application server because of a fault in a loosely-coupled system, well, that'd
    be pretty crummy.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代系统中，通常有大量的磁盘空间和有限的——相对而言——RAM。使用`SyncSender`可能需要相对较少的可摄入事件或cernan可能消耗大量内存。这些情况通常发生在出口协议失败时，可能是因为远端系统关闭。如果cernan因为一个松散耦合系统中的故障而挤占了应用程序服务器，从而加剧了部分系统故障，那么，那将是非常糟糕的。
- en: For many users, it's also not acceptable to go totally blind during such a partial
    outage. Tricky constraints for `SyncSender`. Tricky enough, in fact, that we decided
    to write our own MPSC, called hopper ([https://crates.io/crates/hopper](https://crates.io/crates/hopper)).
    Hopper allows endusers to configure the total in-memory consumption of the queue,
    in bytes. That's not far off `SyncSender`, with a bit of calculation. What's special
    about hopper is that it can page to disk.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多用户来说，在这样部分中断期间完全失去视线也是不可接受的。对于`SyncSender`来说，这是一个棘手的约束。实际上足够棘手，以至于我们决定编写我们自己的MPSC，称为hopper
    ([https://crates.io/crates/hopper](https://crates.io/crates/hopper))。hopper允许最终用户配置队列的总内存消耗，以字节为单位。这并不远，通过一些计算就可以达到`SyncSender`的水平。hopper的特殊之处在于它可以分页到磁盘。
- en: Hopper shines where memory constraints are very tight but you do not want to
    shed events. Or, you want to push shedding events off as far as possible. Similarly
    to the in-memory queue, hopper allows endusers to configure the maximum number
    of bytes to be consumed ondisk. A single hopper queue will hold `in_memory_bytes
    + on_disk_bytes` maximally before it's forced to shed events; we'll see the exact
    mechanism here directly. All of this is programmed with an eye toward maximal
    throughput and blocking threads that have no work, to save CPU time.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Hopper在内存限制非常严格但又不希望丢弃事件的情况下表现突出。或者，你希望尽可能地将丢弃事件推迟。与内存队列类似，hopper允许最终用户配置磁盘上可消耗的最大字节数。单个hopper队列在被迫丢弃事件之前，最多可以持有`in_memory_bytes
    + on_disk_bytes`字节；我们将在下面直接看到其确切机制。所有这些编程都是为了实现最大吞吐量和避免阻塞没有工作的线程，以节省CPU时间。
- en: Hopper in use
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hopper的使用
- en: 'Before we dig into the implementation of hopper, it''ll be instructive to see
    it in practice. Let''s adapt the last iteration of our ring program series. For
    reference, here''s what that looked like:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究hopper的实现之前，看看它在实际中的应用将是有益的。让我们调整我们环形程序系列的最后一迭代。为了参考，这里看起来是这样的：
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The hopper version is a touch different. This is the preamble:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: hopper版本略有不同。这是前言：
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'No surprise there. The function that will serve as the `writer` thread is:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么意外的。将作为`writer`线程的功能是：
- en: '[PRE22]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Other than the switch from `mpsc::SyncSender<u32>` to `hopper::Sender<u32>`—all
    hopper senders are impliclty bounded—the only other difference is that `chan`
    must be mutable. Now, for the `reader` thread:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 除了从`mpsc::SyncSender<u32>`切换到`hopper::Sender<u32>`——所有hopper发送者都是隐式有界的外——唯一的另一个区别是`chan`必须是可变的。现在，对于`reader`线程：
- en: '[PRE23]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here there''s a little more to do. In hopper, the receiver is intended to be
    used as an iterator, which *is* a little awkward here as we want to limit the
    total number of received values. The iterator will block on calls to `next`, never
    returning a `None`. However, the `send` of the sender in  hopper is very different
    to that of MPSC''s—`hopper::Sender<T>::send(&mut self, event: T) -> Result<(),
    (T, Error)>`. Ignore `Error` for a second; why return a tuple in the error condition
    that contains a `T`? To be clear, it''s the same `T` that is passed in. When the
    caller sends a `T` into hopper, its ownership is passed into hopper as well, which
    is a problem in the case of an error. The caller might well want that `T` back
    to avoid its loss. Hopper wants to avoid doing a clone of every `T` that comes
    through and, so, hopper smuggles the ownership of the `T` back to the caller.
    What of `Error`? It''s a simple enumeration:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '这里还有更多的事情要做。在hopper中，接收者被设计成用作迭代器，这在某种程度上有些尴尬，因为我们想限制接收到的值的总数。迭代器在调用`next`时会阻塞，永远不会返回`None`。然而，hopper中发送者的`send`与MPSC的非常不同——`hopper::Sender<T>::send(&mut
    self, event: T) -> Result<(), (T, Error)>`。先忽略`Error`，为什么在包含`T`的错误条件下返回一个元组？为了清楚起见，它是指传递进来的同一个`T`。当调用者将`T`发送到hopper时，它的所有权也传递给了hopper，这在出错的情况下是个问题。调用者可能非常希望将`T`拿回来以避免其丢失。Hopper想要避免对每个通过`T`都进行克隆，因此hopper偷偷地将`T`的所有权带回到调用者那里。那么`Error`呢？它是一个简单的枚举：'
- en: '[PRE24]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The important one is `Error::Full`. This is the condition that occurs when
    both the in-memory and disk-backed buffers are full at the time of sending. This
    error is recoverable, but in a way that only the caller can determine. Now, finally,
    the `main` function of our hopper example:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是`Error::Full`。这是在发送时内存和基于磁盘的缓冲区都满了的情况。这个错误是可以恢复的，但只有调用者才能确定。现在，最后，我们的hopper示例的`main`函数：
- en: '[PRE25]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `read_limit` is in place as before, but the big difference is the creation
    of the channel. First, hopper has to have some place to put its disk storage.
    Here we're deferring to some temporary directory—`let dir = tempdir::TempDir::new("queue_root").unwrap();`—to
    avoid cleaning up after running the example. In real use, the disk location is
    chosen carefully. `hopper::channel_with_explicit_capacity` creates the same sender
    as `hopper::channel` except that all the configuration knobs are open to the caller.
    The first argument is the *name* of the channel. This value is important as it
    will be used to create a directory under `dir` for disk storage. It is important
    for the channel name to be unique. `in_memory_capacity` is in bytes, as well as
    `on_disk_capacity`, which is why we have the use of our old friend `mem::size_of`.
    Now, what's that last configuration option there, set to `1`? That's the maximum
    number of *queue files*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_limit`仍然在位，但最大的不同是通道的创建。首先，hopper必须有一个地方来存放其磁盘存储。在这里，我们正在推迟到某个临时目录——`let
    dir = tempdir::TempDir::new("queue_root").unwrap();`——以避免在运行示例后进行清理。在实际使用中，磁盘位置会被仔细选择。`hopper::channel_with_explicit_capacity`创建的发送者与`hopper::channel`相同，除了所有配置旋钮都对调用者开放。第一个参数是通道的*名称*。这个值很重要，因为它将被用来在`dir`下创建一个用于磁盘存储的目录。通道名称必须是唯一的。`in_memory_capacity`和`on_disk_capacity`都是以字节为单位，这就是为什么我们使用了我们那位老朋友`mem::size_of`。现在，最后一个配置选项是什么，设置为`1`？那是*队列文件*的最大数量。'
- en: Hopper's disk storage is broken into multiple *queue files*, each of `on_disk_capacity`
    size. Senders carefully coordinate their writes to avoid over-filling the queue
    files, and the receiver is responsible for destroying them once it's sure there
    are no more writes coming—we'll talk about the signalling mechanism later in this
    chapter. The use of queue files allows hopper to potentially reclaim disk space
    that it may not otherwise have been able to, were one large file to be used in
    a circular fashion. This does incur some complexity in the sender and receiver
    code, as we'll see, but is worth it to provide a less resource-intensive library.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Hopper的磁盘存储被分成多个*队列文件*，每个文件的大小为`on_disk_capacity`。发送者仔细协调他们的写入以避免队列文件过载，接收者负责在确定没有更多写入到来后销毁它们——我们将在本章后面讨论信号机制。使用队列文件允许hopper可能回收它可能无法通过使用一个大型文件以循环方式使用的情况下回收的磁盘空间。这确实会增加发送者和接收者代码的复杂性，但为了提供一个资源消耗更少的库，这是值得的。
- en: A conceptual view of hopper
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: hopper的概念视图
- en: 'Before we dig into the implementation, let''s walk through how the previous
    example works at a conceptual level. We have enough in-memory space for 10 u32s.
    If the writer thread is much faster than the reader thread—or gets more scheduled
    time—we could end up with a situation like this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究实现之前，让我们从概念层面了解一下之前的例子是如何工作的。我们有足够的内存空间来存储10个u32。如果写入线程比读取线程快得多——或者获得更多的调度时间——我们可能会遇到这种情况：
- en: '[PRE26]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'That is, a write of `99` is entering the system when the in-memory buffer is
    totally full and there are three queued writes in the diskbuffer. While it is
    possible for the state of the world to shift between the time a write enters the
    system for queuing and between the time it is queued, let''s assume that no receivers
    pull items between queuing. The result will then be a system that looks like this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，当内存缓冲区完全满并且diskbuffer中有三个排队写入时，写入`99`正在进入系统。虽然世界状态可能在写入进入系统排队和排队之间发生变化，但让我们假设在排队期间没有接收者拉取项目。结果将是一个看起来像这样的系统：
- en: '[PRE27]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The receivers, together, must read only three elements from the diskbuffer,
    then all the in-memory elements, and then a single element from the diskbuffer,
    to maintain the queue's ordering. This is further complicated considering that
    a write may be split by one or more reads from the receivers. We saw something
    analogous in the previous chapter with regard to guarding writes by doing loads
    and checks - the conditions that satisfy a check may change between the load,
    check, and operation. There's a further complication as well; the unified disk
    buffer displayed previously does not actually exist. Instead, there are potentially
    many individual queue files.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接收者总共必须从diskbuffer中读取三个元素，然后是所有内存中的元素，最后是从diskbuffer中读取一个元素，以保持队列的顺序。考虑到写入可能被接收者的一个或多个读取操作分割，这进一步增加了复杂性。我们在上一章中看到了类似的情况，即通过执行加载和检查来保护写入——满足检查的条件可能在加载、检查和操作之间发生变化。还有一个更复杂的因素；之前显示的统一磁盘缓冲区实际上并不存在。相反，可能存在许多单独的队列文件。
- en: 'Let''s say that hopper has been configured to allow for 10 `u32` in-memory,
    as mentioned, and 10 on-disk but split across five possible queue files. Our revised
    after-write system is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 假设hopper已经被配置为允许10个`u32`的内存中配置，正如之前提到的，以及10个在磁盘上，但分布在五个可能的队列文件中。我们的修改后的写入系统如下：
- en: '[PRE28]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The senders are responsible for creating queue files and filling them to their
    max. The `Receiver` is responsible for reading from the queue file and deleting
    the said file when it's exhausted. The mechanism for determining that a queue
    file is exhausted is simple; when the `Sender` exits a queue file, it moves on
    to create a new queue file, and marks the old path as read-only in the filesystem.
    When the `Receiver` attempts to read bytes from disk and finds there are none,
    it checks the write status of the file. If the file is still read-write, more
    bytes will come eventually. If the file is read-only, the file is exhausted. There's
    a little trick to it yet, and further unexplained cooperation between `Sender`
    and `Receiver`, but that should be enough abstract detail to let us dig in effectively.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 发送者负责创建队列文件并将它们填满到最大容量。`Receiver`负责从队列文件中读取，并在文件耗尽时删除该文件。确定队列文件耗尽机制的简单方法是：当`Sender`退出一个队列文件时，它将创建一个新的队列文件，并在文件系统中将旧路径标记为只读。当`Receiver`尝试从磁盘读取字节并发现没有字节时，它会检查文件的写入状态。如果文件仍然是读写状态，最终会有更多的字节到来。如果文件是只读的，则文件已耗尽。这里还有一些小技巧，以及`Sender`和`Receiver`之间未解释的进一步协作，但这应该足以让我们有效地深入研究。
- en: The deque
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双端队列（deque）
- en: Roughly speaking, hopper is a concurrent deque with two cooperating finite state
    machines layered on top. We'll start in with the deque, defined in `src/deque.rs`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 大致来说，hopper是一个具有两个协作的有限状态机的并发双端队列。我们将从`src/deque.rs`中定义的双端队列开始。
- en: The discussion of hopper that follows lists almost all of its source code. We'll
    call out the few instances where the reader will need to refer to the listing
    in the book's source repository.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的讨论列出了几乎所有的hopper源代码。我们将指出读者需要参考书中源代码仓库中的几个实例。
- en: 'To be totally clear, a deque is a data structure that allows for queuing and
    dequeuing at either end of the queue. Rust''s `stdlib` has `VecDeque<T>`, which
    is very useful. Hopper is unable to use it, however, as one of its design goals
    is to allow for parallel sending and receiving against the hopper queue and `VecDeque`
    is not thread-safe. Also, while there are concurrent deque implementations in
    the crate ecosystem, the hopper deque is closely tied to the finite state machines
    it supports and to hopper''s internal ownership model. That is, you probably can''t
    use hopper''s deque in your own project without some changes. Anyhow, here''s
    the preamble:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完全清楚，双端队列是一种数据结构，允许在队列的两端进行入队和出队操作。Rust的`stdlib`有`VecDeque<T>`，这非常有用。然而，hopper无法使用它，因为其设计目标之一是允许对hopper队列进行并行发送和接收，而`VecDeque`不是线程安全的。此外，尽管在crate生态系统中存在并发双端队列实现，但hopper双端队列与其支持的有限状态机以及hopper的内部所有权模型紧密相关。也就是说，你可能无法在不做任何修改的情况下在自己的项目中使用hopper的双端队列。无论如何，这里是一些前置说明：
- en: '[PRE29]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The only unfamiliar piece here are the things imported from `std::sync::atomic`.
    We''ll be covering atomics in more detail in the next chapter, but we''re going
    to breeze over them at a high-level as needed in the explanation to follow. Note
    as well the unsafe declarations of `send` and `sync` for some as yet unknown type
    `Queue<T, S>`. We''re about to go off-road:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里唯一不熟悉的部分是从`std::sync::atomic`导入的内容。我们将在下一章更详细地介绍原子操作，但我们将根据需要以高级别概述它们。请注意，对于某些尚不为人知的类型`Queue<T,
    S>`，`send`和`sync`的声明是不安全的。我们即将进入非标准领域：
- en: '[PRE30]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `Queue<T, S>` definition is similar to what we saw in previous chapters:
    a simple structure wrapping an inner structure, here called `InnerQueue<T, S>`.
    The `InnerQueue` is wrapped in an `Arc,` meaning there''s only one allocated `InnerQueue`
    on the heap. As you might expect, the clone of `Queue` is a copy of the `Arc`
    into a new struct:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`Queue<T, S>`的定义与我们之前章节中看到的是相似的：一个简单的结构封装了一个内部结构，这里称为`InnerQueue<T, S>`。`InnerQueue`被封装在一个`Arc`中，这意味着堆上只有一个分配的`InnerQueue`。正如你所期望的，`Queue`的克隆是将`Arc`复制到一个新的结构体中：'
- en: '[PRE31]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'It''s important that every thread that interacts with `Queue` sees the same
    `InnerQueue`. Otherwise, the threads are dealing with distinct areas of memory
    and have no relationship with one another. Let''s look at `InnerQueue`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，每个与`Queue`交互的线程都能看到相同的`InnerQueue`。否则，线程正在处理不同的内存区域，彼此之间没有关系。让我们看看`InnerQueue`：
- en: '[PRE32]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Okay, this is much more akin to the internals we saw in Rust itself, and there's
    a lot going on. The field `capacity` is the maximum number of elements that the
    `InnerQueue` will hold in data. Like the first `Ring` in the previous chapter,
    `InnerQueue` uses a contiguous block of memory to store its `T` elements, exploding
    a `Vec` to get that contiguous block. Also, like the first `Ring`, we store `Option<T>`
    elements in the contiguous block of memory. Technically, we could deal with a
    contiguous block of raw pointers or copy memory blocks in and out. But the use
    of `Option<T>` simplifies the code, both for insertion and removal, at the cost
    of a single byte of memory per element. The added complication just isn't worth
    it for the performance goals hopper is trying to hit.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这更接近我们在 Rust 本身看到的内部结构，而且有很多事情在进行中。字段 `capacity` 是 `InnerQueue` 将在数据中保留的最大元素数量。就像前一章中的第一个
    `Ring` 一样，`InnerQueue` 使用连续的内存块来存储其 `T` 元素，通过将 `Vec` 拆分来获得这个连续块。同样，就像第一个 `Ring`
    一样，我们在连续的内存块中存储 `Option<T>` 元素。技术上，我们可以处理原始指针的连续块或复制内存块。但使用 `Option<T>` 简化了代码，无论是插入还是删除，但每个元素需要牺牲一个字节的内存。这种额外的复杂性对于
    hopper 尝试达到的性能目标来说并不值得。
- en: 'The `size` field is an atomic `usize.` Atomics will be covered in more detail
    in the next chapter, but the behavior of `size` is going to be important. For
    now, think of it as a very small piece of synchronization between threads; a little
    hook that will allow us to order memory accesses that happens also to act like
    a `usize.` The condvar `not_empty` is used to signal to any potential readers
    waiting for new elements to pop that there are, in fact, elements to pop. The
    use of condvar greatly reduces the CPU load of hopper without sacrificing latency
    to busy loops with sleeps. Now, `back_lock` and `front_lock`. What''s going on
    here? Either side of the deque is protected by a mutex, meaning there can be only
    one enqueuer and one dequeuer at a time, but these can be running in parallel
    to each other. Here are the definitions of the two inner values of the mutexes:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`size` 字段是一个原子 `usize`。原子将在下一章中详细介绍，但 `size` 的行为将非常重要。现在，将其视为线程之间非常小的同步机制；一个小的钩子，它将允许我们按顺序进行内存访问，同时也像
    `usize` 一样操作。`not_empty` condvar 用于向任何等待新元素弹出的潜在读者发出信号，实际上确实有元素可以弹出。condvar 的使用大大减少了
    hopper 的 CPU 负载，而没有牺牲对忙循环的延迟。现在，`back_lock` 和 `front_lock`。这里发生了什么？双端队列的每一侧都由互斥锁保护，这意味着一次只能有一个入队者和一个出队者，但它们可以相互并行运行。以下是互斥锁的两个内部值的定义：'
- en: '[PRE33]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`FrontGuardInner`   is the easier to explain of the two. The only field is
    `offset`, which defines the offset from the first pointer of `InnerGuard`''s data
    of the thread manipulating the front of the queue. This contiguous store is also
    used in a ring buffer fashion. In `BackGuardInner`, we see the same offset, but
    an additional `inner`, `S`. What is this? As we''ll see, the threads manipulating
    the back of the buffer need extra coordination between them. Exactly what that
    is, the queue does not care. Therefore, we make it a type parameter and allow
    the caller to sort everything out, being careful to pass the data around as needed.
    In this fashion, the queue smuggles state through itself but does not have to
    inspect it.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`FrontGuardInner` 是两个中较容易解释的。唯一的字段是 `offset`，它定义了从 `InnerGuard` 数据的第一个指针到操作队列前端的线程的偏移量。这种连续存储也以环形缓冲区的方式使用。在
    `BackGuardInner` 中，我们看到相同的偏移量，但还有一个额外的 `inner`，`S`。这是什么？正如我们将看到的，操作缓冲区后端的线程需要它们之间的额外协调。这究竟是什么，队列并不关心。因此，我们将其作为类型参数，并允许调用者整理一切，同时小心地按需传递数据。以这种方式，队列通过自身传递状态，但不需要检查它。'
- en: 'Let''s start on the implementation of `InnerQueue`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 `InnerQueue` 的实现开始：
- en: '[PRE34]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The type lacks a `new() -> InnerQueue`, as there was no call for it to be made.
    Instead, there's only `with_capacity` and that's quite similar to what we saw
    of `Ring`'s `with_capacity`—a vector is allocated, and exploded into a raw pointer,
    and the original reference is forgotten before the pointer is loaded into a newly
    minted struct.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该类型缺少一个 `new() -> InnerQueue`，因为没有必要调用它。相反，只有 `with_capacity`，这与我们之前看到的 `Ring`
    的 `with_capacity` 非常相似——分配了一个向量，并将其拆分为原始指针，然后在指针加载到新创建的结构体之前，原始引用被遗忘。
- en: 'The type `S` has to implement a default for initialization that is sufficient,
    as the caller''s smuggled state will always be the same value, which is more than
    adequately definable as a default. If this deque were intended for general use,
    we''d probably need to offer a `with_capacity` that also took an `S` directly.
    Now, a few further functions in the implementation that we''ll just breeze right
    past:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 类型 `S` 必须实现一个默认初始化，该初始化足够充分，因为调用者的走私状态总是相同的值，这可以充分定义为默认值。如果这个去队列打算用于通用用途，我们可能需要提供一个
    `with_capacity`，它也直接接受 `S`。现在，我们快速浏览一下实现中的几个其他函数：
- en: '[PRE35]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The next function, `push_back`, is very important and subtle:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数 `push_back` 非常重要且微妙：
- en: '[PRE36]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`InnerQueue::push_back` is responsible for placing a `T` onto the current back of
    the ring buffer, or failing capacity to signal that the buffer is full. When we
    discussed Ring, we noted that the `size == capacity` check was a race. Not so
    in `InnerQueue`, thanks to the atomic nature of the size. `self.size.load(Ordering::Acquire)`
    performs a memory load of the `self.size` but does so with certainty that it''s
    the only thread with `self.size` as a manipulable value. Subsequent memory operations
    in the thread will be ordered after `Acquire`, at least until a store of `Ordering::Release`
    happens. A store of that nature does happen just a handful of lines down—`self.size.fetch_add(1,
    Ordering::Release)`. Between these two points, we see the element `T` loaded into
    the buffer—with a prior check to ensure that we''re not stomping a `Some` value—and
    a wrapping bump of the `BackGuardInner`''s offset. Just like in the last chapter.
    Where this implementation differs is the return of `Ok(must_wake_dequeuers)`.
    Because the inner `S` is being guarded, it''s not possible for the queue to know
    if there will be any further work that needs to be done before the mutex can be
    given up. As a result, the queue cannot itself signal that there''s a value to
    read, even though it''s already been written to memory by the time the function
    returns. The caller has to run the notification. That''s a sharp edge. If the
    caller forgets to notify a thread blocked on the condvar, the blocked thread will
    stay that way forever.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`InnerQueue::push_back` 负责将 `T` 放置在环形缓冲区的当前尾部，或者在容量不足时发出信号表示缓冲区已满。当我们讨论环形缓冲区时，我们注意到
    `size == capacity` 检查是一个竞争条件。但在 `InnerQueue` 中不是这样，多亏了大小的原子性。`self.size.load(Ordering::Acquire)`
    执行对 `self.size` 的内存加载，但确信它是唯一一个可以将 `self.size` 作为可操作值的线程。线程中的后续内存操作将在 `Acquire`
    之后排序，至少直到发生 `Ordering::Release` 的存储操作。这种类型的存储操作就在几行之后发生——`self.size.fetch_add(1,
    Ordering::Release)`。在这两点之间，我们看到元素 `T` 被加载到缓冲区中——在之前检查确保我们不会覆盖一个 `Some` 值——并且对
    `BackGuardInner` 的偏移量进行包装增加。就像在上一章中一样。这个实现的不同之处在于返回 `Ok(must_wake_dequeuers)`。因为内部
    `S` 被保护，队列无法知道在可以放弃互斥锁之前是否需要做更多的工作。因此，队列本身无法发出有值可读的信号，尽管在函数返回时该值已经被写入内存。调用者必须运行通知。这是一个尖锐的边缘。如果调用者忘记通知在
    condvar 上阻塞的线程，那么阻塞的线程将永远保持这种状态。'
- en: 'The `InnerQueue::push_front` is a little simpler and not a radical departure
    from `push_back`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`InnerQueue::push_front` 比较简单，并且与 `push_back` 没有太大差异：'
- en: '[PRE37]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The thread popping front, because it does not have to coordinate, is able to
    take the front lock itself, as there's no state that needs to be threaded through.
    After receiving the lock, the thread then enters a condition check loop to guard
    against spurious wake-ups on `not_empty`, replacing the item at offset with `None`
    when the thread is able to wake up. The usual offset maintenance occurs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 弹出前端的线程，因为它不需要协调，能够自己获取前端锁，因为没有需要通过线程传递的状态。在获取锁后，线程进入一个条件检查循环，以防止在 `not_empty`
    上的虚假唤醒。当线程能够唤醒时，用 `None` 替换偏移量处的项目。通常的偏移量维护也会发生。
- en: 'The implementation of `Queue<T, S>` is pretty minimal in comparison to the
    inner structure. Here''s `push_back`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与内部结构相比，`Queue<T, S>` 的实现相当简单。以下是 `push_back` 的实现：
- en: '[PRE38]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The only function that''s substantially new to `Queue` is `notify_not_empty(&self,
    _guard: &MutexGuard<FrontGuardInner>) -> ()`. The caller is responsible for calling
    this whenever `push_back` signals that the dequeuer must be notified and, while
    the guard is not used, one rough edge of the library is smoothed down by requiring
    that it be passed in, proving that it''s held.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`Queue` 中唯一实质上新的功能是 `notify_not_empty(&self, _guard: &MutexGuard<FrontGuardInner>)
    -> ()`。调用者负责在 `push_back` 信号指示需要通知去队列时调用此功能，并且尽管守卫未使用，但通过要求传入守卫，该库的一个粗糙边缘得到了平滑处理，证明它被持有。'
- en: That's the deque at the core of hopper. This structure was very difficult to
    get right. Any slight re-ordering of the load and store on the atomic size with
    respect to other memory operations will introduce bugs, such as parallel access
    to the same memory of a region without coordination. These bugs are *very* subtle
    and don't manifest immediately. Liberal use of helgrind plus quickcheck testing
    across x86 and ARM processors—more on that later—will help drive up confidence
    in the implementation. Test runs of hours were not uncommon to find bugs that
    were not deterministic but could be reasoned about, given enough time and repeat
    examples. Building concurrent data structures out of very primitive pieces is
    *hard*.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是跳转器核心的双端队列。这个结构非常难以正确实现。任何对原子大小相对于其他内存操作的加载和存储的轻微重新排序都会引入错误，例如在没有协调的情况下对同一区域内存的并行访问。这些错误非常微妙，不会立即显现。在
    x86 和 ARM 处理器上广泛使用 helgrind 加 quickcheck 测试——关于这一点稍后还会详细介绍——将有助于提高对实现的信心。测试运行数小时并不罕见，可以发现那些不是确定性的但可以通过足够的时间和重复示例进行推理的错误。从非常原始的组件构建并发数据结构是*困难的*。
- en: The Receiver
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接收器
- en: 'Now that we''ve covered the deque, let''s jump on to the `Receiver`, defined
    in `src/receiver.rs`. As mentioned previously, the receiver is responsible for
    either pulling a value out of memory or from disk in the style of an iterator.
    The `Receiver` declares the usual machinery for transforming itself into an iterator,
    and we won''t cover that in this book, mostly just because it''s a touch on the
    tedious side. That said, there are two important functions to cover in the `Receiver`,
    and neither of them are in the public interface. The first is `Receiver::next_value`,
    called by the iterator version of the receiver. This function is defined as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经涵盖了双端队列，让我们跳转到定义在 `src/receiver.rs` 中的 `Receiver`。如前所述，接收器负责以迭代器的风格从内存或磁盘中拉取值。`Receiver`
    声明将自身转换为迭代器的通常机制，我们在这本书中不会涉及这一点，主要是因为它有点繁琐。话虽如此，`Receiver` 中有两个重要的函数需要介绍，而且它们都不在公共接口中。第一个是
    `Receiver::next_value`，由接收器的迭代器版本调用。此函数定义如下：
- en: '[PRE39]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'A hopper defined to move `T` values does not define a deque—as discussed in
    the previous section—over `T`s. Instead, the deque actually holds `Placement<T>`.
    Placement, defined in `src/private.rs`, is a small enumeration:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 定义为移动 `T` 值的跳转器并不定义一个在 `T`s 上的双端队列——如前文所述——而是实际上它持有 `Placement<T>`。`Placement`
    在 `src/private.rs` 中定义，是一个小的枚举：
- en: '[PRE40]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This is the trick that makes hopper work. The primary challenge with a concurrent
    data structure is providing sufficient synchronization between threads that your
    results can remain coherent despite the chaotic nature of scheduling, but not
    require so much synchronization that you're underwater compared to a sequential
    solution.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是使跳转器工作的小技巧。并发数据结构的主要挑战在于在线程之间提供足够的同步，以便您的结果可以在调度混乱的情况下保持一致性，但又不要求与顺序解决方案相比同步过多。
- en: That does happen.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况确实会发生。
- en: Now, recall that maintaining order is a key design need for any channel-style
    queue. The Rust standard library MPSC achieves this with atomic flags, aided by
    the fact that, ultimately, there's only one place for inserted elements to be
    stored and one place for them to be removed from. Not so in hopper. But, that
    one-stop-shop is a very useful, low synchronization approach. That's where `Placement`
    comes in. When the `Memory` variant is hit, the `T` is present already and the
    receiver simply returns it. When `Disk(usize)` is returned, that sends a signal
    to the receiver to flip itself into *disk mode*. In disk mode, when `self.disk_writes_to_read`
    is not zero, the receiver preferentially reads a value from disk. Only when there
    are no more disk values to be read does the receiver attempt to read from memory
    again. This mode-flipping approach maintains ordering but also has the added benefit
    of requiring no synchronization when in disk mode, saving critical time when reading
    from a slow disk.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回想一下，保持顺序是任何通道式队列的关键设计需求。Rust 标准库的 MPSC 通过原子标志实现这一点，得益于最终只有一个地方可以存储插入的元素，以及一个地方可以从中移除。但在跳转器中并非如此。但是，这个一站式商店是一个非常有用且低同步的方法。这就是
    `Placement` 发挥作用的地方。当遇到 `Memory` 变体时，`T` 已经存在，接收器只需返回它。当返回 `Disk(usize)` 时，这向接收器发送一个信号，使其切换到*磁盘模式*。在磁盘模式下，当
    `self.disk_writes_to_read` 不为零时，接收器优先从磁盘读取一个值。只有当没有更多磁盘值可以读取时，接收器才会尝试再次从内存中读取。这种模式切换方法保持了顺序，同时也具有在磁盘模式下不需要同步的额外好处，这有助于在从慢速磁盘读取时节省关键时间。
- en: 'The second important function to examine is `read_disk_value`, referenced in
    `next_value`. It''s long and mostly book-keeping, but I did want to call out the
    first part of that function here:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 需要检查的第二个重要函数是`read_disk_value`，它在`next_value`中被引用。这个函数很长，主要是记录，但我确实想在这里指出该函数的第一部分：
- en: '[PRE41]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This small chunk of code uses two very useful libraries. Hopper stores its
    disk-slop elements to disk in bincode format. Bincode ([https://crates.io/crates/bincode](https://crates.io/crates/bincode))
    was invented for the servo project and is a serialization library intended for
    IPC - more or less the exact use hopper has for it. The advantage of bincode is
    that it''s fast to serialize and deserialize but with the disadvantage of not
    being a standard and not having a guaranteed binary format from version to version.
    The second library to be called out is almost invisible in this example; byteorder.
    You can see it here: `self.fp.read_u32::<BigEndian>`. Byteorder extends `std::io::Read`
    to allow for the deserialization of primitive types from byte-buffers. It is possible
    to do this yourself by hand but it''s error-prone and tedious to repeat. Use byteorder.
    So, what we''re seeing here is hopper reading a 32-bit length big-ending length
    prefix from `self.fp`—a `std::io::BufReader` pointed to the current on-disk queue
    file—and using the said prefix to read exactly that many bytes from disk, before
    passing those on into the deserializer. That''s it. All hopper on-disk slop elements
    are a 32 bit length prefix chased by that many bytes.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这段小代码使用了两个非常实用的库。Hopper将其磁盘上的元素以bincode格式存储到磁盘上。Bincode（[https://crates.io/crates/bincode](https://crates.io/crates/bincode)）是为servo项目发明的，是一个用于IPC的序列化库——几乎就是hopper用它来做的。bincode的优点是序列化和反序列化速度快，但缺点是它不是一个标准，并且版本之间没有保证的二进制格式。需要指出的第二个库在这个例子中几乎看不见；byteorder。你可以在这里看到它：`self.fp.read_u32::<BigEndian>`。Byteorder扩展了`std::io::Read`，以便从字节缓冲区中反序列化原始类型。你可以手动做这件事，但这样做容易出错，而且重复起来很麻烦。使用byteorder。所以，我们在这里看到的是hopper从`self.fp`——一个指向当前磁盘队列文件的`std::io::BufReader`——读取32位长度的big-ending长度前缀，并使用这个前缀从磁盘上读取恰好这么多字节，然后再将这些字节传递给反序列化器。就是这样。所有hopper磁盘上的元素都是32位长度的前缀，后面跟着这么多字节。
- en: The Sender
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发送者
- en: 'Now that we''ve covered the `Receiver`, all that remains is the `Sender.` It''s
    defined in `src/sender.rs`. The most important function in the `Sender` is `send.`
    The `Sender` follows the same disk/memory mode idea that `Receiver` uses but is
    more complicated in its operation. Let''s dig in:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了`Receiver`，剩下的就是`Sender`。它在`src/sender.rs`中定义。`Sender`中最重要的函数是`send`。`Sender`遵循与`Receiver`相同的磁盘/内存模式理念，但在其操作上更为复杂。让我们深入探讨：
- en: '[PRE42]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Recall that the deque allows the holder of the back guard to smuggle state
    for coordination through it. We''re seeing that pay off here. The `Sender`''s
    internal state is called `SenderSync` and is defined as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，双端队列允许后端守护者通过它走私状态以进行协调。我们在这里看到了它的回报。`Sender`的内部状态被称为`SenderSync`，其定义如下：
- en: '[PRE43]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Every sender thread has to be able to write to the current disk queue file,
    which `sender_fp` points to. Likewise, `bytes_written` tracks how many bytes have
    been, well, written to disk. The `Sender` must keep track of this value in order
    to correctly roll queue files over when they grow too large. `sender_seq_num`
    defines the name of the current writable queue file, being as they are named sequentially
    from zero on up. The key field for us is `total_disk_writes`. Notice that a memory
    write—`self.mem_buffer.push_back(placed_event, &mut back_guard)`—might fail with
    a `Full` error. In that case, `self.write_to_disk` is called to write the `T`
    to disk, increasing the total number of disk writes. This write mode was prefixed
    with a check into the cross-thread `SenderSync` to determine if there were outstanding
    disk writes. Remember, at this point, the `Receiver` has no way to determine that
    there has been an additional write go to disk; the sole communication channel
    with the `Receiver` is through the in-memory deque. To that end, the next `Sender`
    thread will flip into a different write mode:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 每个发送线程都必须能够写入当前的磁盘队列文件，该文件由 `sender_fp` 指向。同样，`bytes_written` 跟踪已经写入磁盘的字节数。`Sender`
    必须跟踪这个值，以便在队列文件变得太大时正确地滚动队列文件。`sender_seq_num` 定义了当前可写入队列文件的名称，因为它们是从零开始的顺序命名。对我们来说，关键字段是
    `total_disk_writes`。请注意，内存写入——`self.mem_buffer.push_back(placed_event, &mut back_guard)`——可能会因为
    `Full` 错误而失败。在这种情况下，会调用 `self.write_to_disk` 将 `T` 写入磁盘，从而增加磁盘写入的总数。这种写入模式前面有一个检查跨线程
    `SenderSync`，以确定是否有挂起的磁盘写入。记住，在这个时候，`Receiver` 没有办法确定是否有额外的写入操作已经写入磁盘；与 `Receiver`
    的唯一通信渠道是通过内存中的双端队列。为此，下一个 `Sender` 线程将切换到不同的写入模式：
- en: '[PRE44]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Once there's a single write to disk, the `Sender` flips into disk preferential
    write mode. At the start of this branch the `T` goes to disk, is flushed. Then,
    the `Sender` attempts to push a `Placement::Disk` onto the in-memory deque, which
    may fail if the `Receiver` is slow or unlucky in its scheduling assignment. Should
    it succeed, however, the `total_disk_writes` is set to zero—there are no longer
    any outstanding disk writes—and the `Receiver` is woken if need be to read its
    new events. The next time a `Sender` thread rolls through it may or may not have
    space in the in-memory deque to perform a memory placement but that's the next
    thread's concern.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦有单个写入磁盘，`Sender` 就切换到磁盘优先写入模式。在这个分支的开始，`T` 被写入磁盘，并进行刷新。然后，`Sender` 尝试将一个 `Placement::Disk`
    推送到内存中的双端队列，如果 `Receiver` 的调度分配缓慢或不走运，可能会失败。然而，如果成功，`total_disk_writes` 就会被设置为零——不再有任何挂起的磁盘写入，并且如果需要，`Receiver`
    会被唤醒以读取其新的事件。下一次 `Sender` 线程遍历时，它可能或可能没有足够的空间在内存中的双端队列中执行内存放置，但这将是下一个线程的问题。
- en: That's the heart of `Sender`. While there is another large function in-module,
    `write_to_disk`, we won't list it here. The implementation is primarily book-keeping
    inside a mutex, a topic that has been covered in detail in this and the previous
    chapter, plus filesystem manipulation. That said, the curious reader is warmly
    encouraged to read through the code.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 `Sender` 的核心。虽然模块中还有一个大的函数 `write_to_disk`，但我们在这里不会列出它。实现主要是使用互斥锁进行账目管理，这个主题在本章和上一章中已经详细讨论过，还包括文件系统操作。话虽如此，我们热情地鼓励好奇的读者阅读代码。
- en: Testing concurrent data structures
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试并发数据结构
- en: Hopper is a subtle beast and proves to be quite tricky to get correct, owing
    to the difficulty of manipulating the same memory across multiple threads. Slight
    synchronization bugs were common in the writing of the implementation, bugs that
    highlighted fundamental mistakes but were rare to trigger and even harder to reproduce.
    Traditional unit testing is not sufficient here. There are no clean units to be
    found, being that the computer's non-deterministic behavior is fundamental to
    the end result of the program's run. With that in mind, there are three key testing
    strategies used on hopper: randomized testing over random inputs searching for
    logic bugs (QuickCheck), randomized testing over random inputs searching for crashes
    (fuzz testing), and multiple-million runs of the same program searching for consistency
    failures. In the rest of the chapter, we'll discuss each of these in turn.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Hopper是一个微妙的生物，证明要正确地处理它相当棘手，因为要在多个线程之间操作相同的内存很困难。在实现编写过程中，轻微的同步错误很常见，这些错误突出了基本的错误，但很少触发，甚至更难重现。传统的单元测试在这里是不够的。找不到干净的单元，因为计算机的非确定性行为是程序运行最终结果的基本因素。考虑到这一点，hopper使用了三种关键的测试策略：在随机输入上执行随机测试以寻找逻辑错误（QuickCheck）、在随机输入上执行随机测试以寻找崩溃（模糊测试），以及运行数百万次相同的程序以寻找一致性失败。在接下来的章节中，我们将依次讨论这些策略。
- en: QuickCheck and loops
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QuickCheck和循环
- en: 'Previous chapters discussed QuickCheck at length and we''ll not duplicate that
    here. Instead, let''s dig into a test from `hopper`, defined in `src/lib.rs`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的章节详细讨论了QuickCheck，我们在这里不会重复。相反，让我们深入探讨一个来自`hopper`的测试，该测试在`src/lib.rs`中定义：
- en: '[PRE45]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This test sets up a multiple sender, single receiver round trip environment,
    being careful to reject inputs that allow less space in the in-memory buffer than
    64 bits, no senders, or the like. It defers to another function, `multi_thread_concurrent_snd_and_rcv_round_trip_exp`,
    to actually run the test.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个测试设置了一个多发送者、单接收者的往返环境，并小心地拒绝那些在内存缓冲区中允许小于64位空间、没有发送者或类似情况的输入。它将任务委托给另一个函数`multi_thread_concurrent_snd_and_rcv_round_trip_exp`来实际运行测试。
- en: 'This setup is awkward, admittedly, but has the benefit of allowing `multi_thread_concurrent_snd_and_rcv_round_trip_exp`
    to be run over explicit inputs. That is, when a bug is found you can easily re-play
    that test by creating a manual—or *explicit*, in `hopper` testing terms—test.
    The inner test function is complicated and we''ll consider it in parts:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置确实有些尴尬，但它的好处是允许`multi_thread_concurrent_snd_and_rcv_round_trip_exp`在明确的输入上运行。也就是说，当发现错误时，你可以通过创建手动测试或*明确地*（在hopper测试术语中）测试来轻松地重新播放那个测试。内部测试函数很复杂，我们将分部分考虑：
- en: '[PRE46]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Much like our example usage of hopper at the start of this section, the inner
    test function uses tempdir ([https://crates.io/crates/tempdir](https://crates.io/crates/tempdir)) to
    create a temporary path for passing into `channel_with_explicit_capacity`. Except,
    we're careful not to unwrap here. Because hopper makes use of the filesystem and
    because Rust QuickCheck is aggressively multi-threaded, it's possible that any
    individual test run will hit a temporary case of file-handler exhaustion. This
    throws QuickCheck off, with the test failure being totally unrelated to the inputs
    of this particular execution.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在本节开头对hopper的示例用法一样，内部测试函数使用tempdir ([https://crates.io/crates/tempdir](https://crates.io/crates/tempdir))创建一个临时路径，以便传递给`channel_with_explicit_capacity`。但是，我们在这里非常小心，不要解包。因为hopper使用了文件系统，并且因为Rust
    QuickCheck是积极的多线程的，所以任何单个测试运行都可能遇到文件句柄耗尽的情况。这会导致QuickCheck出错，测试失败与这次特定执行的输入完全无关。
- en: 'The next piece is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分如下：
- en: '[PRE47]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Here we have the creation of the sender threads, each of which grabs an equal
    sized chunk of `vals`. Now, because of indeterminacy in thread scheduling, it''s
    not possible for us to model the order in which elements of `vals` will be pushed
    into hopper. All we can do is confirm that there are no lost elements after transmission
    through hopper. They may, in fact, be garbled in terms of order:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了发送者线程，每个线程从`vals`中抓取一个等大小的块。现在，由于线程调度的不可确定性，我们无法模拟`vals`元素被推入hopper的顺序。我们所能做的就是确认在通过hopper传输后没有丢失元素。实际上，它们可能在顺序上被破坏：
- en: '[PRE48]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Another test with a single sender, `single_sender_single_rcv_round_trip`, is
    able to check for correct ordering as well as no data loss:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个只有一个发送者的测试`single_sender_single_rcv_round_trip`能够检查正确的顺序以及没有数据丢失：
- en: '[PRE49]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Like it''s multi-cousin, this QuickCheck test uses an inner function to perform
    the actual test:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 就像它的多堂兄弟一样，这个QuickCheck测试使用一个内部函数来执行实际的测试：
- en: '[PRE50]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This, too, should appear familiar, except that the Receiver thread is now able
    to check order. Where previously a `Vec<u64>` was fed into the function, we now
    stream `0, 1, 2, .. total_vals` through the hopper queue, asserting that order
    and there are no gaps on the other side. Single runs on a single input will fail
    to trigger low-probability race issues reliably, but that's not the goal. We're
    searching for logic goofs. For instance, an earlier version of this library would
    happily allow an in-memory maximum amount of bytes less than the total bytes of
    an element `T`. Another could fit multiple instances of `T` into the buffer but
    if the `total_vals` were odd *and* the in-memory size were small enough to require
    disk-paging then the last element of the stream would never be kicked out. In
    fact, that's still an issue. It's a consequence of the lazy flip to disk mode
    in the sender; without another element to potentially trigger a disk placement
    to the in-memory buffer, the write will be flushed to disk but the receiver will
    never be aware of it. To that end, the sender does expose a `flush` function,
    which you see in use in the tests. In practice, in cernan, flushing is unnecessary.
    But, it's a corner of the design that the authors did not expect and may well
    have had a hard time noticing had this gone out into the wild.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点也应该很熟悉，只是现在接收线程能够检查顺序。之前是将`Vec<u64>`输入到函数中，我们现在通过跳转队列流式传输`0, 1, 2, .. total_vals`，断言顺序并且另一侧没有间隙。对单个输入的单次运行将无法可靠地触发低概率竞态问题，但这不是目标。我们正在寻找逻辑错误。例如，这个库的早期版本会愉快地允许内存中的最大字节数小于元素`T`的总字节数。另一个版本可以将多个`T`实例放入缓冲区，但如果`total_vals`是奇数并且内存大小足够小以至于需要磁盘分页，那么流中的最后一个元素将永远不会被踢出。实际上，这仍然是一个问题。这是发送者中懒惰翻转至磁盘模式的结果；如果没有其他元素可能触发磁盘放置到内存缓冲区，写入将被刷新到磁盘，但接收者永远不会意识到这一点。为此，发送者确实提供了一个`flush`函数，您可以在测试中看到它的使用。在实践中，在cernan中，刷新是不必要的。但是，这是设计者没有预料到的设计角落，如果这个设计被广泛使用，可能很难注意到这个问题。
- en: 'The inner test of our single-sender variant is also used for the repeat-loop
    variant of hopper testing:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们单发送者变体的内部测试也用于跳转测试的重复循环变体：
- en: '[PRE51]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Notice that here the inner loop is only for 2,500 iterations. This is done in
    deference to the needs of the CI servers which, don't care to run high CPU load
    code for hours at a time. In development, that 2,500 will be adjusted up. But
    the core idea is apparent; check that a stream of ordered inputs returns through
    the hopper queue in order and intact over and over and over again. QuickCheck
    searches the dark corners of the state space and more traditional manual testing
    hammers the same spot to dig in to computer indeterminism.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这里的内部循环仅进行2,500次迭代。这是为了尊重CI服务器的需求，它们不介意连续几小时运行高CPU负载的代码。在开发过程中，这个2,500次将会调整增加。但核心思想是明显的；检查一系列有序输入是否能够一次又一次地通过跳转队列按顺序且完整地返回。QuickCheck搜索状态空间中的暗角，而更传统的手动测试则反复敲打同一位置以挖掘计算机的不确定性。
- en: Searching for crashes with AFL
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AFL搜索崩溃
- en: The repeat-loop variant of testing in the previous section is an interesting
    one. It's not, like QuickCheck, a test wholly focused on finding logic bugs. We
    know that the test will work for most iterations. What's being sought out there
    is a failure to properly control the indeterminism of the underlying machine.
    In some sense that variant of test is using a logical check to search for crashes.
    It is a kind of fuzz test.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节中测试的重复循环变体很有趣。它不像QuickCheck那样，完全专注于寻找逻辑错误。我们知道测试将在大多数迭代中工作。在那里寻找的是未能正确控制底层机器的不确定性。在某种程度上，这种测试变体使用逻辑检查来搜索崩溃。它是一种模糊测试。
- en: Hopper interacts with the filesystem, spawns threads, and manipulates bytes
    up from files. Each of these activities is subject to failure for want of resources,
    offsetting errors, or fundamental misunderstandings of the medium. An early version
    of hopper, for instance, assumed that small atomic writes to the disk would not
    interleave, which is true for XFS but not true for most other filesystems. Or,
    another version of hopper always created threads in-test by use of the more familiar
    `thread::spawn`. It turns out, however, that this function will panic if no thread
    can be created, which is why the tests use the `thread::Builder` pattern instead,
    allowing for recovery.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Hopper 与文件系统交互，创建线程，并从文件中操作字节。这些活动都可能因为资源不足、偏移错误或对介质的基本误解而失败。例如，hopper 的早期版本假设对磁盘的小原子写入不会交错，这在
    XFS 中是正确的，但在大多数其他文件系统中则不正确。或者，hopper 的另一个版本总是通过更熟悉的 `thread::spawn` 来创建测试中的线程。然而，如果无法创建线程，这个函数会引发恐慌，这就是为什么测试使用
    `thread::Builder` 模式而不是，允许恢复。
- en: 'Crashes are important to figure out in their own right. To this end, hopper
    has a fuzzing setup, based on AFL. To avoid producing an unnecessary binary on
    users'' systems, the hopper project does not host its own fuzzing code as of this
    writing. Instead, it lives in `blt/hopper-fuzz` ([https://github.com/blt/hopper-fuzz](https://github.com/blt/hopper-fuzz)).
    This is, admittedly, awkward, but fuzz testing, being uncommon, often is. Fuzz
    rounds easily run for multiples of days and do not fit well into modern CI systems.
    AFL itself is not a tool that admits easy automation, either, compounding the
    problem. Fuzz runs tend to be done in small batches on users'' private machines,
    at least for open-source projects with small communities. Inside hopper-fuzz,
    there''s a single program for fuzzing, the preamble of which is:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 崩溃本身很重要。为此，hopper 有一个基于 AFL 的模糊设置。为了避免在用户系统上产生不必要的二进制文件，截至本文撰写时，hopper 项目没有托管自己的模糊代码。相反，它位于
    `blt/hopper-fuzz` ([https://github.com/blt/hopper-fuzz](https://github.com/blt/hopper-fuzz))。诚然，这很尴尬，但模糊测试，由于其不常见，通常是这样的。模糊测试可能持续数天，而且不适合现代
    CI 系统。AFL 本身也不是一个容易自动化的工具，这也加剧了问题。模糊测试通常在用户私有机器上的小批量中进行，至少对于社区规模较小的开源项目是这样的。在
    hopper-fuzz 中，有一个用于模糊测试的单个程序，其前言如下：
- en: '[PRE52]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The fairly straightforward, based on what we''ve seen so far. Getting input
    into a fuzz program is sometimes non-trivial, especially when the input is not
    much more than a set of inputs rather than, say in the case of a parser, an actual
    unit of work for the program. Your author''s approach is to rely on serde to deserialize
    the byte buffer that AFL will fling into the program, being aware that most payloads
    will fail to decode but not minding all that much. To that end, the top of your
    author''s fuzz programs usually have an input struct filled with control data,
    and this program is no different:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们迄今为止所看到的，相当直接。将输入传递给模糊程序有时并不简单，尤其是当输入不仅仅是输入集而不是，比如说，解析器的情况下的程序的实际工作单元。作者的方法是依赖
    serde 来反序列化 AFL 将投入程序的字节缓冲区，知道大多数有效载荷将无法解码，但并不太在意。为此，作者模糊程序的顶部通常有一个填充了控制数据的输入结构体，这个程序也不例外：
- en: '[PRE53]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The seeds are for `XorShiftRng`, whereas `max_in_memory_bytes` and `max_disk_bytes`
    are for hopper. A careful tally of the bytesize of input is made to avoid fuzz
    testing the bincode deserializer's ability to reject abnormally large inputs.
    While AFL is not blind—it has instrumented the branches of the program, after
    all—it is also not very smart. It's entirely possible that what you, the programmer,
    intends to fuzz is not what gets fuzzed to start. It's not unheard of to shake
    out bugs in any additional libraries brought into the fuzz project. It's to keep
    the libraries drawn in to a minimum and their use minimal.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 种子是为 `XorShiftRng`，而 `max_in_memory_bytes` 和 `max_disk_bytes` 是为 hopper。仔细计算输入的字节大小，以避免模糊测试
    bincode 反序列化器拒绝异常大输入的能力。虽然 AFL 不是盲目的——毕竟它已经对程序的分支进行了仪器化——但它也不是很聪明。完全有可能，程序员想要模糊测试的内容并不是一开始就模糊测试的内容。在模糊项目中引入任何额外的库中找出错误并不罕见。这是为了将库的使用限制在最小范围内，并使其使用最小化。
- en: 'The `main` function of the fuzz program starts off with a setup similar to
    other hopper tests:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊程序的 `main` 函数开始于与其他 hopper 测试类似的设置：
- en: '[PRE54]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The only oddity is the production of prefix. The entropy in the `tempdir` name
    is relatively low, compared to the many, many millions of tests that AFL will
    run. We want to be especially sure that no two hopper runs are given the same
    data directory, as that is undefined behavior:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一奇怪的是前缀的产生。与AFL将要运行的数以百万计的测试相比，`tempdir`名称中的熵相对较低。我们特别想要确保没有两个料斗运行具有相同的数据目录，因为这将是未定义的行为：
- en: '[PRE55]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The meat of the AFL test is surprising:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: AFL测试的核心内容令人惊讶：
- en: '[PRE56]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'This program pulled in a random number generator to switch off between performing
    a read and performing a write into the hopper channel. Why no threads? Well, it
    turns out, AFL struggles to accommodate threading in its model. AFL has a stability
    notion, which is its ability to re-run a program and achieve the same results
    in terms of instruction execution and the like. This is not going to fly with
    a multi-threaded use of hopper. Still, despite missing out on probing the potential
    races between sender and receiver threads, this fuzz test found:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序引入了一个随机数生成器，在执行读取和写入料斗通道操作之间进行切换。为什么没有使用线程？好吧，结果是，AFL在模型中很难适应线程。AFL有一个稳定性概念，即它能够重新运行一个程序，并在指令执行等方面达到相同的结果。在多线程使用料斗的情况下，这行不通。尽管如此，尽管错过了探测发送者和接收者线程之间潜在竞争条件的机会，这个模糊测试发现：
- en: File-descriptor exhaustion crashes
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件描述符耗尽导致的崩溃
- en: Incorrect offset computations in deque
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双端队列中的错误偏移计算
- en: Incorrect offset computations at the Sender/Receiver level
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发送者/接收者级别的错误偏移计算
- en: Deserialization of elements into a non-cleared buffer, resulting in phantom
    elements
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将元素反序列化到未清除的缓冲区中，导致产生幽灵元素
- en: Arithmetic overflow/underflow crashes
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算术溢出/下溢导致的崩溃
- en: A failure to allocate enough space for serialization.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分配足够空间进行序列化失败。
- en: Issues with the interior deque that happen only in a multi-threaded context
    will be missed, of course, but the preceding list is nothing to sneeze at.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在多线程环境中发生的内部双端队列问题将会被忽略，当然，但前面的列表并非微不足道。
- en: Benchmarking
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试
- en: Okay, by now we can be reasonably certain that hopper is fit for purpose, at
    least in terms of not crashing and producing the correct results. But, it also
    needs to be *fast*. To that end, hopper ships with the criterion ([https://crates.io/crates/criterion](https://crates.io/crates/criterion))
    benchmarks. As the time of writing, criterion is a rapidly evolving library that
    performs statistical analysis on bench run results that Rust's built-in, nightly-only benchmarking
    library does not. Also, criterion is available for use on the stable channel.
    The target to match is standard library's MPSC, and that sets the baseline for
    hopper. To that end, the benchmark suite performs a comparison, living in `benches/stdlib_comparison.rs`
    in the hopper repository.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，到目前为止，我们可以合理地确信料斗适合使用，至少在不会崩溃并产生正确结果方面。但是，它也需要*快速*。为此，料斗附带criterion（[https://crates.io/crates/criterion](https://crates.io/crates/criterion)）基准测试。截至写作时，criterion是一个快速发展的库，它对基准运行结果进行统计分析，而Rust内置的夜间基准测试库则没有。此外，criterion还可在稳定渠道上使用。要匹配的目标是标准库的MPSC，这为料斗设定了基线。为此，基准测试套件执行了比较，位于料斗存储库中的`benches/stdlib_comparison.rs`。
- en: 'The preamble is typical:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 前置代码很典型：
- en: '[PRE57]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Note that we''ve pulled in both MPSC and hopper. The function for MPSC that
    we''ll be benching is:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已经引入了MPSC和料斗。我们将要基准测试的MPSC函数是：
- en: '[PRE58]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Some sender threads get made, and a receiver exists and pulls the values from
    MPSC as rapidly as possible. This is not a logic check in any sense and the collected
    materials are immediately discarded. Like with the fuzz testing, the input to
    the function is structured data. `MpscInput` is defined as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一些发送线程被创建，一个接收者存在，并尽可能快地从MPSC中提取值。这根本不是逻辑检查，收集到的材料立即被丢弃。就像模糊测试一样，函数的输入是结构化数据。`MpscInput`定义如下：
- en: '[PRE59]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The hopper version of this function is a little longer, as there are more error
    states to cope with, but it''s nothing we haven''t seen before:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的料斗版本稍微长一些，因为需要处理更多的错误状态，但我们之前都见过：
- en: '[PRE60]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The same is true of `HopperInput`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 同样适用于`HopperInput`：
- en: '[PRE61]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Criterion has many options for running benchmarks, but we''ve chosen here to
    run over inputs. Here''s the setup for MPSC:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Criterion提供了许多运行基准测试的选项，但在这里我们选择在输入上运行。以下是MPSC的设置：
- en: '[PRE62]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'To explain, we''ve got a function, `mpsc_benchmark`, that takes the mutable
    criterion structure, which is opaque to use but in which criterion will store
    run data. This structure exposes `bench_function_over_inputs`, which consumes
    a closure that we can thread our `mpsc_test` through. The sole input is listed
    in a vector. The following is a setup that does the same thing, but for hopper:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释，我们有一个函数，`mpsc_benchmark`，它接受可变的criterion结构，这个结构在使用时是透明的，但其中criterion会存储运行数据。这个结构暴露了`bench_function_over_inputs`，它消耗一个闭包，我们可以通过`mpsc_test`来传递。唯一的输入列在一个向量中。以下是一个设置，它做的是同样的事情，但针对hopper：
- en: '[PRE63]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Notice now that we have two inputs, one guaranteed to be all in-memory and
    the other guaranteed to require disk paging. The disk paging input is sized appropriately
    to match the MSPC run. There''d be no harm in doing an in-memory comparison for
    both hopper and MPSC, but your author has a preference for pessimistic benchmarks,
    being an optimistic sort. The final bits needed by criterion are more or less
    stable across all the benchmarks we''ll see in the rest of this book:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 注意现在我们有两个输入，一个是保证全部在内存中，另一个保证需要磁盘分页。磁盘分页输入的大小适当，以匹配MSPC运行。对于hopper和MPSC进行内存比较并无害处，但作者更倾向于悲观的基准测试，因为作者是一个乐观主义者。以下是我们将在本书剩余部分看到的基准测试中需要的最终位几乎都是稳定的：
- en: '[PRE64]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: I encourage you to run the benchmarks yourself. We see times for hopper that
    are approximately three times faster for the systems we intended hopper for. That's
    more than fast enough.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你自己运行基准测试。我们看到hopper的时间大约是我们打算为hopper设计的系统的三倍快。这已经足够快了。
- en: Summary
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered the remainder of the essential non-atomic Rust synchronization
    primitives, doing a deep-dive on the postmates/hopper libraries to explore their
    use in a production code base. After having digested [Chapter 4](5a332d94-37e4-4748-8920-1679b07e2880.xhtml),
    *Sync and Send – the Foundation of Rust Concurrency*, and this chapter, the reader
    should be in a fine position to build lock-based, concurrent data structures in
    Rust. For readers that need even more performance, we'll explore the topic of
    atomic programming in [Chapter 6](d42acb0b-a05e-4068-894f-81365d147bf4.xhtml),
    *Atomics – the Primitives of Synchronization*, and in [Chapter 7](2099e79d-45cd-46cb-bf58-fc27b27b84ec.xhtml),
    *Atomics – Safely Reclaiming Memory*.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了剩余的基本非原子Rust同步原语，对postmates/hopper库进行了深入研究，以探索其在生产代码库中的应用。在消化了[第4章](5a332d94-37e4-4748-8920-1679b07e2880.xhtml)，*同步与发送
    – Rust并发的基石*，以及本章内容后，读者应该能够很好地在Rust中构建基于锁的并发数据结构。对于需要更高性能的读者，我们将在[第6章](d42acb0b-a05e-4068-894f-81365d147bf4.xhtml)，*原子操作
    – 同步的原始操作*，和[第7章](2099e79d-45cd-46cb-bf58-fc27b27b84ec.xhtml)，*原子操作 – 安全地回收内存*中探讨原子编程主题。
- en: If you thought lock-based concurrency was hard, wait  until you see atomics.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为基于锁的并发很困难，那么等你看到原子操作。
- en: Further reading
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Building concurrent data structure is a broad field of wide concern. These notes
    cover much the same space as the notes from [Chapter 4](5a332d94-37e4-4748-8920-1679b07e2880.xhtml), *Sync
    and Send – the Foundation of Rust Concurrency*. Please do refer back to those
    notes.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 构建并发数据结构是一个广泛关注的领域。这些笔记涵盖了与[第4章](5a332d94-37e4-4748-8920-1679b07e2880.xhtml)，*同步与发送
    – Rust并发的基石*中笔记几乎相同的空间。请务必参考那些笔记。
- en: '*The Little Book of Semaphores*, Allen Downey, available at [http://greenteapress.com/wp/semaphores/](http://greenteapress.com/wp/semaphores/).This
    is a charming book of concurrency puzzles, suitable for undergraduates but challenging
    enough in Rust for the absence of semaphores. We''ll revisit this book in the
    next chapter when we build concurrency primitives out of atomics.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《信号量小书》*，艾伦·道尼，可在[http://greenteapress.com/wp/semaphores/](http://greenteapress.com/wp/semaphores/)找到。这是一本关于并发谜题的迷人书籍，适合本科生阅读，但在Rust中由于没有信号量，挑战性也足够大。我们将在下一章构建原子并发原语时再次回顾这本书。'
- en: '*The Computability of Relaxed Data Structures: Queues and Stacks as Examples*,
    Nir Shavit and Gadi Taubenfeld. This chapter discussed the implementation of a
    concurrent queue based on the presentation of *The Art of Multiprocessor Programming*
    and the author''s knowledge of Erlang''s process queue. Queues are a common concurrent
    data structure and there are a great many possible approaches. This paper discusses
    an interesting notion. Namely, if we relax the ordering constraint of the queue,
    can we squeeze out more performance from a modern machine?'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《松散数据结构的可计算性：队列和栈为例》，作者：尼尔·沙维特和加迪·塔本菲尔德。本章讨论了基于《多处理器编程的艺术》的演示和作者对Erlang进程队列的了解实现的并发队列。队列是一种常见的并发数据结构，有许多可能的实现方法。本文讨论了一个有趣的概念。也就是说，如果我们放松队列的排序约束，我们能否从现代机器中挤出更多的性能？'
- en: '*Is Parallel Programming Hard, and, If So, What Can You Do About It?*, Paul
    McKenney. This book covers roughly the same material as [Chapter 4](5a332d94-37e4-4748-8920-1679b07e2880.xhtml), *Sync
    and Send – the Foundation of Rust Concurrency,* and [Chapter 5](e2de1ba7-c291-494e-82da-37fee7323c1d.xhtml), *Locks
    – Mutex, Condvar, Barriers, and RWLock*, but in significantly more detail. I highly
    encourage readers to look into getting a copy and reading it, especially the eleventh
    chapter on validating implementations.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《并行编程难吗？如果难，你能做什么？》*，作者：保罗·麦肯尼。这本书涵盖了与[第4章](5a332d94-37e4-4748-8920-1679b07e2880.xhtml)，*同步与发送——Rust
    并发的基础*和[第5章](e2de1ba7-c291-494e-82da-37fee7323c1d.xhtml)，*锁——互斥锁、条件变量、屏障和读写锁*大致相同的材料，但内容更为详细。我强烈建议读者寻找这本书的副本并阅读它，尤其是关于验证实现的第十一章。'

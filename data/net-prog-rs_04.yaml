- en: Data Serialization, Deserialization, and Parsing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据序列化、反序列化和解析
- en: In the previous chapter, we covered writing simple socket servers in Rust. Transport
    protocols such as TCP and UDP only provide mechanisms to transport messages, so
    it is up to a higher-level protocol to actually craft and send those messages.
    Also, TCP and UDP protocols always deal with bytes; we saw this when we called
    `as_bytes` on our strings before sending those out on the socket. This process
    of converting a piece of data into a format that can be stored or transmitted
    (a stream of bytes in the case of networking) is called serialization. The reverse
    process is deserialization, which turns a raw data format into a data structure.
    Any networking software must deal with serializing and deserializing data that
    has been received, or is about to be sent out. This simple conversion is not always
    possible for more complex types such as user-defined types, or even simple collection
    types. The Rust ecosystem has special crates that can handle these in a wide range
    of cases.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了在 Rust 中编写简单的套接字服务器。传输协议，如 TCP 和 UDP，仅提供传输消息的机制，因此需要更高层次的协议来实际构建和发送这些消息。此外，TCP
    和 UDP 协议始终处理字节；我们在将字符串发送到套接字之前调用 `as_bytes` 时看到了这一点。将数据转换为可以存储或传输的格式（在网络的案例中是字节流）的过程称为序列化。相反的过程是反序列化，它将原始数据格式转换为数据结构。任何网络软件都必须处理接收到的或即将发送的数据的序列化和反序列化。对于更复杂的数据类型，如用户定义的数据类型，或甚至是简单的集合类型，这种简单的转换并不总是可能的。Rust
    生态系统有一些特殊的包可以处理各种情况。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Serialization and deserialization using Serde. We will start with basic usage
    and then move on to writing custom serializers using Serde.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Serde 进行序列化和反序列化。我们将从基本用法开始，然后继续介绍如何使用 Serde 编写自定义序列化器。
- en: Parsing textual data using nom.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 nom 解析文本数据。
- en: The last topic will be on parsing binary data, a very frequently used technique
    in networking.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个主题将是解析二进制数据，这是网络中非常常用的一种技术。
- en: Serialization and deserialization using Serde
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Serde 进行序列化和反序列化
- en: Serde is the de-facto standard way of serializing and deserializing data in
    Rust. Serde supports a number of data structures that it can serialize out of
    the box to a number of given data formats (including JSON, and TOML, CSV). The
    easiest way to understand Serde is to think of it as an invertible function that
    transforms a given data structure into a stream of bytes. Other than standard
    data types, Serde also provides a few macros that can be implemented on user defined
    data types, making them (de)serializable.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Serde 是 Rust 中序列化和反序列化数据的既定标准方式。Serde 支持多种数据结构，它可以直接将这些数据结构序列化成多种给定的数据格式（包括
    JSON、TOML 和 CSV）。理解 Serde 的最简单方式是将其视为一个可逆函数，它将给定的数据结构转换成字节流。除了标准数据类型外，Serde 还提供了一些宏，这些宏可以应用于用户定义的数据类型，使它们（反）序列化。
- en: 'In [Chapter 2](part0031.html#TI1E0-e803f047c8b7448c90887daa96419287), *Introduction
    to Rust and its Ecosystem*, we discussed how procedural macros can be used to
    implement custom derives for given data types. Serde uses that mechanism to provide
    two custom derives, named `Serialize` and `Deserialize`, that can be implemented
    for user-defined data types that are composed of data types that Serde supports.
    Let us look at a small example of how this works. We start with creating the empty
    project using Cargo:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 2 章](part0031.html#TI1E0-e803f047c8b7448c90887daa96419287)，“Rust 及其生态系统简介”中，我们讨论了如何使用过程宏为给定的数据类型实现自定义推导。Serde
    使用该机制提供两个自定义推导，分别命名为 `Serialize` 和 `Deserialize`，这些推导可以应用于由 Serde 支持的数据类型组成的用户定义数据类型。让我们看看一个小例子，看看它是如何工作的。我们首先使用
    Cargo 创建一个空项目：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is what the Cargo manifest should look like:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Cargo 清单应该看起来像的样子：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `serde` crate is the core of the Serde ecosystem. The `serde_derive` crate
    provides necessary instrumentation that uses procedural macros for deriving `Serialize`
    and `Deserialize`. The next two crates provide Serde-specific functionality to
    and from JSON and YAML, respectively:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`serde` 包是 Serde 生态系统的核心。`serde_derive` 包提供了必要的工具，使用过程宏来推导 `Serialize` 和 `Deserialize`。接下来的两个包分别提供了
    Serde 特定的功能，用于将数据序列化和反序列化为 JSON 和 YAML：'
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Since the `serde_derive` crate exports macros, we will need to mark it with
    a `macro_use` declaration; we then declare all our dependencies as `extern` crates.
    Having set this up, we can define our custom data type. In this case, we are interested
    in a config for a server that has a bunch of parameters of different types. The
    `auth_server` parameter is optional and that is why it is wrapped in an `Option`.
    Our struct derives the two traits from Serde, and also the compiler-provided `Debug`
    trait that we will use later to display after deserialization. In our main function,
    we instantiate our class and call `serde_yaml::to_string` on it to serialize it
    to a string; the reverse of this is `serde_yaml::from_str`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `serde_derive` crate 导出宏，我们需要用 `macro_use` 声明来标记它；然后我们声明所有依赖项为 `extern` crate。设置好这些后，我们可以定义我们的自定义数据类型。在这种情况下，我们感兴趣的是具有不同类型参数的服务器配置。`auth_server`
    参数是可选的，这就是为什么它被包裹在 `Option` 中。我们的 struct 从 Serde 继承了两个特质，还继承了编译器提供的 `Debug` 特质，我们将在反序列化后使用它来显示。在我们的主函数中，我们实例化我们的类，并对其调用
    `serde_yaml::to_string` 来将其序列化为字符串；其逆操作是 `serde_yaml::from_str`。
- en: 'A sample run should look like this:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例运行应该看起来像这样：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let us move on to a more advanced example of using Serde over a network. In
    this example, we will set up a TCP server and a client. This part will be exactly
    the same as how we did it in the last chapter. But this time, our TCP server will
    function as a calculator that takes in a point in a 3D space with three components
    along the three axes, and returns its distance from the origin in the same reference
    frame. Let us set up our Cargo project like this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续一个更高级的示例，展示如何在网络上使用 Serde。在这个例子中，我们将设置一个 TCP 服务器和一个客户端。这部分将与我们上一章中做的一样。但这次，我们的
    TCP 服务器将作为一个计算器运行，它接受一个沿三个轴有三个分量的 3D 空间中的点，并返回它在同一参考系中与原点的距离。让我们这样设置我们的 Cargo
    项目：
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The manifest should look like this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 清单应该看起来像这样：
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With this, we can then move on to defining our code. In this example, the server
    and the client will be in the same binary. The application will take in a flag
    that dictates whether it should run as the server or the client. As we did in
    the last chapter, in the server case, we will bind to all local interfaces on
    a known port and listen for incoming connections. The client case will connect
    to the server on that known port and wait for user input on the console. The client
    expects input as a comma-separated list of three integers, one for each axis.
    On getting the input, the client constructs a struct of a given definition, serializes
    it using Serde, and sends the stream of bytes to the server. The server deserializes
    the stream into a struct of the same type. It then computes the distance and sends
    back the result, which the client then displays. The code is as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们就可以继续定义我们的代码。在这个例子中，服务器和客户端将在同一个二进制文件中。应用程序将接受一个标志，指示它应该作为服务器还是客户端运行。正如我们在上一章中做的那样，在服务器的情况下，我们将绑定到已知端口上的所有本地接口并监听传入的连接。客户端的情况将连接到该已知端口，并在控制台上等待用户输入。客户端期望输入为三个整数，每个轴一个，用逗号分隔。在获取输入后，客户端构建一个给定定义的
    struct，使用 Serde 进行序列化，并将字节流发送到服务器。服务器将流反序列化为相同类型的 struct。然后它计算距离并发送回结果，客户端随后显示该结果。代码如下：
- en: '[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We start with setting up Serde as we did in the last example. We then define
    our 3D point as a struct of three elements. In our main function, we handle CLI
    arguments and branch out to the client or the server, depending on what was passed.
    In both cases, we signal the end of transmission by sending a newline character.
    The client reads a line from `stdin`, cleans it, and creates an instance of the
    struct in a loop. In both cases, we wrap our streams in a `BufReader` for easier
    handling. We run our code using Cargo. An example session on the server is as
    follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先设置 Serde，就像在上一例子中做的那样。然后我们定义我们的 3D 点为一个包含三个元素的 struct。在我们的主函数中，我们处理 CLI
    参数，并根据传递的内容分支到客户端或服务器。在两种情况下，我们都通过发送换行符来表示传输结束。客户端从 `stdin` 读取一行，清理它，并在循环中创建 struct
    的实例。在两种情况下，我们都用 `BufReader` 包装我们的流，以便更容易处理。我们使用 Cargo 运行我们的代码。服务器上的一个示例会话如下：
- en: '[PRE7]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And, on the client side, we see the following interaction with the server.
    As expected, the client reads input, serializes that, and sends it to the server.
    It then waits for a response and, when it gets one, prints the result to standard
    output:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端方面，我们看到以下与服务器交互。正如预期的那样，客户端读取输入，对其进行序列化，并将其发送到服务器。然后它等待响应，并在收到响应时将结果打印到标准输出：
- en: '[PRE8]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Custom serialization and deserialization
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义序列化和反序列化
- en: As we saw before, Serde provides built-in serialization and deserialization
    for all primitive data types, and a number of complex data types, via macros.
    In some cases, however, Serde might fail to auto-implement. This might happen
    for more complex data types. In those cases, you will need to implement these
    manually. These cases demonstrate advanced usage of Serde, which also allows renaming
    fields in the output. For everyday usage, using these advanced feature is almost
    never necessary. These might be more common for networking, to handle a new protocol,
    and more.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所见，Serde 通过宏为所有原始数据类型以及许多复杂数据类型提供了内置的序列化和反序列化功能。然而，在某些情况下，Serde 可能无法自动实现。这可能发生在更复杂的数据类型上。在这些情况下，您将需要手动实现这些。这些示例展示了
    Serde 的高级用法，这也允许在输出中重命名字段。对于日常使用，几乎从不必要使用这些高级功能。这些可能更常见于网络通信，处理新的协议等情况。
- en: 'Let''s say we have a struct of three fields. We will just assume that Serde
    fails to implement `Serialize` and `Deserialize` on this, and so we will need
    to implement those manually. We initialize our project using Cargo:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含三个字段的结构体。我们将假设 Serde 无法实现 `Serialize` 和 `Deserialize`，因此我们需要手动实现这些。我们使用
    Cargo 初始化我们的项目：
- en: '[PRE9]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then declare our dependencies; the resulting Cargo config file should look
    like this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们声明我们的依赖项；生成的 Cargo 配置文件应如下所示：
- en: '[PRE10]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Our struct looks like this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结构体看起来像这样：
- en: '[PRE11]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We need to derive `Debug` and `PartialEq` for Serde to use internally. In the
    real world, it might be necessary to manually implement those as well. Now, we
    will need to implement the `Serialize` trait for kubeconfig. This trait looks
    like this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为 Serde 使用内部功能而派生 `Debug` 和 `PartialEq`。在现实世界中，可能还需要手动实现这些。现在，我们需要为 kubeconfig
    实现 `Serialize` 特征。这个特征看起来像这样：
- en: '[PRE12]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The basic workflow for serializing our struct will simply be to serialize the
    struct name, then each of the elements, and then signal the end of serialization,
    in that order. Serde has built-in methods to serialize that can work with all
    basic types, therefore an implementation does not need to worry about handling
    built-in types. Let''s look at how we can serialize our struct:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 序列化我们的结构体的基本工作流程将是简单地序列化结构体名称，然后是每个元素，然后按此顺序发出序列化结束的信号。Serde 内置了可以与所有基本类型一起工作的序列化方法，因此实现不需要担心处理内置类型。让我们看看我们如何序列化我们的结构体：
- en: '[PRE13]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Serialization of a struct will always begin with a call of `serialize_struct`
    with the struct name and number of fields as parameter (there are similarly named
    methods for other types). We then serialize each field in the order they appear
    while passing a key name that will be used in the resultant json. Once done, we
    call the special `end` method as a signal.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 结构体的序列化将始终以调用 `serialize_struct` 方法开始，该方法以结构体名称和字段数量作为参数（对于其他类型也有类似命名的函数）。然后，我们按顺序序列化每个字段，同时传递一个将在结果
    json 中使用的键名。一旦完成，我们调用特殊的 `end` 方法作为信号。
- en: 'Implementing deserialization is a bit more involved, with a bit of boilerplate
    code. The related trait looks like this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实现反序列化稍微复杂一些，有一些样板代码。相关的特征看起来像这样：
- en: '[PRE14]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Implementing this for a type requires implementing the visitor pattern. Serde
    defines a special `Visitor` trait, as shown in the following sample. Note that
    this has `visit_*` methods for all built-in types, those are not shown here. Also,
    in the following sample, we use the symbol `...` to indicate that there are more
    methods here that are not important for our discussion.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类型的实现，需要实现访问者模式。Serde 定义了一个特殊的 `Visitor` 特征，如下所示样本所示。请注意，这为所有内置类型提供了 `visit_*`
    方法，但在此处未显示。此外，在以下样本中，我们使用符号 `...` 来表示还有更多方法，这些方法对于我们的讨论并不重要。
- en: '[PRE15]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'An implementation of this trait is used internally by the deserializer to construct
    the resultant type. In our case, it will look like this:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特征的实现被反序列化器内部使用，以构建结果类型。在我们的情况下，它将如下所示：
- en: '[PRE16]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, the input to the deserializer is json, which can be treated as a map.
    Thus, we will only need to implement `visit_map` from the `Visitor` trait. If
    any non-json data is passed to our deserializer, it will error out on a call to
    some other function from that trait. Most of the previous implementation is boilerplate.
    It boils down to a few parts: implementing `Visitor` for the fields, and implementing
    `visit_str` (since all of our fields are strings). At this point, we should be
    able to deserialize individual fields. The second part is to implement `Visitor`
    for the overall struct, and to implement `visit_map`. Errors must be handled appropriately
    in all cases. In the end, we can call `deserializer.deserialize_struct` and pass
    the name of the struct, the list of fields, and the visitor implementation for
    the whole struct.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，反序列化器的输入是 json，它可以被视为一个映射。因此，我们只需要实现 `Visitor` 特征中的 `visit_map`。如果将任何非 json
    数据传递给我们的反序列化器，它将在调用该特征中的某些其他函数时出错。大部分之前的实现都是样板代码。它归结为几个部分：为字段实现 `Visitor`，以及实现
    `visit_str`（因为我们的所有字段都是字符串）。在这个阶段，我们应该能够反序列化单个字段。第二部分是实现整体结构的 `Visitor`，并实现 `visit_map`。在所有情况下都必须适当地处理错误。最后，我们可以调用
    `deserializer.deserialize_struct` 并传递结构体的名称、字段列表以及整个结构的访问者实现。
- en: 'This implementation will look like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现将看起来像这样：
- en: '[PRE17]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Serde also provides a crate that can be used to unit test custom serializers
    and deserializers using a token-stream-like interface. To use it, we will need
    to add `serde_test` to our `Cargo.toml` and declare it as an extern crate in our
    main file. Here is a test for our deserializer:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Serde 还提供了一个可以用于使用类似令牌流界面的接口来对自定义序列化器和反序列化器进行单元测试的 crate。要使用它，我们需要将 `serde_test`
    添加到我们的 `Cargo.toml` 中，并在我们的主文件中将其声明为外部 crate。以下是我们反序列化器的测试用例：
- en: '[PRE18]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `assert_de_tokens` call checks if the given stream of tokens deserializes
    to our struct or not, thereby testing our deserializer. We can also add a main
    function to drive the serializer, like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`assert_de_tokens` 调用检查给定的令牌流是否反序列化为我们的结构体，从而测试我们的反序列化器。我们还可以添加一个主函数来驱动序列化器，如下所示：'
- en: '[PRE19]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'All this can now be run using Cargo. Using `cargo test` should run the test
    we just wrote, which should pass. `cargo run` should run the main function and
    print the serialized json:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些现在都可以使用 Cargo 运行。使用 `cargo test` 应该运行我们刚刚编写的测试，它应该通过。`cargo run` 应该运行主函数并打印序列化的
    json：
- en: '[PRE20]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parsing textual data
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析文本数据
- en: Data parsing is a problem closely related to that of deserialization. The most
    common way of thinking about parsing is to start with a formal grammar and construct
    parsers based on that. This results in a bottom-up parser where smaller rules
    parse smaller components of the whole input. A final combinatorial rule combines
    all smaller rules in a given order to form the final parser. This way of formally
    defining a finite set of rules is called a **Parsing Expression Grammar** (**PEG**).
    This ensures that parsing is unambiguous; that there is only one valid parse tree
    if parsing succeeds. In the Rust ecosystem, there are a few distinct ways of implementing
    PEGs, and each of those have their own strengths and weaknesses. The first way
    is using macros to define a domain-specific language for parsing.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据解析是与反序列化密切相关的问题。思考解析的最常见方式是从形式语法开始，并基于此构建解析器。这导致了一个自下而上的解析器，其中较小的规则解析整个输入的较小组件。一个最终的组合规则按照给定的顺序将所有较小的规则组合起来形成最终的解析器。这种形式定义有限规则集的方式称为
    **解析表达式语法**（**PEG**）。这确保了解析是无歧义的；如果解析成功，则只有一个有效的解析树。在 Rust 生态系统中，有几种不同的方法可以实现
    PEG，每种方法都有其自身的优点和缺点。第一种方法是使用宏来定义解析的领域特定语言。
- en: This method integrates well with the compiler through the new macro system,
    and can produce fast code. However, this is often harder to debug and maintain.
    Since this method does not allow overloading operators, the implementation must
    define a DSL, which might be more of a cognitive load for a learner. The second
    method is using the trait system. This method helps in defining custom operators
    and is often easier to debug and maintain. An example of a parser that uses the
    first approach is nom; examples of parsers using the second approach are pom and
    pest.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法通过新的宏系统很好地与编译器集成，并可以生成快速代码。然而，这通常更难调试和维护。由于此方法不允许重载运算符，实现必须定义一个 DSL，这可能会给学习者带来更大的认知负担。第二种方法是使用特征系统。这种方法有助于定义自定义运算符，并且通常更容易调试和维护。使用第一种方法的解析器示例是
    nom；使用第二种方法的解析器示例是 pom 和 pest。
- en: Our use case for parsing is mostly in the context of networking applications.
    In these cases, sometimes it is more useful to deal with raw strings (or byte
    streams) and parse required information instead of deserializing to a complex
    data structure. A common case for this is any text-based protocol, such as HTTP.
    A server might receive a raw request as a stream of bytes over a socket and parse
    it to extract information. In this section, we will study some common parsing
    techniques in the Rust ecosystem.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解析的使用场景主要在网络应用程序的上下文中。在这些情况下，有时处理原始字符串（或字节流）并解析所需信息比反序列化到复杂的数据结构更有用。这种情况的一个常见例子是任何基于文本的协议，如HTTP。服务器可能会通过套接字接收一个原始请求作为字节流，并将其解析以提取信息。在本节中，我们将研究Rust生态系统中的常见解析技术。
- en: 'Now, nom is a parser combinator framework, which means it can combine smaller
    parsers to build more powerful parsers. This is a bottom-up approach that usually
    starts with writing very specific parsers that parse a well-defined thing from
    the input. The framework then provides methods to chain these small parsers into
    a complete one. This approach is in contrast to a top-down approach in the case
    of lex and yacc, where one would start with defining the grammar. It can handle
    both byte streams (binary data) or strings, and provides all of Rust''s usual
    guarantees. Let us start with parsing a simple string, which in this case is an
    HTTP GET or POST request. Like all cargo projects, we will first set up the structure:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，nom是一个解析器组合框架，这意味着它可以组合较小的解析器来构建更强大的解析器。这是一个自下而上的方法，通常从编写非常具体的解析器开始，这些解析器从输入中解析一个定义良好的东西。然后框架提供方法将这些小型解析器链接成一个完整的解析器。这种方法与lex和yacc的情况中的自顶向下方法形成对比，在那里人们会从定义语法开始。它可以处理字节流（二进制数据）或字符串，并提供Rust的所有常用保证。让我们从解析一个简单的字符串开始，在这个例子中是一个HTTP
    GET或POST请求。像所有cargo项目一样，我们首先设置结构：
- en: '[PRE21]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then we will add our dependencies (nom in this case). The resultant manifest
    should look like this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将添加我们的依赖项（在这个例子中是nom）。生成的清单应该看起来像这样：
- en: '[PRE22]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The crate provides a few extra features that are often useful for debugging;
    these are disabled by default and can be turned on by passing the list to the
    `features` flag, as shown in the preceding sample. Now, let''s move on to our
    main file:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该包提供了一些额外的功能，这些功能在调试时通常很有用；默认情况下这些功能是禁用的，可以通过传递列表到`features`标志来打开，如前例所示。现在，让我们继续到我们的主文件：
- en: '[PRE23]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As might be obvious, nom makes heavy use of macros for code generation, the
    most important one being `named!`, which takes in a function signature and defines
    a parser based on that. A nom parser returns an instance of the `IResult` type;
    this is defined as an enum and has three variants:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如可能显而易见，nom在代码生成中大量使用宏，其中最重要的一个是`named!`，它接受一个函数签名并根据该签名定义一个解析器。nom解析器返回`IResult`类型的实例；这被定义为枚举，并具有三个变体：
- en: The `Done(rest, value)` variant represents the case where the current parser
    was successful. In this case, the value will have the current parsed value and
    the rest will have the remaining input to be parsed.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Done(rest, value)`变体表示当前解析器成功的情形。在这种情况下，值将包含当前解析的值，而剩余的将包含需要解析的剩余输入。'
- en: The `Error(Err<E>)` variant represents an error during parsing. The underlying
    error will have the error code, position in error, and more. In a large parse
    tree, this can also hold pointers to more errors.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Error(Err<E>)`变体表示解析过程中的错误。底层错误将包含错误代码、错误位置以及更多信息。在一个大的解析树中，这也可以包含指向更多错误的指针。'
- en: The last variant, `Incomplete(needed)`, represents the case where parsing was
    incomplete for some reason. Needed is an enum that again has two variants; the
    first one represents the case where it is not known how much data is needed. The
    second one represents the exact size of data needed.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个变体`Incomplete(needed)`表示由于某种原因解析不完整的情况。需要的枚举再次有两个变体；第一个变体表示不知道需要多少数据。第二个变体表示所需数据的精确大小。
- en: We start with representations for HTTP methods and the full request as structs.
    In our toy example, we will only deal with GET and POST, and ignore everything
    else. We then define a parser for the HTTP method; our parser will take in a slice
    of bytes and return the `Method` enum. This is simply done by reading the input
    and looking for the strings GET or POST. In each case, the base parser is constructed
    using the `tag!` macro, which parses input to extract the given string. And, if
    the parsing was successful, we convert the result to `Method` using the `map!`
    macro, which maps the result of a parser to a function. Now, for parsing the method,
    we will either have a POST or a GET, but never both. We use the `alt!` macro to
    express the logical OR of both the parsers we constructed before. The `alt!` macro
    will construct a parser that parses the input if any one of it's constituent macros
    can parse the given input. Finally, all this is wrapped in the `return_error!`
    macro, which returns early if parsing fails in the current parser, instead of
    passing onto the next parser in the tree.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从HTTP方法的表示和完整的请求作为结构体开始。在我们的玩具示例中，我们只处理GET和POST方法，忽略其他所有内容。然后我们定义了一个用于HTTP方法的解析器；我们的解析器将接受一个字节数组切片并返回`Method`枚举。这很简单，只需读取输入并查找字符串GET或POST。在每种情况下，基础解析器都是使用`tag!`宏构建的，该宏解析输入以提取给定的字符串。如果解析成功，我们使用`map!`宏将结果转换为`Method`，该宏将解析器的结果映射到函数。现在，对于解析方法，我们可能有一个POST或一个GET，但不可能两者都有。我们使用`alt!`宏来表示之前构建的两个解析器的逻辑或。`alt!`宏将构建一个解析器，如果其构成宏中的任何一个可以解析给定的输入，则解析输入。最后，所有这些都被`return_error!`宏包裹起来，如果在当前解析器中解析失败，则提前返回，而不是传递到树中的下一个解析器。
- en: We then move on to parsing the whole request by defining `parse_request`. We
    start with trimming extra whitespace from the input using the `ws!` macro. We
    then invoke the `do_parse!` macro that chains multiple sub-parsers. This one is
    different from other combinators because this allows storing results from intermediate
    parsers. This is useful in constructing instances of our structs while returning
    results. In `do_parse!`, we first call `parse_method` and store its result in
    a variable. Having removed the method from a request, we should encounter empty
    whitespace before we find the location of the object. This is handled by the `take_until!("
    ")` call, which consumes input till it finds an empty space. The result is converted
    to a `str` using `map_res!`. The next parser in the list is one that removes the
    sequence `HTTP/` using the `tag!` macro. Next, we parse the HTTP version by reading
    input till we see a `\r`, and map it back to a `str`. Once we are done with all
    the parsing, we construct a `Request` object and return it. Note the use of the
    `>>` symbol as a separator between parsers in the sequence.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续通过定义`parse_request`来解析整个请求。我们首先使用`ws!`宏从输入中去除额外的空白。然后我们调用`do_parse!`宏，该宏链接着多个子解析器。这个与其他组合器不同，因为它允许存储中间解析器的结果。这在构建结构体实例的同时返回结果时很有用。在`do_parse!`中，我们首先调用`parse_method`并将结果存储在一个变量中。从请求中移除方法后，我们应该在找到对象位置之前遇到空白的空格。这由`take_until!("
    ")`调用处理，它消耗输入直到找到空格。结果使用`map_res!`转换为`str`。列表中的下一个解析器是使用`tag!`宏移除序列`HTTP/`的解析器。接下来，我们通过读取输入直到看到`\r`来解析HTTP版本，并将其映射回`str`。一旦完成所有解析，我们就构建一个`Request`对象并返回它。注意在解析器序列中使用`>>`符号作为分隔符。
- en: 'We also define a helper function called `run_parser` to run our parsers in
    a given input and print the result. This function calls the parser and matches
    on the result to display either the resultant structure or error. We then define
    our main function with three HTTP requests, the first two being valid, and the
    last one being invalid since the method is wrong. On running this, the output
    is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个名为`run_parser`的辅助函数，用于在给定输入中运行我们的解析器并打印结果。该函数调用解析器并匹配结果以显示结果结构或错误。然后我们定义我们的主函数，包含三个HTTP请求，前两个是有效的，最后一个无效，因为方法错误。运行此函数后，输出如下：
- en: '[PRE24]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the first two cases, everything was parsed as expected and we got the result
    back. As expected, parsing failed in the last case with the custom error being
    returned.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两种情况下，所有内容都按预期解析，并返回了结果。正如预期的那样，在最后一种情况下解析失败，并返回了自定义错误。
- en: As we discussed before, a common problem with nom is debugging, since it is
    much harder to debug macros. Macros also encourage the use of specific DSLs (like
    using the `>>` separator), which some people might find difficult to work with.
    At the time of writing, some error messages from nom are not helpful enough in
    finding what is wrong with a given parser. These will definitely improve in the
    future, but in the meantime, nom provides a few helper macros for debugging.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，nom 的一个常见问题是调试，因为宏的调试要困难得多。宏还鼓励使用特定的 DSL（如使用 `>>` 分隔符），这可能会让一些人难以使用。在撰写本文时，nom
    的一些错误消息在查找给定解析器的问题时并不足够有帮助。这些将在未来肯定会有所改进，但在此期间，nom 提供了一些辅助宏来帮助调试。
- en: 'For instance, `dbg!` prints the result and the input if the underlying parser
    did not return a `Done`. The `dbg_dump!` macro is similar but also prints out
    a hex dump of the input buffer. In our experience, a few techniques can be used
    for debugging:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`dbg!` 在底层解析器没有返回 `Done` 时打印结果和输入。`dbg_dump!` 宏类似，但还会打印出输入缓冲区的十六进制转储。根据我们的经验，可以使用一些技术进行调试：
- en: 'Expanding the macro by passing compiler options to `rustc`. Cargo enables this
    using the following invocations: `cargo rustc -- -Z unstable-options --pretty=expanded`
    expands and pretty prints all macros in the given project. One might find it useful
    to expand the macros to trace execution and debug. A related command in Cargo,
    `rustc -- -Z trace-macros`, only expands the macros.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过向 `rustc` 传递编译器选项来扩展宏。Cargo 通过以下调用启用此功能：`cargo rustc -- -Z unstable-options
    --pretty=expanded` 将展开并格式化打印给定项目中的所有宏。有人可能会发现展开宏以跟踪执行和调试是有用的。Cargo 中相关的命令 `rustc
    -- -Z trace-macros` 仅展开宏。
- en: Running smaller parsers in isolation. Given a series of parsers and another
    one combining those, it might be easier to run each of the sub-parsers till one
    of those errors out. Then, one can go on to debug only the small parser that is
    failing. This is very useful in isolating faults.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立运行较小的解析器。给定一系列解析器和另一个组合这些解析器的解析器，运行每个子解析器直到其中一个出错可能更容易。然后，可以继续调试仅失败的较小解析器。这在隔离故障时非常有用。
- en: Using the provided debugging macros `dbg!` and `dbg_dump!`. These can be used
    like debugging print statements to trace execution.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用提供的调试宏 `dbg!` 和 `dbg_dump!`。这些可以像调试打印语句一样用于跟踪执行。
- en: '`pretty=expanded` is an unstable compiler option right now. Sometime in the
    future, it will be stabilized (or removed). In that case, one will not need to
    pass the `-Z unstable-options` flag to use it.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`pretty=expanded` 目前是一个不稳定的编译器选项。在未来某个时候，它将被稳定化（或删除）。在这种情况下，将不需要传递 `-Z unstable-options`
    标志来使用它。'
- en: 'Let us look at an example of another parser combinator called `pom`. As we
    discussed before, this one relies heavily on traits and operator-overloading to
    implement parser combinations. As the time of writing, the current version is
    1.1.0, and we will use that for our example project. Like always, the first step
    is to set up our project and add `pom` to our dependencies:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看另一个名为 `pom` 的解析器组合器的例子。正如我们之前讨论的，这个解析器组合器在很大程度上依赖于特性和运算符重载来实现解析器组合。在撰写本文时，当前版本是
    1.1.0，我们将使用这个版本作为我们的示例项目。像往常一样，第一步是设置我们的项目并将 `pom` 添加到我们的依赖项中：
- en: '[PRE25]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `Cargo.toml` file will look like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cargo.toml` 文件将看起来像这样：'
- en: '[PRE26]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In this example, we will parse an example HTTP request, like last time. This
    is how it will look:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将解析一个示例 HTTP 请求，就像上次一样。它看起来是这样的：
- en: '[PRE27]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We start with declaring our dependency on `pom`. In our main function, we define
    the final parser as a sequence of multiple sub-parsers. The `*` operator has been
    overloaded to make it apply multiple parsers in sequence. The `seq` operator is
    a built-in parser that matches the given string from input. The `|` operator does
    a logical OR of the two operands. We define a function called `space()` that represents
    empty white spaces in input. This function takes one of each empty whitespace
    characters, repeats it 0 or more times, and then discards it. Consequently, the
    function returns a `Parser` with no return type, indicated by `()`. The string
    function is similarly defined to be one of the characters in the English alphabet,
    repeated 0 or more times, and then converted to an `std::String`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先声明对`pom`的依赖。在我们的主函数中，我们将最终的解析器定义为多个子解析器的序列。`*`运算符被重载，以便按顺序应用多个解析器。`seq`运算符是一个内置解析器，用于匹配输入中的给定字符串。`|`运算符对两个操作数进行逻辑或操作。我们定义了一个名为`space()`的函数，它表示输入中的空格。此函数接受每种空格字符中的一个，重复0次或多次，然后丢弃它。因此，该函数返回一个没有返回类型的`Parser`，表示为`()`。字符串函数被定义为英语字母中的一个字符，重复0次或多次，然后转换为`std::String`。
- en: 'The return type of this function is a `Parser` that has a `String`, as expected.
    Having set those up, our main parser will have a space, followed by the symbol
    `/`, followed by a string, a symbol `/`, a space again, and ending with the sequence
    `HTTP/1.1`. And, as expected, when we parse an example string with the parser
    we wrote, it produces an `Ok`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的返回类型是一个`Parser`，它有一个`String`，正如预期的那样。设置好这些之后，我们的主解析器将有一个空格，后面跟着符号`/`，然后是一个字符串，一个符号`/`，再次是一个空格，并以序列`HTTP/1.1`结束。正如预期的那样，当我们用我们编写的解析器解析一个示例字符串时，它产生了一个`Ok`：
- en: '[PRE28]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: PEG-based parser combinators can be easier to debug and to work with. They also
    tend to produce better error messages, but, unfortunately, those are not mature
    enough right now. The community around them is not as large as the community around
    nom. Consequently, it is often easier to get help with nom issues. At the end
    of the day, it is up to the programmer to choose something that works for them.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 基于PEG的解析器组合器可能更容易调试和操作。它们还倾向于产生更好的错误消息，但不幸的是，目前它们还不够成熟。围绕它们的社区不如nom周围的社区大。因此，在nom问题上通常更容易获得帮助。最终，选择适合他们的解决方案取决于程序员。
- en: Parsing binary data
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析二进制数据
- en: 'A related problem is that of parsing binary data. Common cases where this is
    applicable include parsing binary files and binary protocols. Let us look at how
    `nom` can be used to parse binary data. In our toy example, we will write a parser
    for the IPv6 header. Our `Cargo.toml` will look exactly the same as last time.
    Set up the project using the CLI:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关的问题是解析二进制数据。这种情况下常见的例子包括解析二进制文件和二进制协议。让我们看看`nom`如何被用来解析二进制数据。在我们的玩具示例中，我们将编写一个解析IPv6头部的解析器。我们的`Cargo.toml`将和上次一样。使用CLI设置项目：
- en: '[PRE29]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Our main file will look like this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要文件将看起来像这样：
- en: '[PRE30]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, we start with declaring a struct for the IPv6 fixed header, as defined
    in RFC 2460 ([https://tools.ietf.org/html/rfc2460](https://tools.ietf.org/html/rfc2460)).
    We first define a helper function called `to_ipv6_address` that takes in a slice
    of `u8` and converts to an IPv6 address. To do that, we need another helper function
    that converts a slice to a fixed-size array (`16` in this case). Having set those
    up, we define a number of parsers for parsing each of the members of the struct
    using the `named!` macro.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先声明一个结构体，用于表示在RFC 2460中定义的IPv6固定头部（[https://tools.ietf.org/html/rfc2460](https://tools.ietf.org/html/rfc2460)）。我们首先定义一个辅助函数`to_ipv6_address`，它接受一个`u8`切片并将其转换为IPv6地址。为此，我们需要另一个辅助函数，该函数将切片转换为固定大小的数组（在这个例子中是`16`）。设置好这些之后，我们定义了多个解析器，使用`named!`宏来解析结构体成员的每个成员。
- en: The `parse_version` function takes in a slice of bytes and returns the version
    as a `u8`. This is done by reading 4 bits from the input as a `u8`, using the
    `take_bits!` macro. That is then wrapped in the `bits!` macro which transforms
    the input to a bit stream for the underlying parser. In the same way, we go on
    to define parsers for all the other fields in the header structure. For each one,
    we take the number of bits they occupy according to the RFC and convert to a type
    large enough to hold it. The last case of parsing the address is different. Here,
    we read 16 bytes using the `take!` macro and map it to the `to_ipv6_address` function
    to convert the byte stream, using the `map!` macro.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`parse_version` 函数接收一个字节数组切片并返回一个 `u8` 类型的版本。这是通过从输入中读取 4 位作为 `u8`，使用 `take_bits!`
    宏来完成的。然后它被 `bits!` 宏包装，该宏将输入转换为底层解析器的位流。以同样的方式，我们继续定义头部结构中所有其他字段的解析器。对于每一个，我们根据
    RFC 中它们占用的位数将它们转换为足够大的类型。解析地址的最后一种情况不同。在这里，我们使用 `take!` 宏读取 16 个字节并将它们映射到 `to_ipv6_address`
    函数，使用 `map!` 宏将字节流转换为地址。'
- en: At this point, all small pieces to parse the whole struct are ready, and we
    can define a function using the `do_parse!` macro. In there, we accumulate results
    in temporary variables and construct an instance of the `IPv6Header` struct, which
    is then returned. In our main function, we have an array of bytes that was taken
    from a IPv6 packet dump and should represent a valid IPv6 header. We parse that
    using the parser we defined and assert that the output matches what is expected.
    Thus, a successful run of our parser previously will not throw an exception.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有用于解析整个结构体的小片段都已准备就绪，我们可以使用 `do_parse!` 宏来定义一个函数。在那里，我们在临时变量中累积结果并构建一个
    `IPv6Header` 结构体的实例，然后返回。在我们的主函数中，有一个从 IPv6 数据包转储中取出的字节数组，它应该代表一个有效的 IPv6 头部。我们使用定义的解析器来解析它，并断言输出与预期相符。因此，我们解析器的成功运行之前不会抛出异常。
- en: 'Let us recap all the macros from `nom` that we used so far:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下到目前为止使用的所有 `nom` 宏：
- en: '| **Macro** | **Purpose** |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **宏** | **目的** |'
- en: '| `named!` | Creates a parsing function by combining smaller functions. This
    (or a variant) is always the top-level call in a chain. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `named!` | 通过组合较小的函数创建一个解析函数。这（或其变体）总是链中的顶级调用。|'
- en: '| `ws!` | Enables a parser to consume all whitespaces (`\t`, `\r` and `\n`)
    between tokens. |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `ws!` | 启用解析器消耗标记之间的所有空白字符（`\t`、`\r` 和 `\n`）。|'
- en: '| `do_parse!` | Applies subparsers in a given sequence, can store intermediate
    results. |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `do_parse!` | 以给定顺序应用子解析器，可以存储中间结果。|'
- en: '| `tag!` | Declares a static sequence of bytes that the enclosing parser should
    recognize. |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| `tag!` | 声明一个静态字节序列，封装的解析器应该识别。|'
- en: '| `take_until!` | Consumes input till the given tag. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| `take_until!` | 消耗输入直到给定的标记。|'
- en: '| `take_bits!` | Consumes the given number of bits from the input and casts
    them to the given type. |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| `take_bits!` | 从输入中消耗给定数量的位并将它们转换为给定的类型。|'
- en: '| `take!` | Consumes the specified number of bytes from input. |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| `take!` | 从输入中消耗指定数量的字节。|'
- en: '| `map_res!` | Maps a function (returning a result) on the output of a parser.
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `map_res!` | 将函数（返回结果）映射到解析器的输出。|'
- en: '| `map!` | Maps a function to the output of a parser. |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| `map!` | 将函数映射到解析器的输出。|'
- en: '| `bits!` | Transforms the given slice to a bit stream. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `bits!` | 将给定的切片转换为位流。|'
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this section, we studied handling data in more detail. Specifically, (de)serializing
    and parsing. At the time of writing, Serde and related crates are the community-supported
    way of (de)serializing data in Rust, while `nom` is the most frequently used parser
    combinator. These tools tend to produce better error messages on the nightly compiler,
    and with a few feature flags turned on, since they often depend on a few cutting
    edge night-only features. With time, these features will be available in the stable
    compiler, and these tools will work seamlessly.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们更详细地研究了处理数据的方法。具体来说，是序列化和解析。在撰写本文时，Serde 和相关 crate 是 Rust 社区支持的数据序列化和反序列化方式，而
    `nom` 是最常用的解析组合器。这些工具在 nightly 编译器上通常会生成更好的错误信息，并且当开启一些功能标志时，因为它们通常依赖于一些仅夜间的前沿特性。随着时间的推移，这些特性将可用于稳定编译器，并且这些工具将无缝工作。
- en: In the next chapter, we will talk about the next steps after having made sense
    of incoming data on a socket. More often than not, this involves dealing with
    application-level protocols.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论在套接字上理解传入数据之后的下一步。通常情况下，这涉及到处理应用层协议。

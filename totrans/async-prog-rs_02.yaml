- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How Programming Languages Model Asynchronous Program Flow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered asynchronous program flow, concurrency,
    and parallelism in general terms. In this chapter, we’ll narrow our scope. Specifically,
    we’ll look into different ways to model and deal with concurrency in programming
    languages and libraries.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to keep in mind that threads, futures, fibers, goroutines, promises,
    etc. are abstractions that give us a way to model an asynchronous program flow.
    They have different strengths and weaknesses, but they share a goal of giving
    programmers an easy-to-use (and importantly, hard to misuse), efficient, and expressive
    way of creating a program that handles tasks in a non-sequential, and often unpredictable,
    order.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The lack of precise definitions is prevalent here as well; many terms have a
    name that stems from a concrete implementation at some point in time but has later
    taken on a more general meaning that encompasses different implementations and
    varieties of the same thing.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: We’ll first go through a way of grouping different abstractions together based
    on their similarities before we go on to discuss the pros and cons of each of
    them. We’ll also go through important definitions that we’ll use throughout the
    book and discuss OS threads in quite some detail.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: The topics we discuss here are quite abstract and complicated so don’t feel
    bad if you don’t understand everything immediately. As we progress through the
    book and you get used to the different terms and techniques by working through
    some examples, more and more pieces will fall into place.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the following topics will be covered:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Definitions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threads provided by the operating system
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Green threads/stackfull coroutines/fibers
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Callback based approaches
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Promises, futures, and async/await
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Definitions
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can broadly categorize abstractions over concurrent operations into two
    groups:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '`async`/`await` in Rust and JavaScript.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Non-cooperative**: Tasks that don’t necessarily yield voluntarily. In such
    a system, the scheduler must be able to **pre-empt** a running task, meaning that
    the scheduler can stop the task and take control over the CPU even though the
    task would have been able to do work and progress. Examples of this are OS threads
    and Goroutines (after GO version 1.14).'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Non-cooperative vs. cooperative multitasking](img/B20892_Figure_02.1.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Non-cooperative vs. cooperative multitasking
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: In a system where the scheduler can pre-empt running tasks, tasks can also yield
    voluntarily as they do in a cooperative system, and it’s rare with a system that
    *only* relies on pre-emption.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'We can further divide these abstractions into two broad categories based on
    the characteristics of their implementation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '**Stackful**: Each task has its own call stack. This is often implemented as
    a stack that’s similar to the stack used by the operating system for its threads.
    Stackful tasks can suspend execution at any point in the program as the whole
    stack is preserved.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Stackless**: There is not a separate stack for each task; they all run sharing
    the same call stack. A task can’t be suspended in the middle of a stack frame,
    limiting the runtime’s ability to pre-empt the task. However, they need to store/restore
    less information when switching between tasks so they can be more efficient.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are more nuances to these two categories that you’ll get a deep understanding
    of when we implement an example of both a stackful coroutine (fiber) and a stackless
    coroutine (Rust futures generated by `async`/`await`) later in the book. For now,
    we keep the details to a minimum to just provide an overview.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We keep referring to threads all throughout this book, so before we get too
    far in, let’s stop and give “thread” a good definition since it’s one of those
    fundamental terms that causes a lot of confusion.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: In the most general sense, a thread refers to a **thread of execution**, meaning
    a set of instructions that need to be executed sequentially. If we tie this back
    to the first chapter of this book, where we provided several definitions under
    the Concurrency vs. Parallelism subsection, a thread of execution is similar to
    what we defined as a **task** with multiple steps that need resources to progress.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The generality of this definition can be a cause of some confusion. A thread
    to one person can obviously refer to an OS thread, and to another person, it can
    simply refer to any abstraction that represents a thread of execution on a system.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Threads are often divided into two broad categories:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '**OS threads**: These threads are created by the OS and managed by the OS scheduler.
    On Linux, this is known as a **kernel thread**.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User-level threads**: These threads are created and managed by us as programmers
    without the OS knowing about them.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, this is where things get a bit tricky: OS threads on most modern operating
    systems have a lot of similarities. Some of these similarities are dictated by
    the design of modern CPUs. One example of this is that most CPUs assume that there
    is a stack it can perform operations on and that it has a register for the stack
    pointer and instructions for stack manipulation.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: User-level threads can, in their broadest sense, refer to *any* implementation
    of a system (runtime) that creates and schedules tasks, and you can’t make the
    same assumptions as you do with OS threads. They can closely resemble OS threads
    by using separate stacks for each task, as we’ll see in [*Chapter 5*](B20892_05.xhtml#_idTextAnchor092)
    when we go through our fiber/green threads example, or they can be radically different
    in nature, as we’ll see when we go through how Rust models concurrent operations
    later on in Part 3 of this book.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: No matter the definition, a set of tasks needs something that manages them and
    decides who gets what resources to progress. The most obvious resource on a computer
    system that all tasks need to progress is CPU time. We call the “something” that
    decides who gets CPU time to progress a **scheduler**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Most likely, when someone refers to a “thread” without adding extra context,
    they refer to an OS thread/kernel thread, so that’s what we’ll do going forward.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: I’ll also keep referring to a thread of execution as simply a **task**. I find
    the topic of asynchronous programming easier to reason about when we limit the
    use of terms that have different assumptions associated with them depending on
    the context as much as possible.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: With that out of the way, let’s go through some defining characteristics of
    OS threads while we also highlight their limitations.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Important!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Definitions will vary depending on what book or article you read. For example,
    if you read about how a specific operating system works, you might see that processes
    or threads are abstractions that represent “tasks”, which will seem to contradict
    the definitions we use here. As I mentioned earlier, the choice of reference frame
    is important, and it’s why we take so much care to define the terms we use thoroughly
    as we encounter them throughout the book.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The definition of a thread can also vary by operating system, even though most
    popular systems share a similar definition today. Most notably, Solaris (pre-Solaris
    9, which was released in 2002) used to have a two-level thread system that differentiated
    between application threads, lightweight processes, and kernel threads. This was
    an implementation of what we call M:N threading, which we’ll get to know more
    about later in this book. Just beware that if you read older material, the definition
    of a thread in such a system might differ significantly from the one that’s commonly
    used today.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve gone through the most important definitions for this chapter,
    it’s time to talk more about the most popular ways of handling concurrency when
    programming.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Threads provided by the operating system
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: We call this 1:1 threading. Each task is assigned one OS thread.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Since this book will not focus specifically on OS threads as a way to handle
    concurrency going forward, we treat them more thoroughly here.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the obvious. To use threads provided by the operating system,
    you need, well, an operating system. Before we discuss the use of threads as a
    means to handle concurrency, we need to be clear about what kind of operating
    systems we’re talking about since they come in different flavors.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Embedded systems are more widespread now than ever before. This kind of hardware
    might not have the resources for an operating system, and if they do, you might
    use a radically different kind of operating system tailored to your needs, as
    the systems tend to be less general purpose and more specialized in nature.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Their support for threads, and the characteristics of how they schedule them,
    might be different from what you’re used to in operating systems such as Windows
    or Linux.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Since covering all the different designs is a book on its own, we’ll limit the
    scope to talk about treads, as they’re used in Windows and Linux-based systems
    running on popular desktop and server CPUs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: OS threads are simple to implement and simple to use. We simply let the OS take
    care of everything for us. We do this by spawning a new OS thread for each task
    we want to accomplish and write code as we normally would.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: The runtime we use to handle concurrency for us is the operating system itself.
    In addition to these advantages, you get parallelism for free. However, there
    are also some drawbacks and complexities resulting from directly managing parallelism
    and shared resources.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Creating new threads takes time
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating a new OS thread involves some bookkeeping and initialization overhead,
    so while switching between two existing threads in the same process is pretty
    fast, creating new ones and discarding ones you don’t use anymore involves work
    that takes time. All the extra work will limit throughput if a system needs to
    create and discard a lot of them. This can be a problem if you have huge amounts
    of small tasks that need to be handled concurrently, which often is the case when
    dealing with a lot of I/O.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Each thread has its own stack
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll cover stacks in detail later in this book, but for now, it’s enough to
    know that they occupy a fixed size of memory. Each OS thread comes with its own
    stack, and even though many systems allow this size to be configured, they’re
    still fixed in size and can’t grow or shrink. They are, after all, the cause of
    stack overflows, which will be a problem if you configure them to be too small
    for the tasks you’re running.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: If we have many small tasks that only require a little stack space but we reserve
    much more than we need, we will occupy large amounts of memory and possibly run
    out of it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Context switching
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you now know, threads and schedulers are tightly connected. Context switching
    happens when the CPU stops executing one thread and proceeds with another one.
    Even though this process is highly optimized, it still involves storing and restoring
    the register state, which takes time. Every time that you yield to the OS scheduler,
    it can choose to schedule a thread from a different process on that CPU.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: You see, threads created by these systems belong to a **process**. When you
    start a program, it starts a process, and the process creates at least one initial
    thread where it executes the program you’ve written. Each process can spawn multiple
    threads that share the same **address space**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: That means that threads within the same process can access shared memory and
    can access the same resources, such as files and file handles. One consequence
    of this is that when the OS switches contexts by stopping one thread and resuming
    another within the same process, it doesn’t have to save and restore all the state
    associated with that process, just the state that’s specific to that thread.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着同一进程内的线程可以访问共享内存，并且可以访问相同的资源，例如文件和文件句柄。这一结果之一是，当操作系统通过停止同一进程中的一个线程并恢复另一个线程来切换上下文时，它不需要保存和恢复与该进程相关的所有状态，只需保存和恢复特定于该线程的状态。
- en: On the other hand, when the OS switches from a thread associated with one process
    to a thread associated with another, the new process will use a different address
    space, and the OS needs to take measures to make sure that process “A” doesn’t
    access data or resources that belong to process “B”. If it didn’t, the system
    wouldn’t be secure.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当操作系统从一个与一个进程关联的线程切换到与另一个进程关联的线程时，新进程将使用不同的地址空间，操作系统需要采取措施确保进程“A”不会访问属于进程“B”的数据或资源。如果不是这样，系统将不会安全。
- en: The consequence is that caches might need to be flushed and more state might
    need to be saved and restored. In a highly concurrent system under load, these
    context switches can take extra time and thereby limit the throughput in a somewhat
    unpredictable manner if they happen frequently enough.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，可能需要刷新缓存，并且可能需要保存和恢复更多的状态。在高并发系统负载下，这些上下文切换可能会花费额外的时间，如果它们频繁发生，可能会以某种不可预测的方式限制吞吐量。
- en: Scheduling
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度
- en: The OS can schedule tasks differently than you might expect, and *every time
    you yield to the OS*, you’re put in the same queue as all other threads and processes
    on the system.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统可以以你意想不到的方式调度任务，并且每次你向操作系统让步时，你都会被放入与系统上所有其他线程和进程相同的队列中。
- en: Moreover, since there is no guarantee that the thread will resume execution
    on the same CPU core as it left off or that two tasks won’t run in parallel and
    try to access the same data, you need to synchronize data access to prevent data
    races and other pitfalls associated with multicore programming.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于无法保证线程将在离开时相同的CPU核心上恢复执行，或者两个任务不会并行运行并尝试访问相同的数据，因此你需要同步数据访问以防止数据竞争和其他与多核编程相关的陷阱。
- en: Rust as a language will help you prevent many of these pitfalls, but synchronizing
    data access will require extra work and add to the complexity of such programs.
    We often say that using OS threads to handle concurrency gives us parallelism
    for free, but it isn’t free in terms of added complexity and the need for proper
    data access synchronization.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Rust作为一种语言将帮助你防止许多这些陷阱，但同步数据访问将需要额外的工作，并增加此类程序复杂性。我们经常说，使用操作系统线程处理并发给我们带来了免费的可并行性，但从增加的复杂性和需要适当的数据访问同步的角度来看，这并不是免费的。
- en: The advantage of decoupling asynchronous operations from OS threads
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将异步操作与操作系统线程解耦的优势
- en: Decoupling asynchronous operations from the concept of threads has a lot of
    benefits.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 将异步操作与线程概念解耦有很多好处。
- en: First of all, using OS threads as a means to handle concurrency requires us
    to use what essentially is an OS abstraction to represent our tasks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用操作系统线程作为处理并发的手段要求我们使用本质上是一种操作系统抽象来表示我们的任务。
- en: Having a separate layer of abstraction to represent concurrent tasks gives us
    the freedom to choose how we want to handle concurrent operations. If we create
    an abstraction over concurrent operations such as a future in Rust, a promise
    in JavaScript, or a goroutine in GO, it is up to the runtime implementor to decide
    how these concurrent tasks are handled.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个单独的抽象层来表示并发任务，这给了我们选择如何处理并发操作的自由。如果我们创建了一个表示并发操作（如Rust中的future、JavaScript中的promise或GO中的goroutine）的抽象，那么这些并发任务的处理方式将由运行时实现者来决定。
- en: A runtime could simply map each concurrent operation to an OS thread, they could
    use fibers/green threads or state machines to represent the tasks. The programmer
    that writes the asynchronous code will not necessarily have to change anything
    in their code if the underlying implementation changes. In theory, the same asynchronous
    code could be used to handle concurrent operations on a microcontroller without
    an OS if there’s just a runtime for it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时可以将每个并发操作映射到一个操作系统线程，它们可以使用纤程/绿色线程或状态机来表示任务。编写异步代码的程序员在底层实现发生变化时，不一定需要在他们的代码中进行任何更改。理论上，如果只是有一个运行时，相同的异步代码就可以用来处理没有操作系统的情况下在微控制器上的并发操作。
- en: 'To sum it up, using threads provided by the operating system to handle concurrency
    has the following advantages:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Simple to understand
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to use
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Switching between tasks is reasonably fast
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You get parallelism for free
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, they also have a few drawbacks:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: OS-level threads come with a rather large stack. If you have many tasks waiting
    simultaneously (as you would in a web server under heavy load), you’ll run out
    of memory pretty fast.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context switching can be costly and you might get an unpredictable performance
    since you let the OS do all the scheduling.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OS has many things it needs to handle. It might not switch back to your
    thread as fast as you’d wish.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is tightly coupled to an OS abstraction. This might not be an option on some
    systems.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we’ll not spend more time talking about OS threads in this book, we’ll
    go through a short example so you can see how they’re used:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: ch02/aa-os-threads
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, we simply spawn several OS threads and put them to sleep. Sleeping
    is essentially the same as yielding to the OS scheduler with a request to be re-scheduled
    to run after a certain time has passed. To make sure our main thread doesn’t finish
    and exit (which will exit the process) before our children thread has had time
    to run we `join` them at the end of our `main` function.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the example, we’ll see how the operations occur in a different order
    based on how long we yielded each thread to the scheduler:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So, while using OS threads is great for a number of tasks, we also outlined
    a number of good reasons to look at alternatives by discussing their limitations
    and downsides. The first alternatives we’ll look at are what we call fibers and
    green threads.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Fibers and green threads
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: This is an example of **M:N threading**. Many tasks can run concurrently on
    one OS thread. Fibers and green threads are often referred to as stackful coroutines.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: The name “green threads” originally stems from an early implementation of an
    M:N threading model used in Java and has since been associated with different
    implementations of M:N threading. You will encounter different variations of this
    term, such as “green processes” (used in Erlang), which are different from the
    ones we discuss here. You’ll also see some that define green threads more broadly
    than we do here.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: The way we define green threads in this book makes them synonymous with fibers,
    so both terms refer to the same thing going forward.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of fibers and green threads implies that there is a runtime
    with a scheduler that’s responsible for scheduling what task (M) gets time to
    run on the OS thread (N). There are many more tasks than there are OS threads,
    and such a system can run perfectly fine using only one OS thread. The latter
    case is often referred to as **M:1 threading**.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Goroutines is an example of a specific implementation of stackfull coroutines,
    but it comes with slight nuances. The term “coroutine” usually implies that they’re
    cooperative in nature, but Goroutines can be pre-empted by the scheduler (at least
    since version 1.14), thereby landing them in somewhat of a grey area using the
    categories we present here.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Green threads and fibers use the same mechanisms as an OS, setting up a stack
    for each task, saving the CPU’s state, and jumping from one task(thread) to another
    by doing a context switch.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: We yield control to the scheduler (which is a central part of the runtime in
    such a system), which then continues running a different task.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: The state of execution is stored in each stack, so in such a solution, there
    would be no need for `async`, `await`, `Future`, or `Pin`. In many ways, green
    threads mimic how an operating system facilitates concurrency, and implementing
    them is a great learning experience.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: A runtime using fibers/green threads for concurrent tasks can have a high degree
    of flexibility. Tasks can, for example, be pre-empted and context switched at
    any time and at any point in their execution, so a long-running task that hogs
    the CPU could in theory be pre-empted by the runtime, acting as a safeguard from
    having tasks that end up blocking the whole system due to an edge-case or a programmer
    error.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: This gives the runtime scheduler almost the same capabilities as the OS scheduler,
    which is one of the biggest advantages of systems using fibers/green threads.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical flow goes as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: You run some non-blocking code
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You make a blocking call to some external resource
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CPU jumps to the main thread, which schedules a different thread to run
    and jumps to that stack
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You run some non-blocking code on the new thread until a new blocking call or
    the task is finished
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CPU jumps back to the main thread, schedules a new thread that is ready
    to make progress, and jumps to that thread
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Program flow using fibers/green threads](img/B20892_Figure_02.2.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Program flow using fibers/green threads
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Each stack has a fixed space
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As fibers and green threads are similar to OS threads, they do have some of
    the same drawbacks as well. Each task is set up with a stack of a fixed size,
    so you still have to reserve more space than you actually use. However, these
    stacks can be growable, meaning that once the stack is full, the runtime can grow
    the stack. While this sounds easy, it’s a rather complicated problem to solve.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'We can’t simply grow a stack as we grow a tree. What actually needs to happen
    is one of two things:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: You allocate a new piece of continuous memory and handle the fact that your
    stack is spread over two disjointed memory segments
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You allocate a new larger stack (for example, twice the size of the previous
    stack), move all your data over to the new stack, and continue from there
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first solution sounds pretty simple, as you can leave the original stack
    as it is, and you can basically context switch over to the new stack when needed
    and continue from there. However, modern CPUs can work extremely fast if they
    can work on a contiguous piece of memory due to caching and their ability to predict
    what data your next instructions are going to work on. Spreading the stack over
    two disjointed pieces of memory will hinder performance. This is especially noticeable
    when you have a loop that happens to be just at the stack boundary, so you end
    up making up to two context switches for each iteration of the loop.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个解决方案听起来相当简单，因为你可以将原始栈保持原样，并在需要时基本切换到新栈，并从那里继续。然而，由于缓存和它们预测你接下来要处理的数据的能力，现代CPU可以在连续的内存块上工作得非常快。将栈分散到两块不连续的内存中将会阻碍性能。这在你有一个恰好位于栈边界处的循环时尤为明显，因此你可能会为循环的每次迭代进行多达两次的上下文切换。
- en: The second solution solves the problems with the first solution by having the
    stack as a contiguous piece of memory, but it comes with some problems as well.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个解决方案通过将栈作为连续的内存块来解决第一个解决方案的问题，但它也带来了一些问题。
- en: 'First, you need to allocate a new stack and move all the data over to the new
    stack. But what happens with all pointers and references that point to something
    located on the stack when everything moves to a new location? You guessed it:
    every pointer and reference to anything located on the stack needs to be updated
    so they point to the new location. This is complex and time-consuming, but if
    your runtime already includes a garbage collector, you already have the overhead
    of keeping track of all your pointers and references anyway, so it might be less
    of a problem than it would for a non-garbage collected program. However, it does
    require a great deal of integration between the garbage collector and the runtime
    to do this every time the stack grows, so implementing this kind of runtime can
    get very complicated.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要分配一个新的栈并将所有数据移动到新栈上。但是，当一切移动到新位置时，所有指向栈上位置的指针和引用怎么办？你已经猜到了：指向栈上任何内容的指针和引用都需要更新，以便它们指向新位置。这是复杂且耗时的，但如果你的运行时已经包括垃圾回收器，你已经在跟踪所有指针和引用方面有了开销，所以这可能比非垃圾回收程序的问题要小。然而，它确实需要在垃圾回收器和运行时之间进行大量的集成，以便每次栈增长时都执行此操作，因此实现这种运行时可能会变得非常复杂。
- en: Secondly, you have to consider what happens if you have a lot of long-running
    tasks that only require a lot of stack space for a brief period of time (for example,
    if it involves a lot of recursion at the start of the task) but are mostly I/O
    bound the rest of the time. You end up growing your stack many times over only
    for one specific part of that task, and you have to make a decision whether you
    will accept that the task occupies more space than it needs or at some point move
    it back to a smaller stack. The impact this will have on your program will of
    course vary greatly based on the type of work you do, but it’s still something
    to be aware of.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，你必须考虑如果你有很多长时间运行的任务，这些任务在短时间内只需要大量的栈空间（例如，如果任务开始时涉及大量的递归）但大部分时间都是I/O密集型的情况。你最终会只为任务的一个特定部分增长栈多次，你必须决定你是否会接受任务占用比它需要的更多空间，或者在某些时候将其移回较小的栈。这种影响当然会根据你所做的工作类型而有很大差异，但这仍然是一件需要注意的事情。
- en: Context switching
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文切换
- en: Even though these fibers/green threads are lightweight compared to OS threads,
    you still have to save and restore registers at every context switch. This likely
    won’t be a problem most of the time, but when compared to alternatives that don’t
    require context switching, it can be less efficient.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这些纤程/绿色线程与操作系统线程相比很轻量级，你仍然需要在每次上下文切换时保存和恢复寄存器。这很可能不会成为问题，但与不需要上下文切换的替代方案相比，它可能效率较低。
- en: Context switching can also be pretty complex to get right, especially if you
    intend to support many different platforms.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文切换也可能非常复杂，特别是如果你打算支持许多不同的平台。
- en: Scheduling
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度
- en: When a fiber/green thread yields to the runtime scheduler, the scheduler can
    simply resume execution on a new task that’s ready to run. This means that you
    avoid the problem of being put in the same run queue as every other task in the
    system every time you yield to the scheduler. From the OS perspective, your threads
    are busy doing work all the time, so it will try to avoid pre-empting them if
    it can.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: One unexpected downside of this is that most OS schedulers make sure all threads
    get some time to run by giving each OS thread a time slice where it can run before
    the OS pre-empts the thread and schedules a new thread on that CPU. A program
    using many OS threads might be allotted more time slices than a program with fewer
    OS threads. A program using M:N threading will most likely only use a few OS threads
    (one thread per CPU core seems to be the starting point on most systems). So,
    depending on whatever else is running on the system, your program might be allotted
    fewer time slices in total than it would be using many OS threads. However, with
    the number of cores available on most modern CPUs and the typical workload on
    concurrent systems, the impact from this should be minimal.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: FFI
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since you create your own stacks that are supposed to grow/shrink under certain
    conditions and might have a scheduler that assumes it can pre-empt running tasks
    at any point, you will have to take extra measures when you use FFI. Most FFI
    functions will assume a normal OS-provided C-stack, so it will most likely be
    problematic to call an FFI function from a fiber/green thread. You need to notify
    the runtime scheduler, context switch to a different OS thread, and have some
    way of notifying the scheduler that you’re done and the fiber/green thread can
    continue. This naturally creates overhead and added complexity both for the runtime
    implementor and the user making the FFI call.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Advantages
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is simple to use for the user. The code will look like it does when using
    OS threads.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context switching is reasonably fast.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abundant memory usage is less of a problem when compared to OS threads.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are in full control over how tasks are scheduled and if you want you can
    prioritize them as you see fit.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s easy to incorporate pre-emption, which can be a powerful feature.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stacks need a way to grow when they run out of space creating additional work
    and complexity
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You still need to save the CPU state on every context switch
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s complicated to implement correctly if you intend to support many platforms
    and/or CPU architectures
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FFI can have a lot of overhead and add unexpected complexity
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Callback based approaches
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: This is another example of M:N threading. Many tasks can run concurrently on
    one OS thread. Each task consists of a chain of callbacks.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: You probably already know what we’re going to talk about in the next paragraphs
    from JavaScript, which I assume most know.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: The whole idea behind a callback-based approach is to save a pointer to a set
    of instructions we want to run later together with whatever state is needed. In
    Rust, this would be a closure.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Implementing callbacks is relatively easy in most languages. They don’t require
    any context switching or pre-allocated memory for each task.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: However, representing concurrent operations using callbacks requires you to
    write the program in a radically different way from the start. Re-writing a program
    that uses a normal sequential program flow to one using callbacks represents a
    substantial rewrite, and the same goes the other way.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Callback-based concurrency can be hard to reason about and can become very complicated
    to understand. It’s no coincidence that the term “callback hell” is something
    most JavaScript developers are familiar with.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Since each sub-task must save all the state it needs for later, the memory usage
    will grow linearly with the number of callbacks in a task.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Advantages
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Easy to implement in most languages
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No context switching
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relatively low memory overhead (in most cases)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Memory usage grows linearly with the number of callbacks.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programs and code can be hard to reason about.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s a very different way of writing programs and it will affect almost all
    aspects of the program since all yielding operations require one callback.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ownership can be hard to reason about. The consequence is that writing callback-based
    programs without a garbage collector can become very difficult.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing state between tasks is difficult due to the complexity of ownership
    rules.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging callbacks can be difficult.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coroutines: promises and futures'
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note!
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: This is another example of M:N threading. Many tasks can run concurrently on
    one OS thread. Each task is represented as a state machine.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '**Promises** in JavaScript and **futures** in Rust are two different implementations
    that are based on the same idea.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: There are differences between different implementations, but we’ll not focus
    on those here. It’s worth explaining promises a bit since they’re widely known
    due to their use in JavaScript. Promises also have a lot in common with Rust’s
    futures.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: First of all, many languages have a concept of promises, but I’ll use the one
    from JavaScript in the following examples.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Promises are one way to deal with the complexity that comes with a callback-based
    approach.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can do:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The latter approach is also referred to as **the continuation-passing style**.
    Each subtask calls a new one once it’s finished.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between callbacks and promises is even more substantial under
    the hood. You see, promises return a state machine that can be in one of three
    states: `pending`, `fulfilled`, or `rejected`.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: When we call `timer(200)` in the previous example, we get back a promise in
    the `pending` state.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Now, the continuation-passing style does fix some of the issues related to callbacks,
    but it still retains a lot of them when it comes to complexity and the different
    ways of writing programs. However, they enable us to leverage the compiler to
    solve a lot of these problems, which we’ll discuss in the next paragraph.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines and async/await
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Coroutines come in two flavors: **asymmetric** and **symmetric**. Asymmetric
    coroutines yields to a scheduler, and they’re the ones we’ll focus on. Symmetric
    coroutines yield a specific destination; for example, a different coroutine.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: While coroutines are a pretty broad concept in general, the introduction of
    coroutines as `objects` in programming languages is what really makes this way
    of handling concurrency rival the ease of use that OS threads and fibers/green
    threads are known for.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: You see when you write `async` in Rust or JavaScript, the compiler re-writes
    what looks like a normal function call into a future (in the case of Rust) or
    a promise (in the case of JavaScript). **Await**, on the other hand, yields control
    to the runtime scheduler, and the task is suspended until the future/promise you’re
    awaiting has finished.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: This way, we can write programs that handle concurrent operations in almost
    the same way we write our normal sequential programs.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Our JavaScript program can now be written as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can consider the `run` function as a pausable task consisting of several
    sub-tasks. On each “await” point, it yields control to the scheduler (in this
    case, it’s the well-known JavaScript event loop).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Once one of the sub-tasks changes state to either `fulfilled` or `rejected`,
    the task is scheduled to continue to the next step.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'When using Rust, you can see the same transformation happening with the function
    signature when you write something such as this:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The function wraps the return object, and instead of returning the type `()`,
    it returns a `Future` with an output type of `()`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Syntactically, Rust’s futures 0.1 was a lot like the promise example we just
    showed, and the Rust futures we use today have a lot in common with how `async`/`await`
    works in JavaScript..
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: This way of rewriting what look like normal functions and code into something
    else has a lot of benefits, but it’s not without its drawbacks.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: As with any stackless coroutine implementation, full pre-emption can be hard,
    or impossible, to implement. These functions have to yield at specific points,
    and there is no way to suspend execution in the middle of a stack frame in contrast
    to fibers/green threads. Some level of pre-emption is possible by having the runtime
    or compiler insert pre-emption points at every function call, for example, but
    it’s not the same as being able to pre-empt a task at any point during its execution.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Pre-emption points
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Pre-emption points can be thought of as inserting code that calls into the scheduler
    and asks it if it wishes to pre-empt the task. These points can be inserted by
    the compiler or the library you use before every new function call for example.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, you need compiler support to make the most out of it. Languages
    that have metaprogramming abilities (such as macros) can emulate much of the same,
    but this will still not be as seamless as it will when the compiler is aware of
    these special async tasks.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Debugging is another area where care must be taken when implementing futures/promises.
    Since the code is re-written as state machines (or generators), you won’t have
    the same stack traces as you do with normal functions. Usually, you can assume
    that the caller of a function is what precedes it both in the stack and in the
    program flow. For futures and promises, it might be the runtime that calls the
    function that progresses the state machine, so there might not be a good backtrace
    you can use to see what happened before calling the function that failed. There
    are ways to work around this, but most of them will incur some overhead.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Advantages
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can write code and model programs the same way you normally would
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No context switching
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be implemented in a very memory-efficient way
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s easy to implement for various platforms
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pre-emption can be hard, or impossible, to fully implement, as the tasks can’t
    be stopped in the middle of a stack frame
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It needs compiler support to leverage its full advantages
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging can be difficult both due to the non-sequential program flow and the
    limitations on the information you get from the backtraces.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You’re still here? That’s excellent! Good job on getting through all that background
    information. I know going through text that describes abstractions and code can
    be pretty daunting, but I hope you see why it’s so valuable for us to go through
    these higher-level topics now at the start of the book. We’ll get to the examples
    soon. I promise!
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we went through a lot of information on how we can model and
    handle asynchronous operations in programming languages by using both OS-provided
    threads and abstractions provided by a programming language or a library. While
    it’s not an extensive list, we covered some of the most popular and widely used
    technologies while discussing their advantages and drawbacks.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: We spent quite some time going in-depth on threads, coroutines, fibers, green
    threads, and callbacks, so you should have a pretty good idea of what they are
    and how they’re different from each other.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will go into detail about how we do system calls and create
    cross-platform abstractions and what OS-backed event queues such as Epoll, Kqueue,
    and IOCP really are and why they’re fundamental to most async runtimes you’ll
    encounter out in the wild.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL

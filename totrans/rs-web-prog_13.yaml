- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Best Practices for a Clean Web App Repository
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we have been building out our application piece by piece
    and adding automation scripts and tools to help us with testing and deploying
    our application. However, although this path is useful for learning tools and
    concepts, the structure of our projects in previous chapters has not been optimal
    for running a project for production.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will create a new repository, lift our Rust code into that
    repository, and then structure the code for clean database migrations, tests,
    and optimized Docker builds for our application so that it can be deployed smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The general layout of a clean repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting our configuration from environment variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a local development database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing variables in Postman tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building distroless tiny server Docker images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a clean test pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building continuous integration with GitHub Actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to structure a repository with
    scripts, Docker builds, and tests that will make development smooth and easy to
    add new features. You will also be able to build **distroless** Docker images
    for the application, making them secure and dropping the size of our server images
    from 1.5 GB to 45 MB!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be referencing parts of the code defined in [*Chapter
    9*](B18722_09.xhtml#_idTextAnchor182), *Testing Our Application Endpoints and
    Components*. This can be found at the following URL: [https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter09](https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter09).'
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter13](https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter13).
  prefs: []
  type: TYPE_NORMAL
- en: The general layout of a clean repository
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to a clean layout, we must have directories in the repository
    that have a single focus, just like our isolated code, which is modularized. In
    the clean approach taken in this chapter, our repository will have the following
    layout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'These files and directories have the following responsibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Cargo.toml`: Defines the requirements for the Rust build.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`README.md`: Renders on a GitHub page when visited, telling the reader what
    the project is about and how to interact with the project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.dockerignore`: Tells the Docker build what to ignore when copying directories
    and files into the Docker image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.gitignore`: Tells git what to ignore when committing code to the git repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.github`: A directory that houses GitHub Actions workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`builds`: A directory that houses different builds for Docker, depending on
    the chip architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`database`: A directory that houses all scripts and Docker builds required
    to handle database migrations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker-compose.yml`: Defines the containers needed to run a development build.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scripts`: A directory that houses all the Bash scripts needed to run dev servers
    or tests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src`: A directory that houses all the Rust code to build the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tests`: A directory that houses a `docker-compose` configuration and a Postman
    collection to enable a fully integrated test. We must remember that unit tests
    are coded within the `src` directory and are conditionally compiled when the `test`
    command in Cargo is executed. In standard and release builds, the unit tests are
    excluded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know what our repository structure is like, we can add some rules
    and files to ensure that our builds and git commits behave in exactly the right
    way. It is good to do this at the very start of a project to avoid accidentally
    adding unwanted code to the git history or Docker builds.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will start with the `.gitignore` file, which has the following rules
    defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that we avoid anything in the `target` directory, which gets
    filled up with a lot of files when performing Rust builds and tests. These files
    add nothing to the project’s development and will balloon the size of your project
    very quickly. If you like to use JetBrains or are using a Mac, I have added `.idea`
    and `.DS_Store` as these files can sneak into repositories; they are not required
    for running any of the web application’s code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at our `.dockerignore` file, which has the following rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: These rules should make sense. We do not want to add our build files, scripts,
    database migrations, or GitHub workflows to our Docker builds.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now defined all our rules for the repository. Before we move on to
    the next section, we might as well define the general layout of our application.
    Here, we can lift the source Rust code from the existing to-do application into
    our new repository with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you created an `src` directory in the clean app repository before running
    the preceding command, you must delete the `src` directory in the clean app repository;
    otherwise, you will end up with two `src` directories, where the copied `src`
    is inside the existing `src`. Our `Cargo.tml` file has the same dependencies as
    our existing web application; however, we can change its name with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check if lifting our code works with the following `test` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This should give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This output shows that our code compiles and that our `Cargo.tml` file is properly
    defined. Now that we have confirmed that our unit tests have passed, we have some
    assurance that our code is working. However, how we define our configuration will
    give us some hurdles when we are deploying applications to the cloud. In the next
    section, we will smooth out our deployments by using environment variables to
    configure our web applications.
  prefs: []
  type: TYPE_NORMAL
- en: Getting our configuration from environment variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have been loading configuration variables from YML files. This has
    a few issues. First, we must move these files around where our deployment is.
    Also, files do not work efficiently with orchestration tools such as Kubernetes.
    Kubernetes uses ConfigMaps, which essentially define environment variables for
    each container they are running. Environment variables also work well with tools
    such as Secret Manager and AWS credentials. We can also directly overwrite the
    environment variables in `docker-compose`. With all these advantages in mind,
    we will switch our configuration values from files to environment variables. To
    map where we have implemented configuration variables from a file, all we must
    do is delete our `src/config.rs` file and the module declaration of that `config`
    module in the `main.rs` file. Then, we can run the `cargo test` command again
    to get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we used `config` in the `jwt`, `database`, and `counter` modules. This
    makes sense because we must connect to external structures when using these modules.
    To fix the breaking imports, all we must do is replace the config references with
    environment variable references. To demonstrate this, we can use the `src/counter.rs`
    file. First, we must delete the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must replace the preceding lines of code with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can follow this format for the `JWT` and `database` modules as well. In
    the `JWT` module, there is one variable that is not a string and has to be converted
    into an integer, which is `expire minutes`. This can be done with the following
    line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the `cargo test` command now, we will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Our tests ran, so we know that compiling the code worked. However, some of
    the JWT tests are failing. If we scroll further down the log, we will see the
    following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This is telling us that our environment variables are not present. Considering
    that this is failing in the `JWT` module, we can be sure that it will also fail
    in the `database` and `counter` modules. Therefore, before we run or test our
    application, we need to define these environment variables. We can build a test
    pipeline for our application with environment variables by building a `scripts/run_unit_tests.sh`
    script with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, we navigate to the root directory, export the environment variables, and
    then run the `test` command. Running the preceding script results in all the unit
    tests passing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'While I like putting as much as possible in the Bash script as it acts like
    documentation for other developers to see all the moving parts, there are other
    approaches. For instance, you may find that the approach outlined previously is
    clunky as this approach deviates from running the standard `cargo test` command.
    Other approaches include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Manually injecting variables into the test'
  prefs: []
  type: TYPE_NORMAL
- en: '- Using the `dotenv` crate to load environment variables from a file (https://github.com/dotenv-rs/dotenv)'
  prefs: []
  type: TYPE_NORMAL
- en: '- Having sensible defaults for environment variables'
  prefs: []
  type: TYPE_NORMAL
- en: 'How would you create the script that runs the dev server? This would be a good
    time for you to try and write the script yourself. If you have attempted writing
    the script yourself, your `scripts/run_dev_server.sh` script should look something
    like the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if we try and run the preceding script, it will crash because we cannot
    connect to the Redis database. We need to define our dev services in the `docker-compose.yml`
    file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now that our dev services have been defined, we can spin up our `docker-compose`
    and run our `run_dev_server.sh` script, resulting in our dev server running. However,
    if we try and perform any requests, the server will crash. This is because we
    have not performed migrations on the database. In the next section, we will perform
    migrations on our dev database.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a local development database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to migrations, there is the advantage of decoupling the programming
    language that we are using from the migrations. In the past, I have had to switch
    servers from one language to another and simply wished that the migration’s implementation
    was not coupled with the language. This is also a deployment issue. For instance,
    in Kubernetes, deploying a new server or an update might require a migration to
    be run. Ideally, you want to run the migration automatically through what we call
    *init Pods*. This is a container that is spun up and executed before the main
    server is deployed. This init Pod can perform a database migration command. However,
    if the init Pod requires something such as Rust to be present to execute the migration,
    this can greatly increase the size of the init pod. Therefore, I built an open
    source Bash tool that is only dependent on the `psql` and `wget` libraries. It
    can create new migrations and roll the database up and down versions. However,
    it must be stressed that this tool is not for every use. To quote the documentation
    of the migrations tool I wrote ([https://github.com/yellow-bird-consult/build_tools/tree/develop#use-yb-database-migrations-if-you-have-the-following)](https://github.com/yellow-bird-consult/build_tools/tree/develop#use-yb-database-migrations-if-you-have-the-following)),
    you should choose to use the migrations tool for projects if you have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A light throughput of migrations**: Migrations are not timestamped; they
    are simply numbered. The design of the tool is simple to keep track of what’s
    going on. Light applications in microservices are an ideal environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Well-tested code**: There are no guardrails. If there is an error in part
    of your SQL script, your database will be scarred with a partly run migration.
    You should have testing regimes with Docker databases before implementing migrations
    on a live production database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**You plan on writing your own SQL**: Because this tool is completely decoupled
    from any programming language, you have to write your own SQL scripts for each
    migration. This is not as daunting as you might think and gives you more control.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**You want complete control**: SQL migrations and the simple implementation
    are essentially defined in a single Bash script. This simple implementation gives
    you 100% control. Nothing is stopping you from opening up your database in a GUI
    and directly altering the version number or manually running particular sections
    of the migration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we know what we are getting ourselves in for, we can navigate to the
    `database` directory and install the migration tool with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This installs a couple of Bash scripts in your home directory. You may have
    to refresh your terminal to get the command alias. Not all operating systems will
    support the command alias. If your command alias does work, we can create a new
    set of migrations by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if the alias does not work, you can run all your commands through
    the Bash script as each Bash script is 100% self-contained. All we must do is
    pass the same arguments in with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `init` command, we get the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the same as the Diesel migrations tool but with just plain numbers.
    We have two migrations from our to-do application, so we can create them with
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have done this, we can create our migration files. The `database_management/1/up.sql`
    file creates the `to_do` table with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `database_management/1/down.sql` file drops the `to_do` table with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `database_management/2/up.sql` file creates the `user` table and links
    all existing items to a placeholder user with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `database_management/2/down.sql` file drops the `users` table with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Our migrations are now ready. However, we need to connect to our database to
    get information and perform migrations. We can spin up our `docker-compose` to
    get the dev database up and running. Once this is done, we must define our database
    URL in the environment variables. The migration tool looks for the URL in environment
    variables. However, if there is a `.env` file in the current working directory,
    the migration tool will also load all the variables in this file. In our `database_management/.env`
    file, we can define the database URL with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our database is running and we have our URL defined, we can get what
    migration level the database is currently at with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Right now, we should get a `-1`. This means that there is no migrations versioning
    table at all on the database. If there is, but no migrations have been performed
    on the database, the version will be `0`. If there are any migrations, then the
    response will be the migration number that it is currently at. We can use the
    following `db` commands when using the build tool to perform commands on the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '`set`: Creates a migrations version table if there is not one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`up`: Goes up one migration version by applying the `up.sql` script'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`down`: Goes down one migration version by applying the `down.sql` script'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new`: Creates a new migration folder if you are on the latest version'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rollup`: Creates a new migrations version table if there is not one and then
    loops up all the versions in the `database_management` directory, starting from
    the current version of the database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will run the `rollup` command with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This will perform migrations on the database. If you run the `get` command,
    you will see that the version of the database is now `2`. Our database is now
    ready to be queried by our application.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Migrations can also be achieved using the `sqlx-cli` crate, which can be found
    at the following link: https://crates.io/crates/sqlx-cli.'
  prefs: []
  type: TYPE_NORMAL
- en: However, Cargo is needed to install `sqlx-cli`, which will complicate the creation
    of init Pods for executing these migrations.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of randomly making requests, in the next section, we will refine our
    Postman tests so that we can run a series of requests and check that our application
    runs in the way that we want it to.
  prefs: []
  type: TYPE_NORMAL
- en: Managing variables in Postman tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [*Chapter 9*](B18722_09.xhtml#_idTextAnchor182), *Testing Our Application
    Endpoints and Components*, we built a Postman collection. However, it was a bit
    ropey as we had to rely on Python to load the new token into the Newman collection.
    While this was important to use as using Python as glue code between processes
    is a useful skill, our old version of readying a Newman collection with Python
    is not the cleanest approach. At the start of our collection, we will add two
    new requests. The first one will create a user with the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Create user Postman request](img/Figure_13.1_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – Create user Postman request
  prefs: []
  type: TYPE_NORMAL
- en: 'With the create user request, we get the following JavaScript in the **Tests**
    tab in Postman:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'With this, the first request of our collection will create the user and throw
    an error if the request was not successful. Then, we can create the second request
    for our collection, which consists of logging in, with the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Login Postman request](img/Figure_13.2_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 – Login Postman request
  prefs: []
  type: TYPE_NORMAL
- en: 'With this request, we must check the response and set a collection variable
    as the token that we just got from the login by running the following JavaScript
    in the **Tests** tab in Postman:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have set our collection variable, we will be able to reference our
    token throughout the rest of the collection. To do this, we must update the authorization
    for the entire collection so that our new token value will propagate through all
    our requests. To access the authorization settings, click on the header of a `create`
    request to get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Header of the request](img/Figure_13.3_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 – Header of the request
  prefs: []
  type: TYPE_NORMAL
- en: 'On the right-hand side of the previous screenshot, we can see that there is
    a **Go to authorization** button. If we click on this, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – Configuring authorization](img/Figure_13.4_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4 – Configuring authorization
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the value has been changed to `{{login_token}}`. If we save
    this and then export the collection JSON file to the `tests` directory in our
    repository, the value that belongs to `{{login_token}}` will be inserted into
    the collection JSON file.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a Postman collection that updates itself with a fresh token after
    the login request without having to rely on Python to glue processes together.
    This is much cleaner; however, we want to ensure that the rest of our testing
    pipeline mimics as much of a production setting as possible. In the next section,
    we will build Docker images that contain our application that are a fraction of
    the size of what our server images were in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Building distroless tiny server Docker images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, our server Docker images were roughly around 1.5 GB. This
    is pretty big and not ideal when we want to distribute our Rust images on servers
    or to other developers. Note that there is a shell that we can access in the Docker
    container when the image is running. This is useful in development but not great
    in production because if anyone manages to gain access to the Docker container,
    they will be able to look around and run commands in the Docker container. If
    the permissions on the server are not locked down, the hacker could even start
    running commands on the cluster that you have. I have seen cryptojacking happen
    through this method, where a hacker spun up a load of mining Pods at the expense
    of the owner of the AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to solve these problems by using distroless images. These distroless
    images are tiny in size and do not have shells. So, if someone manages to gain
    access to our server, they will not be able to do anything because there are no
    shells. We will be able to drop the size of our image from 1.5 GB to 45 MB! This
    is something we want. However, before we start building our distroless images,
    we must know that distroless images have close to nothing on them. This means
    that if we compile our application and stuff it into a distroless image, it will
    not work. For instance, if we make a connection to a database, we need the `libpq`
    library in our distroless image. As the distroless image does not contain the
    library, the image will not be able to run because our static binary will not
    be able to locate the `libpq` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that our 1.5 GB image runs because it contains everything and the kitchen
    sink. We can use our 1.5 GB to inspect what dependencies the static binary has
    in the image. We can do this by moving to our `deployment` directory where we
    wrote code to deploy our application on AWS and spinning up `docker-compose` there.
    Once this is running, we can inspect our containers with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Your IDs will be different, but we will use these IDs to SSH into our Rust
    app by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This opens an interactive shell so that we can navigate the Docker container.
    Here, we must remember that the static binary – that is, the Rust server – is
    called `web_app` and that this is in the root directory, so we do not need to
    go anywhere within the container. We can list the dependencies by using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: There are 29 dependencies in total. On the left of the list, there is the name
    of the library. On the right of the list, there is the path to where the library
    is. We can see that the database `libpq` library is needed alongside other libraries.
    Your paths may look different. This is because I am running this image on a MacBook
    M1, which has an ARM chip architecture. If you do not have this, then you will
    have `x86_64-linux-gnu` in your path as opposed to `aarch64-linux-gnu`. This is
    fine – we will supply both Docker files in the GitHub repository online.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our Docker build, we must copy these libraries into our distroless image.
    In our `clean_web_app/builds` directory, we must create two files: `aarch64_build`
    and `x86_64_build`. Both these files are essentially the same Dockerfiles but
    with different references to libraries. At the time of writing, I wish that there
    was a smarter way to achieve builds with different chips in one Docker file; however,
    Docker builds are terrible at passing variables throughout the build as each step
    is isolated, and conditional logic is limited at best. It is easier to just have
    two different files. Also, if the builds change in the future, then the two different
    chip builds are decoupled. In our `clean_web_app/builds/arch_build` file, we must
    get the Rust image, install the database library, copy over the code of the application
    to be compiled, and define what type of build we are doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the environment is set to `"PRODUCTION"` by default. If there
    is an accident and the environment is not defined, it should be `"PRODUCTION"`
    by default. Accidentally taking longer to compile on a test build is much better
    than accidentally deploying a non-production server into production. Then, we
    compile using the release flag if it is production and switch the static binary
    into the release directory if it is not compiled using the release flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, our application has been compiled. Everything we have covered
    is independent of what type of chip we are using, so the `x86_64_build` file will
    contain the same code that we have just laid out in the `aarch64_build` file.
    For both build files, we can also get our distroless image with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, this is where the build scripts differ. In the ARM chip build, we must
    copy the libraries needed from the previous Rust image into our distroless image,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Including them all would simply provide needless bloat for the book and again,
    these files are available in this book’s GitHub repository. What we must note,
    however, is that the directory for the first part of each copy is the directory
    listed when we explored the Docker image of our large application. The second
    part is the same path; however, if there is a `/usr/lib/` at the start of the
    path, it is shortened to `/lib/`. There is no shell or users in the distroless
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all the libraries have been copied over, we must copy the static binary
    of our web application into the root of our image, expose the port, and define
    the entry point, which is the static binary, with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: With this, our distroless image is done. Right now, both builds are stored away,
    and we will get them out depending on the chip type in a bash script to be built.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t have to build our distroless application manually. Instead, we can
    use Apko via the following link: [https://github.com/chainguard-dev/apko](https://github.com/chainguard-dev/apko).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can copy your chosen build to the root directory of the repository under
    the `Dockerfile` filename. Then, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: When you list your Docker images, you will see that this image is 46.5 MB! This
    is a massive reduction from 1.5 GB. In the next section, we will include these
    build files in a test pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Building a clean test pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to testing our application, we want to package it in the Docker
    image that we wish to deploy onto the servers, run migrations on the database
    as we would on the servers, and run a series of Postman requests and tests to
    mimic a user making a series of requests. This can be orchestrated with one Bash
    script in the `scripts/run_full_release_test.sh` file. First, we must find out
    what chip we are running on with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we pull the correct build depending on the type of chip. Depending on
    the computer you are using, this might be different. I am using a Mac M1, so when
    I call the `uname -m` command in the terminal, I get an `arm64` output. If you
    are not using an arch or ARM chip, you do not need the conditional logic. Instead,
    you just need to pull the `x86_64_build` file. Then, we must move to the `tests`
    directory and build our `docker-compose` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to run our tests and clean up the images with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we run this, however, we need to build our `docker-compose` in our `tests`
    directory. Our `tests/docker-compose.yml` file has the following outline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we will focus on the test server. Seeing as we are running a test, we
    need to point to the build, pass a `NOT PRODUCTION` argument into the build, define
    the environment variables for the server to utilize, and then wait for the Redis
    database to be operational before spinning it up. We can do this with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, `docker-compose` is a powerful tool. A few tags can result in
    some complex orchestration. Then, we can move to our database and Redis containers
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'These databases are nothing new. However, in the last service, we create an
    init container that spins up briefly just to run the migrations on the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, there must be a Docker build in the `database` directory for
    our init container to make a database migration before closing. This means that
    our init container must have `psql` installed, our migrations tool, and the `rollup`
    command as the entry point. Initially, we install what we need in our `database/Dockerfile`
    file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that we get the `psql` library from the `postgres` Docker
    image. Then, we install `wget` and use this to install our migrations build tool.
    Finally, we copy the `database.sh` Bash script from the home directory into the
    root directory of the image so that we do not have to worry about aliases. Once
    we have configured our installments, we must copy the migrations SQL files from
    the current directory into the root directory of the image and define the migration
    command as the entry point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This will work fine; however, we do have to define a `database/.dockerignore`
    file with the following content to avoid the environment variable being passed
    into the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: If we do not stop this environment variable from being copied into the image,
    then whatever variables we pass into the init container through `docker-compose`
    could get overwritten.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have everything we need in place, so all we must do is run our `scripts/run_full_release.sh`
    script. This will produce a lengthy printout of building the images, spinning
    up `docker-compose`, and running the API tests via Newman. The last output should
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_13.5_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 – Result of a full test run
  prefs: []
  type: TYPE_NORMAL
- en: We can see that all the tests ran and passed. Our distroless build works and
    our init container for the database also makes the migrations. Nothing is stopping
    us from putting this infrastructure on AWS, with the difference of pointing to
    images on Docker Hub as opposed to local builds. Considering how small our distroless
    server is, pulling the image from Docker Hub and spinning it up will be very quick.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now got all the ingredients to build continuous integration for our GitHub
    repository to ensure that tests are run when we create pull requests. In the next
    and final section, we will configure continuous integration through **GitHub Actions**.
  prefs: []
  type: TYPE_NORMAL
- en: Building continuous integration with GitHub Actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to ensuring that code quality is maintained, it can be handy to
    have a continuous integration pipeline that will run every time a pull request
    is done. We can do this with GitHub Actions. It must be noted that with GitHub
    Actions, you get several free minutes every month; then, you must pay for the
    minutes you go over. So, be careful and keep an eye on how much time you’re spending
    using GitHub Actions.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Actions gives us flexibility when it comes to implementing tasks. We
    can run workflows when a pull request is merged or made and when an issue is created
    and much more. We can also be selective about the type of branches we use. In
    this example, we will merely focus on a pull request on any branch to run unit
    tests and then full integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a workflow called `tests`, we need to create a file called `.github/workflows/run-tests.yml`.
    In this file, we will define the general outline of the unit and integration tests
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have defined the name of the workflow and the conditions that the workflow
    is triggered on pull requests for all branches. Then, we define two jobs – one
    to run unit tests and the other to run integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each job has steps. We can also define dependencies for our steps. We can define
    our unit test job with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Here, we used the `checkout` action. If we do not use the `checkout` action,
    we will not be able to access any of the files in the GitHub repository. Then,
    we export the environment variables that are needed for the unit tests to run,
    and then we run the unit tests using Cargo. Also, note that we define a timeout.
    Defining a timeout is important just in case something ends up in a loop and you
    do not burn all your minutes in one job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s move on to our integration test job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we move into the `tests` directory, get the server build Docker file,
    spin up `docker-compose`, and then use the `newman` action to run the Newman tests.
    If we make a pull request, the actions will be shown on the pull request. If we
    click on the GitHub Actions button, we can access the status and results, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.6 – GitHub Actions options](img/Figure_13.6_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 – GitHub Actions options
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can click on the test to see the steps of the job, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.7 – GitHub Actions job view](img/Figure_13.7_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7 – GitHub Actions job view
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we click on a step in the job, it will expand. We will see that our
    Newman tests work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8 – Newman step result](img/Figure_13.8_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8 – Newman step result
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, our continuous integration works! We have now come to the end
    of this chapter as our repository is clean and functional.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have finally made it to the end of structuring a web application in Rust
    and building the infrastructure around the application to make ongoing development
    of new features safe and easy to integrate. We have structured our repository
    into one that’s clean and easy to use where directories have individual purposes.
    Like in well-structured code, our well-structured repository can enable us to
    slot tests and scripts in and out of the repository easily. Then, we used pure
    bash to manage migrations for our database without any code dependencies so that
    we can use our migrations on any application, regardless of the language being
    used. Then, we built init containers to automate database migrations, which will
    work even when deployed on a server or cluster. We also refined the Docker builds
    for our server, making them more secure and reducing the size from 1.5 GB to 45
    MB. After, we integrated our builds and tests into an automated pipeline that
    is fired when new code is merged into the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: This brings a natural end to building a web application and deploying it on
    a server. In the following chapters, we will dive deeper into web programming
    with Rust, looking at lower-level frameworks so that we can build custom protocols
    over TCP sockets. This will enable you to build lower-level applications for web
    servers or even local processes. In the next chapter, we will explore the Tokio
    framework, a building block of async programs such as TCP servers.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Database migrations documentation and repository: [https://github.com/yellow-bird-consult/build_tools](https://github.com/yellow-bird-consult/build_tools)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GitHub Actions documentation: [https://docs.github.com/en/actions/guides](https://docs.github.com/en/actions/guides)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bash migrations tool uses incremental single-digit integers to denote migrations.
    What is the big downside to this?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are distroless servers more secure?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How did we remove the need for Python when running our Newman tests that required
    a fresh token?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the advantages of using environment variables for configuration values?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using incremental single-digit integers exposes the migrations to clashes. So,
    if one developer writes migrations on one branch while another developer writes
    migrations on a different branch, there will be a conflict of migrations when
    they both merge. GitHub should pick this up, but it’s important to keep the traffic
    of migrations low, plan out database alterations properly, and keep the services
    using the migrations small. If this is a concern for you, however, please use
    a different migrations tool that is heavier but has more guardrails.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distroless servers do not have shells. This means that if a hacker manages to
    access our server container, they cannot run any commands or inspect the contents
    of the container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the login request, we get the token that is returned from the server in the
    test script and assign it to a collection variable that can be accessed by other
    requests, removing the reliance on Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Environment variables are simply easier to implement when deploying our application
    to the cloud. For instance, Kubernetes’s ConfigMaps use environment variables
    to pass variables into Docker containers. It is also easier to implement services
    such as Secrets Manager on AWS by using environment variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Part 6:Exploring Protocol Programming and Async Concepts with Low-Level Network
    Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web programming has evolved to more than just simple applications that interact
    with databases. In this part, we cover more advanced concepts with async Rust
    by covering the basics of async Rust, Tokio, and Hyper. With Tokio and Hyper,
    we leverage async Rust and the actor model to implement async designs such as
    passing messages between actors in different threads, queuing tasks in Redis to
    be consumed by multiple workers, and processing byte streams with Tokio framing
    and TCP ports. By the end of this part, you will be able to implement more complex
    event-processing solutions on your server to handle more complex problems. You
    will also have practical knowledge of how to implement async Rust, which is an
    up-and-coming field.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part includes the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B18722_14.xhtml#_idTextAnchor279), *Exploring the Tokio Framework*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 15*](B18722_15.xhtml#_idTextAnchor291), *Accepting TCP Traffic with
    Tokio*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 16*](B18722_16.xhtml#_idTextAnchor306), *Building Protocols on Top
    of TCP*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 17*](B18722_17.xhtml#_idTextAnchor323), *Implementing Actors and
    Async with the Hyper Framework*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 18*](B18722_18.xhtml#_idTextAnchor335), *Queuing Tasks with Redis*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

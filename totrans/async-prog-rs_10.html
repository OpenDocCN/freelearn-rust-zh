<html><head></head><body>
		<div><h1 id="_idParaDest-179" class="chapter-number"><a id="_idTextAnchor178"/>10</h1>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor179"/>Creating Your Own Runtime</h1>
			<p>In the last few chapters, we covered a lot of aspects that are relevant to asynchronous programming in Rust, but we did that by implementing alternative and simpler abstractions than what we have in Rust today.</p>
			<p>This last chapter will focus on bridging that gap by changing our runtime so that it works with Rust futures and async/await instead of our own futures and coroutine/wait. Since we’ve pretty much covered everything there is to know about coroutines, state machines, futures, wakers, runtimes, and pinning, adapting what we have now will be a relatively easy task.</p>
			<p>When we get everything working, we’ll do some experiments with our runtime to showcase and discuss some of the aspects that make asynchronous Rust somewhat difficult for newcomers today.</p>
			<p>We’ll also take some time to discuss what we might expect in the future with asynchronous Rust before we summarize what we’ve done and learned in this book.</p>
			<p>We’ll cover the following main topics:</p>
			<ul>
				<li>Creating our own runtime with futures and async/await</li>
				<li>Experimenting with our runtime</li>
				<li>Challenges with asynchronous Rust</li>
				<li>The future of asynchronous Rust</li>
			</ul>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor180"/>Technical requirements</h1>
			<p>The examples in this chapter will build on the code from the last chapter, so the requirements are the same. The example is cross-platform and will work on all platforms that Rust (<a href="https://doc.rust-lang.org/beta/rustc/platform-support.html#tier-1-with-host-tools">https://doc.rust-lang.org/beta/rustc/platform-support.html#tier-1-with-host-tools</a>) and <code>mio</code> (https://github.com/tokio-rs/mio#platforms) support.</p>
			<p>The only thing you need is Rust installed and the book’s repository downloaded locally. All the code in this chapter can be found in the <code>ch10</code> folder.</p>
			<p>We’ll use <code>delayserver</code> in this example as well, so you need to open a separate terminal, enter the <code>delayserver</code> folder at the root of the repository, and type <code>cargo run</code> so it’s ready and available for the examples going forward.</p>
			<p>Remember to change the ports in the code if for some reason you have to change what port <code>delayserver</code> listens on.</p>
			<pre class="source-code">
Creating our own runtime with futures and async/await</pre>			<p>Okay, so we’re in the home stretch; the last thing we’ll do is change our runtime so it uses the Rust <code>Future</code> trait, <code>Waker</code>, and <code>async/await</code>. This will be a relatively easy task for us now that we’ve pretty much covered the most complex aspects of asynchronous programming in Rust by building everything up ourselves. We have even gone into quite some detail on the design decisions that Rust had to make along the way.</p>
			<p>The asynchronous programming model Rust has today is the result of an evolutionary process. Rust started in its early stages with green threads, but this was before it reached version 1.0. At the point of reaching version 1.0, Rust didn’t have the notion of futures or asynchronous operations in its standard library at all. This space was explored on the side in the futures-rs crate (<a href="https://github.com/rust-lang/futures-rs">https://github.com/rust-lang/futures-rs</a>), which still serves as a nursery for async abstractions today. However, it didn’t take long before Rust settled around a version of the <code>Future</code> trait similar to what we have today, often referred to as <em class="italic">futures 0.1</em>. Supporting coroutines created by async/await was something that was in the works already at that point but it took a few years before the design reached its final stage and entered the stable version of the standard library.</p>
			<p>So, many of the choices we had to make with our async implementation are real choices that Rust had to make along the way. However, it all brings us to this point, so let’s get to it and start adapting our runtime so it works with Rust futures.</p>
			<p>Before we get to the example, let’s cover the things that are different from our current implementation:</p>
			<ul>
				<li>The <code>Future</code> trait Rust uses is slightly different from what we have now. The biggest difference is that it takes something called <code>Context</code> instead of <code>Waker</code>. The other difference is that it returns an enum called <code>Poll</code> instead of <code>PollState</code>.</li>
				<li><code>Context</code> is a wrapper around Rust’s <code>Waker</code> type. Its only purpose is to future-proof the API so it can hold additional data in the future without having to change anything related to <code>Waker</code>.</li>
				<li>The <code>Poll</code> enum returns one of two states, <code>Ready(T)</code> or <code>Pending</code>. This is slightly different from what we have now with our <code>PollState</code> enum, but the two states mean the same as <code>Ready(T)/NotReady</code> in our current implementation.</li>
				<li><code>Wakers</code> in Rust is slightly more complex to create than what we’re used to with our current <code>Waker</code>. We’ll go through how and why later in the chapter.</li>
			</ul>
			<p>Other than the differences outlined above, everything else can stay pretty much as is. For the most part, we’re renaming and refactoring this time.</p>
			<p>Now that we’ve got an idea of what we need to do, it’s time to set everything up so we can get our new example up and running.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Even though we create a runtime to run futures properly in Rust, we still try to keep this simple by avoiding error handling and not focusing on making our runtime more flexible. Improving our runtime is certainly possible, and while it can be a bit tricky at times to use the type system correctly and please the borrow checker, it has relatively little to do with <em class="italic">async</em> Rust and more to do with Rust being Rust.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor181"/>Setting up our example</h2>
			<p class="callout-heading">Tip</p>
			<p class="callout">You’ll find this example in the book’s repository in the <code>ch1</code><code>0</code><code>/a-rust-futures</code> folder.</p>
			<p>We’ll continue where we left off in the last chapter, so let’s copy everything we had over to a new project:</p>
			<ol>
				<li>Create a new folder called <code>a-rust-futures</code>.</li>
				<li>Copy everything from the example in the previous chapter. If you followed the naming I suggested, it would be stored in the <code>e-coroutines-pin</code> folder.</li>
				<li>You should now have a folder containing a copy of our previous example, so the last thing to do is to change the project name in <code>Cargo.toml</code> to <code>a-rust-futures</code>.</li>
			</ol>
			<p>Okay, so let’s start with the program we want to run. Open <code>main.rs</code>.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor182"/>main.rs</h2>
			<p>We’ll go back to the simplest<a id="_idIndexMarker627"/> version of our program and get it running before we try anything more complex. Open <code>main.rs</code> and replace all the code in that file with this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/main.rs</p>
			<pre class="source-code">
mod http;
mod runtime;
use crate::http::Http;
fn main() {
    let mut executor = runtime::init();
    executor.block_on(async_main());
}
async fn async_main() {
    println!("Program starting");
    let txt = Http::get("/600/HelloAsyncAwait").await;
    println!("{txt}");
    let txt = Http::get("/400/HelloAsyncAwait").await;
    println!("{txt}");
}</pre>			<p>No need for <code>corofy</code> or <a id="_idIndexMarker628"/>anything special this time. The compiler will rewrite this for us.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Notice that we’ve removed the declaration of the <code>future</code> module. That’s because we simply don’t need it anymore. The only exception is if you want to retain and use the <code>join_all</code> function we created to join multiple futures together. You can either try to rewrite that yourself or take a look in the repository and locate the <code>ch1</code><code>0</code><code>/a-rust-futures-bonus/src/future.rs</code> file, where you’ll find the same version of our example, only this version retains the future module with a <code>join_all</code> function that works with Rust futures.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor183"/>future.rs</h2>
			<p>You can delete this file<a id="_idIndexMarker629"/> altogether as we don’t need our own <code>Future</code> trait anymore.</p>
			<p>Let’s move right along to <code>http.rs</code> and see what we need to change there.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor184"/>http.rs</h2>
			<p>The first thing we<a id="_idIndexMarker630"/> need to change is our dependencies. We’ll no longer rely on our own <code>Future</code>, <code>Waker</code>, and <code>PollState</code>; instead, we’ll depend on <code>Future</code>, <code>Context</code>, and <code>Poll</code> from the standard library. Our dependencies should look like this now:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/http.rs</p>
			<pre class="source-code">
use crate::runtime::{self, reactor};
use mio::Interest;
use std::{
    future::Future,
    io::{ErrorKind, Read, Write},
    pin::Pin,
    task::{Context, Poll},
};</pre>			<p>We have to do some minor refactoring in the <code>poll</code> implementation for <code>HttpGetFuture</code>.</p>
			<p>First, we need to change<a id="_idIndexMarker631"/> the signature of the <code>poll</code> function so it complies with the new <code>Future</code> trait:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/http.rs</p>
			<pre class="source-code">
fn poll(mut self: Pin&lt;&amp;mut Self&gt;, <code>cx</code>, we have to change what we pass in to <code>set_waker</code> with the following:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/http.rs</p>
			<pre class="source-code">
runtime::reactor().set_waker(<code>Poll</code> instead of <code>PollState</code>. To do that, locate the <code>poll</code> method and start by changing the signature so it matches the <code>Future</code> trait from the standard library:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/http.rs</p>
			<pre class="source-code">
fn poll(mut self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context) -&gt; <strong class="bold">Poll&lt;Self::Output&gt;</strong></pre>			<p>Next, we need to change our return types wherever we return from the function (I’ve only presented the relevant part of the function body here):</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/http.rs</p>
			<pre class="source-code">
loop {
            match self.stream.as_mut().unwrap().read(&amp;mut buff) {
                Ok(0) =&gt; {
                    let s = String::from_utf8_lossy(&amp;self.buffer).to_string();
                    runtime::reactor().deregister(self.stream.as_mut().unwrap(), id);
                    break <strong class="bold">Poll::Ready(s.to_string())</strong>;
                }
                Ok(n) =&gt; {
                    self.buffer.extend(&amp;buff[0..n]);
                    continue;
                }
                Err(e) if e.kind() == ErrorKind::WouldBlock =&gt; {
                    // always store the last given Waker
                    runtime::reactor().set_waker(cx, self.id);
                    break <strong class="bold">Poll::Pending</strong>;
                }
                Err(e) =&gt; panic!("{e:?}"),
            }
        }</pre>			<p>That’s it for this file. Not bad, huh? Let’s take a look at what we need to change in our executor <a id="_idIndexMarker632"/>and open <code>executor.rs</code>.</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor185"/>executor.rs</h2>
			<p>The first thing we need to change in <code>executor.rs</code> is our dependencies. This time, we only rely on types<a id="_idIndexMarker633"/> from the standard library, and our <code>dependencies</code> section should now look like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/runtime/executor.rs</p>
			<pre class="source-code">
use std::{
    cell::{Cell, RefCell},
    collections::HashMap,
    <strong class="bold">future::Future</strong>,
    pin::Pin,
    sync::{Arc, Mutex},
    <strong class="bold">task::{Poll, Context, Wake, Waker},</strong>
    thread::{self, Thread},
};</pre>			<p>Our coroutines will no longer be limited to only output String, so we can safely use a more sensible <code>Output</code> type for our top-level futures:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/runtime/executor.rs</p>
			<pre class="source-code">
type Task = Pin&lt;Box&lt;dyn Future&lt;Output = <code>Waker</code> since the changes we make here will result in several other changes to this file.</p>
			<p>Creating a waker in Rust can be quite a complex task since Rust wants to give us maximum flexibility on how we choose to implement wakers. The reason for this is twofold:</p>
			<ul>
				<li>Wakers must work just as well on a server as it does on a microcontroller</li>
				<li>A waker must be a zero-cost abstraction</li>
			</ul>
			<p>Realizing that most programmers never need to create their own wakers, the cost that the lack of ergonomics has was deemed acceptable.</p>
			<p>Until quite recently, the only way to construct a waker in Rust was to create something very similar to a trait object without being a trait object. To do so, you had to go through quite a complex process of constructing a <em class="italic">v-table</em> (a set of function pointers), combining that with a pointer to<a id="_idIndexMarker634"/> the data that the waker stored, and creating  <code>RawWaker</code>.</p>
			<p>Fortunately, we don’t actually have to go through this process anymore as Rust now has the <code>Wake</code> trait. The <code>Wake</code> trait works if the <code>Waker</code> type we create is placed in <code>Arc</code>.</p>
			<p>Wrapping <code>Waker</code> in an <code>Arc</code> results in a heap allocation, but for most <code>Waker</code> implementations on the kind of systems we’re talking about in this book, that’s perfectly fine and what most production runtimes do. This simplifies things for us quite a bit.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">This is an example of Rust adopting what turns out to be best practices from the ecosystem. For a long time, a popular way to construct wakers was by implementing a trait called <code>ArcWake</code> provided by the <code>futures</code> crate (<a href="https://github.com/rust-lang/futures-rs">https://github.com/rust-lang/futures-rs</a>). The <code>futures</code> crate is not a part of the language but it’s in the <code>rust-lang</code> repository and can be viewed much like a toolbox and nursery for abstractions that might end up in the language at some point in the future.</p>
			<p>To avoid confusion by having multiple things with the same name, let’s rename our concrete <code>Waker</code> type to <code>MyWaker</code>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/runtime/executor.rs</p>
			<pre class="source-code">
#[derive(Clone)]
pub struct <strong class="bold">MyWaker</strong> {
    thread: Thread,
    id: usize,
    ready_queue: Arc&lt;Mutex&lt;Vec&lt;usize&gt;&gt;&gt;,
}</pre>			<p>We can keep the implementation of <code>wake</code> pretty much the same, but we put it in the implementation of the <code>Wake</code> trait instead<a id="_idIndexMarker635"/> of just having a <code>wake</code> function on <code>MyWaker</code>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/runtime/executor.rs</p>
			<pre class="source-code">
impl Wake for MyWaker {
    fn wake(self: Arc&lt;Self&gt;) {
        self.ready_queue
            .lock()
            .map(|mut q| q.push(self.id))
            .unwrap();
        self.thread.unpark();
    }
}</pre>			<p>You’ll notice that the <code>wake</code> function takes a <code>self: Arc&lt;Self&gt;</code> argument, much like we saw when working with the <code>Pin</code> type. Writing the function signature this way means that <code>wake</code> is only callable on <code>MyWaker</code> instances that are wrapped in <code>Arc</code>.</p>
			<p>Since our <code>waker</code> has changed slightly, there are a few places we need to make some minor corrections. The first is in the <code>get_waker</code> function:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/runtime/executor.rs</p>
			<pre class="source-code">
fn get_waker(&amp;self, id: usize) -&gt; <strong class="bold">Arc&lt;MyWaker&gt;</strong> {
    <strong class="bold">Arc::new(</strong>MyWaker {
        id,
        thread: thread::current(),
        ready_queue: CURRENT_EXEC.with(|q| q.ready_queue.clone()),
    }<strong class="bold">)</strong>
}</pre>			<p>So, not a big change here. The only difference is that we heap-allocate the waker by placing it in <code>Arc</code>.</p>
			<p>The next place we need to make a change is in the <code>block_on</code> function.</p>
			<p>First, we need to change its <a id="_idIndexMarker636"/>signature so that it matches our new definition of a top-level future:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/runtime/executor.rs</p>
			<pre class="source-code">
pub fn block_on&lt;F&gt;(&amp;mut self, future: F)
    where
        F: Future&lt;Output = <strong class="bold">()</strong>&gt; + 'static,
    {</pre>			<p>The next step is to change how we create a waker and wrap it in a <code>Context</code> struct in the <code>block_on</code> function:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/runtime/executor.rs</p>
			<pre class="source-code">
...
                // guard against false wakeups
                    None =&gt; continue,
                };
<strong class="bold">                let waker: Waker = self.get_waker(id).into();</strong>
<strong class="bold">                let mut cx = Context::from_waker(&amp;waker);</strong>
                match future.as_mut().poll(&amp;mut cx) {
...</pre>			<p>This change is<a id="_idIndexMarker637"/> a little bit complex, so we’ll go through it step by step:</p>
			<ol>
				<li>First, we get <code>Arc&lt;MyWaker&gt;</code> by calling the <code>get_waker</code> function just like we did before.</li>
				<li>We convert <code>MyWaker</code> into a simple <code>Waker</code> by specifying the type we expect with <code>let waker: Waker</code> and calling <code>into()</code> on <code>MyWaker</code>. Since every instance of <code>MyWaker</code> is also a kind of <code>Waker</code>, this will convert it into the <code>Waker</code> type that’s defined in the standard library, which is just what we need.</li>
				<li>Since <code>Future::poll</code> expects  <code>Context</code> and not <code>Waker</code>, we create a new <code>Context</code> struct with a reference to the waker we just created.</li>
			</ol>
			<p>The last place we need to make changes is to the signature of our <code>spawn</code> function so that it takes the new definition of top-level futures as well:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/runtime/executor.rs</p>
			<pre class="source-code">
pub fn spawn&lt;F&gt;(future: F)
where
    F: Future&lt;Output = <code>reactor.rs</code>.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor186"/>reactor.rs</h2>
			<p>The first thing we do is to <a id="_idIndexMarker638"/>make sure our dependencies are correct. We have to remove the dependency on our old <code>Waker</code> implementation and instead pull in these types from the standard library. The <code>dependencies</code> section should look like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/runtime/reactor.rs</p>
			<pre class="source-code">
use mio::{net::TcpStream, Events, Interest, Poll, Registry, Token};
use std::{
    collections::HashMap,
    sync::{
        atomic::{AtomicUsize, Ordering},
        Arc, Mutex, OnceLock,
    },
    thread, <strong class="bold">task::{Context, Waker},</strong>
};</pre>			<p>There are two<a id="_idIndexMarker639"/> minor changes we need to make. The first one is that our <code>set_waker</code> function now accepts <code>Context</code> from which it needs to get a <code>Waker</code> object:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/runtime/reactor.rs</p>
			<pre class="source-code">
pub fn set_waker(&amp;self, <strong class="bold">cx: &amp;Context</strong>, id: usize) {
        let _ = self
            .wakers
            .lock()
            .map(|mut w| w.insert(id, <strong class="bold">cx.waker().clone()</strong>).is_none())
            .unwrap();
    }</pre>			<p>The last change is that we need to call a slightly different method when calling <code>wake</code> in the <code>event_loop</code> function:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/a-rust-futures/src/runtime/reactor.rs</p>
			<pre class="source-code">
if let Some(waker) = wakers.get(&amp;id) {
    waker.<strong class="bold">wake_by_ref()</strong>;
}</pre>			<p>Since calling <code>wake</code> now consumes <code>self</code>, we call the version that takes <code>&amp;self</code> instead since we want to hold on to that waker for later.</p>
			<p>That’s it. Our runtime<a id="_idIndexMarker640"/> can now run and take advantage of the full power of asynchronous Rust. Let’s try it out by typing <code>cargo run</code> in the terminal.</p>
			<p>We should get the same output as we’ve seen before:</p>
			<pre class="source-code">
Program starting
FIRST POLL - START OPERATION
main: 1 pending tasks. Sleep until notified.
HTTP/1.1 200 OK
content-length: 15
<strong class="bold">[==== ABBREVIATED ====]</strong>
HelloAsyncAwait
main: All tasks are finished</pre>			<p>That’s pretty neat, isn’t it?</p>
			<p>So, now we have created our own async runtime that uses Rust’s <code>Future</code>, <code>Waker</code>, <code>Context</code>, and <code>async/await</code>.</p>
			<p>Now that we can pride ourselves on being runtime implementors, it’s time to do some experiments. I’ll choose a few that will also teach us a few things about runtimes and futures in Rust. We’re not done learning just yet.</p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor187"/>Experimenting with our runtime</h1>
			<p class="callout-heading">Note</p>
			<p class="callout">You’ll find this example in the book’s repository in the <code>ch1</code><code>0</code><code>/b-rust-futures-experiments</code> folder. The different experiments will be implemented as different versions of the <code>async_main</code> function numbered chronologically. I’ll indicate which function corresponds with which function in the repository example in the heading of the code snippet.</p>
			<p>Before we <a id="_idIndexMarker641"/>start experimenting, let’s copy everything we have now to a new folder:</p>
			<ol>
				<li>Create a new folder called <code>b-rust-futures-experiments</code>.</li>
				<li>Copy everything from the <code>a-rust-futures</code> folder to the new folder.</li>
				<li>Open <code>Cargo.toml</code> and change the <code>name</code> attribute to <code>b-rust-futures-experiments</code>.</li>
			</ol>
			<p>The first experiment will be to exchange our very limited HTTP client with a proper one.</p>
			<p>The easiest way to do that is to simply pick another production-quality HTTP client library that supports async Rust and use that instead.</p>
			<p>So, when trying to find a suitable replacement for our HTTP client, we check the list of the most popular high-level HTTP client libraries and find <code>reqwest</code> at the top. That might work for our purposes, so let’s try that first.</p>
			<p>The first thing we do is add <code>reqwest</code> as a dependency in <code>Cargo.toml</code> by typing the following:</p>
			<pre class="console">
cargo add reqwest@0.11</pre>			<p>Next, let’s change our <code>async_main</code> function so we use <code>reqwest</code> instead of our own HTTP client:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/b-rust-futures-examples/src/main.rs (async_main2)</p>
			<pre class="source-code">
async fn async_main() {
    println!("Program starting");
    let url = "http://127.0.0.1:8080/600/HelloAsyncAwait<strong class="bold">1</strong>";
    let res = reqwest::get(url).await.unwrap();
    let txt = res.text().await.unwrap();
    println!("{txt}");
    let url = "http://127.0.0.1:8080/400/HelloAsyncAwait<strong class="bold">2</strong>";
    let res = reqwest::get(url).await.unwrap();
    let txt = res.text().await.unwrap();
    println!("{txt}");
}</pre>			<p>Besides using the <code>reqwest</code> API, I also changed the message we send. Most HTTP clients don’t return the raw HTTP<a id="_idIndexMarker642"/> response to us and usually only provide a convenient way to get the <em class="italic">body</em> of the response, which up until now was similar for both our requests.</p>
			<p>That should be all we need to change, so let’s try to run our program by writing <code>cargo run</code>:</p>
			<pre class="console">
     Running `target\debug\a-rust-futures.exe`
Program starting
thread 'main' panicked at C:\Users\cf\.cargo\registry\src\index.crates.io-6f17d22bba15001f\tokio-1.35.0\src\net\tcp\stream.rs:160:18:
there is no reactor running, must be called from the context of a Tokio 1.x runtime</pre>			<p>Okay, so the error tells us that there is no reactor running and that it must be called from the context of a Tokio 1.x runtime. Well, we know there is a reactor running, just not the one <code>reqwest</code> expects, so let’s see how we can fix this.</p>
			<p>We obviously need to add Tokio to our program, and since Tokio is heavily feature-gated (meaning that it has very few features enabled by default), we’ll make it easy on ourselves and enable all of them:</p>
			<pre class="source-code">
cargo add tokio@1 --features full</pre>			<p>According to the documentation, we need to start a Tokio runtime and explicitly enter it to enable the reactor. The <code>enter</code> function will return <code>EnterGuard</code> to us that we can hold on to it as long as we need the reactor up and running.</p>
			<p>Adding this to the<a id="_idIndexMarker643"/> top of our <code>async_main</code> function should work:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/b-rust-futures-examples/src/main.rs (async_main2)</p>
			<pre class="source-code">
use tokio::runtime::Runtime;
async fn async_main
    <strong class="bold">let rt = Runtime::new().unwrap();</strong>
    <strong class="bold">let _guard = rt.enter();</strong>
    println!("Program starting");
    let url = "http://127.0.0.1:8080/600/HelloAsyncAwait1";
    ...</pre>			<p class="callout-heading">Note</p>
			<p class="callout">Calling <code>Runtime::new</code> creates a multithreaded Tokio runtime, but Tokio also has a single-threaded runtime that you can create by using the runtime builder like this: <code>Builder::new_current_thread().enable_all().build().unwrap()</code>. If you do that, you end up with a peculiar problem: a deadlock. The reason for that is interesting and one that you should know about.</p>
			<p class="callout">Tokio’s single-threaded runtime uses only the thread it’s called on for both the executor and the reactor. This is very similar to what we did in the first version of our runtime in <a href="B20892_08.xhtml#_idTextAnchor138"><em class="italic">Chapter 8</em></a>. We used the <code>Poll</code> instance to park our executor directly. When both our reactor and executor execute on the same thread, they must have the same mechanism to park themselves and wait for new events, which means there will be a tight coupling between them.</p>
			<p class="callout">When handling an event, the reactor has to wake up first to call <code>Waker::wake</code>, but the executor is the last one to park the thread. If the executor parked itself by calling <code>thread::park</code> (like we do), the reactor is parked as well and will never wake up since they’re running on the same thread. The only way for this to work is that the executor parks on something shared with the reactor (like we did with <code>Poll</code>). Since we’re not tightly integrated with Tokio, all we get is a deadlock.</p>
			<p>Now, if we try to <a id="_idIndexMarker644"/>run our program once more, we get the following output:</p>
			<pre class="console">
Program starting
main: 1 pending tasks. Sleep until notified.
main: 1 pending tasks. Sleep until notified.
main: 1 pending tasks. Sleep until notified.
HelloAsyncAwait1
main: 1 pending tasks. Sleep until notified.
main: 1 pending tasks. Sleep until notified.
main: 1 pending tasks. Sleep until notified.
HelloAsyncAwait2
main: All tasks are finished</pre>			<p>Okay, so now everything works as expected. The only difference is that we get woken up a few extra times, but the program finishes and produces the expected result.</p>
			<p>Before we discuss what we just witnessed, let’s do one more experiment.</p>
			<p><strong class="bold">Isahc</strong> is an HTTP client <a id="_idIndexMarker645"/>library that promises to be <em class="italic">executor agnostic</em>, meaning that it doesn’t rely on any specific executor. Let’s put that to the test.</p>
			<p>First, we add a dependency on <code>isahc</code> by typing the following:</p>
			<pre class="source-code">
cargo add isahc@1.7</pre>			<p>Then, we rewrite our <code>main</code> function so it looks like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch10/b-rust-futures-examples/src/main.rs (async_main3)</p>
			<pre class="source-code">
use isahc::prelude::*;
async fn async_main() {
    println!("Program starting");
    let url = "http://127.0.0.1:8080/600/HelloAsyncAwait1";
    let mut res = isahc::get_async(url).await.unwrap();
    let txt = res.text().await.unwrap();
    println!("{txt}");
    let url = "http://127.0.0.1:8080/400/HelloAsyncAwait2";
    let mut res = isahc::get_async(url).await.unwrap();
    let txt = res.text().await.unwrap();
    println!("{txt}");
}</pre>			<p>Now, if we run our <a id="_idIndexMarker646"/>program by writing <code>cargo run</code>, we get the following output:</p>
			<pre class="source-code">
Program starting
main: 1 pending tasks. Sleep until notified.
main: 1 pending tasks. Sleep until notified.
main: 1 pending tasks. Sleep until notified.
HelloAsyncAwait1
main: 1 pending tasks. Sleep until notified.
main: 1 pending tasks. Sleep until notified.
main: 1 pending tasks. Sleep until notified.
HelloAsyncAwait2
main: All tasks are finished</pre>			<p>So, we get the expected output without having to jump through any hoops.</p>
			<p><em class="italic">Why does all this have to be </em><em class="italic">so unintuitive?</em></p>
			<p>The answer to that brings us to the topic of common challenges that we all face when programming with async Rust, so let’s cover some of the most noticeable ones and explain the reason<a id="_idIndexMarker647"/> they exist so we can figure out how to best deal with them.</p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor188"/>Challenges with asynchronous Rust</h1>
			<p>So, while we’ve seen with our own eyes that the executor and reactor could be loosely coupled, which in turn<a id="_idIndexMarker648"/> means that you could in theory mix and match reactors and executors, the question is why do we encounter so much friction when trying to do just that?</p>
			<p>Most programmers that have used async Rust have experienced problems caused by incompatible async libraries, and we saw an example of the kind of error message you would get previously.</p>
			<p>To understand this, we have to dive a little bit deeper into the existing async runtimes in Rust, specifically those we typically use for desktop and server applications.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor189"/>Explicit versus implicit reactor instantiation</h2>
			<p class="callout-heading">Info</p>
			<p class="callout">The type of future we’ll talk about going forward is leaf futures, the kind that actually represents an I/O operation (for example, <code>HttpGetFuture</code>).</p>
			<p>When you create a runtime in Rust, you<a id="_idIndexMarker649"/> also need to create non-blocking primitives of the Rust standard library. Mutexes, channels, timers, TcpStreams, and so on are all things that need an async equivalent.</p>
			<p>Most of these can be implemented as different kinds of reactors, but the question that then comes up is: how is that reactor started?</p>
			<p>In both our own runtime and in Tokio, the reactor is started as part of the runtime initialization. We have a <code>runtime::init()</code> function that calls <code>reactor::start()</code>, and Tokio has a <code>Runtime::new()</code> and <code>Runtime::enter()</code> function.</p>
			<p>If we try to create a leaf future (the only one we created ourselves is <code>HttpGetFuture</code>) without the reactor started, both our runtime and Tokio will panic. The reactor has to be instantiated <em class="italic">explicitly</em>.</p>
			<p>Isahc, on the other hand, brings its own kind of reactor. Isahc is built on <code>libcurl</code>, a highly portable C<a id="_idIndexMarker650"/> library for <code>libcurl</code> accepts a callback that is called when an operation is ready. So, Isahc passes the waker<a id="_idIndexMarker651"/> it receives to this callback and makes sure that <code>Waker::wake</code> is called when the callback is executed. This is a bit oversimplified, but it’s essentially what happens.</p>
			<p>In practice, that means that Isahc brings its own reactor since it comes with the machinery to store wakers and call <code>wake</code> on them when an operation is ready. The reactor is started <em class="italic">implicitly</em>.</p>
			<p>Incidentally, this is also one of the major differences between <code>async_std</code> and Tokio. Tokio requires <em class="italic">explicit</em> instantiation, and <code>async_std</code> relies on <em class="italic">implicit</em> instantiation.</p>
			<p>I’m not going into so much detail on this just for fun; while this seems like a minor difference, it has a rather big impact on how intuitive asynchronous programming in Rust is.</p>
			<p>This problem mostly arises when you start programming using a different runtime than Tokio and then have to use a library that internally relies on a Tokio reactor being present.</p>
			<p>Since you can’t have two Tokio instances running on the same thread, the library can’t implicitly start a Tokio reactor. Instead, what often happens is that you try to use that library and get an error like we did in the preceding example.</p>
			<p>Now, you have to solve this by starting a Tokio reactor yourself, use some kind of compatibility wrapper created by someone else, or seeing whether the runtime you use has a built-in mechanism for running futures that rely on a Tokio reactor being present.</p>
			<p>For most people who don’t know about reactors, executors, and different kinds of leaf futures, this can be quite<a id="_idIndexMarker652"/> unintuitive and cause quite a bit of frustration.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The problem we describe here is quite common, and it’s not helped by the fact that async libraries rarely explain this well or even try to be explicit about what kind of runtime they use. Some libraries might only mention that they’re built on top of Tokio somewhere in the <code>README </code>file, and some might simply state that they’re built on top of Hyper, for example, assuming that you know that Hyper is built on top of Tokio (at least by default).</p>
			<p class="callout">But now, you know that you should check this to avoid any surprises, and if you encounter this issue, you know exactly what the problem is.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor190"/>Ergonomics versus efficiency and flexibility</h2>
			<p>Rust is good at being ergonomic <em class="italic">and</em> efficient, and that almost makes it difficult to remember that when <a id="_idIndexMarker653"/>Rust is faced with the choice between being efficient <em class="italic">or</em> ergonomic, it will choose to be efficient. Many of the most popular crates in the ecosystem echo these values, and that includes async runtimes.</p>
			<p>Some tasks can be more efficient if they’re tightly integrated with the executor, and therefore, if you use them in your library, you will be dependent on that specific runtime.</p>
			<p>Let’s take <strong class="bold">timers</strong> as an example, but <a id="_idIndexMarker654"/>task notifications where <em class="italic">Task A</em> notifies <em class="italic">Task B</em> that it can continue is another example with some of the same trade-offs.</p>
			<p class="callout-heading">Tasks</p>
			<p class="callout">We’ve used the terms tasks and futures without making the difference explicitly clear, so let’s clear that up here. We first covered tasks in <a href="B20892_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, and they still retain the same general meaning, but when talking about runtimes in Rust, they have a more specific definition. A task is a <em class="italic">top-level future</em>, the one that we spawn onto our executor. The executor schedules between different tasks. Tasks in a runtime in many ways represent the same abstraction that threads do in an OS. Every task is a future in Rust, but every future is not a task by this definition.</p>
			<p>You can think of <code>thread::sleep</code> as a timer, and we often need something like this in an asynchronous context, so our asynchronous runtime will therefore need to have a <code>sleep</code> equivalent that tells <a id="_idIndexMarker655"/>the executor to park this task for a specified duration.</p>
			<p>We could implement this as a reactor and have separate OS-thread sleep for a specified duration and then wake the correct <code>Waker</code>. That would be simple and executor agnostic since the executor is oblivious to what happens and only concern itself with scheduling the task when <code>Waker::wake</code> is called. However, it’s also not optimally efficient for all workloads (even if we used the same thread for all timers).</p>
			<p>Another, and more common, way to solve this is to delegate this task to the executor. In our runtime, this could be done by having the executor store an ordered list of instants and a corresponding <code>Waker</code>, which is used to determine whether any timers have expired before it calls <code>thread::park</code>. If none have expired, we can calculate the duration until the next timer expires and use something such as <code>thread::park_timeout</code> to make sure that we at least wake up to handle that timer.</p>
			<p>The algorithms used to store the timers can be heavily optimized and you avoid the need for one extra thread just for timers with the additional overhead of synchronization between these threads just to signal that a timer has expired. In a multithreaded runtime, there might even be contention when multiple executors frequently add timers to the same reactor.</p>
			<p>Some timers are implemented reactor-style as separate libraries, and for many tasks, that will suffice. The important point here is that by using the defaults, you end up being tied to one specific runtime, and you have to make careful considerations if you want to avoid your library being tightly coupled to a specific runtime.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor191"/>Common traits that everyone agrees about</h2>
			<p>The last topic that causes friction in<a id="_idIndexMarker656"/> async Rust is the lack of universally agreed-upon traits and interfaces for typical async operations.</p>
			<p>I want to preface this segment by pointing out that this is one area that’s improving day by day, and there is a nursery for the traits and abstractions for asynchronous Rust in the <code>futures-rs</code> crate (<a href="https://github.com/rust-lang/futures-rs">https://github.com/rust-lang/futures-rs</a>). However, since it’s still early days for async Rust, it’s something worth mentioning in a book like this.</p>
			<p>Let’s take spawning as an example. When you write a high-level async library in Rust, such as a web server, you’ll likely want to be able to spawn new tasks (top-level futures). For example, each connection to the server will most likely be a new task that you want to spawn onto the executor.</p>
			<p>Now, spawning is specific to each executor, and Rust doesn’t have a trait that defines how to spawn a task. There is a trait suggested for spawning in the <code>future-rs</code> crate, but creating a spawn trait that is both zero-cost and flexible enough to support all kinds of runtimes turns out to be very difficult.</p>
			<p>There are ways around this. The popular HTTP library Hyper (<a href="https://hyper.rs/">https://hyper.rs/</a>), for example, uses a trait to represent the executor and internally uses that to spawn new tasks. This makes it possible for users to implement this trait for a different executor and hand it back to Hyper. By implementing this trait for a different executor, Hyper will use a different spawner than its default option (which is the one in Tokio’s executor). Here is an example of how this is used for <code>async_std</code> with Hyper: <a href="https://github.com/async-rs/async-std-hyper">https://github.com/async-rs/async-std-hyper</a>.</p>
			<p>However, since there is no universal way of making this work, most libraries that rely on executor-specific functionality do one of two things:</p>
			<ol>
				<li>Choose a runtime and stick with it.</li>
				<li>Implement two versions of the library supporting different popular runtimes that users choose by enabling the correct features.</li>
			</ol>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor192"/>Async drop</h2>
			<p>Async drop, or async destructors, is an aspect of async Rust that’s somewhat unresolved at the time of writing this book. Rust uses a pattern called RAII, which means that when a type is created, so are its <a id="_idIndexMarker657"/>resources, and when a type is dropped, the resources are freed as well. The compiler automatically inserts a call to drop on objects when they go out of scope.</p>
			<p>If we take our runtime as an example, when resources are dropped, they do so in a blocking manner. This is normally not a big problem since a drop likely won’t block the executor for too long, but it isn’t always so.</p>
			<p>If we have a drop implementation that takes a long time to finish (for example, if the drop needs to manage I/O, or makes a blocking call to the OS kernel, which is perfectly legal and sometimes even unavoidable in Rust), it can potentially block the executor. So, an async drop would somehow be able to yield to the scheduler in such cases, and this is not possible at the moment.</p>
			<p>Now, this isn’t a rough edge of async Rust you’re likely to encounter as a user of async libraries, but it’s worth knowing about since right now, the only way to make sure this doesn’t cause issues is to be careful what you put in the drop implementation for types that are used in an async context.</p>
			<p>So, while this is not an extensive list of everything that causes friction in async Rust, it’s some of the points I find most noticeable and worth knowing about.</p>
			<p>Before we round off this chapter, let’s spend a little time talking about what we should expect in the future when it comes to asynchronous programming in Rust.</p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor193"/>The future of asynchronous Rust</h1>
			<p>Some of the things that make async Rust different from other languages are unavoidable. Asynchronous Rust is <a id="_idIndexMarker658"/>very efficient, has low latency, and is backed by a very strong type system due to how the language is designed and its core values.</p>
			<p>However, much of the perceived complexity today has more to do with the ecosystem and the kind of issues that result from a lot of programmers having to agree on the best way to solve different problems without any formal structure. The ecosystem gets fragmented for a while, and together with the fact that asynchronous programming is a topic that’s difficult for a lot of programmers, it ends up adding to the cognitive load associated with asynchronous Rust.</p>
			<p>All the issues and pain points I’ve mentioned in this chapter are constantly getting better. Some points that would have been on this list a few years ago are not even worth mentioning today.</p>
			<p>More and more common traits and abstractions will end up in the standard library, making async Rust more ergonomic since everything that uses them will “just work.”</p>
			<p>As different experiments and designs gain more traction than others, they become the de facto standard, and even though you will still have a lot of choices when programming <a id="_idIndexMarker659"/>asynchronous Rust, there will be certain paths to choose that cause a minimal amount of friction for those that want something that “just works.”</p>
			<p>With enough knowledge about asynchronous Rust and asynchronous programming in general, the issues I’ve mentioned here are, after all, relatively minor, and since you know more about asynchronous Rust than most programmers, I have a hard time imagining that any of these issues will cause you a lot of trouble.</p>
			<p>That doesn’t mean it’s not something worth knowing about since chances are your fellow programmers will struggle with some of these issues at some point.</p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor194"/>Summary</h1>
			<p>So, in this chapter, we did two things. First, we made some rather minor changes to our runtime so it works as an actual runtime for Rust futures. We tested the runtime using two external HTTP client libraries to learn a thing or two about reactors, runtimes, and async libraries in Rust.</p>
			<p>The next thing we did was to discuss some of the things that make asynchronous Rust difficult for many programmers coming from other languages. In the end, we also talked about what to expect going forward.</p>
			<p>Depending on how you’ve followed along and how much you’ve experimented with the examples we created along the way, it’s up to you what project to take on yourself if you want to learn more.</p>
			<p>There is an important aspect of learning that only happens when you experiment on your own. Pick everything apart, see what breaks, and how to fix it. Improve the simple runtime we created to learn new stuff.</p>
			<p>There are enough interesting projects to pick from, but here are some suggestions:</p>
			<ul>
				<li>Change out the parker implementation where we used <code>thread::park</code> with a proper parker. You can choose one from a library or create a parker yourself (I added a small bonus at the end of the <code>ch1</code><code>0</code> folder called <code>parker-bonus</code> where you get a simple parker implementation).</li>
				<li>Implement a simple <code>delayserver</code> using the runtime you’ve created yourself. To do this, you have to be able to write some raw HTTP responses and create a simple server. If you went through the free introductory book called <em class="italic">The Rust Programming Language</em>, you created a simple server in one of the last chapters (<a href="https://doc.rust-lang.org/book/ch20-02-multithreaded.html">https://doc.rust-lang.org/book/ch20-02-multithreaded.html</a>), which gives you the basics you need. You also need to create a timer as we discussed above or use an existing crate for async timers.</li>
				<li>You can create a “proper” multithreaded runtime and explore the possibilities that come with having a global task queue, or as an alternative, implement a work-stealing scheduler that can steal tasks from other executors’ local queues when they’re done with their own.</li>
			</ul>
			<p>Only your imagination sets the limits on what you can do. The important thing to note is that there is a certain joy in doing something just because you can and just for fun, and I hope that you get some of the same enjoyment from this as I do.</p>
			<p>I’ll end this chapter with a few words on how to make your life as an asynchronous programmer as easy as possible.</p>
			<p>The first thing is to realize that an async runtime is not just another library that you use. It’s extremely invasive and impacts almost everything in your program. It’s a layer that rewrites, schedules tasks, and reorders the program flow from what you’re used to.</p>
			<p>My clear recommendation if you’re not specifically into learning about runtimes, or have very specific needs, is to pick one runtime and stick to it for a while. Learn everything about it – not necessarily <em class="italic">everything</em> from the start, but as you need more and more functionality from it, you will learn everything eventually. This is almost like getting comfortable with everything in Rust’s standard library.</p>
			<p>What runtime you start with depends a bit on what crates you’re using the most. Smol and <code>async-std</code> share a lot of implementation details and will behave similarly. Their big selling point is that their API strives to stay as close as possible to the standard library. Combined with the fact that the reactors are instantiated implicitly, this can result in a slightly more intuitive experience and a more gentle learning curve. Both are production-quality runtimes and see a lot of use. Smol was originally created with the goal of having a code base that’s easy for programmers to understand and learn from, which I think is true today as well.</p>
			<p>With that said, the most popular alternative for users looking for a general-purpose runtime at the time of writing is <strong class="bold">Tokio</strong> (<a href="https://tokio.rs/">https://tokio.rs/</a>). Tokio is one of the oldest async runtimes in Rust. It is actively developed and has a welcoming and active community. The documentation is excellent. Being one of the most popular runtimes also means there is a good chance that you’ll find a library that does exactly what you need with support for Tokio out of the box. Personally, I tend to reach for Tokio for the reasons mentioned, but you can’t really go wrong with either of these runtimes unless you have very specific requirements.</p>
			<p>Finally, let’s not forget to mention the <code>futures-rs</code> crate (<a href="https://github.com/rust-lang/futures-rs">https://github.com/rust-lang/futures-rs</a>). I mentioned this crate earlier, but it’s really useful to know about as it contains several traits, abstractions, and executors (<a href="https://docs.rs/futures/latest/futures/executor/index.html">https://docs.rs/futures/latest/futures/executor/index.html</a>) for async Rust. It serves the purpose of an async toolbox that comes in handy in many situations.</p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor195"/>Epilogue</h1>
			<p>So, you have reached the end. First of all, congratulations! You’ve come to the end of quite a journey!</p>
			<p>We started by talking about concurrency and parallelism in <a href="B20892_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>. We even covered a bit about the history, CPUs and OSs, hardware, and interrupts. In <a href="B20892_02.xhtml#_idTextAnchor043"><em class="italic">Chapter 2</em></a>, we discussed how programming languages modeled asynchronous program flow. We introduced coroutines and how stackful and stackless coroutines differ. We discussed OS threads, fibers/green threads, and callbacks and their pros and cons.</p>
			<p>Then, in <a href="B20892_03.xhtml#_idTextAnchor063"><em class="italic">Chapter 3</em></a>, we took a look at OS-backed event queues such as <code>epoll</code>, <code>kqueue</code>, and IOCP. We even took quite a deep dive into syscalls and cross-platform abstractions.</p>
			<p>In <a href="B20892_04.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>, we hit some quite difficult terrain when implementing our own mio-like event queue using epoll. We even had to learn about the difference between edge-triggered and level-triggered events.</p>
			<p>If <a href="B20892_04.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a> was somewhat rough terrain, <a href="B20892_05.xhtml#_idTextAnchor092"><em class="italic">Chapter 5</em></a> was more like climbing Mount Everest. No one expects you to remember everything covered there, but you read through it and have a working example you can use to experiment with. We implemented our own fibers/green threads, and while doing so, we learned a little bit about processor architectures, ISAs, ABIs, and calling conventions. We even learned quite a bit about inline assembly in Rust. If you ever felt insecure about the stack versus heap difference, you surely understand it now that you’ve created stacks that we made our CPU jump to ourselves.</p>
			<p>In <em class="italic">Chapter 6</em>, we got a high-level introduction to asynchronous Rust, before we took a deep dive from <a href="B20892_07.xhtml#_idTextAnchor122"><em class="italic">Chapter 7</em></a> and onward, starting with creating our own coroutines and our own <code>coroutine/wait</code> syntax. In <a href="B20892_08.xhtml#_idTextAnchor138"><em class="italic">Chapter 8</em></a>, we created the first versions of our own runtime while discussing basic runtime design. We also deep-dived into reactors, executors, and wakers.</p>
			<p>In <a href="B20892_09.xhtml#_idTextAnchor156"><em class="italic">Chapter 9</em></a>, we improved our runtime and discovered the dangers of self-referential structs in Rust. We then took a thorough look at pinning in Rust and how that helped us solve the problems we got into.</p>
			<p>Finally, in <a href="B20892_10.xhtml#_idTextAnchor178"><em class="italic">Chapter 10</em></a>, we saw that by making some rather minor changes, our runtime became a fully functioning runtime for Rust futures. We rounded everything off by discussing some well-known challenges with asynchronous Rust and some expectations for the future.</p>
			<p>The Rust community is very inclusive and welcoming, and we’d happily welcome you to engage and contribute if you find this topic interesting and want to learn more. One of the ways asynchronous Rust gets better is through contributions by people with all levels of experience. If you want to get involved, then the async work group (<a href="https://rust-lang.github.io/wg-async/welcome.html">https://rust-lang.github.io/wg-async/welcome.html</a>) is a good place to start. There is also a very active community centered around the Tokio project (<a href="https://github.com/tokio-rs/tokio/blob/master/CONTRIBUTING.md">https://github.com/tokio-rs/tokio/blob/master/CONTRIBUTING.md</a>), and many, many more depending on what specific area you want to dive deeper into. Don’t be afraid to join the different channels and ask questions.</p>
			<p>Now that we’re at the end I want to thank you for reading all the way to the end. I wanted this book to feel like a journey we took together, not like a lecture. I wanted you to be the focus, not me.</p>
			<p>I hope I succeeded with that, and I genuinely hope that you learned something that you find useful and can take with you going forward. If you did, then I’m sincerely happy that my work was of value to you. I wish you the best of luck with your asynchronous programming going forward.</p>
			<p>Until the next time!</p>
			<p>Carl Fredrik</p>
		</div>
	</body></html>
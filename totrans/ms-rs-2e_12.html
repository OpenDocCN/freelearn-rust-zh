<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Network Programming in Rust</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll take a look at what Rust has to offer for network programming. We'll start by exploring existing networking primitives in the standard library by building a simple Redis clone. This will help us get familiar with the default synchronous network I/O model and its limitations. Next, we'll explain how asynchrony is a better approach when dealing with network I/O on a large scale. In the process, we'll get to know about the abstractions provided by the Rust ecosystem for building asynchronous network applications and refactor our Redis server to make it asynchronous using third-party crates.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Network programming prelude</li>
<li>Synchronous network I/O</li>
<li>Building a simple Redis server</li>
<li>Asynchronous network I/O</li>
<li>An introduction to <kbd>futures</kbd> and <kbd>tokio</kbd> crates</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network programming prelude</h1>
                </header>
            
            <article>
                
<div class="packt_quote">"A program is like a poem: you cannot write a poem without writing it."</div>
<p class="CDPAlignRight CDPAlign"><span>– <em>E. W. Dijkstra</em></span></p>
<p>Building a medium through which machines can communicate with each other over the internet is a complicated task. There are different kinds of devices that communicate over the internet, running different OS and different versions of applications, and they need a set of agreed upon rules to exchange messages with one another. These rules of communication are called network protocols and the messages devices send to each other are referred to as network packets.</p>
<p class="mce-root"/>
<p>For the separation of concerns of various aspects, such as reliability, discoverability, and encapsulation, these protocols are divided into layers with higher-layer protocols stacked over the lower-layers. Each network packet is composed of information from all of these layers. These days, modern operating systems already ship with a network protocol stack implementation. In this implementation, each layer provides support for the layers above it.</p>
<p>At the lowest layer, we have the Physical layer and the Data Link layer protocol for specifying how packets are transmitted through wires across nodes on the internet and how they move in and out of network cards in computers. The protocols on this layer are the Ethernet and Token Ring protocols. Above that, we have the IP layer, which employs the concept of unique IDs, called IP addresses, to identify nodes on the internet. Above the IP layer, we have the Transport layer, which is a protocol that provides point-to-point delivery between two processes on the internet. Protocols such as TCP and UDP exist at this layer. Above the Transport layer, we have Application layer protocols such as HTTP and FTP, both of which are used to build rich applications. This allows for a higher level of communication, such as a chat application running on mobile devices. The entire protocol stack works in tandem to facilitate these kinds of complex interactions between applications running on computers, spread across the internet.</p>
<p>With devices connecting to each other over the internet and sharing information, distributed application architectures started to proliferate. Two models emerged: the decentralized model, popularly known as the peer-to-peer model, and the centralized model, which is widely known as the client-server model. The later is more common out of the two these days. Our focus in this chapter will be on the client-server model of building network applications, especially on the Transport layer.</p>
<p>In major operating systems, the Transport layer of the network stack is exposed to developers under a family of APIs named <strong>Sockets</strong>. It includes a set of interfaces, which are used to set up a communication link between two processes. Sockets allow you to communicate data back and forth between two processes, either locally or remotely, without requiring the developer to have an understanding of the underlying network protocol.</p>
<p>The Socket API's roots lie in the <strong>Berkley Software Distribution (BSD)</strong>, which was the first operating system to provide a networking stack implementation with a socket API in 1983. It serve as the reference implementation for networking stacks in major operating systems today. In Unix-like systems, a socket follows the same philosophy of <em>everything is a file</em> and exposes a file descriptor API. This means that one can read and write data from a socket just like files.</p>
<p class="mce-root"/>
<div class="packt_infobox">Sockets are file descriptors (an integer) that point to a descriptor table of the process that's managed by the kernel. The descriptor table contains a mapping of file descriptors to <strong>file entry</strong> structures, which contains the actual buffer for the data that's sent to the socket.</div>
<p>The Socket API acts primarily at the TCP/IP layer. On this layer, the sockets that we create are categorized on various levels:</p>
<ul>
<li><strong>Protocol</strong>: Depending on the protocol, we can either have a TCP socket or a UDP socket. TCP is a stateful streaming protocol that provides the ability to deliver messages in a reliable fashion, whereas UDP is a stateless and unreliable protocol.</li>
<li><strong>Communication kind</strong>: Depending on whether we are communicating with processes on the same machine or processes on remote machines, we can either have internet sockets or Unix domain sockets<em>.</em> Internet sockets are used for exchanging messages between processes on remote machines. It is represented by a tuple of an IP address and a port. Two processes that want to communicate remotely must use IP sockets. Unix domain sockets are used for communication between processes that run on the same machine. Here, instead of an IP address-port pair, it takes a filesystem path. For instance, databases use Unix domain sockets to expose connection endpoints.</li>
<li><strong>I/O model</strong>: Depending on how we read and write data to a socket, we can create sockets of two kinds: blocking sockets and non-blocking sockets.</li>
</ul>
<p>Now that we know more about sockets, let's explore the client-server model a bit more. In this model of networking, the usual flow of setting up two machines to communicate with each other follows this process: the server creates a socket and binds it to an IP address-port pair before specifying a protocol, which can be TCP or UDP. It then starts listening for connections from clients. The client, on the other hand, creates a connecting socket and connects to the given IP address and port. In Unix, processes can create a socket using the <kbd>socket</kbd> system. This call gives back a file descriptor that the program can use to perform read and write calls to the client or to the server.</p>
<p>Rust provides us with the <kbd>net</kbd> module in the standard library. This contains the aforementioned networking primitives on the Transport layer. For communicating over TCP, we have the <kbd>TcpStream</kbd> and <kbd>TcpListener</kbd> types. For communicating over UDP, we have the <kbd>UdpSocket</kbd> type. The <kbd>net</kbd> module also provides proper data types for representing IP addresses and supports both v4 and v6 versions.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Building network applications that are reliable involves several considerations. If you are okay with few of the packets getting dropped between message exchanges, you can go with UDP sockets, but if you cannot afford to have packets dropped or want to have message delivery in sequence, you must use TCP sockets. The UDP protocol is fast and came much later to cater to needs where you require minimal latency in the delivery of packets and can deal with a few packets being dropped. For example, a video chat application uses UDP, but you aren't particularly affected if a few of the frames drop from the video stream. UDPs are used in cases where you are tolerant of no delivery guarantees. We'll focus our discussion on TCP sockets in this chapter, as it's the most used protocol by the majority of network applications that need to be reliable.</p>
<p>Another factor to consider, is how well and efficient your application is able to serve clients. From a technical standpoint, this translates to choosing the I/O model of sockets.</p>
<div class="packt_infobox">I/O is an acronym for Input/Output, and in this context, it is a catch-all phrase that simply denotes reading and writing bytes to sockets.</div>
<p>Choosing between blocking and non-blocking sockets changes its architecture, the way we write our code, and how it scales to clients. Blocking sockets give you a <strong>synchronous I/O</strong> model, while non-blocking sockets let you do <strong>asynchronous I/O</strong>. On platforms that implement the Socket API, such as Unix, sockets are created in blocking mode by default. This entails the default I/O model in major network stacks following the synchronous model. Let's explore both of these models next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Synchronous network I/O</h1>
                </header>
            
            <article>
                
<p>As we said previously, a socket is created in blocking mode by default. A server in blocking mode is synchronous in the sense that each read and write call on the socket blocks and waits until it is complete. If another client tries to connect to the server, it needs to wait until the server is done serving the previous client. This is to say that until the TCP read and write buffers are full, your application blocks on the respective I/O operation and any new client connections must wait until the buffers are empty and full again.</p>
<div class="packt_infobox">The TCP protocol implementation contains its own read and write buffers on the kernel level, apart from the application maintaining any buffers of its own.</div>
<p class="mce-root"/>
<p>Rust's standard library networking primitives provide the same synchronous API for sockets. To see this model in action, we'll implement something more than an echo server. We'll build a stripped down version of Redis. Redis is a data structure server and is often used as an in-memory data store. Redis clients and servers speak the RESP protocol, which is a simple line-based protocol. While the protocol is agnostic of TCP or UDP, Redis implementations mostly use the TCP protocol. TCP is a stateful stream-based protocol with no way for servers and clients to identify how many bytes to read from the socket to construct a protocol message. To account for that, most protocols follow this pattern of using a length byte, followed by the same length of payload bytes.</p>
<p>A message in the RESP protocol is similar to most line-based protocols in TCP, with the initial byte being a marker byte followed by the length of the payload, followed by the payload itself. The message ends with a terminating marker byte. The RESP protocol supports various kinds of messages, ranging from simple strings, integers, arrays, and bulk strings and so on. A message in the RESP protocol ends with a <kbd>\r\n</kbd> byte sequence. For instance, a success message from the server to the client is encoded and sent as <kbd>+OK\r\n</kbd> (without quotes). <kbd>+</kbd> indicates a success reply, and then follows the strings. The command ends with <kbd>\r\n</kbd>. To indicate if a query has failed, the Redis server replies with <kbd>-Nil\r\n</kbd>.</p>
<p>Commands such as <kbd>get</kbd> and <kbd>set</kbd> are sent as arrays of bulk strings. For instance, a <kbd>get foo</kbd> command will be sent as follows:</p>
<pre class="graf graf--pre graf-after--p">*2\r\n$3\r\n<strong>get</strong>\r\n$3\r\n<strong>foo</strong>\r\n</pre>
<p>In the preceding message, <kbd>*2</kbd> indicates that we have an array of <kbd>2</kbd> commands and is delimited by <kbd>\r\n</kbd>. Following that, <kbd>$3</kbd> indicates that we have a string of length <kbd>3</kbd>, i.e., the <kbd>GET</kbd> command followed by a <kbd>$3</kbd> for the string <kbd>foo</kbd>. The command ends with <kbd>\r\n</kbd>. That's the basics on RESP. We don't have to worry about the low-level details of parsing RESP messages, as we'll be using a fork of a crate called <kbd>resp</kbd> to parse incoming byte streams from our client into a valid RESP message.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a synchronous redis server</h1>
                </header>
            
            <article>
                
<p>To make this example short and easy to follow, our Redis clone will implement a very small subset of the RESP protocol and will be able to process only <kbd>SET</kbd> and <kbd>GET</kbd> calls. We'll use the official <kbd>redis-cli</kbd> that comes with the official <em>Redis</em> package to make queries against our server. To use the <kbd>redis-cli</kbd>, we can install Redis on Ubuntu by running <kbd>apt-get install redis-server.</kbd></p>
<p class="mce-root"/>
<p>Let's create a new project by running <kbd>cargo new rudis_sync</kbd> and adding the following dependencies in our <kbd>Cargo.toml</kbd> file:</p>
<pre>rudis_sync/Cargo.toml<br/><br/>[dependencies]<br/>lazy_static = "1.2.0"<br/>resp = { git = "https://github.com/creativcoder/resp" }</pre>
<p>We have named our project <kbd>rudis_sync</kbd>. We depend on two crates:</p>
<ul>
<li><kbd>lazy_static</kbd>: We'll use this to store our in-memory database.</li>
<li><kbd>resp</kbd>: This is a forked crate that resides on my GitHub repository. We'll use this to parse the stream of bytes from the client.</li>
</ul>
<p>To make the implementation easier to follow, <kbd>rudis_sync</kbd> has very minimal error-handling integration. When you are done experimenting with the code, you are encouraged to implement better error-handling strategies.</p>
<p>Let's start with the contents of our <kbd>main.rs</kbd> file:</p>
<pre>// rudis_sync/src/main.rs<br/><br/>use lazy_static::lazy_static;<br/>use resp::Decoder;<br/>use std::collections::HashMap;<br/>use std::env;<br/>use std::io::{BufReader, Write};<br/>use std::net::Shutdown;<br/>use std::net::{TcpListener, TcpStream};<br/>use std::sync::Mutex;<br/>use std::thread;<br/><br/>mod commands;<br/>use crate::commands::process_client_request;<br/><br/>type STORE = Mutex&lt;HashMap&lt;String, String&gt;&gt;;<br/><br/>lazy_static! {<br/>    static ref RUDIS_DB: STORE = Mutex::new(HashMap::new());<br/>}<br/><br/>fn main() {<br/>    let addr = env::args()<br/>        .skip(1)<br/>        .next()<br/>        .unwrap_or("127.0.0.1:6378".to_owned());<br/><br/>    let listener = TcpListener::bind(&amp;addr).unwrap();<br/>    println!("rudis_sync listening on {} ...", addr);<br/><br/>    for stream in listener.incoming() {<br/>        let stream = stream.unwrap();<br/>        println!("New connection from: {:?}", stream);<br/>        handle_client(stream);<br/>    }<br/>}</pre>
<p>We have a bunch of imports, followed by an in-memory <kbd>RUDIS_DB</kbd> hashmap that's declared in a <kbd>lazy_static!</kbd> block. We are using this as an in-memory database to store key and value pairs that are sent by clients. In our <kbd>main</kbd> function, we create a listening address in <kbd>addr</kbd> from the user-provided argument or use <kbd>127.0.0.1:6378</kbd> as the default. We then create a <kbd>TcpListener</kbd> instance by calling the associated <kbd>bind</kbd> method, passing the <kbd>addr</kbd>. This creates a TCP listening socket. Later, we call the <kbd>incoming</kbd> method on <kbd>listener</kbd>, which then returns an iterator of new client connections. For each client connection <kbd>stream</kbd> that is of the <kbd>TcpStream</kbd> type (a client socket), we call the <kbd>handle_client</kbd> function, passing in the <kbd>stream</kbd>.</p>
<p>In the same file, the <kbd>handle_client</kbd> function is responsible for parsing queries that are sent from the client, which would be one of the <kbd>GET</kbd> or <kbd>SET</kbd> queries:</p>
<pre>// rudis_sync/src/main.rs<br/><br/>fn handle_client(stream: TcpStream) {<br/>    let mut stream = BufReader::new(stream);<br/>    let decoder = Decoder::new(&amp;mut stream).decode();<br/>    match decoder {<br/>        Ok(v) =&gt; {<br/>            let reply = process_client_request(v);<br/>            stream.get_mut().write_all(&amp;reply).unwrap();<br/>        }<br/>        Err(e) =&gt; {<br/>            println!("Invalid command: {:?}", e);<br/>            let _ = stream.get_mut().shutdown(Shutdown::Both);<br/>        }<br/>    };<br/>}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The <kbd>handle_client</kbd> function receives the client <kbd>TcpStream</kbd> socket in the <kbd>stream</kbd> variable. We wrap our client <kbd>stream</kbd> in a <kbd>BufReader</kbd>, which is then passed as a mutable reference to the <kbd>Decoder::new</kbd> method from the <kbd>resp</kbd> crate. The <kbd>Decoder</kbd> reads bytes from the <kbd>stream</kbd> to create a RESP <kbd>Value</kbd> type. We then have a match block to check whether our decoding succeeded. If it fails, we print an error message and close the socket by calling <kbd>shutdown()</kbd> and requesting both the reader part and writer part of our client socket connection to be closed with the <kbd>Shutdown::Both</kbd> value. The <kbd>shutdown</kbd> method needs a mutable reference, so we call <kbd>get_mut()</kbd> before that. In a real-world implementation, you obviously need to handle this error gracefully.</p>
<p>If the decoding succeeds, we call <kbd>process_client_request</kbd>, which returns the <kbd>reply</kbd> to send back to the client. We write this <kbd>reply</kbd> to the client by calling <kbd>write_all</kbd> on the client <kbd>stream</kbd>. The <kbd>process_client_request</kbd> function is defined in <kbd>commands.rs</kbd> as follows:</p>
<pre>// rudis_sync/src/commands.rs<br/><br/>use crate::RUDIS_DB;<br/>use resp::Value;<br/><br/>pub fn process_client_request(decoded_msg: Value) -&gt; Vec&lt;u8&gt; {<br/>    let reply = if let Value::Array(v) = decoded_msg {<br/>        match &amp;v[0] {<br/>            Value::Bulk(ref s) if s == "GET" || s == "get" =&gt; handle_get(v),<br/>            Value::Bulk(ref s) if s == "SET" || s == "set" =&gt; handle_set(v),<br/>            other =&gt; unimplemented!("{:?} is not supported as of now", other),<br/>        }<br/>    } else {<br/>        Err(Value::Error("Invalid Command".to_string()))<br/>    };<br/><br/>    match reply {<br/>        Ok(r) | Err(r) =&gt; r.encode(),<br/>    }<br/>}<br/><br/></pre>
<p>This function takes the decoded <kbd>Value</kbd> and matches it on the parsed query. In our implementation, we expect the client to send an array of bulk strings, so we match on the <kbd>Value::Array</kbd> variant of <kbd>Value</kbd>, using <kbd>if let</kbd>, and store the array in <kbd>v</kbd>. If we match as an <kbd>Array</kbd> value in the <kbd>if</kbd> branch, we take that array and match on the first entry in <kbd>v</kbd>, which will be our command type, that is, <kbd>GET</kbd> or <kbd>SET</kbd>. This is again a <kbd>Value::Bulk</kbd> variant that wraps the command as a string.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>We take the reference to the inner string as <kbd>s</kbd> and match only if the string has a <kbd>GET</kbd> or <kbd>SET</kbd> as a value. In the case of <kbd>GET</kbd>, we call <kbd>handle_get</kbd>, passing the <kbd>v</kbd> array, and in the case of <kbd>SET</kbd>, we call <kbd>handle_set</kbd>. In the <kbd>else</kbd> branch, we simply send a <kbd>Value::Error</kbd> reply to the client with <kbd>invalid Command</kbd> as the description.</p>
<p>The value that's returned by both branches is assigned to the <kbd>reply</kbd> variable. It is then matched for the inner type <kbd>r</kbd> and turned into <kbd>Vec&lt;u8&gt;</kbd> by invoking the <kbd>encode</kbd> method on it, which is then returned from the function.</p>
<p>Our <kbd>handle_set</kbd> and <kbd>handle_get</kbd> functions are defined in the same file as follows:</p>
<pre>// rudis_sync/src/commands.rs<br/><br/>use crate::RUDIS_DB;<br/>use resp::Value;<br/><br/>pub fn handle_get(v: Vec&lt;Value&gt;) -&gt; Result&lt;Value, Value&gt; {<br/>    let v = v.iter().skip(1).collect::&lt;Vec&lt;_&gt;&gt;();<br/>    if v.is_empty() {<br/>        return Err(Value::Error("Expected 1 argument for GET command".to_string()))<br/>    }<br/>    let db_ref = RUDIS_DB.lock().unwrap();<br/>    let reply = if let Value::Bulk(ref s) = &amp;v[0] {<br/>        db_ref.get(s).map(|e| Value::Bulk(e.to_string())).unwrap_or(Value::Null)<br/>    } else {<br/>        Value::Null<br/>    };<br/>    Ok(reply)<br/>}<br/><br/>pub fn handle_set(v: Vec&lt;Value&gt;) -&gt; Result&lt;Value, Value&gt; {<br/>    let v = v.iter().skip(1).collect::&lt;Vec&lt;_&gt;&gt;();<br/>    if v.is_empty() || v.len() &lt; 2 {<br/>        return Err(Value::Error("Expected 2 arguments for SET command".to_string()))<br/>    }<br/>    match (&amp;v[0], &amp;v[1]) {<br/>        (Value::Bulk(k), Value::Bulk(v)) =&gt; {<br/>            let _ = RUDIS_DB<br/>                .lock()<br/>                .unwrap()<br/>                .insert(k.to_string(), v.to_string());<br/>        }<br/>        _ =&gt; unimplemented!("SET not implemented for {:?}", v),<br/>    }<br/><br/>    Ok(Value::String("OK".to_string()))<br/>}</pre>
<p>In <kbd>handle_get()</kbd>, we first check whether the <kbd>GET</kbd> command has no key present in the query and fails with an error message. Next, we match on <kbd>v[0]</kbd>, which is the key for the <kbd>GET</kbd> command, and check whether it exists in our database. If it exists, we wrap it in <kbd>Value::Bulk</kbd> using the map combinator, otherwise we return a <kbd>Value::Null</kbd> reply:</p>
<pre>db_ref.get(s).map(|e| Value::Bulk(e.to_string())).unwrap_or(Value::Null)</pre>
<p>We then store it in a <kbd>reply</kbd> variable and return it as a <kbd>Result</kbd> type, that is, <kbd>Ok(reply)</kbd>.</p>
<p>A similar thing happens in <kbd>handle_set</kbd>, where we bail out if we don't have enough arguments to the <kbd>SET</kbd> command. Next, we match on our key and value using <kbd>&amp;v[0]</kbd> and <kbd>&amp;v[1]</kbd> and insert it into <kbd>RUDIS_DB</kbd>. As an acknowledgement of the <kbd>SET</kbd> query., we reply with <kbd>Ok</kbd>.</p>
<p>Back in our <kbd>process_client_request</kbd> function, once we create the reply bytes, we match on the <kbd>Result</kbd> type and convert them into a <kbd>Vec&lt;u8&gt;</kbd> by calling <kbd>encode()</kbd>, which is then written to the client. With that walk-through out of the way, it's time to test our client with the official <kbd>redis-cli</kbd> tool. We'll run it by invoking <kbd>redis-cli -p 6378</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d782e87d-c9b7-4271-b22d-730d1142a9c0.png" style="width:28.50em;height:7.67em;"/></p>
<p>In the preceding session, we did a few <kbd>GET</kbd> and <kbd>SET</kbd> queries with an expected reply from <kbd>rudis_sync</kbd>. Also, here's our output log from the <kbd>rudis_server</kbd> of our new connection(s):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/25c5a814-0b1d-4c5e-a664-4ff20689f2b8.png" style="width:52.58em;height:5.25em;"/></p>
<p class="mce-root"/>
<p>But the problem with our server is that we have to wait until the initial client has finished being served. To demonstrate this, we'll introduce a bit of delay in our <kbd>for</kbd> loop that handles new client connections:</p>
<pre>    for stream in listener.incoming() {<br/>        let stream = stream.unwrap();<br/>        println!("New connection from: {:?}", stream);<br/>        handle_client(stream);<br/>        <strong>thread::sleep(Duration::from_millis(3000));</strong><br/>    }</pre>
<p>The <kbd>sleep</kbd> call simulates a delay in request processing. To see the latencies, we'll start two clients at almost the same time, where one of them makes a <kbd>SET</kbd> request and the other one makes a <kbd>GET</kbd> request on the same key. Here's our first client, which does the <kbd>SET</kbd> request:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8266832f-8fdc-4618-bdd9-93013e591282.png" style="width:32.50em;height:4.25em;"/></p>
<p>Here's our second client, which does a <kbd>GET</kbd> request on the same key, <kbd>foo</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0dbc71fc-b4e1-489e-acb4-725744aa02e2.png" style="width:33.50em;height:5.50em;"/></p>
<p class="mce-root"/>
<p>As you can see, the second client had to wait for almost three seconds to get the second <kbd>GET</kbd> reply.</p>
<p>Due to its nature, the synchronous mode becomes a bottleneck when you need to process more than 100,000 (say) clients at the same time, with each client taking varying amounts of processing time. To get around this, you usually need to spawn a thread for handling each client connection. Whenever a new client connection is made, we spawn a new thread and offload the <kbd>handle_client</kbd> invocation from the main thread, allowing the main thread to accept other client connections. We can achieve this by using a single line change in our <kbd>main</kbd> function, like so:</p>
<pre>    for stream in listener.incoming() {<br/>        let stream = stream.unwrap();<br/>        println!("New connection from: {:?}", stream);<strong><br/>        thread::spawn(|| handle_client(stream));</strong><br/>    }</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>This removes the blocking nature of our server, but introduces the overhead of spawning a new thread every time a new client connection is received. First, there is an overhead of spawning threads and, second, the context switch time between threads adds another overhead.</p>
<p>As we can see, our <kbd>rudis_sync</kbd> server works as expected. But it will soon be bottlenecked by the amount of threads our machine can handle. This threading model of handling connections worked well until the internet began gaining a wider audience and more and more clients connecting to the internet became the norm. Today, however, things are different and we need highly efficient servers that can handle millions of requests at the same time. It turns out that we can tackle the problem of handling more clients on a more foundational level, that is, by using non-blocking sockets. Let's explore them next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Asynchronous network I/O</h1>
                </header>
            
            <article>
                
<p>As we saw in our <kbd>rudis_sync</kbd> server implementation, the synchronous I/O model can be a major bottleneck in handling multiple clients in a given period of time. One has to use threads to process more clients. However, there's a better way to scale our server. Instead of coping with the blocking nature of sockets, we can make our sockets non-blocking. With non-blocking sockets, any read, write, or connect operation, on the socket will return immediately, regardless of whether the operation completed successfully or not, that is, they don't block the calling code if the read and write buffers are partially filled. This is the asynchronous I/O model as no client needs to wait for their request completion, and is instead notified later of the completion or failure of the request.</p>
<p>The asynchronous model is very efficient compared to threads, but it adds more complexity to our code. In this model, because an initial read or write call on the socket is unlikely to succeed, we need to retry the interested operation again at a later time. This process of retrying the operation on the socket is called polling. We need to poll the sockets from time to time to see if any of our read/write/connect operations can be completed and also maintain state on how many bytes we have read or written so far. With large number of incoming socket connections, using non-blocking sockets entails having to deal with polling and maintenance of state.This soon blows up as a complex state machine. In addition to that polling  is a very in-efficient operation. Even if we don't have any events on our sockets. There are better approaches, though.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>On Unix-based platforms, polling mechanism on sockets is done through <kbd>poll</kbd> and <kbd>select</kbd> system calls, which are available on all Unix platforms. Linux has a better <kbd>epoll</kbd> API in addition to them. Instead of polling the sockets by ourselves, which is an inefficient operation, these APIs can tell us when the socket is ready to read or write. Where poll and select run a for loop on each requested socket, <kbd>epoll</kbd> runs in <kbd>O(1)</kbd> to notify any interested party of a new socket event.</p>
<p>The asynchronous I/O model allows you to handle a considerably larger amount of sockets than would be possible with the synchronous model, because we are doing operations in small chunks and quickly switching to serving other clients. Another efficiency is that we don't need to spawn threads, as everything happens in a single thread.</p>
<p>To write asynchronous network applications with non-blocking sockets, we have several high quality crates in the Rust ecosystem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Async abstractions in Rust</h1>
                </header>
            
            <article>
                
<p>The async network I/O is advantageous, but programming them in their raw form is hard. Fortunately, Rust provides us with convenient abstractions in the form of third-party crates for working with asynchronous I/O. It alleviates the developer from most of the complex state machine handling when dealing with non-blocking sockets and the underlying socket polling mechanism. Two of the lower-layer abstractions that are available as crates are the <kbd>futures</kbd> and <kbd>mio</kbd> crates. Let's understand them in brief.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mio</h1>
                </header>
            
            <article>
                
<p>When working with non-blocking sockets, we need a way to check whether the socket is ready for the desired operation. The situation is worse when we have thousands, or more, sockets to manage. We can use the very inefficient way of running a loop, checking for the socket state, and performing the operation once it's ready. But there are better ways to do this. In Unix, we had the <kbd>poll</kbd> system call, to which you give the list of file descriptors you want to be monitored for events. It was then replaced by the <kbd>select</kbd> system call, which improved things a bit. However, both <kbd>select</kbd> and <kbd>poll</kbd> were not scalable as they were basically for loops under the hood and the iteration time went up linearly as more and more sockets were added to its monitor list.</p>
<p>Under Linux, then came <kbd>epoll</kbd>, which is the current and most efficient file descriptor multiplexing API. This is used by most network and I/O applications that want to do asynchronous I/O. Other platforms have similar abstractions, such as <strong>kqueue</strong> in macOS and BSD. On Windows, we have <strong>IO Completion Ports (IOCP)</strong>.</p>
<p class="mce-root"/>
<p>It is these low-level abstractions that <kbd>mio</kbd> abstracts over, providing a cross-platform, highly efficient interface to all of these I/O multiplexing APIs. Mio is quite a low-level library, but it provides a convenient way to set up a reactor for socket events. It provides the same kind of networking primitives such as the <kbd>TcpStream</kbd> type as the standard library does, but these are non-blocking by default.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Futures</h1>
                </header>
            
            <article>
                
<p>Juggling with mio's socket polling state machine is not very convenient. To provide a higher-level API that can be used by application developers, we have the <kbd>futures</kbd> crate. The <kbd>futures</kbd> crate provides a trait named <kbd>Future</kbd>, which is the core component of the crate. A <kbd>future</kbd> represents the idea of a computation that is not immediately available, but might be available later. Let's look at its type signature of the <kbd>Future</kbd> trait to get more information about it:</p>
<pre>pub trait Future {<br/>    type Item;<br/>    type Error;<br/>    fn poll(&amp;mut self) -&gt; Poll&lt;Self::Item, Self::Error&gt;;<br/>}</pre>
<p>A <kbd>Future</kbd> is an associated type trait that defines two types: an <kbd>Item</kbd> type representing the value that the <kbd>Future</kbd> will resolve to and an <kbd>Error</kbd> type that specifies what error type the future will fail with. They are quite similar to the <kbd>Result</kbd> type in the standard library, but instead of getting the result right away, they don't compute the immediately.</p>
<p>A <kbd>Future</kbd> value on its own cannot be used to build asynchronous applications. You need some kind of reactor and an event loop to progress the future toward completion. By design, the only way to have them succeed with a value or fail with an error is to poll them. This operation is represented by the single require method known as <kbd>poll</kbd>. The method <kbd>poll</kbd> specifies what should be done to progress the future. A future can be composed of several things, chained one after another. To progress a future, we need a reactor and an event loop implementation, and that is provided by the <kbd>tokio</kbd> crate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tokio</h1>
                </header>
            
            <article>
                
<p>Combining both of the above mentioned abstractions, and along a work stealing scheduler, event loop and a timer implementation we have the <kbd>tokio</kbd> crate, which provides a runtime for driving these futures to completion. With the <kbd>tokio</kbd> framework, you can spawn many futures and have them run concurrently.</p>
<p class="mce-root"/>
<p>The <kbd>tokio</kbd> crate was born to provide a go-to solution for building robust and high-performance asynchronous networking applications that are agnostic of the protocol, yet provides abstractions for general patterns that are common in all networking applications. The <kbd>tokio</kbd> crate is technically a runtime consisting of a thread pool, and event loop, and a reactor for I/O events based on mio. By runtime, we mean that every web application developed with tokio will have the mentioned components above running as part of the application.</p>
<p>Futures in the tokio framework run inside a task. A task is similar to a user space thread or a green thread. An executor is responsible for scheduling tasks for execution.</p>
<p>When a future does not have any data to resolve or is waiting for data to arrive at the socket in case of a <kbd>TcpStream</kbd> client read, it returns a <kbd>NotReady</kbd> status. But, in doing this it also needs to register interest with the reactor to be notified again of any new data on the server.</p>
<p><span>When a future is created, no work is performed. For the work defined by the future to happen, the future must be submitted to an executor. In tokio, tasks are user-level threads that can execute futures. In its implementation of the <kbd>poll</kbd> method, a task has to arrange itself to be polled later in case no progress can be made. For doing this it has to pass its task handler to the reactor thread. The reactor in case of Linux is mio the crate.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building an asynchronous redis server</h1>
                </header>
            
            <article>
                
<p>Now that we're familiar with the asynchronous I/O solutions that the Rust ecosystem provides, it's time to revisit our Redis server implementation. We'll port our <kbd>rudis_sync</kbd> server to the asynchronous version using the <kbd>tokio</kbd> and <kbd>futures</kbd> crates. As with any asynchronous code, using <kbd>futures</kbd> and <kbd>tokio</kbd> can be daunting at first, and it can take time getting used to its API. However, We'll try to make things easy to understand here. Let's start by creating our project by running <kbd>cargo new rudis_async</kbd> with the following dependencies in <kbd>Cargo.toml</kbd>:</p>
<pre># rudis_async/Cargo.toml<br/><br/>[dependencies]<br/>tokio = "0.1.13"<br/>futures = "0.1.25"<br/>lazy_static = "1.2.0"<br/>resp = { git = "https://github.com/creativcoder/resp" }<br/>tokio-codec = "0.1.1"<br/>bytes = "0.4.11"</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>We are using a bunch of crates here:</p>
<ul>
<li><kbd>futures</kbd>: Provides a cleaner abstraction for dealing with async code</li>
<li><kbd>tokio</kbd>: Encapsulates mio and provides a runtime for running asynchronous code</li>
<li><kbd>lazy_static</kbd>: Allows us to create a dynamic global variable that can be mutated</li>
<li><kbd>resp</kbd>: A crate that can parse Redis protocol messages</li>
<li><kbd>tokio-codec</kbd>: This allows you to convert a stream of bytes from the network into a given type, which is parsed as a definite message according to the specified codec. A codec converts stream of bytes into a parsed message termed as a <strong>Frame</strong> in the tokio ecosystem.</li>
<li><kbd>bytes</kbd>: This is used with the tokio codec to efficiently convert a stream of bytes into a given <em>Frame</em></li>
</ul>
<p>Our initial code in <kbd>main.rs</kbd> follows a similar structure:</p>
<pre>// rudis_async/src/main.rs<br/><br/>mod codec;<br/>use crate::codec::RespCodec;<br/><br/>use lazy_static::lazy_static;<br/>use std::collections::HashMap;<br/>use std::net::SocketAddr;<br/>use std::sync::Mutex;<br/>use tokio::net::TcpListener;<br/>use tokio::net::TcpStream;<br/>use tokio::prelude::*;<br/>use tokio_codec::Decoder;<br/>use std::env;<br/><br/>mod commands;<br/>use crate::commands::process_client_request;<br/><br/>lazy_static! {<br/>    static ref RUDIS_DB: Mutex&lt;HashMap&lt;String, String&gt;&gt; = Mutex::new(HashMap::new());<br/>}</pre>
<p>We have a bunch of imports and the same <kbd>RUDIS_DB</kbd> in a <kbd>lazy_static!</kbd> block. We then have our function <kbd>main</kbd>:</p>
<pre>// rudis_async/main.rs<br/><br/>fn main() -&gt; Result&lt;(), Box&lt;std::error::Error&gt;&gt; {<br/>    let addr = env::args()<br/>        .skip(1)<br/>        .next()<br/>        .unwrap_or("127.0.0.1:6378".to_owned());<br/>    let addr = addr.parse::&lt;SocketAddr&gt;()?;<br/><br/>    let listener = TcpListener::bind(&amp;addr)?;<br/>    println!("rudis_async listening on: {}", addr);<br/><br/>    let server_future = listener<br/>        .incoming()<br/>        .map_err(|e| println!("failed to accept socket; error = {:?}", e))<br/>        .for_each(handle_client);<br/><br/>    tokio::run(server_future);<br/>    Ok(())<br/>}</pre>
<p>We parse the string that's been passed in as an argument or use a default address of <kbd>127.0.0.1:6378</kbd>. We then create a new <kbd>TcpListener</kbd> instance with <kbd>addr</kbd>. This returns us a future in <kbd>listener</kbd>. We then chain on this future by calling <kbd>incoming</kbd> on and invoke <kbd>for_each</kbd> on it which takes in a closure and call <kbd>handle_client</kbd> on it. This future gets stored as <kbd>server_future</kbd>.In the end, we call <kbd>tokio::run</kbd> passing in <kbd>server_future</kbd>. This creates a main tokio task and schedules the future for execution.</p>
<p>In the same file, our <kbd>handle_client</kbd> function is defined like so:</p>
<pre>// rudis_async/src/main.rs<br/><br/>fn handle_client(client: TcpStream) -&gt; Result&lt;(), ()&gt; {<br/>    let (tx, rx) = RespCodec.framed(client).split();<br/>    let reply = rx.and_then(process_client_request);<br/>    let task = tx.send_all(reply).then(|res| {<br/>        if let Err(e) = res {<br/>            eprintln!("failed to process connection; error = {:?}", e);<br/>        }<br/>        Ok(())<br/>    });<br/><br/>    tokio::spawn(task);<br/>    Ok(())<br/>}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>In <kbd>handle_client</kbd>, we first split our <kbd>TcpStream</kbd> into a writer (<kbd>tx</kbd>) and reader (<kbd>rx</kbd>) half by first converting the stream to a framed future calling framed on <kbd>RespCodec</kbd> receives the <kbd>client</kbd> connection and converts it into a framed future by calling framed on <kbd>RudisFrame</kbd>. Following that, we call <kbd>split</kbd> on it, which converts the frame into a <kbd>Stream</kbd> and <kbd>Sink</kbd> future, respectively. This simply gives us a <kbd>tx</kbd> and <kbd>rx</kbd> to read and write from the client socket. However, when we read this, we get the decoded message. When we write anything to <kbd>tx</kbd>, we write the encoded byte sequence.</p>
<p>On <kbd>rx</kbd>, we call <kbd>and_then</kbd> passing the <kbd>process_client_request</kbd> function, which will resolve the future to a decoded frame. We then take the writer half <kbd>tx</kbd>, and call <kbd>send_all</kbd> with the <kbd>reply</kbd>. We then spawn the future task by calling <kbd>tokio::spawn</kbd>.</p>
<p>In our <kbd>codec.rs</kbd> file, we have defined <kbd>RudisFrame</kbd>, which implements <kbd>Encoder</kbd> and <kbd>Decoder</kbd> traits from the <kbd>tokio-codec</kbd> crate:</p>
<pre>// rudis_async/src/codec.rs<br/><br/>use std::io;<br/>use bytes::BytesMut;<br/>use tokio_codec::{Decoder, Encoder};<br/>use resp::{Value, Decoder as RespDecoder};<br/>use std::io::BufReader;<br/>use std::str;<br/><br/>pub struct RespCodec;<br/><br/>impl Encoder for RespCodec {<br/>    type Item = Vec&lt;u8&gt;;<br/>    type Error = io::Error;<br/><br/>    fn encode(&amp;mut self, msg: Vec&lt;u8&gt;, buf: &amp;mut BytesMut) -&gt; io::Result&lt;()&gt; {<br/>        buf.reserve(msg.len());<br/>        buf.extend(msg);<br/>        Ok(())<br/>    }<br/>}<br/><br/>impl Decoder for RespCodec {<br/>    type Item = Value;<br/>    type Error = io::Error;<br/><br/>    fn decode(&amp;mut self, buf: &amp;mut BytesMut) -&gt; io::Result&lt;Option&lt;Value&gt;&gt; {<br/>        let s = if let Some(n) = buf.iter().rposition(|b| *b == b'\n') {<br/>            let client_query = buf.split_to(n + 1);<br/><br/>            match str::from_utf8(&amp;client_query.as_ref()) {<br/>                Ok(s) =&gt; s.to_string(),<br/>                Err(_) =&gt; return Err(io::Error::new(io::ErrorKind::Other, "invalid string")),<br/>            }<br/>        } else {<br/>            return Ok(None);<br/>        };<br/><br/>        if let Ok(v) = RespDecoder::new(&amp;mut BufReader::new(s.as_bytes())).decode() {<br/>            Ok(Some(v))<br/>        } else {<br/>            Ok(None)<br/>        }<br/>    }<br/>}</pre>
<p>The <kbd>Decoder</kbd> implementation specify how to parse incoming bytes into a <kbd>resp::Value</kbd> type, whereas the <kbd>Encoder</kbd> trait specifies how to encode a <kbd>resp::Value</kbd> to a stream of bytes to the client.</p>
<p>Our <kbd>commands.rs</kbd> file implementation is the same as the previous one so we'll skip going through that. With that said, let's try our new server by running <kbd>cargo run</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/29a2da81-24b7-486d-90bd-114ef84b39e5.png" style="width:56.67em;height:2.42em;"/></p>
<p>With the official redis-cli client, we can connect to our server by running:</p>
<pre><strong>$ redis-cli -p 6378</strong></pre>
<p>Here's a session of running <kbd>redis-cli</kbd> against <kbd>rudis_async</kbd> server:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b063d555-3e61-454c-a1c7-d34439dbc648.png" style="width:21.50em;height:4.50em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Rust is very well-equipped and suitable for providing higher-performance, quality, and security for network applications. While built-in primitives are well-suited to a synchronous application model, for asynchronous I/O, Rust provides rich libraries with well-documented APIs that help you build high-performance applications.</p>
<p>In the next chapter, we'll step up the network protocol stack and and learn how to build web applications with Rust.</p>


            </article>

            
        </section>
    </body></html>
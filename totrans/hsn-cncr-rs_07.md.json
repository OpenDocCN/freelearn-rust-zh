["```rs\npub struct Arc<T: ?Sized> {\n    ptr: Shared<ArcInner<T>>,\n    phantom: PhantomData<T>,\n}\n```", "```rs\nstruct ArcInner<T: ?Sized> {\n    strong: atomic::AtomicUsize,\n\n    // the value usize::MAX acts as a sentinel for temporarily \n    // \"locking\" the ability to upgrade weak pointers or \n    // downgrade strong ones; this is used to avoid races\n    // in `make_mut` and `get_mut`.\n    weak: atomic::AtomicUsize,\n\n    data: T,\n}\n```", "```rs\n    fn clone(&self) -> Arc<T> {\n        let old_size = self.inner().strong.fetch_add(1, Relaxed);\n\n        if old_size > MAX_REFCOUNT {\n            unsafe {\n                abort();\n            }\n        }\n\n        Arc { ptr: self.ptr, phantom: PhantomData }\n    }\n```", "```rs\n    pub fn new(data: T) -> Arc<T> {\n        // Start the weak pointer count as 1 which is the weak \n        // pointer that's held by all the strong pointers \n        // (kinda), see std/rc.rs for more info\n        let x: Box<_> = box ArcInner {\n            strong: atomic::AtomicUsize::new(1),\n            weak: atomic::AtomicUsize::new(1),\n            data,\n        };\n        Arc { ptr: Shared::from(Box::into_unique(x)), \n              phantom: PhantomData }\n    }\n```", "```rs\nclone Arc\n    INCR strong, Relaxed\n    guard against strong wrap-around\n    allocate New Arc\nmove New Arc into B\ndrop Arc\n```", "```rs\n    fn drop(&mut self) {\n        if self.inner().strong.fetch_sub(1, Release) != 1 {\n            return;\n        }\n\n        atomic::fence(Acquire);\n\n        unsafe {\n            let ptr = self.ptr.as_ptr();\n\n            ptr::drop_in_place(&mut self.ptr.as_mut().data);\n\n            if self.inner().weak.fetch_sub(1, Release) == 1 {\n                atomic::fence(Acquire);\n                Heap.dealloc(ptr as *mut u8, Layout::for_value(&*ptr))\n            }\n        }\n    }\n```", "```rs\nimpl<T: ?Sized> Deref for Arc<T> {\n    type Target = T;\n\n    #[inline]\n    fn deref(&self) -> &T {\n        &self.inner().data\n    }\n}\n```", "```rs\n    pub fn get_mut(this: &mut Self) -> Option<&mut T> {\n        if this.is_unique() {\n            // This unsafety is ok because we're guaranteed that the\n            // pointer returned is the *only* pointer that will ever\n            // be returned to T. Our reference count is guaranteed\n            // to be 1 at this point, and we required the Arc itself\n            // to be `mut`, so we're returning the only possible \n            // reference to the inner data.\n            unsafe {\n                Some(&mut this.ptr.as_mut().data)\n            }\n        } else {\n            None\n        }\n    }\n```", "```rs\n    fn is_unique(&mut self) -> bool {\n        if self.inner().weak.compare_exchange(1, usize::MAX, \n                                              Acquire, Relaxed).is_ok() {\n            let unique = self.inner().strong.load(Relaxed) == 1;\n            self.inner().weak.store(1, Release);\n            unique\n        } else {\n            false\n        }\n    }\n```", "```rs\nuse std::sync::atomic::{fence, AtomicPtr, AtomicUsize, Ordering};\nuse std::{mem, ptr};\n\nunsafe impl<T: Send> Send for Stack<T> {}\nunsafe impl<T: Send> Sync for Stack<T> {}\n\nstruct Node<T> {\n    references: AtomicUsize,\n    next: *mut Node<T>,\n    data: Option<T>,\n}\n\nimpl<T> Node<T> {\n    fn new(t: T) -> Self {\n        Node {\n            references: AtomicUsize::new(1),\n            next: ptr::null_mut(),\n            data: Some(t),\n        }\n    }\n}\n\npub struct Stack<T> {\n    head: AtomicPtr<Node<T>>,\n}\n\nimpl<T> Stack<T> {\n    pub fn new() -> Self {\n        Stack {\n            head: AtomicPtr::default(),\n        }\n    }\n\n    pub fn pop(&self) -> Option<T> {\n        loop {\n            let head: *mut Node<T> = self.head.load(Ordering::Relaxed);\n\n            if head.is_null() {\n                return None;\n            }\n            let next: *mut Node<T> = unsafe { (*head).next };\n\n            if self.head.compare_and_swap(head, next, \n            Ordering::Relaxed) == head {\n                let mut head: Box<Node<T>> = unsafe {\n                    Box::from_raw(head) \n                };\n                let data: Option<T> = mem::replace(&mut (*head).data, \n                                                   None);\n                unsafe {\n                    assert_eq!(\n                        (*(*head).next).references.fetch_sub(1, \n                             Ordering::Release),\n                        2\n                    );\n                }\n                drop(head);\n                return data;\n            }\n        }\n    }\n\n    pub fn push(&self, t: T) -> () {\n        let node: *mut Node<T> = Box::into_raw(Box::new(Node::new(t)));\n        loop {\n            let head = self.head.load(Ordering::Relaxed);\n            unsafe {\n                (*node).next = head;\n            }\n\n            fence(Ordering::Acquire);\n            if self.head.compare_and_swap(head, node, \n            Ordering::Release) == head {\n                // node is now self.head\n                // head is now self.head.next\n                if !head.is_null() {\n                    unsafe {\n                        assert_eq!(1, \n                                   (*head).references.fetch_add(1, \n                                       Ordering::Release)\n                        );\n                    }\n                }\n                break;\n            }\n        }\n    }\n}\n```", "```rs\nuse std::sync::atomic::{self, AtomicPtr};\nuse std::marker::PhantomData;\nuse std::ptr;\nuse {Guard, add_garbage_box};\n```", "```rs\npub struct Treiber<T> {\n    /// The head node.\n    head: AtomicPtr<Node<T>>,\n    /// Make the `Sync` and `Send` (and other OIBITs) transitive.\n    _marker: PhantomData<T>,\n}\n```", "```rs\nstruct Node<T> {\n    /// The data this node holds.\n    item: T,\n    /// The next node.\n    next: *mut Node<T>,\n}\n```", "```rs\npub struct Guard<T: 'static + ?Sized> {\n    /// The inner hazard.\n    hazard: hazard::Writer,\n    /// The pointer to the protected object.\n    pointer: &'static T,\n}\n```", "```rs\npub struct Writer {\n    /// The pointer to the heap-allocated hazard.\n    ptr: &'static AtomicPtr<u8>,\n}\n```", "```rs\n    pub fn try_new<F, E>(ptr: F) -> Result<Guard<T>, E>\n    where F: FnOnce() -> Result<&'static T, E> {\n        // Increment the number of guards currently being created.\n        #[cfg(debug_assertions)]\n        CURRENT_CREATING.with(|x| x.set(x.get() + 1));\n```", "```rs\nthread_local! {\n    /// Number of guards the current thread is creating.\n    static CURRENT_CREATING: Cell<usize> = Cell::new(0);\n}\n```", "```rs\n        // Get a hazard in blocked state.\n        let hazard = local::get_hazard();\n```", "```rs\npub fn get_hazard() -> hazard::Writer {\n    if STATE.state() == thread::LocalKeyState::Destroyed {\n        // The state was deinitialized, so we must rely on the\n        // global state for creating new hazards.\n        global::create_hazard()\n    } else {\n        STATE.with(|s| s.borrow_mut().get_hazard())\n    }\n}\n```", "```rs\nthread_local! {\n    /// The state of this thread.\n    static STATE: RefCell<State> = RefCell::new(State::default());\n}\n```", "```rs\n#[derive(Default)]\nstruct State {\n    garbage: Vec<Garbage>,\n    available_hazards: Vec<hazard::Writer>,\n    available_hazards_free_before: usize,\n}\n```", "```rs\n    fn get_hazard(&mut self) -> hazard::Writer {\n        // Check if there is hazards in the cache.\n        if let Some(hazard) = self.available_hazards.pop() {\n            // There is; we don't need to create a new hazard.\n            //\n            // Since the hazard popped from the cache is not \n            // blocked, we must block the hazard to satisfy \n            // the requirements of this function.\n            hazard.block();\n            hazard\n        } else {\n            // There is not; we must create a new hazard.\n            global::create_hazard()\n        }\n    }\n```", "```rs\n    if STATE.state() == thread::LocalKeyState::Destroyed {\n        // The state was deinitialized, so we must rely on the \n        // global state for creating new hazards.\n        global::create_hazard()\n    } else {\n```", "```rs\npub fn create_hazard() -> hazard::Writer {\n    STATE.create_hazard()\n}\n```", "```rs\nlazy_static! {\n    /// The global state.\n    ///\n    /// This state is shared between all the threads.\n    static ref STATE: State = State::new();\n}\n```", "```rs\nstruct State {\n    /// The message-passing channel.\n    chan: mpsc::Sender<Message>,\n    /// The garbo part of the state.\n    garbo: Mutex<Garbo>,\n}\n```", "```rs\nenum Message {\n    /// Add new garbage.\n    Garbage(Vec<Garbage>),\n    /// Add a new hazard.\n    NewHazard(hazard::Reader),\n}\n```", "```rs\nimpl State {\n    /// Initialize a new state.\n    fn new() -> State {\n        // Create the message-passing channel.\n        let (send, recv) = mpsc::channel();\n\n        // Construct the state from the two halfs of the channel.\n        State {\n            chan: send,\n            garbo: Mutex::new(Garbo {\n                chan: recv,\n                garbage: Vec::new(),\n                hazards: Vec::new(),\n            })\n        }\n    }\n```", "```rs\n    fn create_hazard(&self) -> hazard::Writer {\n        // Create the hazard.\n        let (writer, reader) = hazard::create();\n        // Communicate the new hazard to the global state \n        // through the channel.\n        self.chan.send(Message::NewHazard(reader));\n        // Return the other half of the hazard.\n        writer\n    }\n```", "```rs\npub fn create() -> (Writer, Reader) {\n    // Allocate the hazard on the heap.\n    let ptr = unsafe {\n        &*Box::into_raw(Box::new(AtomicPtr::new(\n                        &BLOCKED as *const u8 as *mut u8)))\n    };\n\n    // Construct the values.\n    (Writer {\n        ptr: ptr,\n    }, Reader {\n        ptr: ptr,\n    })\n}\n```", "```rs\nstruct Garbo {\n    /// The channel of messages.\n    chan: mpsc::Receiver<Message>,\n    /// The to-be-destroyed garbage.\n    garbage: Vec<Garbage>,\n    /// The current hazards.\n    hazards: Vec<hazard::Reader>,\n}\n```", "```rs\n        // This fence is necessary for ensuring that `hazard` does not\n        // get reordered to after `ptr` has run.\n        // TODO: Is this fence even necessary?\n        atomic::fence(atomic::Ordering::SeqCst);\n\n        // Right here, any garbage collection is blocked, due to the\n        // hazard above. This ensures that between the potential \n        // read in `ptr` and it being protected by the hazard, there\n        // will be no premature free.\n\n        // Evaluate the pointer through the closure.\n        let res = ptr();\n\n        // Decrement the number of guards currently being created.\n        #[cfg(debug_assertions)]\n        CURRENT_CREATING.with(|x| x.set(x.get() - 1));\n\n        match res {\n            Ok(ptr) => {\n                // Now that we have the pointer, we can protect it by \n                // the hazard, unblocking a pending garbage collection\n                // if it exists.\n                hazard.protect(ptr as *const T as *const u8);\n\n                Ok(Guard {\n                    hazard: hazard,\n                    pointer: ptr,\n                })\n            },\n            Err(err) => {\n                // Set the hazard to free to ensure that the hazard \n                // doesn't remain blocking.\n                hazard.free();\n\n                Err(err)\n            }\n        }\n    }\n```", "```rs\nimpl Reader {\n    /// Get the state of the hazard.\n    ///\n    /// It will spin until the hazard is no longer in a blocked state, \n    /// unless it is in debug mode, where it will panic given enough\n    /// spins.\n    pub fn get(&self) -> State {\n        // In debug mode, we count the number of spins. In release \n        // mode, this should be trivially optimized out.\n        let mut spins = 0;\n\n        // Spin until not blocked.\n        loop {\n            let ptr = self.ptr.load(atomic::Ordering::Acquire) \n                          as *const u8;\n\n            // Blocked means that the hazard is blocked by another \n            // thread, and we must loop until it assumes another \n            // state.\n            if ptr == &BLOCKED {\n                // Increment the number of spins.\n                spins += 1;\n                debug_assert!(spins < 100_000_000, \"\\\n                    Hazard blocked for 100 millions rounds. Panicking \n                    as chances are that it will \\\n                    never get unblocked.\\\n                \");\n\n                continue;\n            } else if ptr == &FREE {\n                return State::Free;\n            } else if ptr == &DEAD {\n                return State::Dead;\n            } else {\n                return State::Protect(ptr);\n            }\n```", "```rs\n    pub fn push(&self, item: T)\n    where T: 'static {\n        let mut snapshot = Guard::maybe_new(|| unsafe {\n            self.head.load(atomic::Ordering::Relaxed).as_ref()\n        });\n\n        let mut node = Box::into_raw(Box::new(Node {\n            item: item,\n            next: ptr::null_mut(),\n        }));\n\n        loop {\n            let next = snapshot.map_or(ptr::null_mut(), \n                                       |x| x.as_ptr() as *mut _);\n            unsafe { (*node).next = next; }\n\n            match Guard::maybe_new(|| unsafe {\n                self.head.compare_and_swap(next, node, \n                    atomic::Ordering::Release).as_ref()\n            }) {\n                Some(ref new) if new.as_ptr() == next => break,\n                None if next.is_null() => break,\n                // If it fails, we will retry the CAS with updated \n                // values.\n                new => snapshot = new,\n            }\n        }\n    }\n```", "```rs\n    pub fn pop(&self) -> Option<Guard<T>> {\n        let mut snapshot = Guard::maybe_new(|| unsafe {\n            self.head.load(atomic::Ordering::Acquire).as_ref()\n        });\n\n        while let Some(old) = snapshot {\n            snapshot = Guard::maybe_new(|| unsafe {\n                self.head.compare_and_swap(\n                    old.as_ptr() as *mut _,\n                    old.next as *mut Node<T>,\n                    atomic::Ordering::Release,\n                ).as_ref()\n            });\n\n            if let Some(ref new) = snapshot {\n                if new.as_ptr() == old.as_ptr() {\n                    unsafe { add_garbage_box(old.as_ptr()); }\n                    return Some(old.map(|x| &x.item));\n                }\n            } else {\n                break;\n            }\n        }\n\n        None\n    }\n```", "```rs\nextern crate conc;\n#[macro_use]\nextern crate lazy_static;\nextern crate num_cpus;\nextern crate quantiles;\n\nuse conc::sync::Treiber;\nuse quantiles::ckms::CKMS;\nuse std::sync::Arc;\nuse std::sync::atomic::{AtomicUsize, Ordering};\nuse std::{thread, time};\n\nlazy_static! {\n    static ref WORKERS: AtomicUsize = AtomicUsize::new(0);\n    static ref COUNT: AtomicUsize = AtomicUsize::new(0);\n}\nstatic MAX_I: u32 = 67_108_864; // 2 ** 26\n\nfn main() {\n    let stk: Arc<Treiber<(u64, u64, u64)>> = Arc::new(Treiber::new());\n\n    let mut jhs = Vec::new();\n\n    let cpus = num_cpus::get();\n    WORKERS.store(cpus, Ordering::Release);\n\n    for _ in 0..cpus {\n        let stk = Arc::clone(&stk);\n        jhs.push(thread::spawn(move || {\n            for i in 0..MAX_I {\n                stk.push((i as u64, i as u64, i as u64));\n                stk.pop();\n                COUNT.fetch_add(1, Ordering::Relaxed);\n            }\n            WORKERS.fetch_sub(1, Ordering::Relaxed)\n        }))\n    }\n\n    let one_second = time::Duration::from_millis(1_000);\n    let mut iter = 0;\n    let mut cycles: CKMS<u32> = CKMS::new(0.001);\n    while WORKERS.load(Ordering::Relaxed) != 0 {\n        let count = COUNT.swap(0, Ordering::Relaxed);\n        cycles.insert((count / cpus) as u32);\n        println!(\n            \"CYCLES PER SECOND({}):\\n  25th: \\\n            {}\\n  50th: {}\\n  75th: \\\n            {}\\n  90th: {}\\n  max:  {}\\n\",\n            iter,\n            cycles.query(0.25).unwrap().1,\n            cycles.query(0.50).unwrap().1,\n            cycles.query(0.75).unwrap().1,\n            cycles.query(0.90).unwrap().1,\n            cycles.query(1.0).unwrap().1\n        );\n        thread::sleep(one_second);\n        iter += 1;\n    }\n\n    for jh in jhs {\n        jh.join().unwrap();\n    }\n}\n```", "```rs\nCYCLES PER SECOND(0):\n  25th: 0\n  50th: 0\n  75th: 0\n  90th: 0\n  max:  0\n\nCYCLES PER SECOND(1):\n  25th: 0\n  50th: 0\n  75th: 1124493\n  90th: 1124493\n  max:  1124493\n\n...\n\nCYCLES PER SECOND(56):\n  25th: 1139055\n  50th: 1141656\n  75th: 1143781\n  90th: 1144324\n  max:  1145284\n\nCYCLES PER SECOND(57):\n  25th: 1139097\n  50th: 1141656\n  75th: 1143792\n  90th: 1144324\n  max:  1145284\nCYCLES PER SECOND(58):\n  25th: 1139097\n  50th: 1141809\n  75th: 1143792\n  90th: 1144398\n  max:  1152384\n```", "```rs\n> perf stat --event task-clock,context-switches,page-faults,cycles,instructions,branches,branch-misses,cache-references,cache-misses target/release/conc_stack > /dev/null\n\n Performance counter stats for 'target/release/conc_stack':\n\n     230503.932161      task-clock (msec)         #    3.906 CPUs utilized\n               943      context-switches          #    0.004 K/sec\n             9,140      page-faults               #    0.040 K/sec\n   665,734,124,408      cycles                    #    2.888 GHz \n   529,230,473,047      instructions              #    0.79  insn per cycle \n    99,146,219,517      branches                  #  430.128 M/sec \n     1,140,398,326      branch-misses             #    1.15% of all branches\n    12,759,284,944      cache-references          #   55.354 M/sec\n       124,487,844      cache-misses              #    0.976 % of all cache refs\n\n      59.006842741 seconds time elapsed\n```", "```rs\nCYCLES PER SECOND(0):\n  25th: 0\n  50th: 0\n  75th: 0\n  90th: 0\n  max:  0\nCYCLES PER SECOND(1):\n  25th: 0\n  50th: 0\n  75th: 150477\n  90th: 150477\n  max:  150477\n\n...\n\nCYCLES PER SECOND(462):\n  25th: 137736\n  50th: 150371\n  75th: 150928\n  90th: 151129\n  max:  151381\n\nCYCLES PER SECOND(463):\n  25th: 137721\n  50th: 150370\n  75th: 150928\n  90th: 151129\n  max:  151381\n```", "```rs\n> perf stat --event task-clock,context-switches,page-faults,cycles,instructions,branches,branch-misses,cache\n-references,cache-misses target/release/conc_stack > /dev/null\n\n Performance counter stats for 'target/release/conc_stack':\n\n    1882745.172714      task-clock (msec)         #    3.955 CPUs utilized\n                 0      context-switches          #    0.000 K/sec\n             3,212      page-faults               #    0.002 K/sec\n 2,109,536,434,019      cycles                    #    1.120 GHz\n 1,457,344,087,067      instructions              #    0.69  insn per cycle\n   264,210,403,390      branches                  #  140.333 M/sec\n    36,052,283,984      branch-misses             #   13.65% of all branches\n   642,854,259,575      cache-references          #  341.445 M/sec\n     2,401,725,425      cache-misses              #    0.374 % of all cache refs\n\n     476.095973090 seconds time elapsed\n```", "```rs\nuse std::sync::atomic::Ordering::{Acquire, Relaxed, Release};\nuse std::ptr;\n\nuse epoch::{self, Atomic, Owned};\n```", "```rs\n#[derive(Debug)]\npub struct TreiberStack<T> {\n    head: Atomic<Node<T>>,\n}\n\n#[derive(Debug)]\nstruct Node<T> {\n    data: T,\n    next: Atomic<Node<T>>,\n}\n```", "```rs\nimpl<T> TreiberStack<T> {\n    /// Create a new, empty stack.\n    pub fn new() -> TreiberStack<T> {\n        TreiberStack {\n            head: Atomic::null(),\n        }\n    }\n```", "```rs\n    pub fn push(&self, t: T) {\n        let mut n = Owned::new(Node {\n            data: t,\n            next: Atomic::null(),\n        });\n        let guard = epoch::pin();\n        loop {\n            let head = self.head.load(Relaxed, &guard);\n            n.next.store(head, Relaxed);\n            match self.head.compare_and_set(head, n, Release, &guard) {\n                Ok(_) => break,\n                Err(e) => n = e.new,\n            }\n        }\n    }\n```", "```rs\n    pub fn try_pop(&self) -> Option<T> {\n        let guard = epoch::pin();\n        loop {\n            let head_shared = self.head.load(Acquire, &guard);\n            match unsafe { head_shared.as_ref() } {\n                Some(head) => {\n                    let next = head.next.load(Relaxed, &guard);\n                    if self.head\n                        .compare_and_set(head_shared, next, Release, \n                         &guard)\n                        .is_ok()\n                    {\n                        unsafe {\n                            guard.defer(move || \n                            head_shared.into_owned());\n                            return Some(ptr::read(&(*head).data));\n                        }\n                    }\n                }\n                None => return None,\n            }\n        }\n    }\n```", "```rs\n    guard.defer(move || head_shared.into_owned());\n    return Some(ptr::read(&(*head).data));\n```", "```rs\npub struct Atomic<T> {\n    data: AtomicUsize,\n    _marker: PhantomData<*mut T>,\n}\n\nunsafe impl<T: Send + Sync> Send for Atomic<T> {}\nunsafe impl<T: Send + Sync> Sync for Atomic<T> {}\n```", "```rs\n    pub fn null() -> Atomic<T> {\n        Self {\n            data: ATOMIC_USIZE_INIT,\n            _marker: PhantomData,\n        }\n    }\n```", "```rs\n    pub fn new(value: T) -> Atomic<T> {\n        Self::from(Owned::new(value))\n    }\n```", "```rs\nimpl<T> Owned<T> {\n    pub fn new(value: T) -> Owned<T> {\n        Self::from(Box::new(value))\n    }\n```", "```rs\nimpl<T> From<Box<T>> for Owned<T> {\n    fn from(b: Box<T>) -> Self {\n        unsafe { Self::from_raw(Box::into_raw(b)) }\n    }\n}\n```", "```rs\npub unsafe fn from_raw(raw: *mut T) -> Owned<T> {\n    ensure_aligned(raw);\n    Self::from_data(raw as usize)\n}\n```", "```rs\n    pub fn load<'g>(&self, ord: Ordering, _: &'g Guard) \n        -> Shared<'g, T> \n    {\n        unsafe { Shared::from_data(self.data.load(ord)) }\n    }\n```", "```rs\npub struct Shared<'g, T: 'g> {\n    data: usize,\n    _marker: PhantomData<(&'g (), *const T)>,\n}\n```", "```rs\n    pub fn compare_and_set<'g, O, P>(\n        &self,\n        current: Shared<T>,\n        new: P,\n        ord: O,\n        _: &'g Guard,\n    ) -> Result<Shared<'g, T>, CompareAndSetError<'g, T, P>>\n    where\n        O: CompareAndSetOrdering,\n        P: Pointer<T>,\n    {\n        let new = new.into_data();\n        self.data\n            .compare_exchange(current.into_data(), new,\n                              ord.success(), ord.failure())\n            .map(|_| unsafe { Shared::from_data(new) })\n            .map_err(|current| unsafe {\n                CompareAndSetError {\n                    current: Shared::from_data(current),\n                    new: P::from_data(new),\n                }\n            })\n    }\n```", "```rs\npub trait CompareAndSetOrdering {\n    /// The ordering of the operation when it succeeds.\n    fn success(&self) -> Ordering;\n\n    /// The ordering of the operation when it fails.\n    ///\n    /// The failure ordering can't be `Release` or `AcqRel` and must be \n    /// equivalent or weaker than the success ordering.\n    fn failure(&self) -> Ordering;\n}\n\nimpl CompareAndSetOrdering for Ordering {\n    #[inline]\n    fn success(&self) -> Ordering {\n        *self\n    }\n\n    #[inline]\n    fn failure(&self) -> Ordering {\n        strongest_failure_ordering(*self)\n    }\n}\n```", "```rs\nfn strongest_failure_ordering(ord: Ordering) -> Ordering {\n    use self::Ordering::*;\n    match ord {\n        Relaxed | Release => Relaxed,\n        Acquire | AcqRel => Acquire,\n        _ => SeqCst,\n    }\n}\n```", "```rs\npub trait Pointer<T> {\n    /// Returns the machine representation of the pointer.\n    fn into_data(self) -> usize;\n\n    /// Returns a new pointer pointing to the tagged pointer `data`.\n    unsafe fn from_data(data: usize) -> Self;\n}\n```", "```rs\nguard.defer(move || head_shared.into_owned());\nreturn Some(ptr::read(&(*head).data));\n```", "```rs\n    pub unsafe fn defer<F, R>(&self, f: F)\n    where\n        F: FnOnce() -> R,\n    {\n        let garbage = Garbage::new(|| drop(f()));\n\n        if let Some(local) = self.local.as_ref() {\n            local.defer(garbage, self);\n        }\n    }\n```", "```rs\npub struct Garbage {\n    func: Deferred,\n}\n\nunsafe impl Sync for Garbage {}\nunsafe impl Send for Garbage {}\n```", "```rs\nimpl Garbage {\n    /// Make a closure that will later be called.\n    pub fn new<F: FnOnce()>(f: F) -> Self {\n        Garbage { func: Deferred::new(move || f()) }\n    }\n}\n\nimpl Drop for Garbage {\n    fn drop(&mut self) {\n        self.func.call();\n    }\n}\n```", "```rs\n        if let Some(local) = self.local.as_ref() {\n            local.defer(garbage, self);\n        }\n    }\n```", "```rs\npub struct Guard {\n    pub(crate) local: *const Local,\n}\n```", "```rs\npub fn pin() -> Guard {\n    HANDLE.with(|handle| handle.pin())\n}\n```", "```rs\nthread_local! {\n    /// The per-thread participant for the default garbage collector.\n    static HANDLE: Handle = COLLECTOR.register();\n}\n```", "```rs\nlazy_static! {\n    /// The global data for the default garbage collector.\n    static ref COLLECTOR: Collector = Collector::new();\n}\n```", "```rs\npub struct Collector {\n    pub(crate) global: Arc<Global>,\n}\n\nunsafe impl Send for Collector {}\nunsafe impl Sync for Collector {}\n\nimpl Collector {\n    /// Creates a new collector.\n    pub fn new() -> Self {\n        Collector { global: Arc::new(Global::new()) }\n    }\n\n    /// Registers a new handle for the collector.\n    pub fn register(&self) -> Handle {\n        Local::register(self)\n    }\n}\n```", "```rs\npub struct Global {\n    /// The intrusive linked list of `Local`s.\n    locals: List<Local>,\n\n    /// The global queue of bags of deferred functions.\n    queue: Queue<(Epoch, Bag)>,\n\n    /// The global epoch.\n    pub(crate) epoch: CachePadded<AtomicEpoch>,\n}\n```", "```rs\n    pub fn register(collector: &Collector) -> Handle {\n        unsafe {\n            // Since we dereference no pointers in this block, it is \n            // safe to use `unprotected`.\n\n            let local = Owned::new(Local {\n                entry: Entry::default(),\n                epoch: AtomicEpoch::new(Epoch::starting()),\n                collector: UnsafeCell::new(\n                    ManuallyDrop::new(collector.clone())\n                ),\n                bag: UnsafeCell::new(Bag::new()),\n                guard_count: Cell::new(0),\n                handle_count: Cell::new(1),\n                pin_count: Cell::new(Wrapping(0)),\n            }).into_shared(&unprotected());\n            collector.global.locals.insert(local, &unprotected());\n            Handle { local: local.as_raw() }\n        }\n    }\n```", "```rs\nimpl Drop for Guard {\n    #[inline]\n    fn drop(&mut self) {\n        if let Some(local) = unsafe { self.local.as_ref() } {\n            local.unpin();\n        }\n    }\n}\n```", "```rs\n    pub fn unpin(&self) {\n        let guard_count = self.guard_count.get();\n        self.guard_count.set(guard_count - 1);\n\n        if guard_count == 1 {\n            self.epoch.store(Epoch::starting(), Ordering::Release);\n\n            if self.handle_count.get() == 0 {\n                self.finalize();\n            }\n        }\n    }\n```", "```rs\n    fn finalize(&self) {\n        debug_assert_eq!(self.guard_count.get(), 0);\n        debug_assert_eq!(self.handle_count.get(), 0);\n\n        // Temporarily increment handle count. This is required so that  \n        // the following call to `pin` doesn't call `finalize` again.\n        self.handle_count.set(1);\n        unsafe {\n            // Pin and move the local bag into the global queue. It's \n            // important that `push_bag` doesn't defer destruction \n            // on any new garbage.\n            let guard = &self.pin();\n            self.global().push_bag(&mut *self.bag.get(), guard);\n        }\n        // Revert the handle count back to zero.\n        self.handle_count.set(0);\n```", "```rs\npub struct Bag {\n    /// Stashed objects.\n    objects: ArrayVec<[Garbage; MAX_OBJECTS]>,\n}\n```", "```rs\n#[cfg(not(feature = \"strict_gc\"))]\nconst MAX_OBJECTS: usize = 64;\n#[cfg(feature = \"strict_gc\")]\nconst MAX_OBJECTS: usize = 4;\n```", "```rs\n    pub fn push_bag(&self, bag: &mut Bag, guard: &Guard) {\n        let bag = mem::replace(bag, Bag::new());\n\n        atomic::fence(Ordering::SeqCst);\n\n        let epoch = self.epoch.load(Ordering::Relaxed);\n        self.queue.push((epoch, bag), guard);\n    }\n```", "```rs\n    pub fn defer(&self, mut garbage: Garbage, guard: &Guard) {\n        let bag = unsafe { &mut *self.bag.get() };\n\n        while let Err(g) = bag.try_push(garbage) {\n            self.global().push_bag(bag, guard);\n            garbage = g;\n        }\n    }\n```", "```rs\n    pub fn pin(&self) -> Guard {\n        let guard = Guard { local: self };\n\n        let guard_count = self.guard_count.get();\n        self.guard_count.set(guard_count.checked_add(1).unwrap());\n```", "```rs\n        if guard_count == 0 {\n            let global_epoch = \n         self.global().epoch.load(Ordering::Relaxed);\n            let new_epoch = global_epoch.pinned();\n```", "```rs\n              self.epoch.store(new_epoch, Ordering::Relaxed);\n              atomic::fence(Ordering::SeqCst);\n```", "```rs\n            let count = self.pin_count.get();\n            self.pin_count.set(count + Wrapping(1));\n\n            // After every `PINNINGS_BETWEEN_COLLECT` try advancing the \n            // epoch and collecting some garbage.\n            if count.0 % Self::PINNINGS_BETWEEN_COLLECT == 0 {\n                self.global().collect(&guard);\n            }\n        }\n\n        guard\n    }\n```", "```rs\n    pub fn collect(&self, guard: &Guard) {\n        let global_epoch = self.try_advance(guard);\n```", "```rs\n        let condition = |item: &(Epoch, Bag)| {\n            // A pinned participant can witness at most one epoch \n            advancement. Therefore, any bag\n            // that is within one epoch of the current one cannot be \n            destroyed yet.\n            global_epoch.wrapping_sub(item.0) >= 2\n        };\n\n        let steps = if cfg!(feature = \"sanitize\") {\n            usize::max_value()\n        } else {\n            Self::COLLECT_STEPS\n        };\n\n        for _ in 0..steps {\n            match self.queue.try_pop_if(&condition, guard) {\n                None => break,\n                Some(bag) => drop(bag),\n            }\n        }\n    }\n```", "```rs\nextern crate crossbeam;\n#[macro_use]\nextern crate lazy_static;\nextern crate num_cpus;\nextern crate quantiles;\n\nuse crossbeam::sync::TreiberStack;\nuse quantiles::ckms::CKMS;\nuse std::sync::Arc;\nuse std::sync::atomic::{AtomicUsize, Ordering};\nuse std::{thread, time};\n\nlazy_static! {\n    static ref WORKERS: AtomicUsize = AtomicUsize::new(0);\n    static ref COUNT: AtomicUsize = AtomicUsize::new(0);\n}\nstatic MAX_I: u32 = 67_108_864; // 2 ** 26\n\nfn main() {\n    let stk: Arc<TreiberStack<(u64, u64, u64)>> = Arc::new(TreiberStack::new());\n\n    let mut jhs = Vec::new();\n\n    let cpus = num_cpus::get();\n    WORKERS.store(cpus, Ordering::Release);\n\n    for _ in 0..cpus {\n        let stk = Arc::clone(&stk);\n        jhs.push(thread::spawn(move || {\n            for i in 0..MAX_I {\n                stk.push((i as u64, i as u64, i as u64));\n                stk.pop();\n                COUNT.fetch_add(1, Ordering::Relaxed);\n            }\n            WORKERS.fetch_sub(1, Ordering::Relaxed)\n        }))\n    }\n\n    let one_second = time::Duration::from_millis(1_000);\n    let mut iter = 0;\n    let mut cycles: CKMS<u32> = CKMS::new(0.001);\n    while WORKERS.load(Ordering::Relaxed) != 0 {\n        let count = COUNT.swap(0, Ordering::Relaxed);\n        cycles.insert((count / cpus) as u32);\n        println!(\n            \"CYCLES PER SECOND({}):\\n  25th: \\\n             {}\\n  50th: {}\\n  75th: \\\n             {}\\n  90th: {}\\n  max:  {}\\n\",\n            iter,\n            cycles.query(0.25).unwrap().1,\n            cycles.query(0.50).unwrap().1,\n            cycles.query(0.75).unwrap().1,\n            cycles.query(0.90).unwrap().1,\n            cycles.query(1.0).unwrap().1\n        );\n        thread::sleep(one_second);\n        iter += 1;\n    }\n\n    for jh in jhs {\n        jh.join().unwrap();\n    }\n}\n```", "```rs\nCYCLES PER SECOND(0):\n  25th: 0\n  50th: 0\n  75th: 0\n  90th: 0\n  max:  0\n\nCYCLES PER SECOND(1):\n  25th: 0\n  50th: 0\n  75th: 1739270\n  90th: 1739270\n  max:  1739270\n\n...\n\nCYCLES PER SECOND(37):\n  25th: 1738976\n  50th: 1739528\n  75th: 1740474\n  90th: 1757650\n  max:  1759459\n\nCYCLES PER SECOND(38):\n  25th: 1738868\n  50th: 1739528\n  75th: 1740452\n  90th: 1757650\n  max:  1759459\n```", "```rs\n> perf stat --event task-clock,context-switches,page-faults,cycles,instructions,branches,branch-misses,cache-references,cache-misses target/release/epoch_stack > /dev/null\n\n Performance counter stats for 'target/release/epoch_stack':\n\n     148830.337380      task-clock (msec)         #    3.916 CPUs utilized\n             1,043      context-switches          #    0.007 K/sec\n             1,039      page-faults               #    0.007 K/sec\n   429,969,505,981      cycles                    #    2.889 GHz\n   161,901,052,886      instructions              #    0.38  insn per cycle\n    27,531,697,676      branches                  #  184.987 M/sec\n       627,050,474      branch-misses             #    2.28% branches\n    11,885,394,803      cache-references          #   79.859 M/sec\n         1,772,308      cache-misses              #    0.015 % cache refs\n\n      38.004548310 seconds time elapsed\n```", "```rs\nCYCLES PER SECOND(0):\n  25th: 0\n  50th: 0\n  75th: 0\n  90th: 0\n  max:  0\n\nCYCLES PER SECOND(1):\n  25th: 0\n  50th: 0\n  75th: 921743\n  90th: 921743\n  max:  921743\n\n...\n\nCYCLES PER SECOND(71):\n  25th: 921908\n  50th: 922326\n  75th: 922737\n  90th: 923235\n  max:  924084\n\nCYCLES PER SECOND(72):\n  25th: 921908\n  50th: 922333\n  75th: 922751\n  90th: 923235\n  max:  924084\n```", "```rs\n> perf stat --event task-clock,context-switches,page-faults,cycles,instructions,branches,branch-misses,cache-references,cache-misses target/release/epoch_stack > /dev/null\n\n Performance counter stats for 'target/release/epoch_stack':\n\n     304880.898057      task-clock (msec)         #    3.959 CPUs utilized\n                 0      context-switches          #    0.000 K/sec\n               248      page-faults               #    0.001 K/sec\n   364,139,169,537      cycles                    #    1.194 GHz\n   215,754,991,724      instructions              #    0.59  insn per cycle\n    28,019,208,815      branches                  #   91.902 M/sec\n     3,525,977,068      branch-misses             #   12.58% branches\n   105,165,886,677      cache-references          #  344.941 M/sec\n     1,450,538,471      cache-misses              #    1.379 % cache refs\n\n      77.014340901 seconds time elapsed\n```"]
<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Parallelism and Rayon</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li class="mce-root">Parallelizing iterators</li>
<li class="mce-root">Running two operations together</li>
<li>Sending data across threads</li>
<li>Sharing resources in multithreaded closures</li>
<li class="mce-root">Accessing resources in parallel with RwLocks</li>
<li>Atomically accessing primitives</li>
<li>Putting it all together in a connection handler</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p class="mce-root">There used to be a time when your code got faster every year automatically, as processors got better and better. But nowadays, as Herb Sutter famously stated, <em>The Free Lunch Is Over</em> (<a href="http://www.gotw.ca/publications/concurrency-ddj.htm">http://www.gotw.ca/publications/concurrency-ddj.htm</a>). The age of not better, but more numerous processor cores arrived a long time ago. Not all programming languages are well suited for this radical change towards omnipresent concurrency.</p>
<p class="mce-root">Rust was designed with exactly this problem in mind. Its borrow checker makes sure that most concurrent algorithms work fine. It goes even further: your code won't even compile if it's not parallelizable, even if you don't yet use more than one thread. Because of these unique guarantees, one of Rust's main selling points has been dubbed <em>fearless concurrency</em>.</p>
<p class="mce-root">And we are about to find out why.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parallelizing iterators</h1>
                </header>
            
            <article>
                
<p>Wouldn't it be cool to have a magic button that allowed you to just make any algorithm parallel, without you doing anything? Well, as long as your algorithm uses iterators, <kbd>rayon</kbd> is exactly that!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create a Rust project to work on during this chapter with <kbd>cargo new chapter-seven</kbd>.</li>
<li>Navigate into the newly-created <kbd>chapter-seven</kbd> folder. For the rest of this chapter, we will assume that your command line is currently in this directory.</li>
<li>Open the <kbd>Cargo.toml</kbd> file that has been generated for you.</li>
<li>Under <kbd>[dependencies]</kbd>, add the following line:</li>
</ol>
<pre style="padding-left: 60px">rayon = "1.0.0"</pre>
<p style="padding-left: 60px">If you want, you can go to <kbd>rayon</kbd>'s crates.io page (<a href="https://crates.io/crates/rayon">https://crates.io/crates/rayon</a>) to check for the newest version and use that one instead.</p>
<ol start="5">
<li>Inside the <kbd>src</kbd> folder, create a new folder called <kbd>bin</kbd>.</li>
<li>Delete the generated <kbd>lib.rs</kbd> file, as we are not creating a library.</li>
<li>In the <kbd>src/bin</kbd> folder, create a file called <kbd>par_iter.rs</kbd>.</li>
<li>Add the following code and run it with <kbd>cargo run --bin par_iter</kbd>:</li>
</ol>
<pre style="padding-left: 60px">1  extern crate rayon;<br/>2  use rayon::prelude::*;<br/>3 <br/>4  fn main() {<br/>5    let legend = "Did you ever hear the tragedy of Darth Plagueis <br/>     The Wise?";<br/>6    let words: Vec&lt;_&gt; = legend.split_whitespace().collect();<br/>7 <br/>8   // The following will execute in parallel,<br/>9   // so the exact order of execution is not foreseeable<br/>10  words.par_iter().for_each(|val| println!("{}", val));<br/>11 <br/>12   // par_iter can do everything that a normal iterator does, but<br/>13   // in parallel. This way you can easily parallelize any <br/>       algorithm<br/>14   let words_with_a: Vec&lt;_&gt; = words<br/>15       .par_iter()<br/>16       .filter(|val| val.find('a').is_some())<br/>17       .collect();<br/>18 <br/>19    println!(<br/>20       "The following words contain the letter 'a': {:?}",<br/>21        words_with_a<br/>22    );<br/>23  }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><kbd>rayon</kbd> implements the trait <kbd>ParallelIterator</kbd> for every type that implements its standard library equivalent <kbd>Iterator</kbd>, which we got to know in <a href="977b8621-cb73-43de-9a2b-4bc9f5583542.xhtml">Chapter 2</a>, <em>Working with Collections; </em><em>Access Collections as iterators</em>. In fact, you can use all the knowledge from said recipe again here. The methods provided by the <kbd>ParallelIterator</kbd> trait are nearly the same as the ones provided by <kbd>Iterator</kbd>, so in virtually all cases where you notice an iterator operation taking too long and bottlenecking you, you can simply replace <kbd>.iter()</kbd> with <kbd>.par_iter()</kbd>[10]. Similarly, for moving iterators, you can use <kbd>.into_par_iter()</kbd> instead of <kbd>.into_iter()</kbd>.</p>
<p><kbd>rayon</kbd> handles all the tedious work for you, as it automatically distributes the work evenly between all of your available cores. Just keep in mind that despite this magic, you're still dealing with parallelism here, so you have no guarantees about the order in which the items in your iterator are going to be handled, as evidenced by line [10], which will print in a different order each time you execute the program:</p>
<pre style="padding-left: 30px">words.par_iter().for_each(|val| println!("{}", val));</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Access collections as iterators</em> recipe in <a href="977b8621-cb73-43de-9a2b-4bc9f5583542.xhtml">Chapter 2</a>, <em>Working with Collections</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running two operations together</h1>
                </header>
            
            <article>
                
<p>The parallel iterators from the last recipe are internally built upon a more fundamental function, <kbd>rayon::join</kbd>, which takes two closures and <em>potentially</em> runs them in parallel. This way, even the balance of performance gain versus the overhead of spawning a thread has been done for you.</p>
<p>If you have an algorithm that doesn't use iterators but still consists of some clearly separated parts that could benefit from running concurrently, consider using <kbd>rayon::join</kbd> for that.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>
<p>Open the <kbd>Cargo.toml</kbd> file that was generated earlier for you.</p>
</li>
<li>If you didn't do so in the last recipe, under <kbd>[dependencies]</kbd>, add the following line:</li>
</ol>
<pre style="padding-left: 60px">rayon = "1.0.0"</pre>
<ol start="3">
<li>If you want, you can go to <kbd>rayon</kbd>'s crates.io page (<a href="https://crates.io/crates/rayon">https://crates.io/crates/rayon</a>) to check for the newest version and use that one instead.</li>
<li>
<p>In the folder <kbd>bin</kbd>, create a file called <kbd>join.rs</kbd>.</p>
</li>
<li>
<p>Add the following code and run it with <kbd>cargo run --bin join</kbd>:</p>
</li>
</ol>
<pre style="padding-left: 60px">1 extern crate rayon;<br/>2 <br/>3  #[derive(Debug)]<br/>4  struct Rectangle {<br/>5    height: u32,<br/>6    width: u32,<br/>7  }<br/>8 <br/>9  impl Rectangle {<br/>10    fn area(&amp;self) -&gt; u32 {<br/>11       self.height * self.width<br/>12    }<br/>13    fn perimeter(&amp;self) -&gt; u32 {<br/>14       2 * (self.height + self.width)<br/>15    }<br/>16  }<br/>17 <br/>18  fn main() {<br/>19    let rect = Rectangle {<br/>20       height: 30,<br/>21       width: 20,<br/>22    };<br/>23    // rayon::join makes closures run potentially in parallel and<br/>24    // returns their returned values in a tuple<br/>25    let (area, perimeter) = rayon::join(|| rect.area(), || <br/>      rect.perimeter());<br/>26    println!("{:?}", rect);<br/>27    println!("area: {}", area);<br/>28    println!("perimeter: {}", perimeter);<br/>29 <br/>30    let fib = fibonacci(6);<br/>31    println!("The sixth number in the fibonacci sequence is {}", <br/>      fib);<br/>32  }<br/>33 <br/>34  fn fibonacci(n: u32) -&gt; u32 {<br/>35    if n == 0 || n == 1 {<br/>36     n<br/>37    } else {<br/>38      // rayon::join can really shine in recursive functions<br/>39      let (a, b) = rayon::join(|| fibonacci(n - 1), || fibonacci(n <br/>          - 2));<br/>40       a + b<br/>41    }<br/>42  }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><kbd>rayon::join</kbd> is pretty simple. It takes two closures, potentially runs them in parallel, and returns their returned values in a tuple [25]. Wait a second, did we just say <em>potentially</em>? Isn't it always better to run things in parallel?</p>
<p>Nope, at least not always. Sure, if you really care about things running together at all times without blocking, say a GUI and its underlying I/O where you definitely don't want the mouse cursor to freeze when opening a file, you always need to have all processes running in their own thread. But most applications for concurrency don't have this requirement. A big part of what makes concurrency so important is its ability to run code that would normally run sequentially (that is, one line after another) in parallel if required. Notice the choice of words here—<em>code that would normally run sequentially</em>. These kinds of algorithms do not inherently need concurrency, but they might get a boost out of it. Now comes the <em>potential</em> part—firing up a thread might not be worth it.</p>
<p>To understand why, let's look at the hardware side of things. We are not going to dive too deep into this territory because:</p>
<p>a) the fact that you're reading this book makes me think you're more of a software person and b) the exact mechanisms of CPUs tend to change very rapidly nowadays and we don't want the information provided here to be outdated in a year.</p>
<p>Your CPU divides its work among its <em>cores</em>. A core is the basic computation unit of the CPU. If the device you're reading this on is not made out of paper and younger than two decades, it most probably contains multiple cores. These kinds of cores are called <em>physical</em> and can work on different things at the same time. A physical core itself also has ways to perform multiple jobs. Some can divide themselves into multiple <em>logical</em> cores, splitting work further. For example, an Intel CPU can use <em>hyper-threading</em>, which means that if a program only uses the integer addition unit of a physical core, a virtual core might start working on the floating points addition unit for another program until the first one is done. <em><br/></em></p>
<p>If you don't care about the available amount of cores and simply start new threads without limit, the operating system will start creating threads that don't actually run concurrently, because it ran out of cores. In this case, it will perform <em>context switching</em>, which means that it stores the current state of the thread, pauses it, works on another thread for a split second, and then resumes the thread again. As you can imagine, this costs quite some resources.</p>
<p>This is why if it's not vital to run two things in parallel, you should first check if there are any cores <em>idle</em> (that is, available) in the first place. Because <kbd>rayon::join</kbd> does this check for you; among other things, it will only run the two closures in parallel if it's actually worth it to do so. If you need to do this work yourself, check out the <kbd>num_cpus</kbd> crate (<a href="https://crates.io/crates/num_cpus">https://crates.io/crates/num_cpus</a>).</p>
<p>By the way, the parallel iterators from the last recipe go even further: If the amount of elements and work in them is so small that it would cost more to initiate a new thread for them than to run it sequentially, they will automatically forego concurrency for you.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The underlying mechanism of <kbd>rayon</kbd> is <em>work stealing</em>. This means that when we call the following function, <span>the current thread will immediately start working on </span><kbd>a</kbd><span> and place </span><kbd>b</kbd><span> in a queue</span>:</p>
<pre style="padding-left: 30px">rayon::join(a, b);</pre>
<p class="mce-root">Meanwhile, whenever a core is idle, <kbd>rayon</kbd> will let it work on the next task in the queue. The new thread then <em>steals</em> the task from the others. In our case, that would be <kbd>b</kbd>. If <kbd>a</kbd> happens to finish before <kbd>b</kbd>, the main thread will look into the queue and try to steal work as well. The queue can contain more than two items if <kbd>rayon::join</kbd> is called multiple times in a recursive function.</p>
<p>The author of <kbd>rayon</kbd>, Niko Matsakis, wrote down the following pseudo Rust code to illustrate this principle in his introductory blog post at <a href="http://smallcultfollowing.com/babysteps/blog/2015/12/18/rayon-data-parallelism-in-rust/">http://smallcultfollowing.com/babysteps/blog/2015/12/18/rayon-data-parallelism-in-rust/</a>:</p>
<pre style="padding-left: 30px">fn join&lt;A,B&gt;(oper_a: A, oper_b: B)<br/>    where A: FnOnce() + Send,<br/>          B: FnOnce() + Send,<br/>{<br/>    // Advertise `oper_b` to other threads as something<br/>    // they might steal:<br/>    let job = push_onto_local_queue(oper_b);<br/>    <br/>    // Execute `oper_a` ourselves:<br/>    oper_a();<br/>    <br/>    // Check whether anybody stole `oper_b`:<br/>    if pop_from_local_queue(oper_b) {<br/>        // Not stolen, do it ourselves.<br/>        oper_b();<br/>    } else {<br/>        // Stolen, wait for them to finish. In the<br/>        // meantime, try to steal from others:<br/>        while not_yet_complete(job) {<br/>            steal_from_others();<br/>        }<br/>        result_b = job.result();<br/>    }<br/>}</pre>
<p>By the way, the recursive Fibonacci implementation provided in this example [34] is easy to look at and illustrates the point of using <kbd>rayon::join</kbd>, but is also really, really inefficient. To learn why, and how to improve on it, check out the <a href="f2c7ca21-145e-40af-8502-8b42b37fe290.xhtml">Chapter 10</a>, <a href="f2c7ca21-145e-40af-8502-8b42b37fe290.xhtml"/><em>Using Experimental Nightly Features</em>; <em>Benchmarking your code</em><em>.</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span><em>Benchmarking your code</em> recipe in</span> <a href="f2c7ca21-145e-40af-8502-8b42b37fe290.xhtml">Chapter 10</a>, <em>Using Experimental Nightly Features</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sharing resources in multithreaded closures</h1>
                </header>
            
            <article>
                
<p>It's time to look at parallelism at a lower level, without any crates to help us. We will now check out how to share a resource across threads so that they all can work with the same object. This recipe will also serve as a refresher on manually creating threads, in case it's been a while since you learned about it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>
<p>In the folder <kbd>bin</kbd>, create a file called <kbd>sharing_in_closures.rs</kbd>.</p>
</li>
<li>
<p>Add the following code and run it with <kbd>cargo run --bin sharing_in_closures</kbd>:</p>
</li>
</ol>
<pre style="padding-left: 60px">1  use std::thread;<br/>2  use std::sync::Arc;<br/>3  <br/>4  fn main() {<br/>5    // An Arc ("Atomically Reference Counted") is used the exact<br/>6    // same way as an Rc, but also works in a parallel context<br/>7    let some_resource = Arc::new("Hello World".to_string());<br/>8 <br/>9    // We use it to give a new thread ownership of a clone of the <br/>        Arc<br/>10   let thread_a = {<br/>11     // It is very common to give the clone the same name as the <br/>          original<br/>12     let some_resource = some_resource.clone();<br/>13     // The clone is then moved into the closure:<br/>14     thread::spawn(move || {<br/>15       println!("Thread A says: {}", some_resource);<br/>16      })<br/>17   };<br/>18   let thread_b = {<br/>19       let some_resource = some_resource.clone();<br/>20       thread::spawn(move || {<br/>21         println!("Thread B says: {}", some_resource);<br/>22        })<br/>23   };<br/>24 <br/>25  // .join() blocks the main thread until the other thread is done<br/>26  thread_a.join().expect("Thread A panicked");<br/>27  thread_b.join().expect("Thread B panicked");<br/>28  }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">A fundamental building block of parallelism in Rust is the <kbd>Arc</kbd>, which stands for <strong>Atomically Reference Counted</strong>. Functionally, it works the same way as an <kbd>Rc</kbd>, which we have looked at in <a href="6b8b0c3c-2644-4684-b1f4-b1e08d62450c.xhtml">Chapter 5</a>, <em>Advanced Data Structures</em>; <em>Sharing ownership with smart pointers</em>. The only difference is that the reference counting is done using <em>atomic primitives</em>, which are versions of primitive data types like <kbd>usize</kbd> that have well-defined parallel interactions. This has two consequences:</p>
<ul>
<li>An <kbd>Arc</kbd> is slightly slower than an <kbd>Rc</kbd>, as the reference counting involves a bit more work</li>
<li>An <kbd>Arc</kbd> can be used safely across threads</li>
</ul>
<p>The constructor of <kbd>Arc</kbd> looks the same as <kbd>Rc</kbd>[7]:</p>
<pre style="padding-left: 30px">let some_resource = Arc::new("Hello World".to_string());</pre>
<p>This creates an <kbd>Arc</kbd> over a <kbd>String</kbd>. A <kbd>String</kbd> is a <kbd>struct</kbd> that is not inherently saved to be manipulated across threads. In Rust terms, we say that <kbd>String</kbd> is not <kbd>Sync</kbd> (more about that later in the recipe <em>Atomically access primitives</em>).</p>
<p>Now let's look at how a thread is initialized. <kbd>thread::spawn()</kbd> takes a closure and executes it in a new thread. Because this is done in parallel, the main thread doesn't wait until the thread is done; it continues working right after its creation.</p>
<p>The following creates a thread that prints out the content of <kbd>some_resource</kbd> and gives us a handle to that thread called <kbd>thread_a</kbd>[10]:</p>
<pre>    let thread_a = {<br/>        let some_resource = some_resource.clone();<br/>        thread::spawn(move || {<br/>            println!("Thread A says: {}", some_resource);<br/>        })<br/>    };</pre>
<p>Afterward (or at the same time), we do the exact same thing in a second thread called <kbd>thread_b</kbd>.</p>
<p>To understand why we need an <kbd>Arc</kbd> and can't just pass the resource directly to the closure, let's take a closer look at how closures work.</p>
<p>Closures in Rust can only operate on three kinds of variables:</p>
<ul>
<li>Arguments passed to them</li>
<li><kbd>static</kbd> variables (variables with the <kbd>'static</kbd> lifetime; see <a href="6b8b0c3c-2644-4684-b1f4-b1e08d62450c.xhtml" target="_blank">Chapter 5</a>, <em>Advanced Data Structures</em>; <em>Creating lazy static objects</em>)</li>
<li>Variables it owns, either by creating them or by moving them into the closure</li>
</ul>
<p>With this in mind, let's look at the most simplistic approach an inexperienced Rust programmer might take:</p>
<pre style="padding-left: 30px">let thread_a = thread::spawn(|| {<br/>    println!("Thread A says: {}", some_resource);<br/>});</pre>
<p>If we try to run this, the compiler tells us the following:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b102a819-7de0-4337-a3a5-008a19f5c43f.png"/></div>
<p>Seems like it doesn't like our usage of <kbd>some_resource</kbd>. Look at the rules for variable usage in closures again:</p>
<ul>
<li><kbd>some_resource</kbd> has not been passed as an argument</li>
<li>It is not <kbd>static</kbd></li>
<li>It was neither created in the closure nor moved into it</li>
</ul>
<p>But what does <em>closure may outlive the current function</em> mean? Well, because closures can be stored in a normal variable, they can be returned from a function. Imagine now if we programmed a function that created a variable called <kbd>some_resource</kbd>, used it inside a closure, and returned it. Since the function owns <kbd>some_resource</kbd>, it would be dropped while returning the closure, making any reference to it invalid. We don't want any invalid variables, so the compiler stops us from potentially enabling them. Instead, it suggests moving the ownership of <kbd>some_resource</kbd> into the closure by using the <kbd>move</kbd> keyword. Let's try that:</p>
<pre>    let thread_a = thread::spawn(move || {<br/>        println!("Thread A says: {}", some_resource);<br/>    });</pre>
<p>The compiler responds with this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/adf522f0-4203-4965-bc7c-ca33345a722e.png"/></div>
<p>Because we moved <kbd>some_resource</kbd> into the closure inside of <kbd>thread_a</kbd>, <kbd>thread_b</kbd> can no longer use it! The solution is to create a clone of the reference to <kbd>some_resource</kbd> and only move the clone into the closure:</p>
<pre>    let some_resource_clone = some_resource.clone();<br/>    let thread_a = thread::spawn(move || {<br/>        println!("Thread A says: {}", some_resource_clone);<br/>    });</pre>
<p>This now runs perfectly fine, but it looks a bit weird, as we are now carrying the mental baggage of the knowledge that the resource we're dealing with is, in fact, a <kbd>clone</kbd>. This can be solved in a more elegant way by putting the clone into a new scope, where it can have the same name as the original, leaving us with the final version of our code:</p>
<pre style="padding-left: 30px">let thread_a = {<br/>    let some_resource = some_resource.clone();<br/>    thread::spawn(move || {<br/>        println!("Thread A says: {}", some_resource);<br/>    })<br/>};</pre>
<p>Looks way clearer, doesn't it? This way of passing <kbd>Rc</kbd> and <kbd>Arc</kbd> variables to a closure is a well-known Rust idiom that we are going to use in all other recipes of the chapter from here on out.</p>
<p>The last thing we are going to do in this recipe is join the two threads by calling <kbd>.join()</kbd> on them [26 and 27]. Joining a thread means blocking the current thread until the joined thread is done with its work. It's called like that because we <em>join the two threads of our program back into a single one</em>. It helps to visually imagine actual sewing threads when thinking about this concept.</p>
<p>We join them before the end of the program, as otherwise, we would have no guarantee that they would actually run all the way through before our program quits. Generally speaking, you should <kbd>join</kbd> your threads when you need their results and can't wait for them any longer, or they're about to be dropped otherwise.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Sharing ownership with smart pointers</em> and <em>Creating lazy static objects</em> recipes in <a href="6b8b0c3c-2644-4684-b1f4-b1e08d62450c.xhtml" target="_blank">Chapter 5</a>, <em>Advanced Data Structures</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sending data across threads</h1>
                </header>
            
            <article>
                
<p>So far, we've looked at threads that work independently. Now, let's take a look at intertwined threads that need to share data. This situation is common when setting up servers, as the thread receiving client messages is usually not the same as the one that actually handles and responds to the client input. Rust gives us the concept of <em>channels</em> as a solution. A channel is split into a <em>sender</em> and a <em>receiver</em> which can share data across threads.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>
<p>Open the <kbd>Cargo.toml</kbd> file that was  generated earlier for you.</p>
</li>
<li>Under <kbd>[dependencies]</kbd>, add the following line:</li>
</ol>
<pre style="padding-left: 60px">rand = "0.4.2"</pre>
<ol start="3">
<li>If you want, you can go to rand's crates.io page (<a href="https://crates.io/crates/rand">https://crates.io/crates/rand</a>) to check for the newest version and use that one instead.</li>
<li>
<p>In the folder <kbd>bin</kbd>, create a file called <kbd>channels.rs</kbd>.</p>
</li>
<li>
<p>Add the following code and run it with <kbd>cargo run --bin channels</kbd>:</p>
</li>
</ol>
<pre style="padding-left: 60px">1  extern crate rand;<br/>2 <br/>3  use rand::Rng;<br/>4  use std::thread;<br/>5  // mpsc stands for "Multi-producer, single-consumer"<br/>6  use std::sync::mpsc::channel;<br/>7 <br/>8  fn main() {<br/>9     // channel() creates a connected pair of a sender and a <br/>        receiver.<br/>10    // They are usually called tx and rx, which stand for<br/>11    // "transmission" and "reception"<br/>12    let (tx, rx) = channel();<br/>13    for i in 0..10 {<br/>14        // Because an mpsc channel is "Multi-producer",<br/>15        // the sender can be cloned infinitely<br/>16        let tx = tx.clone();<br/>17        thread::spawn(move || {<br/>18           println!("sending: {}", i);<br/>19           // send() pushes arbitrary data to the connected <br/>              receiver<br/>20           tx.send(i).expect("Disconnected from receiver");<br/>21        });<br/>22    }<br/>23    for _ in 0..10 {<br/>24       // recv() blocks the current thread<br/>25       // until a message was received<br/>26       let msg = rx.recv().expect("Disconnected from sender");<br/>27       println!("received: {}", msg);<br/>28    }<br/>29 <br/>30    let (tx, rx) = channel();<br/>31    const DISCONNECT: &amp;str = "Goodbye!";<br/>32    // The following thread will send random messages<br/>33    // until a goodbye message was sent<br/>34    thread::spawn(move || {<br/>35        let mut rng = rand::thread_rng();<br/>36        loop {<br/>37          let msg = match rng.gen_range(0, 5) {<br/>38              0 =&gt; "Hi",<br/>39              1 =&gt; DISCONNECT,<br/>40              2 =&gt; "Howdy there, cowboy",<br/>41              3 =&gt; "How are you?",<br/>42              4 =&gt; "I'm good, thanks",<br/>43              _ =&gt; unreachable!(),<br/>44          };<br/>45          println!("sending: {}", msg);<br/>46          tx.send(msg).expect("Disconnected from receiver");<br/>47          if msg == DISCONNECT {<br/>48            break;<br/>49          }<br/>50       }<br/>51   });<br/>52 <br/>53   // An iterator over messages in a receiver is infinite.<br/>54   // It will block the current thread until a message is    <br/>     available<br/>55   for msg in rx {<br/>56        println!("received: {}", msg);<br/>57    }<br/>58  }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">As explained in the comments of the code, calling <kbd>std::sync::mpsc::channel()</kbd> generates a tuple consisting of a <kbd>Sender</kbd> and a <kbd>Receiver</kbd>, which are conventionally called <kbd>tx</kbd> for <em>transmission</em> and <kbd>rx</kbd> for <em>reception</em> [12].</p>
<div class="mce-root packt_infobox">This naming convention doesn't come from Rust, but has been a standard in the telecommunications industry since at least 1960 when the RS-232 (<strong>Recommended Standard 232</strong>) was introduced, detailing how computers and modems should communicate with each other.</div>
<p>These two halves of the same channel can communicate with each other independently of the current thread they're in. The module's name, <kbd>mspc</kbd>, tells us that this channel is a <kbd>Multi-producer, single-consumer</kbd> channel, which means that we can <kbd>clone</kbd> our sender as many times as we want. We can use this fact to our advantage when dealing with closures [16 to 21]:</p>
<pre style="padding-left: 30px">for i in 0..10 {<br/>    let tx = tx.clone();<br/>    thread::spawn(move || {<br/>        println!("sending: {}", i);<br/>        tx.send(i).expect("Disconnected from receiver");<br/>    });<br/>}</pre>
<p>We do not need to wrap our sender in an <kbd>Arc</kbd>, because it natively supports arbitrary cloning! Inside of the closure you can see the sender's main functionality. The <kbd>send()</kbd> method sends data across threads to the receiver. It will return an error if the receiver is not available anymore, as in when it is dropped too early. In this thread here, we will simply send the numbers <kbd>0</kbd> to <kbd>9</kbd> concurrently to the receiver. One thing to note is that because a channel's halves are statically typed, they are only going to be able to send one specific data type around. If the first thing you send is an <kbd>i32</kbd>, your channel will only work with <kbd>i32</kbd>. If you send a <kbd>String</kbd>, it will be a <kbd>String</kbd> channel.</p>
<p>On to the receiver we go [23 to 28]:</p>
<pre style="padding-left: 30px">for _ in 0..10 {<br/>    let msg = rx.recv().expect("Disconnected from sender");<br/>    println!("received: {}", msg);<br/>}</pre>
<p>The <kbd>recv()</kbd> method, which stands for <em>receive</em>, blocks the current thread until a message has arrived. Similar to its counterpart, it returns an error if the sender is unavailable. Because we know that we only sent 10 messages, we only call it 10 times. There is no need to explicitly <kbd>join</kbd> the threads we created for the sender, because <kbd>recv()</kbd> blocked the main thread until no more messages were left, which means that the sender finished sending all they had to send, that is, all the threads already finished their job. This way, we already joined them.</p>
<p>But in real life, you do not have a guarantee about the amount of times a client will send information to you. For a more realistic demonstration, we will now create a thread that sends random messages [37] to the receiver until it finally has enough and quits by sending <kbd>"Goodbye!"</kbd> [48]. Note how we created a new channel pair, as the old one was set to the type <kbd>i32 </kbd> because integer literals such as <kbd>1</kbd> or <kbd>2</kbd> are treated as <kbd>i32</kbd> by default.</p>
<p>While the sending code looks almost identical to the one before, the receiving end looks a bit different [55 to 57]:</p>
<pre>    for msg in rx {<br/>        println!("received: {}", msg);<br/>    }</pre>
<p>As you can see, a receiver can be iterated over. It behaves like an infinite iterator over all messages that will ever come, blocking when waiting for a new one, similar to calling <kbd>recv()</kbd> in a loop. The difference is that the iteration will automatically stop when the sender is unavailable. Because we terminate the sending thread when it sends <kbd>"Goodbye!"</kbd> [48], this iteration over the receiver will also stop when receiving it, as the sender will have been dropped at that point. Because this means that we have a guarantee about the sending thread being finished, we do not need to join it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>A channel is not <kbd>Sync</kbd> and, as such, can only be moved across channels but not shared between them. If you need the channel to be <kbd>Sync</kbd> you can use <kbd>std::sync::mpsc::sync_channel</kbd>, which blocks when a buffer of unanswered messages is full. An example for when this might be necessary is when a web framework offers to manage your types but only works with <kbd>Sync</kbd> structs. You can read more on <kbd>Sync</kbd> in the recipe <em>Atomically access primitives</em>.</p>
<p>The <kbd>mpsc</kbd> channels, as their name suggests, allow many senders but only one receiver. Most of the time, this will be good enough, but if you find yourself needing the exact opposite, as in one sender and multiple receivers, check out Sean McArthur's <kbd>spmc</kbd> crate at <a href="https://crates.io/crates/spmc">https://crates.io/crates/spmc</a>, which provides you with <kbd>Single-producer, multi-consumer</kbd> channels.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span><em>Access collections as Iterator</em> recipe in </span><a href="977b8621-cb73-43de-9a2b-4bc9f5583542.xhtml" target="_blank">Chapter 2</a>, <em>Working with Collections</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Accessing resources in parallel with RwLocks</h1>
                </header>
            
            <article>
                
<p>When we shared resources with an <kbd>Arc</kbd>, we only did so immutably. The moment we want our threads to mutate our resources, we need to use some kind of locking mechanism to secure the golden rule of parallelism: multiple readers or one writer. <kbd>RwLock</kbd> enforces just that rule across threads and blocks them if they violate the rule.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>
<p>In the folder <kbd>bin</kbd>, create a file called <kbd>rw_lock.rs</kbd>.</p>
</li>
<li>
<p>Add the following code and run it with <kbd>cargo run --bin rwlock</kbd>:</p>
</li>
</ol>
<pre style="padding-left: 60px">1 use std::sync::{Arc, RwLock};<br/>2 use std::thread;<br/>3 <br/>4 fn main() {<br/>5 // An RwLock works like the RefCell, but blocks the current<br/>6 // thread if the resource is unavailable<br/>7 let resource = Arc::new(RwLock::new("Hello <br/>      World!".to_string()));<br/>8 <br/>9 // The reader_a thread will print the current content of<br/>10 // our resource fourty times<br/>11 let reader_a = {<br/>12 let resource = resource.clone();<br/>13 thread::spawn(move || {<br/>14 for _ in 0..40 {<br/>15 // Lock resource for reading access<br/>16 let resource = resource<br/>17 .read()<br/>18 .expect("Failed to lock resource for reading");<br/>19 println!("Reader A says: {}", resource);<br/>20 }<br/>21 })<br/>22 };<br/>23 <br/>24 // The reader_b thread will print the current content of<br/>25 // our resource fourty times as well. Because RwLock allows<br/>26 // multiple readers, it will execute at the same time as <br/>        reader_a<br/>27 let reader_b = {<br/>28 let resource = resource.clone();<br/>29 thread::spawn(move || {<br/>30 for _ in 0..40 {<br/>31 // Lock resource for reading access<br/>32 let resource = resource<br/>33 .read()<br/>34 .expect("Failed to lock resource for reading");<br/>35 println!("Reader B says: {}", resource);<br/>36 }<br/>37 })<br/>38 };<br/>39 <br/>40 // The writer thread will modify the resource ten times.<br/>41 // Because RwLock enforces Rust's access rules<br/>42 // (multiple readers xor one writer), this thread will wait <br/>          until<br/>43 // thread_a and thread_b are not using the resource and then <br/>         block<br/>44 // them both until its done.<br/>45 let writer = {<br/>46 let resource = resource.clone();<br/>47 thread::spawn(move || {<br/>48 for _ in 0..10 {<br/>49 // Lock resource for writing access<br/>50 let mut resource = resource<br/>51 .write()<br/>52 .expect("Failed to lock resource for writing");<br/>53 <br/>54 resource.push('!');<br/>55 }<br/>56 })<br/>57 };<br/>58 <br/>59 reader_a.join().expect("Reader A panicked");<br/>60 reader_b.join().expect("Reader B panicked");<br/>61 writer.join().expect("Writer panicked");<br/>62 }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The <kbd>RwLock</kbd> is the parallel equivalent of the <kbd>RefCell</kbd> we worked with in <a href="6b8b0c3c-2644-4684-b1f4-b1e08d62450c.xhtml" target="_blank">Chapter 5</a>, <em>Advanced Data Structures</em><em>;</em> <em>Working with interior mutability</em>. The big difference is that, while <kbd>RefCell</kbd> panics on a violation of Rust's ownership concept, <kbd>RwLock</kbd> simply blocks the current thread until the violation is over.</p>
<p>The analog of the <kbd>borrow()</kbd> method of <kbd>RefCell</kbd> is <kbd>read()</kbd> [17], which locks the resource for immutable access. The analog of <kbd>borrow_mut()</kbd> is <kbd>write()</kbd> [51], which locks the resource for mutable access. Makes sense, doesn't it?</p>
<p>These methods return a <kbd>Result</kbd>, which tells us whether the thread is <em>poisoned</em>. The meaning of poisoning is different for every lock. In an <kbd>RwLock</kbd>, it means that the thread that locked the resource for <kbd>write</kbd> access panicked. This way, you can react to panics in other threads and treat them in some way. One example where this can be useful is sending some logs to a server before a crash happens in order to diagnose the problem. In most cases, though, it will be okay if you simply <kbd>panic</kbd> along, as a <kbd>panic</kbd> usually stands for a critical failure that cannot be mended.</p>
<p>In our example, we demonstrate the concept by setting up two threads that request <kbd>read</kbd> access: <kbd>reader_a</kbd> [11] and <kbd>reader_b</kbd> [27]. Because an <kbd>RwLock</kbd> allows multiple readers, they will concurrently print out the value of our resource [19 and 35]. In the meantime, <kbd>writer</kbd> [45] tries to lock the resource for <kbd>write</kbd> access. It will have to wait until both <kbd>reader_a</kbd> and <kbd>reader_b</kbd> are currently not using the resource. By the same rules, when the <kbd>writer</kbd> gets their turn and mutates the resource [54], both <kbd>reader_a</kbd> and <kbd>reader_b</kbd> have to wait until it's done.</p>
<p>Because all of this happens roughly at the same time, every execution of this example is going to give you slightly different results. I encourage you to run the program multiple times and compare the output.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Despite its nice usability, <kbd>RwLock</kbd> is still no silver bullet for all concurrent problems. There is a concept in concurrent programming called <em>deadlock</em>. It arises when two processes wait for the unlocking of resources that the other holds. This will lead to them waiting forever, as no one is ready to take the first step. Kind of like teenagers in love. An example of this would be a <kbd>writer_a</kbd> requesting access to a file that <kbd>writer_b</kbd> holds. <kbd>writer_b</kbd>, in the meantime, needs some kind of user information from <kbd>writer_a</kbd> before he can give up the file lock. The best way to avoid this problem is to keep it in the back of your mind and remember it when you're about to create processes that depend on each other.</p>
<p>Another lock that is fairly popular in other languages is the <kbd>Mutex</kbd>, which Rust also provides under <kbd>std::sync::Mutex</kbd>. When it locks resources, it treats every process like a writer, so no two threads will <em>ever</em> be able to work at the same time with a <kbd>Mutex</kbd>, even if they don't mutate the data. We are going to create a very simple implementation of it ourselves in the next recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span><em>Working with interior mutability</em> recipe in </span><a href="6b8b0c3c-2644-4684-b1f4-b1e08d62450c.xhtml" target="_blank">Chapter 5</a>, <em>Advanced Data Structures</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Atomically accessing primitives</h1>
                </header>
            
            <article>
                
<p>When reading about all of these parallel structures, you might have wondered how they are implemented. In this recipe, we are going to take a look under the hood and learn about the most basic parallel data types, which are called <em>atomics</em>. We are going to do this by implementing our very own <kbd>Mutex</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>
<p>In the folder <kbd>bin</kbd>, create a file called <kbd>atomic.rs</kbd>.</p>
</li>
<li>
<p>Add the following code and run it with <kbd>cargo run --bin atomic</kbd>:</p>
</li>
</ol>
<pre style="padding-left: 60px">1 use std::sync::Arc;<br/>2 use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering, ATOMIC_BOOL_INIT, ATOMIC_USIZE_INIT};<br/>3 use std::thread;<br/>4 use std::ops::{Deref, DerefMut};<br/>5 use std::cell::UnsafeCell;<br/>6 <br/>7 fn main() {<br/>8 // Atomics are primitive types suited for<br/>9 // well defined concurrent behaviour<br/>10 let some_number = AtomicUsize::new(0);<br/>11 // They are usually initialized by copying them from<br/>12 // their global constants, so the following line does the same:<br/>13 let some_number = ATOMIC_USIZE_INIT;<br/>14 <br/>15 // load() gets the current value of the atomic<br/>16 // Ordering tells the compiler how exactly to handle the <br/>         interactions<br/>17 // with other threads. SeqCst ("Sequentially Consistent") can <br/>         always be used<br/>18 // as it results in the same thing as if no parallelism was <br/>         involved<br/>19 let curr_val = some_number.load(Ordering::SeqCst);<br/>20 println!("The current value of some_number is {}", curr_val);<br/>21 <br/>22 // store() sets the variable<br/>23 some_number.store(123, Ordering::SeqCst);<br/>24 let curr_val = some_number.load(Ordering::SeqCst);<br/>25 println!("The current value of some_number is {}", curr_val);<br/>26 <br/>27 // swap() sets the variable and returns the old value<br/>28 let old_val = some_number.swap(12_345, Ordering::SeqCst);<br/>29 let curr_val = some_number.load(Ordering::SeqCst);<br/>30 println!("The old value of some_number was {}", old_val);<br/>31 println!("The current value of some_number is {}", curr_val);<br/>32 <br/>33 // compare_and_swap only swaps the variable if it<br/>34 // is currently equal to the first argument.<br/>35 // It will always return the old variable<br/>36 let comparison = 12_345;<br/>37 let new_val = 6_789;<br/>38 let old_val = some_number.compare_and_swap(comparison, new_val, <br/>      Ordering::SeqCst);<br/>39 if old_val == comparison {<br/>40 println!("The value has been updated");<br/>41 }<br/>42 <br/>43 // The previous atomic code is equivalent to<br/>44 // the following sequential code<br/>45 let mut some_normal_number = 12_345;<br/>46 let old_val = some_normal_number;<br/>47 if old_val == comparison {<br/>48 some_normal_number = new_val;<br/>49 println!("The value has been updated sequentially");<br/>50 }<br/>51 <br/>52 // fetch_add() and fetch_sub() add/subtract a number from the <br/>         value,<br/>53 // returning the old value<br/>54 let old_val_one = some_number.fetch_add(12, Ordering::SeqCst);<br/>55 let old_val_two = some_number.fetch_sub(24, Ordering::SeqCst);<br/>56 let curr_val = some_number.load(Ordering::SeqCst);<br/>57 println!(<br/>58 "some_number was first {}, then {} and is now {}",<br/>59 old_val_one, old_val_two, curr_val<br/>60 );<br/>61 <br/>62 // fetch_or() performs an "or" ("||") operation on the variable <br/>         and<br/>63 // an argument and sets the variable to the result. It then <br/>         returns the old value.<br/>64 // For the other logical operations, fetch_and(), fetch_nand() <br/>         and fetch_xor also exist<br/>65 let some_bool = ATOMIC_BOOL_INIT;<br/>66 let old_val = some_bool.fetch_or(true, Ordering::SeqCst);<br/>67 let curr_val = some_bool.load(Ordering::SeqCst);<br/>68 println!("({} || true) is {}", old_val, curr_val);<br/>69 <br/>70 // The following is a demonstration of our own Mutex <br/>         implementation,<br/>71 // based on an AtomicBool that checks if it's locked or not<br/>72 let naive_mutex = Arc::new(NaiveMutex::new(1));<br/>73 <br/>74 // The updater thread will set the value in the mutex to 2<br/>75 let updater = {<br/>76 let naive_mutex = naive_mutex.clone();<br/>77 thread::spawn(move || {<br/>78 let mut val = naive_mutex.lock();<br/>79 *val = 2;<br/>80 })<br/>81 };<br/>82 <br/>83 // The updater thread will print the value in the mutex<br/>84 let printer = {<br/>85 let naive_mutex = naive_mutex.clone();<br/>86 thread::spawn(move || {<br/>87 let val = naive_mutex.lock();<br/>88 println!("The value in the naive mutex is: {}", *val);<br/>89 })<br/>90 };<br/>91 <br/>92 // The exact order of execution is unpredictable,<br/>93 // but our mutex guarantees that the two threads will<br/>94 // never access the data at the same time<br/>95 updater.join().expect("The updater thread panicked");<br/>96 printer.join().expect("The printer thread panicked");<br/>97 }</pre>
<ol start="3">
<li>Now comes the implementation of our very own homemade mutex:</li>
</ol>
<pre style="padding-left: 60px">99 // NaiveMutex is an easy, albeit very suboptimal,<br/>100 // implementation of a Mutex, similar to std::sync::Mutex<br/>101 // A mutex is a lock that only allows one thread to access a    <br/>    ressource at all times<br/>102 pub struct NaiveMutex&lt;T&gt; {<br/>103 locked: AtomicBool,<br/>104 // UnsafeCell is the underlying struct of every<br/>105 // internally mutable container such as ours<br/>106 data: UnsafeCell&lt;T&gt;,<br/>107 }<br/>108 <br/>109 // This is a RAII guard, identical to the one from the last                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <br/>    chapter<br/>110 pub struct NaiveMutexGuard&lt;'a, T: 'a&gt; {<br/>111 naive_mutex: &amp;'a NaiveMutex&lt;T&gt;,<br/>112 }<br/>113 <br/>114 impl&lt;T&gt; NaiveMutex&lt;T&gt; {<br/>115 pub fn new(data: T) -&gt; Self {<br/>116 NaiveMutex {<br/>117 locked: ATOMIC_BOOL_INIT,<br/>118 data: UnsafeCell::new(data),<br/>119 }<br/>120 }<br/>121 <br/>122 pub fn lock(&amp;self) -&gt; NaiveMutexGuard&lt;T&gt; {<br/>123 // The following algorithm is called a "spinlock", because it <br/>             keeps<br/>124 // the current thread blocked by doing nothing (it keeps it <br/>             "spinning")<br/>125 while self.locked.compare_and_swap(false, true, <br/>          Ordering::SeqCst) {}<br/>126 NaiveMutexGuard { naive_mutex: self }<br/>127 }<br/>128 }<br/>129 <br/>130 // Every type that is safe to send between threads is automatically<br/>131 // safe to share between threads if wrapped in our mutex, as it<br/>132 // guarantees that no threads will access it ressource at the  <br/>same time<br/>133 unsafe impl&lt;T: Send&gt; Sync for NaiveMutex&lt;T&gt; {}<br/>134 <br/>135 // Automatically unlock the mutex on drop<br/>136 impl&lt;'a, T&gt; Drop for NaiveMutexGuard&lt;'a, T&gt; {<br/>137 fn drop(&amp;mut self) {<br/>138 self.naive_mutex.locked.store(false, Ordering::SeqCst);<br/>139 }<br/>140 }<br/>141 <br/>142 // Automatically dereference to the underlying data<br/>143 impl&lt;'a, T&gt; Deref for NaiveMutexGuard&lt;'a, T&gt; {<br/>144 type Target = T;<br/>145 fn deref(&amp;self) -&gt; &amp;T {<br/>146 unsafe { &amp;*self.naive_mutex.data.get() }<br/>147 }<br/>148 }<br/>149 <br/>150 impl&lt;'a, T&gt; DerefMut for NaiveMutexGuard&lt;'a, T&gt; {<br/>151 fn deref_mut(&amp;mut self) -&gt; &amp;mut T {<br/>152 unsafe { &amp;mut *self.naive_mutex.data.get() }<br/>153 }<br/>154 }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>As of the time of writing, there are four <kbd>atomic</kbd> types in the standard library under the <kbd>std::sync::atomic</kbd> module: <kbd>AtomicBool</kbd>, <kbd>AtomicIsize</kbd>, <kbd>AtomicUsize</kbd>, and <kbd>AtomicPtr</kbd>. Each one of them represents a primitive type, namely <kbd>bool</kbd>, <kbd>isize</kbd>, <kbd>usize</kbd>, and <kbd>*mut</kbd>. We are not going to look at the last, which, being a pointer, you will probably only have to deal with when interfacing with programs written in other languages anyways.</p>
<div class="packt_infobox">In case you haven't encountered <kbd>isize</kbd> and <kbd>usize</kbd> before, they are representations of the smallest amount of bytes needed to address any part of the memory of your machine. On 32-bit targets this is 4 bytes, while 64-bit systems will need 8 bytes. <kbd>isize</kbd> uses those bytes to represent a <em>signed</em> number, as in an integer that can be negative. <kbd>usize</kbd> instead represents an <em>unsigned</em> number, which can only be positive but has a lot more capacity for huge numbers in that direction. They are usually used when dealing with collections capacities. For example, <kbd>Vec</kbd> returns a <kbd>usize</kbd> when calling its <kbd>.len()</kbd> method. Additionally, on the nightly toolchain, there are atomic variants of all other concrete integer types like <kbd>u8</kbd> or <kbd>i32</kbd>.</div>
<p>The <kbd>atomic</kbd> versions of our primitives work the same way as their cousins, with one important distinction: they have well-defined behavior when used in parallel environments. All their methods take a parameter of the type <kbd>atomic::Ordering</kbd>, which stands for which low-level concurrent strategy to use. In this example, we are only going to use <kbd>Ordering::SeqCst</kbd>, which stands for s<em>equentially consistent</em>. This, in turn, means that the behavior is quite intuitive. If some data is stored or modified using this ordering, another thread can see its content after the write as if the two threads ran one after another. Or, in other words, the behavior is <em>consistent</em> with that of a <em>sequential</em> series of events. This strategy will always work with all parallel algorithms. All other orderings merely relax the constraints on the data involved in order to get some kind of performance benefit.</p>
<p class="mce-root">With this knowledge in hand, you should be able to understand most things done in <kbd>main</kbd> up to the usage of <kbd>NaiveMutex</kbd>[72]. Note how some of the <kbd>atomic</kbd> methods are just different ways of doing the same as with our normal primitives, with the added twist of specifying an ordering and most of them returning the old value. For instance, <kbd>some_number.fetch_add(12, Ordering::SeqCst)</kbd>, apart from returning the old value of <kbd>some_number</kbd>, is essentially nothing but <kbd>some_number += 12</kbd>.</p>
<p>A real use case for atomics comes up in the second part of the example code, where we implement our very own <kbd>Mutex</kbd>. A mutex, prominently featured in all modern programming languages, is a kind of lock that does not allow <em>any</em> two threads to access a resource at the same time. After reading the last recipe, you know that you can imagine a <kbd>Mutex</kbd> as a kind of <kbd>RwLock</kbd> that always locks everything in <kbd>write</kbd> mode.</p>
<p class="mce-root">Let's jump a few lines forward in our code to [102]:</p>
<pre style="padding-left: 30px">pub struct NaiveMutex&lt;T&gt; {<br/>    locked: AtomicBool,<br/>    data: UnsafeCell&lt;T&gt;,<br/>}</pre>
<p>As you can see, we are going to base our <kbd>NaiveMutex</kbd> on a simple atomic flag, <kbd>locked</kbd>, which is going to track whether our mutex is available or not. The other member, <kbd>data</kbd>, holds the underlying resource we are interested in locking. Its type, <kbd>UnsafeCell</kbd>, is the underlying type of every struct that implements some kind of interior mutability (see <a href="6b8b0c3c-2644-4684-b1f4-b1e08d62450c.xhtml" target="_blank">Chapter 5</a>, <em>Advanced Data Structures</em>; <em>Working with interior mutability</em>).</p>
<p>The next struct is going to look familiar to you if you've read <a href="d2c7b7cb-3060-40b8-adb4-408eee7940a1.xhtml" target="_blank">Chapter 6</a>, <em>Handling Errors</em>; <em>Understanding RAII</em>, as it's an RAII guard with a reference to its parent [110]:</p>
<pre style="padding-left: 30px">pub struct NaiveMutexGuard&lt;'a, T: 'a&gt; {<br/>    naive_mutex: &amp;'a NaiveMutex&lt;T&gt;,<br/>}</pre>
<p>Let's take a look at how we lock a thread:</p>
<pre style="padding-left: 30px">pub fn lock(&amp;self) -&gt; NaiveMutexGuard&lt;T&gt; {<br/>  while self.locked.compare_and_swap(false, true, Ordering::SeqCst) {}<br/>    NaiveMutexGuard { naive_mutex: self }<br/>}</pre>
<p>Looks a bit weird at first glance, doesn't it? <kbd>compare_and_swap</kbd> is one of the more complex <kbd>atomic</kbd> operations. It works as follows:</p>
<ol>
<li>It compares the value of the atomic with the first parameter</li>
<li>If they are the same, it stores the second parameter in the atomic</li>
<li>Lastly, it returns the value of the atomic from before the function call</li>
</ol>
<p>Let's apply that to our call:</p>
<ul>
<li><kbd>compare_and_swap</kbd> checks if <kbd>self.locked</kbd> contains <kbd>false</kbd></li>
<li>If so, it sets <kbd>self.locked</kbd> to <kbd>true</kbd></li>
<li>In any case, it will return the old value</li>
</ul>
<p>If the returned value is <kbd>true</kbd>, it means our mutex is currently locked. What should our thread do then? Absolutely nothing: <kbd>{ }</kbd>. Because we call this in a <kbd>while</kbd> loop, we will continue doing nothing (this is called <em>spinning</em>) until the situation changes. This algorithm is called <strong>spinlock</strong>.</p>
<p>When our mutex is finally available, we set its <kbd>locked</kbd> flag to <kbd>true</kbd> and return an RAII guard with a reference to our <kbd>NaiveMutex</kbd>.</p>
<div class="packt_infobox">This is not how the real <kbd>std::sync::Mutex</kbd> is implemented. Because exclusively locking a resource is a very basic concurrent task, operating systems natively support it. The <kbd>Mutex</kbd> implemented in the Rust standard library is still built by the RAII pattern as well, but uses the OS's mutex handles instead of our custom logic. Fun fact—the Windows implementation uses SRWLocks (<a href="https://msdn.microsoft.com/en-us/library/windows/desktop/aa904937(v=vs.85).aspx">https://msdn.microsoft.com/en-us/library/windows/desktop/aa904937(v=vs.85).aspx</a>), which are Windows's native version of <kbd>RwLock</kbd>, as they proved to be faster than a native <kbd>Mutex</kbd>. So, on Windows at least, the two types really are very similar.</div>
<p>The implementation of <kbd>NaiveMutexGuard</kbd> provides the counterpart of <kbd>lock</kbd> during its dropping [138]:</p>
<pre style="padding-left: 30px">fn drop(&amp;mut self) {<br/>    self.naive_mutex.locked.store(false, Ordering::SeqCst);<br/>}</pre>
<p class="mce-root">We simply <kbd>store</kbd> the value <kbd>false</kbd> in <kbd>self.locked</kbd> whenever our guard goes out of scope (see <a href="d2c7b7cb-3060-40b8-adb4-408eee7940a1.xhtml" target="_blank">Chapter 6</a>, <em>Handling Errors</em>; <em>Implementing the Drop trait</em><em>)</em>. The next two trait <kbd>NaiveMutexGuard</kbd> implements are <kbd>Deref</kbd> and <kbd>DerefMut</kbd>, which let us call methods of type <kbd>T</kbd> directly on <kbd>NaiveMutexGuard&lt;T&gt;</kbd>. They both share nearly the same implementation [146]:</p>
<pre style="padding-left: 30px">unsafe { &amp;*self.naive_mutex.data.get() }</pre>
<p>Remember when we said that you'll have to deal with pointers on rare occasions? Well, this is one of those times.</p>
<p class="mce-root"><kbd>UnsafeCell</kbd> doesn't guarantee any borrowing safety, hence the name and the <kbd>unsafe</kbd> block. It relies on you to make sure all calls to it are actually safe. Because of this, it gives you a raw mutable pointer, which you can manipulate in any way you want. What we do here is dereference it with <kbd>*</kbd>, so <kbd>*mut T</kbd> becomes only <kbd>T</kbd>. Then we return a normal reference to that with <kbd>&amp;</kbd>[146]. The only thing different in the implementation of <kbd>deref_mut</kbd> is that we instead return a mutable reference with <kbd>&amp;mut</kbd> [152]. All of our <kbd>unsafe</kbd> calls are guaranteed to follow Rust's ownership principles, as we only allow one scope to borrow our resource anyway.</p>
<p>The last thing required for our <kbd>Mutex</kbd> implementation is the following line, which we skipped before:</p>
<pre style="padding-left: 30px">unsafe impl&lt;T: Send&gt; Sync for NaiveMutex&lt;T&gt; {}</pre>
<p>The <kbd>Sync</kbd> trait has a pretty small implementation, right? That's because it is a <em>marker</em>. It belongs to a family of traits that don't actually do anything themselves but only exist to tell the compiler something about the types that implement them. Another trait in the <kbd>std::marker</kbd> module is <kbd>Send</kbd>, which we also use here.</p>
<p>If a type <kbd>T</kbd> implements <kbd>Send</kbd>, it tells the world that it is safe to be moved (<em>sent</em>) between threads by passing it around as a value instead of a reference. Nearly all types of Rust implement <kbd>Send</kbd>.</p>
<p>If <kbd>T</kbd> is <kbd>Sync</kbd>, it tells the compiler that it is safe to be shared between threads (it behaves in a <em>synchronized</em> way) by passing it around per reference, <kbd>&amp;T</kbd>. This is harder to accomplish than <kbd>Send</kbd>, but our <kbd>NaiveMutex</kbd> guarantees that types in it can be shared around, as we only allow one access to its inner type at a time. This is why we implement the <kbd>Sync</kbd> trait for every <kbd>Send</kbd> in our mutex. If it's safe to pass it around, it's automatically also safe to share it within <kbd>NaiveMutex</kbd>.</p>
<p>Back in <kbd>main</kbd> you can now find some usage examples of our <kbd>Mutex</kbd>[75 and 84], similar to the examples in the previous recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Because <kbd>SeqCst</kbd> is good enough for most applications and the complexity involved in all other orderings, we are not going to look at any others. Don't be disappointed, however—Rust uses nearly the same <kbd>atomic</kbd> layout and functionality as C++, so there are plenty of sources to tell you how complex the issue really is. Anthony Williams, author of the well-known book <em>C++: Concurrency In Action</em> (<a href="http://www.cplusplusconcurrencyinaction.com/">http://www.cplusplusconcurrencyinaction.com/</a>), uses an entire 45 pages (!) to simply describe all the atomic orderings and how to use them. An additional 44 pages go into showing examples of all of these orderings. Does an average program benefit from this level of dedication? Let's look at the man's own words, with the background knowledge that <kbd>std::memory_order_seq_cst</kbd> is how C++ calls <kbd>SeqCst</kbd>:</p>
<div class="mce-root packt_quote">The basic premise is: do not use anything other than <kbd>std::memory_order_seq_cst</kbd> (the default) unless (a) you really <strong>really</strong> know what you are doing, and can <strong>prove</strong> that the relaxed usage is safe in all cases, and (b) your profiler demonstrates that the data structure and operations you are intending to use the relaxed orderings with are a bottleneck.</div>
<p class="mce-root">Source: <a href="https://stackoverflow.com/a/9564877/5903309">https://stackoverflow.com/a/9564877/5903309</a></p>
<p>In short, you should wait to learn about the different kinds of orderings until you have a very good reason to use them. This is, by the way, also the approach of Java, which makes all variables marked as <kbd>volatile</kbd> behave in a sequentially consistent way.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span><em>Working with interior mutability</em> recipe in </span><a href="6b8b0c3c-2644-4684-b1f4-b1e08d62450c.xhtml" target="_blank">Chapter 5</a>, <em>Advanced Data Structures</em></li>
<li><span><em>Implementing the Drop trait</em> and <em>Understanding RAII</em> recipe in </span><a href="d2c7b7cb-3060-40b8-adb4-408eee7940a1.xhtml" target="_blank">Chapter 6</a>, <em>Handling Errors</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting it all together in a connection handler</h1>
                </header>
            
            <article>
                
<p>We have looked at a lot of different practices in isolation now. The true strength of these building blocks, however, comes from combining them. This recipe is going to show you how to combine some of them into a realistic starting point for the connection handling part of a server.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>
<p>In the folder <kbd>bin</kbd>, create a file called <kbd>connection_handler.rs</kbd>.</p>
</li>
<li>
<p>Add the following code and run it with <kbd>cargo run --bin connection_handler</kbd>:</p>
</li>
</ol>
<pre style="padding-left: 60px">1 use std::sync::{Arc, RwLock};<br/>2 use std::net::Ipv6Addr;<br/>3 use std::collections::HashMap;<br/>4 use std::{thread, time};<br/>5 use std::sync::atomic::{AtomicUsize, Ordering, ATOMIC_USIZE_INIT};<br/>6 <br/>7 // Client holds whatever state your client might have<br/>8 struct Client {<br/>9 ip: Ipv6Addr,<br/>10 }<br/>11 <br/>12 // ConnectionHandler manages a list of connections<br/>13 // in a parallelly safe way<br/>14 struct ConnectionHandler {<br/>15 // The clients are identified by a unique key<br/>16 clients: RwLock&lt;HashMap&lt;usize, Client&gt;&gt;,<br/>17 next_id: AtomicUsize,<br/>18 }<br/>19 <br/>20 impl Client {<br/>21 fn new(ip: Ipv6Addr) -&gt; Self {<br/>22 Client { ip }<br/>23 }<br/>24 }<br/>25 <br/>26 impl ConnectionHandler {<br/>27 fn new() -&gt; Self {<br/>28 ConnectionHandler {<br/>29 clients: RwLock::new(HashMap::new()),<br/>30 next_id: ATOMIC_USIZE_INIT,<br/>31 }<br/>32 }<br/>33 <br/>34 fn client_count(&amp;self) -&gt; usize {<br/>35 self.clients<br/>36 .read()<br/>37 .expect("Failed to lock clients for reading")<br/>38 .len()<br/>39 }<br/>40 <br/>41 fn add_connection(&amp;self, ip: Ipv6Addr) -&gt; usize {<br/>42 let last = self.next_id.fetch_add(1, Ordering::SeqCst);<br/>43 self.clients<br/>44 .write()<br/>45 .expect("Failed to lock clients for writing")<br/>46 .insert(last, Client::new(ip));<br/>47 last<br/>48 }<br/>49 <br/>50 fn remove_connection(&amp;self, id: usize) -&gt; Option&lt;()&gt; {<br/>51 self.clients<br/>52 .write()<br/>53 .expect("Failed to lock clients for writing")<br/>54 .remove(&amp;id)<br/>55 .and(Some(()))<br/>56 }<br/>57 }</pre>
<p style="padding-left: 60px">Using our connection handler by simulating connecting and disconnecting clients:</p>
<pre style="padding-left: 60px">59 fn main() {<br/>60 let connections = Arc::new(ConnectionHandler::new());<br/>61 <br/>62 // the connector thread will add a new connection every now and <br/>        then<br/>63 let connector = {<br/>64 let connections = connections.clone();<br/>65 let dummy_ip = Ipv6Addr::new(0, 0, 0, 0, 0, 0xffff, 0xc00a, <br/>          0x2ff);<br/>66 let ten_millis = time::Duration::from_millis(10);<br/>67 thread::spawn(move || {<br/>68 for _ in 0..20 {<br/>69 connections.add_connection(dummy_ip);<br/>70 thread::sleep(ten_millis);<br/>71 }<br/>72 })<br/>73 };<br/>74 <br/>75 // the disconnector thread will remove the third connection at <br/>         some point<br/>76 let disconnector = {<br/>77 let connections = connections.clone();<br/>78 let fifty_millis = time::Duration::from_millis(50);<br/>79 thread::spawn(move || {<br/>80 thread::sleep(fifty_millis);<br/>81 connections.remove_connection(2);<br/>82 })<br/>83 };<br/>84 <br/>85 // The main thread will print the active connections in a short <br/>         interval<br/>86 let five_millis = time::Duration::from_millis(5);<br/>87 for _ in 0..40 {<br/>88 let count = connections.client_count();<br/>89 println!("Active connections: {}", count);<br/>90 thread::sleep(five_millis);<br/>91 }<br/>92 <br/>93 connector.join().expect("The connector thread panicked");<br/>94 disconnector<br/>95 .join()<br/>96 .expect("The disconnector thread panicked");<br/>97 }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">This recipe doesn't introduce any new modules or concepts. It's here to provide you with a general idea of how to combine all the things you've learned in this recipe in a somewhat realistic context. Specifically, our context consists of code that manages clients that connect with us in some way.</p>
<p><kbd>Client</kbd> [8] holds all information relevant to a connection. As a basic example, it currently contains the client's IP address. Other possibilities would be the client's username, location, device, ping, and so on.</p>
<p>The <kbd>ConnectionHandler</kbd> [14] itself holds a list, more specifically a <kbd>HashMap</kbd>, of the active connections, indexed by a unique ID. Analogous to that, it also stores the ID for the next connection.</p>
<div class="packt_tip">We are using unique IDs instead of a <kbd>Vec&lt;Client&gt;</kbd> because clients might be able to connect, multiple times, to whatever service we are providing on the same device. The easiest example for this is multiple tabs open in a browser, all accessing the same website. Generally speaking, it is good practice to always hold your data behind unique keys to save yourself from trouble down the road.</div>
<p>The implementations of the structs should be straightforward. Methods that need to modify the <kbd>clients</kbd> member lock it with <kbd>.write()</kbd>, all others with <kbd>.read()</kbd>.</p>
<p>The code used to get a new ID at <kbd>add_connection</kbd> adds one to <kbd>next_id</kbd> and returns its last value, as usual for an <kbd>atomic</kbd>[42]:</p>
<pre style="padding-left: 30px">let last = self.next_id.fetch_add(1, Ordering::SeqCst);</pre>
<p>After adding the connection to the <kbd>clients</kbd>, we return the newly-acquired ID to the caller, so that they can store the ID however they want and reuse it when it's time to kick the client with <kbd>remove_connection</kbd> [50], which in turn returns an <kbd>Option</kbd> telling the caller if the removed ID was in the client list in the first place. We do not return the removed <kbd>Client</kbd> directly because that would reveal unnecessary implementation details to the user of <kbd>ConnectionHandler</kbd>.</p>
<p>The code in <kbd>main</kbd> simulates parallel access to the hypothetical service. A bunch of clients connect to our <kbd>ConnectionHandler</kbd> and some leave again. <kbd>thread::sleep</kbd> [70, 80 and 90] blocks the current thread for a specified time and is used here to simulate the effect of various events happening at irregular intervals, represented by the different waiting times for each task.</p>
<p>As with the <kbd>RwLock</kbd> example, this program will have very different output every time you run it, so try it out multiple times.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">If you need to react to the messages from your clients in a different thread, you can use <kbd>channel</kbd>, which we looked at earlier in the chapter. One use case for this would be programming an online video game. You'll want to aggregate all input from your players, react to it by simulating your world, and then broadcast local changes to the players, with each of these tasks happening concurrently in a single thread.</p>


            </article>

            
        </section>
    </body></html>
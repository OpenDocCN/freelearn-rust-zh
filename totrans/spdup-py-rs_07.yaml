- en: '*Chapter 8*: Structuring an End-to-End Python Package in Rust'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have covered enough Rust and `pyo3` to theoretically build a range
    of real-world solutions, we must be careful. It would not be good if we decided
    to reinvent the wheel in Rust and ended up with a slower outcome after coding
    the solution. Hence, understanding how to solve a problem and testing our implementation
    is important. In this chapter, we will be building a Python package using Rust
    that solves a simplified real-world problem and loads data from files to build
    a catastrophe model. We will structure the package in a manner where we can slot
    in extra functionality if our model gets more complex. Once we build our model,
    we will test it to see whether our implementation is worth it in terms of scaling
    and speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down a catastrophe modeling problem for our package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an end-to-end solution as a package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing and testing our package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter enables us to take what we have learned throughout the book and
    solve a real-world problem and handle data files. Testing our solution will also
    enable us to avoid spending too much time on a solution that will have a slower
    result, preventing us from potentially missing our shot at implementing Rust in
    Python systems at our place of work.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code and data for this chapter can be found at [https://github.com/PacktPublishing/Speed-up-your-Python-with-Rust/tree/main/chapter_eight](https://github.com/PacktPublishing/Speed-up-your-Python-with-Rust/tree/main/chapter_eight).
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down a catastrophe modeling problem for our package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The project that we are going to build is a catastrophe model. This is where
    we calculate the probability of a catastrophe such as a hurricane, flood, or terror
    attack happening in a particular geographical location. We could do this using
    longitude and latitude coordinates. However, if we are going to do this, it is
    going to take a lot of computational power and time with little benefit. For instance,
    if we were going to calculate the probability of the flooding at Charing Cross
    Hospital in London, we could use the coordinates *51.4869° N, 0.2195° W*.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we use the coordinates *51.4865° N, 0.2190° W,* we would still
    be hitting Charing Cross Hospital, despite us changing the coordinates by *0.0004°
    N, 0.0005° W.* We could change the coordinates even more and we would still be
    hitting Charing Cross Hospital. Therefore, we would be doing loads of computations
    to calculate repeatedly the probability of flooding of the same building, which
    is not efficient. To combat this, we can break down the locations into bins and
    give them a numerical value, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Geographical bins for a catastrophe model of an island'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.01_B17720.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Geographical bins for a catastrophe model of an island
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that if a line of data in our model referred to bin `25`, this
    means that the line of data is referring to land in the middle of our island that
    we are concerned with. We can make our calculations even more efficient. For instance,
    we can see that the squares in *Figure 8.1* with the coordinates of `33`, `35`,
    `47`, and `49` and `1`, `2`, `8`, and `9` are in the sea. Therefore, the probability
    of flooding in these squares is zero because it is already water, and there is
    nothing that we care about in terms of flooding in the sea. Because we are merely
    mapping our calculations onto these bins, nothing is stopping us from redefining
    all of the bins inside these squares as one bin.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we must perform only one operation to calculate the risk of flooding
    in all our sea bins and that would be zero because the sea is already flooded.
    In fact, nothing is stopping us from sticking to square classifications for one
    bin. Bin number 1 could be all the squares that are 100% inside the sea, saving
    us a lot of time. We can also go the other way. We can make some of our bins more
    refined. For instance, areas near the coast might have more nuanced gradients
    of flooding, as a small distance closer to the sea could greatly increase the
    risk of flooding; therefore, we could break bin number 26 down into smaller bins.
    To avoid being dragged into the weeds, we will just refer to arbitrary bin numbers
    in our model data. Catastrophe modeling is its own subject, and we are merely
    using it to show how to build Rust Python packages that can solve real problems
    as opposed to trying to build the most accurate catastrophe model. Now that we
    understand how we map geographical data with probabilities, we can move on to
    the calculation of those probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Like with the mapping of geographical data, probability calculations are more
    complex and nuanced than what we are going to cover in this book. Companies like
    OASISLMF work with academic departments at universities to model risks of catastrophes
    and the damage inflicted. However, there is an overarching theme that we must
    do when calculating these probabilities. We will have to calculate the total probability
    of damage using the probability of the event happening in the area, and the probability
    of the event causing damage. To do this, we must multiply these probabilities
    together. We also must break down the probability of the event happening at a
    certain intensity. For instance, a category one hurricane is less likely to cause
    damage to a building compared to a category five hurricane. Therefore, we are
    going to run these probability calculations for each intensity bin.
  prefs: []
  type: TYPE_NORMAL
- en: 'We cannot go any further in designing our process without looking at the data
    that we have available. The data is in the form of `CSV` files and is available
    in our GitHub repository stated in the *Technical requirements* section. The first
    data file that we can inspect is the `footprint.csv` file. This file presents
    the probability of a catastrophe with a certain intensity happening in an area:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Table_8.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that we have taken in a series of event IDs. We can merge the
    `footprint.csv` data with the event IDs we passed in. This enables us to map the
    event IDs that we passed in with an area, intensity, and probability of it happening.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have merged our geographical data, we can now look at our damage
    data in the `vulnerability.csv` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Table_8.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at this, we can merge the damage data of the intensity bin ID, duplicating
    whatever we need. We then must multiply the probabilities to get the total probability.
    The flow can be summed up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Catastrophe model flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.02_B17720.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – Catastrophe model flow
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering the data and flow, we can see that we now have events that have
    an intensity bin ID, damage bin ID, probability of the event happening in the
    area, and the probability of the event causing damage in a certain bin. These
    can then be passed on to another stage, which is the process of calculating financial
    losses. We will stop here, but we must remember that real-world applications need
    to adapt for expansion. For instance, there is interpolation. This is where we
    use a function to estimate the values across a bin, which is demonstrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_8.03_B17720.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Linear interpolation of a distribution
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that if we just use bins, our reading between `2` and `2.9`
    would be the same. We know that the distribution is increasing, so we use a simple
    linear function, and the value of our reading increases as the reading increases.
    There are other more complex functions we can use, but this can increase the accuracy
    of readings if the bins are too wide. While we will not be using interpolation
    in our example, it is a legitimate step that we might want to slot in later. Considering
    this, our processes need to be isolated.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is only one other thing that we must consider when designing our package,
    which is the storage of our model data. Our probabilities will be defined by an
    academic team that collected and analyzed a range of data sources and specific
    knowledge. For instance, damage to buildings requires structural engineering knowledge
    and knowledge of hurricanes. While we might expect our teams to update the models
    in later releases, we do not want the end user to easily manipulate data. We also
    do not want to hardcode the data into our Rust code; therefore, storing `CSV`
    files in our package would be useful for this demonstration. Considering this,
    our package should take the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The structure should be familiar to you. In the preceding file structure, we
    can see that our merge processes for the probability of the event happening and
    the damage are in their own folders. Data structures for the process are housed
    in the `structs.rs` file and functions around the process are defined in the `processes.rs`
    file. The `flitton_oasis_risk_modelling` folder will house our compiled Rust code;
    therefore, our `CSV` files are also stored there.
  prefs: []
  type: TYPE_NORMAL
- en: We state that we are storing our `CSV` files in the `MANIFEST.in` file. Our
    `lib.rs` file is where our interface between our Rust and Python is defined. Now
    that we have defined the process for our catastrophe model, we can move on to
    the next section of building our end-to-end package.
  prefs: []
  type: TYPE_NORMAL
- en: Building an end-to-end solution as a package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we identified what we needed to do to build our catastrophe
    model package. We can achieve it with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a footprint merging process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a vulnerability and probability merging process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a Python interface in Rust.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build an interface in Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build package installation instructions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before we build anything, we must define our dependencies in our `Cargo.toml`
    file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that we are using the `csv` crate to load our data and the
    `serde` crate to serialize the data that we had loaded from the `CSV` file. With
    this approach, it is important that we start by coding the processes first. This
    enables us to know what we need when we get to building our interfaces. Considering
    this, we can start building our footprint merging process.
  prefs: []
  type: TYPE_NORMAL
- en: Building a footprint merging process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our footprint merging process is essentially loading our footprint data and
    merging it with our input IDs. Once this is done, we then return the data to be
    fed into another process. We initially need to build our data structs before we
    build our processes, as our processes will need them. We can build our footprint
    struct in the `src/footprint/structs.rs` file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that we apply the `Deserialize` macro to the struct so that
    when we load data from the file, it can be directly loaded into our `FootPrint`
    struct. We will also want to clone our struct if similar multiple event IDs are
    being passed into our package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our struct, we can build our merging process in our `src/footprint/processes.rs`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to define the imports we need with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We must remember that we did not define our struct in the `src/footprint/mod.rs`
    file, so this will not run yet, but we will define it in time before running our
    code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can now build a function that will read a footprint from the file with the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see our function requires the directory where our data file is
    housed. We then add the filename to the path, open the file, and pass it through
    the `from_reader` function. We then define an empty vector and add the data that
    we deserialize. We now have a vector of `FootPrint` structs, which we return.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have our `load data` function, we can now build our `merge footprints`
    function in the same file with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see that we take in a vector of event IDs and a vector of `FootPrint`
    structs. We then loop through our event IDs. For each event, we then loop through
    all the `FootPrint` structs, adding the struct to our buffer if it matches the
    event ID. We then return the buffer meaning that we have merged all that we need.
    We do not need to code any more processes. To make them useful, we can build an
    interface in the `src/footprint/mod.rs` file.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'So, we must import what we need with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have imported all that we need, we can build our interface in the
    same file with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we merely accept the file path and event IDs and pass them through our
    processes, returning the results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With this, our footprint processes are built, meaning that we can move on to
    the next step of building the vulnerability merge processes.
  prefs: []
  type: TYPE_NORMAL
- en: Building the vulnerability merge process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have merged our event IDs with our footprint data, we have a working
    map of the probabilities of certain events happening at certain intensities within
    a range of geographical locations. We can merge this with the probabilities of
    damage occurring due to the catastrophe by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this process, we must load the vulnerabilities and then merge them with
    our existing data. To facilitate this, we will have to build two structs – one
    for the data that is loaded from the file and another for the result after the
    merge. Because we are loading the data, we will need to use the `serde` crate.
    In our `src/vulnerabilities/structs.rs` file, we import it with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then build our struct to load the file with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We must note here that the probability of the data we are loading is labeled
    under the `probability` field. This is the same with our `FootPrint` struct. Because
    of this, we must rename the `probability` field to avoid clashes during the merge.
    We also need to calculate the total probability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Considering this, our result after the merge takes the form of the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With this, our structs are complete and we can build our processes in our `src/vulnerabilities/processes.rs`
    file. Here, we are going to have two functions, reading the vulnerabilities, and
    then merging them with our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import everything that we need with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see that we are relying on the `FootPrint` struct from our `footprint`
    module.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have everything, we can build our first process, which is loading
    the data with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see that this is similar code to our loading process in our footprint
    module. Refactoring this into a generalized function would be a good exercise.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have our loading function, we can merge `Vec<Vulnerability>` with
    `Vec<FootPrint>` to get `Vec<VulnerabilityFootPrint>`. We can define the function
    with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we can see that we have a new vector called `buffer`, which is where
    the merged data will be stored in the `. . .` placeholder. We can see that we
    loop through the footprints for each vulnerability. If `intensity_bin_id` matches,
    we execute the code in the `. . .` placeholder, which is the following code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we are merely mapping the correct values to the correct fields of our
    `VulnerabilityFootPrint` struct. In the last field, we calculate the total probability
    by multiplying the other probabilities together.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Our processes are finally done, so we move on to building our interface for
    this process in our `src/vulnerabilities/mod.rs` file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import what we need with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With this, we can create a function that takes in a base path for the directory
    of where our data files are and our footprint data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We then pass them through both of our processes, loading and merging, and then
    return our merged data with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have now built our two processes for constructing our data model. We can
    move on to our next step, which is building our Python interface in Rust.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Python interface in Rust
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Python interface is defined in the `src/lib.rs` file, where we use the
    `pyo3` crate to get our Rust code to communicate with the Python system. Here
    are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import what we need with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see that we import what we need from the `pyo3` crate. We will
    be wrapping a `get_model` function with `wrap_pyfunction` and returning a list
    of `PyDict` structs. We also define the process modules, structs, and functions
    that we need to build our model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can then define our function with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It must be noted that we accept a `Python` struct into our function. This is
    automatically filled. If we get the `Python` struct via the `Python` struct, we
    can return the Python structures that we create in the function using the `Python`
    struct that we took in.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the `. . .` placeholder, we create a `PyDict` struct with all the data for
    the model row and push it to our buffer with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see that we can push different types to our `PyDict` struct and
    Rust does not care.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can then wrap our function and define our module with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that all our Rust programming is done, we can move on to building our Python
    interface in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Building our interface in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to our Python interface, we will have to build a function in a
    Python script in the `flitton_oasis_rist_modelling/__init__.py` file. We also
    store our data `CSV` files in the `flitton_oasis_rist_modelling` directory. Remember,
    we do not want our users interfering with the `CSV` files or having to know where
    they are. To do this, we will use the `os` Python module to find the directory
    of our module to load our `CSV` data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we import what we need in the `flitton_oasis_rist_modelling/__init__.py`
    file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember, our Rust code will compile into a binary and be stored in the `flitton_oasis_rist_modelling`
    directory, so we can do a relative import for all the wrapped functions in our
    Rust code. Now, we can code our `construct_model` model function with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that all the user needs to do is pass in the event IDs. However,
    if we tried to install this package using `pip`, we would get errors stating that
    the `CSV` files cannot be found; this is because our setup does not include the
    data files. We can solve this in our next step of building package installation
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Building package installation instructions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To do this, we must state that we want to keep all `CSV` files in our `MANIFEST.in`
    file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have done this, we can move to our `setup.py` file to define our
    setup:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import what we need with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, as we have done before, we fetch the `setuptools_rust` package; although
    it is not essential for the running of the package, it is needed for the installation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can now define our setup parameters with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we can see that we do not need any Python third-party packages. We have
    also defined our Rust extension, set the `include_package_data` parameter to `True`,
    and defined our package data with `package_data={'''': [''*.csv'']}`. With this,
    all `CSV` files will be kept when installing our package.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We are nearly finished; all we have to do is define the `rustflags` environment
    variables in the `.cargo/config` file with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With this, we can upload our code and install it in our Python system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can now use our Python module. We can test this in our module with the terminal
    output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: There's more data that is printed out, but if your printout correlates with
    the preceding output, then there is a high chance that the rest of your data is
    accurate. Here, we have built a real-world solution that loads data and does a
    series of operations and processes to come up with a model. However, it is a basic
    model that would not be used in real-life catastrophe modeling; we have coded
    it in isolated modules so that we can slot in more processes when we need to.
  prefs: []
  type: TYPE_NORMAL
- en: However, we need to ensure that all our effort was not for nothing. We can do
    what we did in this package with a few lines of Python code using pandas, which
    is written in C, so it could be quicker or at the same speed. Considering this,
    we need to test to ensure that we are not wasting our time by testing our code
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing and testing our package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have started building out our solution in a Python package coded in Rust.
    However, we need to justify to our team and ourselves that all this effort was
    worth it. We can test to see whether we should continue with our efforts in a
    single isolated Python script. In this Python script, we can test by following
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a Python construct model using pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build random event ID generator functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Time our Python and Rust implementations with a series of different data sizes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we have carried out all the aforementioned steps, we will know whether
    we should progress further with our module.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our testing script, before we start coding anything, we must import all
    of what we need with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are using the `random` module to generate random event IDs and the
    `time` module to time our implementations. We are using `pandas` to build our
    model, `matplotlib` to plot the outcomes, and our Rust implementation. We can
    now build our model.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Python construct model using pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have imported everything that we need, we can move on to loading
    data from the CSV files in Python and use it to construct a model in Python using
    pandas with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, our function must take in event IDs. We also must load our data from
    our `CSV` files with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have all our data, we can merge our data and rename the `probability`
    column to avoid clashing with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see that we are using less code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we can do our final process, which is merging with the vulnerabilities
    and then calculating the total probability with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With this, our Python model is now complete. We can now move on to our next
    step of building our random event ID generator functions.
  prefs: []
  type: TYPE_NORMAL
- en: Building a random event ID generator function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to our Rust implementation, we need a list of integers. For our
    Python model, we need to pass in a list of dictionaries with an event ID stored
    in it. We can define these functions with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have everything we need, we can carry out the final step of testing
    our implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Timing our Python and Rust implementations with a series of different data sizes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now have everything we need to test our Rust and Python implementation.
    Running both Python and Rust models with timing can be done by carrying out the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To test our implementation, we define our entry point and all the data structures
    for our time graph with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For our testing data, we are going to loop through a list of integers from
    `10` to `3000` in steps of `10` with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Both Python and Rust implementations will be running the same event ID dataset
    sizes, which is why we only have one `x` vector. We can now test our Python implementation
    with the following code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we generate our ID dataset to the size of the integer of the loop. We
    then start our timer, construct our model in Python, finish the timer, and add
    the time taken to our Python data list.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We take the same approach with our Rust test with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our data collection is now complete.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'All we need to do is plot the results when the loop has finished with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We have now written all the code for testing, which should display a graph
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Rust versus Python for the time taken for model generation for
    the size of data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.04_B17720.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – Rust versus Python for the time taken for model generation for
    the size of data
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we see that initially, our Rust implementation is faster
    than our Python pandas implementation. However, once we get past the 1,300 mark,
    our Rust model gets slower than our Python pandas model. This is because our code
    does not scale well. We are performing loops within loops. In our pandas model,
    we vectorize our total probability. pandas is a well-written module where multiple
    developers have optimized the merge functions.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, although our Rust code will be faster than Python and pandas code,
    if our implementation is sloppy and does not scale well, we may even be slowing
    down our program. I have seen poorly implemented C++ be beaten by Python pandas.
    Understanding this nuance is important when trying to implement Rust in your system.
    Rust is a new language, and colleagues will be let down if you promise big gains,
    poorly implement code, and result in slower performance after burning a lot of
    time coding implementation in Rust.
  prefs: []
  type: TYPE_NORMAL
- en: Seeing that this is a book about building Python packages in Rust as opposed
    to data processing in Rust, this is where we stop. However, Xavier Tao implemented
    an efficient merge process in Rust, resulting in Rust taking 75% less time and
    78% less memory. This is noted in the *Further reading* section. There is also
    a Rust implementation of pandas called **Polars**, which also has Python bindings.
    It is faster than standard pandas, and this documentation is also listed in the
    *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: The takeaway message here is that Rust enables us to build fast memory-efficient
    solutions, but we must be careful with our implementation and test to see whether
    what we are doing is sensible. We should be careful, especially if we are trying
    to build a solution from scratch that has an optimized solution in an existing
    Python package.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through the basics of building a simple catastrophe
    model. We then broke down the logic and converted it into steps so that we could
    build the catastrophe model in Rust. This included taking in paths, loading data
    from files, including data in our package, and building a Python interface so
    that our users do not have to know about what is going on under the hood when
    constructing a model. After all of this, we tested our module and ensured that
    we kept increasing the data size of the test to see how it scales. We saw that,
    initially, our Rust solution was faster because Rust is faster than Python and
    pandas. However, our implementation did not scale well, as we did a loop within
    a loop for our merge.
  prefs: []
  type: TYPE_NORMAL
- en: As the data size increased, our Rust code ended up being slower. In previous
    chapters, we have shown multiple times that Rust implementations are generally
    faster. However, this does not counteract the effects of bad code implementation.
    If you are relying on a Python third-party module to perform a complex process,
    it probably is not a good idea to rewrite it in Rust for performance gains. If
    a Rust crate is not available for the same solution, then it is probably best
    to leave that part of the solution to the Python module.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be building a Flask web application to lay the
    groundwork for applying Rust to a Python web application.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Polars documentation for Rust Crate Polars (2021): [https://docs.rs/polars/0.15.1/polars/frame/struct.DataFrame.html](https://docs.rs/polars/0.15.1/polars/frame/struct.DataFrame.html%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Manipulation: Pandas vs Rust*, *Xavier Tao* (2021): [https://able.bio/haixuanTao/data-manipulation-pandas-vs-rust--1d70e7fc](https://able.bio/haixuanTao/data-manipulation-pandas-vs-rust--1d70e7fc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

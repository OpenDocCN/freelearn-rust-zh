- en: Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern day software is rarely written to perform tasks sequentially. It is more
    important today to be able to write programs that do more than one thing at a
    time and do it correctly. As transistors keep getting smaller, computer architects
    are unable to scale CPU clocks frequency due to quantum effects in the transistors.
    This has shifted focus more towards building concurrent CPU architectures that
    employ multiple cores. With this shift, developers need to write highly concurrent
    applications to maintain performance gains that they had for free when Moore's
    law was in effect.
  prefs: []
  type: TYPE_NORMAL
- en: But writing concurrent code is hard and languages that don't provide better
    abstractions make the situation worse. Rust attempts to make things better and
    safer in this space. In this chapter, we will go through the concepts and primitives
    that enable Rust to provide fearless concurrency to developers, allowing them
    to easily express their programs in a way that can safely do more than one thing
    at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Program execution models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency and associated pitfalls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threads as unit of concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Rust provides thread-safety
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency primitives in Rust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other libraries for concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Program execution models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"An evolving system increases its complexity unless work is done to reduce
    it."'
  prefs: []
  type: TYPE_NORMAL
- en: '- *Meir Lehman*'
  prefs: []
  type: TYPE_NORMAL
- en: In the early 1960s, before multitasking was even a thing, programs written for
    computers were limited to a sequential execution model, where they were able to
    run instructions one after the other in chronological order. This was mainly due
    to limitations in how many instructions the hardware could process during that
    time. As we shifted from vacuum tubes to transistors, then to integrated chips,
    the modern day computer opened up possibilities to support multiple points of
    execution in programs. Gone are the days of sequential programming model where
    computers had to wait for an instruction to finish before executing the next one.
    Today, it's more common for computers to be able to do more than one thing at
    a time and do it correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The modern day computer models a concurrent execution model, where a bunch
    of instructions can execute independently of each other with overlapping time
    periods. In this model, instructions need not wait for each other and run nearly
    at the same time, except when they need to share or coordinate with some data.
    If you look at the modern day software, it does many things that appear to happen
    at the same time, as in the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: The user interface of a desktop application continues to work normally even
    though the application connects to the network in the background
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A game updates the state of thousands of entities at the same time, while playing
    a soundtrack in the background and keeping a consistent frame rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A scientific, compute-heavy program splits computation in order to take full
    advantage of all of the cores in the machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A web server handles more than one request at a time in order to maximize throughput
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are some really compelling examples that propel the need to model our
    program as concurrent processes. But what does concurrency really mean? In the
    next section, let's define that.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ability of a program to manage more than one thing at a time while giving
    an illusion of them happening at the same time is called concurrency, and such
    programs are called concurrent programs. Concurrency allows you to structure your
    program in a way that it performs faster if you have a problem that can be split
    into multiple sub-problems. When talking about concurrency, another term called
    parallelism is often thrown in the discussion, and it is important we know the
    differences as the usage of these terms often overlap. Parallelism is when each
    task runs simultaneously on separate CPU coresÂ  with non-overlapping time periods.
    The following diagram illustrates the difference between concurrency and parallelism:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03050029-7f64-46ff-a375-a1e6de196796.png)'
  prefs: []
  type: TYPE_IMG
- en: To put it another way, concurrency is about structuring your program to manage
    more than one thing at a time, while parallelism is about putting your program
    on multiple cores to increase the amount of work it does in a period of time.
    With this definition, it follows that concurrency when done right, does a better
    utilization of the CPU while parallelism might not in all cases. If your program
    runs in parallel but is only dealing with a single dedicated task, you aren't
    gaining much throughput. This is to say that we gain the best of both worlds when
    a concurrent program is made to run on multiple cores.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the support for concurrency is already provided at the lower levels
    by the operating system, and developers mostly program against the higher level
    abstractions provided by programming languages. On top of the low level support,
    there are different approaches to concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches to concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use concurrency to offload parts of our program to run independently. At
    times, these parts may depend on each other and are progressing towards a common
    goal or they may be embarrassingly parallel, which is a term used to refer to
    problems that can be split into independent stateless tasks, for instance, transforming
    each pixel of an image in parallel. As such, the approaches used to make a program
    concurrent depend on what level we are leveraging concurrency and the nature of
    the problem we are trying to solve. In the next section, let's discuss the available
    approaches to concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel-based
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With multitasking being the norm these days, modern operating systems need
    to deal with more than one processes. As such, your operating system kernel already
    provides primitives for writing concurrent programs in one of the following forms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Processes**: In this approach, we can run different parts of a program by
    spawning separate replicas of themselves. On Linux, this can be achieved using
    the `fork` system call. To communicate any data with the spawned processes, one
    can use various **Inter Process Communication** (**IPC**) facilities such as pipes
    and FIFOs. Process based concurrency provides you with features such as fault
    isolation, but also has the overhead of starting a whole new process. There''s
    a limited number of processes you can spawn before the OS runs out of memory and
    kills them. Process-based concurrency is seen in Python''s multiprocessing module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Threads**: Processes under the hood are just threads, specifically called
    the main thread. A process can launch or spawn one or more threads. A thread is
    the smallest schedulable unit of execution. Every process starts with a main thread.
    In addition to that, it can spawn additional threads using the APIs provided by
    the OS. To allow a programmer to use threads, most languages come with threading
    APIs in their standard library. They are lightweight compared to processes. Threads
    share the same address space with the parent process. They don''t need to have
    a separate entry in the **Process Control Block (PCB)** in the kernel, which is
    updated every time we spawn a new process. But taming multiple threads within
    a process is a challenge because, unlike processes, they share the address space
    with their parent process and other child threads and, because scheduling of threads
    is decided by the OS, we cannot rely on the order the threads will execute and
    what memory they will read from or write to. These operations suddenly become
    hard to reason about when we go from a single-threaded program to a multi-threaded
    one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note**: The implementation of threads and processes differ between operating
    systems. Under Linux, they are treated the same by the kernel, except that threads
    don''t have their own process control block entry in the kernel and they share
    the address space with their parent process and any other child threads.'
  prefs: []
  type: TYPE_NORMAL
- en: User-level
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Process- and thread-based concurrency are limited by how many of them we can
    spawn. A lighter and more efficient alternative is to use user space threads,
    popularly known as green threads. They first appeared in Java with the code name
    *green* and the name has stuck since then. Other languages such as Go (goroutines),
    and Erlang also have green threads. The primary motivation in using green threads
    is to reduce the overhead that comes with using process- and thread-based concurrency.
    Green threads are very lightweight to spawn and use less space than a thread.
    For instance, in Go, a goroutine takes only 4 KiB of space compared to the usual
    8MB by a thread.
  prefs: []
  type: TYPE_NORMAL
- en: User space threads are managed and scheduled as part of the language runtime.
    A runtime is any extra bookeeping or managing code that's executed with every
    program you run. This would be your garbage collector or the thread scheduler.Â Internally,
    user space threads are implemented on top of native OS threads. Rust had green
    threads before the 1.0 version, but they were later removed before the language
    hit stable release. Having green threads would have steered away Rust's guarantee
    and its principle of having no runtime costs.
  prefs: []
  type: TYPE_NORMAL
- en: User space concurrency is more efficient, but hard to get right in its implementation.
    Thread-based concurrency, however, is a tried and tested approach and has been
    popular since multi-process operating systems came into existence and it's the
    go- to approach for concurrency. Most mainstream languages provide threading APIs
    that allows users to create threads and easily offload a portion of their code
    for independent execution.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging concurrency in a program follows a multi-step process. First, we
    need to identify parts of our problem that can be run independently. Then, we
    need to look for ways to co-ordinate threads that are split into multiple sub-tasks
    to accomplish a shared goal. In the process, threads might also need to share
    data and they need synchronization for accessing or writing to shared data. With
    all of the benefits that concurrency brings with it, there are a new set of challenges
    and paradigms that developers need to care and plan for. In the next section,
    let's discuss the pitfalls of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The advantages of concurrencyÂ abound, but it brings a whole lot of complexity
    and pitfalls that we have to deal with. Some issues when writing concurrent programs
    code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Race conditions**: As threads are scheduled by the operating system, we don''t
    have a say in what order and how threads will access a shared data. A common use
    case in multi-threaded code is about updating a global state from multiple threads.
    This follows a three step processâread, modify, and write.Â If these three operations
    aren''t performed atomically by threads, we may end up with a race condition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of operations is atomic if they execute together in an indivisble manner.
    For a set of operations to be atomic, it must not be pre-empted in the middle
    of its execution. It must execute completely or not at all.
  prefs: []
  type: TYPE_NORMAL
- en: If two threads try to update a value at a memory location at the same time,
    they might end up overwriting each other's values and only one of the updates
    will ever be written to memory or the value might not get updated at all. This
    is a classic example of a race condition. Both threads are racing to update the
    value without any co-ordination with each other. This leads to other issues such
    as data races.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data race**: When multiple threads try to write data to a certain location
    in memory and when both of them write at the same time, it''s hard to predict
    what values will get written. The end result in the memory could also be garbage
    value. Data race is a consequence of a race condition, as read-modify-update operation
    must happen atomically by any thread to ensure that consistent data gets read
    or written by any thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory unsafety and undefined behavior**: Race conditions can also lead to
    undefined behavior. Consider the following pseudocode:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We have two threads, A and B, that act on a linked list. `Thread A` tries to
    retrieve the head of the list. For doing this safely, it first checks the head
    of the list is not `NULL` and then returns it. `Thread B` sets the head of the
    list to a `NULL` value. Both of these run at nearly the same time and might get
    scheduled by the OS in different order. For instance, in one of the execution
    instances, the point where `Thread A` runs first and asserts that `list.head`,
    is not `NULL`. Right after that, `Thread A` is preempted by the OS and `Thread
    B` is scheduled to run. Now, `Thread B` sets `list.head` to `NULL`. Following
    that, when `Thread A` gets the chance to run, it will try to return `list.head`
    which is a `NULL` value. This would result in a segmentation fault when `list.head`
    is read from. In this case, memory unsafety happens because ordering is not maintained
    for these operations.
  prefs: []
  type: TYPE_NORMAL
- en: There is a common solution to the previously mentioned problemsâsynchronizing
    or serializing access to shared data or code or ensuring that the threads run
    critical sections atomically. This is done using synchronization primitives such
    as a mutex, semaphores, or conditional variables. But even using these primitives
    can lead to other issues such as deadlocks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deadlocks**: Apart from race conditions, another issue that threads face
    is getting starved of resources while holding a lock on a resource. Deadlock is
    a condition where a Thread A holding a resource a and waiting for resource b.
    Another Thread B is holding a resource `b` and is waiting for resource a. The
    following diagram depicts the situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7470c731-80cc-4842-9826-0dad5073d09d.png)'
  prefs: []
  type: TYPE_IMG
- en: Deadlocks are hard to detect but they can be solved by taking locks in the correct
    order. In the preceding case, if both Thread A and Thread B try to take the lock
    first, we can ensure that the locks are released properly.
  prefs: []
  type: TYPE_NORMAL
- en: With the advantages and pitfalls explored, let's go through the APIs that Rust
    provides to write concurrent programs.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency in Rust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rust's concurrency primitives rely on native OS threads. It provides threading
    APIs in the `std::thread` module in the standard library. In this section, we'll
    start with the basics on how to create threads to perform tasks concurrently.
    In subsequent sections,Â  we'll explore how threads can share data with each other.
  prefs: []
  type: TYPE_NORMAL
- en: Thread basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we said, every program starts with a main thread. To create an independent
    execution point from anywhere in the program, the main thread can spawn a new
    thread, which becomes its child thread. Child threads can further spawn their
    own threads. Let''s look at a concurrent program in Rust that uses threads in
    the simplest way possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In `main`, we call theÂ `spawn`Â function from the `thread` module which takes
    a no parameter closure as an argument. Within this closure, we can write any code
    that we want to execute concurrently as a separate thread. In our closure, we
    simply print some text and returnÂ `String`. Compiling and running this program
    gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Strange! We only get to see `"Hello"` being printed. What happened toÂ `println!("Thread");`
    from the child thread ? A call to `spawn` creates the thread and returns immediately
    and the thread starts executing concurrently without blocking the instructions
    after it. The child thread is created in the detached state. Before the child
    thread has any chance to run its code, the program reaches the `print!("Hello");`
    statement and exits the program when it returns from `main`. As a result, code
    within the child thread doesn''t execute at all. To allow the child thread to
    execute its code, we need to wait on the child thread. To do that, we need to
    first assign the value returned by `spawn` to a variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `spawn`Â function returns aÂ `JoinHandle`Â type, which we store in theÂ `child`Â variable.
    This type is a handle to the child thread, which can be used to join a threadâin
    other words, wait for its termination. If we ignore the `JoinHandle`Â type of a
    thread, there is no way to wait for the thread. Continuing with our code, we call
    theÂ `join`Â method on the child before exiting from `main` as in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling `join` blocks the current thread and waits for the child thread to
    finish before executing any line of code following the `join` call. It returns
    a `Result` value. Since we know that this thread does not panic, we call `expect`
    to unwrap the `Result`Â type giving us the string. Joining the thread can fail
    if a thread is joining itself or gets deadlocked, and, in that case, it returns
    anÂ `Err` variant with the value that was passed to theÂ `panic!`Â call though, in
    this case, the returned value is of theÂ `Any` typeÂ which must be downcasted to
    a proper type. Our updated code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the output of the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Great ! We wrote our first concurrent *hello world* program. Let's explore other
    APIs from the `thread` module.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We also have APIs that can be used to configure threads by setting their properties
    such as the name or their stack size. For this, we have the `Builder` type from
    the `thread` module. Here''s a simple program that creates a thread and spawns
    it using the `Builder` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we use the `Builder::new`,Â methodÂ followed by calling
    theÂ `name`Â and `stack_size` methodsÂ to add a name to our thread and its stack
    size respectively. We then call `spawn` on `my_thread`, which consumes the builder
    instance and spawns the thread. This time, within our closure, we `panic!` with
    an `"Oops"` message. Following is the output of this program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We get to see that the thread has the same name we gave it - `"Worker Thread"`.
    Also, notice the `"Child status"` message that's returned as an `Any` type. Values
    returned from panic call in a thread are returned as an `Any` type and must be
    downcasted to a specific type. That's all on the basics of spawning threads.
  prefs: []
  type: TYPE_NORMAL
- en: But the threads we spawned in the preceding code examples aren't doing much.
    We use concurrency to solve problems that can be split into multiple sub-tasks.
    In simple cases, these sub-tasks are independent of each other such as applying
    a filter to each pixel of an image in parallel. In other situations, the sub-tasks
    running in threads might want want to co-ordinate on some shared data.
  prefs: []
  type: TYPE_NORMAL
- en: They might also be contributing to a computation whose end result depends on
    the individual results from the threads, for instance, downloading a file from
    multiple threads in blocks and communicating it to a parent manager thread. Other
    problems might be dependent on a shared state such as an HTTP client sending a
    `POST` request to a server that has to update the database. Here, the database
    is the shared state common to all threads. These are some of the most common use
    cases of concurrency and it's important that threads are able to share or communicate
    data back and forth between each other and with their parent thread.
  prefs: []
  type: TYPE_NORMAL
- en: Let's step up the game a bit and look at how we can access existing data from
    parent threads within child threads.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing data from threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A thread that doesn''t communicate or access data from the parent thread is
    not much. Let''s take a very common pattern of using multiple threads to concurrently
    access items in a list to perform some computation. Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we have `5` numbers in `values` and we spawn `5` threads
    where each one of them accesses the data inÂ `values`. Let''s compile this program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac1bd9b6-7a3d-4362-b894-7349fc9ea2fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Interesting ! The error makes sense if you think about it from a borrowing perspective.
    `nums` comes from the main thread. When we spawn a thread, it is not guaranteed
    to exit before the parent thread and may outlive it. When the parent thread returns,
    the `nums`Â variableÂ is gone andÂ `Vec` it's pointing to is freed. If the preceding
    code was allowed by Rust, the child thread could have accessed `nums` which might
    have some garbage value after `main` returns and it would have undergone a segmentation
    fault.
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the help message from from the compiler, it suggests us to move
    or capture `nums` inside the closure. This way the referenced aÂ `nums`Â variable
    from `main` is moved inside `closure` and it won't be available in the `main`
    thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the code that uses the `move` keyword to move the value from the parent
    thread in its child thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are trying to accessed `my_str` again. This fails
    with the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/247021c7-5046-4d3c-87b5-c143f34aa150.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from the preceding error message, with `move`, you don't get
    to use the data again, even if we are only reading `my_str` from our child thread.
    Here too, we are saved by the compiler. If the child thread frees the data and
    we access `my_str` from `main`, we'll access a freed value which is a use after
    free issue.
  prefs: []
  type: TYPE_NORMAL
- en: As you saw, the same rules of ownership and borrowing work in multi-threaded
    contexts too. This is one of the novel aspects of its design that doesn't require
    additional constructs to enforce correct concurrent code. But, how do we achieve
    the preceding use case of accessing data from threads? Because threads are more
    likely to outlive their parent, we can't have references in threads. Instead,
    Rust provides us with synchronization primitives that allow us to safely share
    and communicate data between threads. Let's explore these primitives. These types
    are usually composed in layers depending on the needs and you only pay for what
    you use.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency models with threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We mainly use threads to perform a task that can be split into sub-problems,
    where the threads might need to communicate or share data with each other. Now,
    using the threading model as the baseline, there are different ways to structure
    our program and control access to shared data. A concurrency model specifies how
    multiple threads interact with instructions and data shared between them and how
    they make progress over time and space (here, memory).
  prefs: []
  type: TYPE_NORMAL
- en: Rust does not prefer any opinionated concurrency model and frees the developer
    in using their own models depending on the problem they are trying to solve through
    third party crates. So, other models of concurrency exist that includes the actor
    model implemented as a library in the `actix` crate. There are other models too,
    such as the work stealing concurrency model implemented by the `rayon` crate.
    Then, there is the `crossbeam` crate, which allows concurrent threads to share
    data from their parent stack frame and are guaranteed to return before the parent
    stack is deallocated.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two popular built-in concurrency models with which Rust provides
    us: sharing data with synchronization and sharing data by message passing.'
  prefs: []
  type: TYPE_NORMAL
- en: Shared state model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using shared state to communicate values to a thread is the most widely used
    approach, and the synchronization primitives to achieve this exist in most mainstream
    languages. Synchronization primitives are types or language constructs that allow
    multiple threads to access or manipulate a value in a thread-safe way. Rust also
    has many synchronization primitives that we can wrap around types to make them
    thread-safe.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in the previous section, we cannot have shared access to any value
    from multiple threads. We need shared ownership here. Back in [Chapter 5](db2c2723-8ca0-43be-b135-afd847342146.xhtml),
    *Memory Management and Safety*, we introduced the `Rc` type. that can provide
    shared ownership of values. Let''s try using this type with our previous example
    of reading data from multiple threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This fails with the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c89bb9a2-3bb7-4903-8e87-f93e0db940b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Rust saves us here too. This is because an `Rc` type is not thread-safe as mentioned
    previously, as the reference count update operation is not atomic. We can only
    use `Rc` in single-threaded code. If we want to have the same kind of shared ownership
    across multi-threaded contexts, we can use the `Arc` type, which is just like
    `Rc`, but has atomic reference counting capability.
  prefs: []
  type: TYPE_NORMAL
- en: Shared ownership with Arc
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preceding code can be made to work with the multi-threaded `Arc` type as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we simply replaced the wrapper of the vector from `Rc`
    to theÂ `Arc` type. Another change is that, before we reference `nums` from a child
    thread, we need to clone it with `Arc::clone()`, which gives us an owned `Arc<Vec<i32>>`
    value that refers to the same `Vec`. With that change, our program compiles and
    provides safe access to the shared `Vec`, with the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now, another use case in multi-threaded code is to mutate a shared value from
    multiple threads. Let's see how to do that next.
  prefs: []
  type: TYPE_NORMAL
- en: Mutating shared data from threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll take a look at a sample program where five threads push data to a shared
    `Vec`. The following program tries to do the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We have the same `nums` wrapped with `Arc`. But we cannot mutate it, as the
    compiler gives the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c617b0b5-c313-45fd-849a-e5d38347357b.png)'
  prefs: []
  type: TYPE_IMG
- en: This doesn't work as cloning `Arc` hands out immutable reference to the inner
    value. To mutate data from multiple threads, we need to use a type that provides
    shared mutability just likeÂ `RefCell`. But similar to `Rc`, `RefCell` cannot be
    used across multiple threads. Instead, we need to use their thread-safe variants
    such as the `Mutex` or `RwLock` wrapper types. Let's explore them next.
  prefs: []
  type: TYPE_NORMAL
- en: Mutex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When safe mutable access to a shared resource is required, the access can be
    provided by the use of mutex. Mutex is a portmanteau for mutual exclusion, a widely
    used synchronization primitive for ensuring that a piece of code is executed by
    only one thread at a time. A `mutex` in general is a guard object which a thread
    acquires to protect data that is meant to be shared or modified by multiple threads.Â It
    works by prohibiting access to a value from more than one thread at a time by
    locking the value. If one of the Â threads has a lock on the `mutex` type, no other
    thread can run the same code until the thread that holds the lock is done with
    it.
  prefs: []
  type: TYPE_NORMAL
- en: The `std::sync` module from the standard library contains the `Mutex` type allowing
    one to mutate data from threads in thread-safe manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code example shows how to use the `Mutex` type from a single
    child thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Running this works as expected. But, this won't work when multiple threads try
    to access the value as `Mutex` doesn't provide shared mutability. To allow a value
    inside a `Mutex` to be mutated from multiple threads, we need to compose it it
    the `Arc` type. Let's see how to do that next.
  prefs: []
  type: TYPE_NORMAL
- en: Shared mutability with Arc and Mutex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having explored the basics of Mutex in single threaded contexts, we''ll revisit
    the example from the previous section. The following code modifies a value using
    a `Mutex` wrapped in an `Arc` from the multiple threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we created a `Mutex` value in `m`. We then spawn a thread.
    The output on your machine may vary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calling `lock` on a mutex will block other threads from calling `lock` until
    the lock is gone. As such, it is important that we structure our code in such
    a way that the is granular. Compiling and running this gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: There is another similar alternative to `Mutex`, which is the `RwLock` type
    that is more aware on the kind of lock you have on your type, and can be more
    performant when reads are more often than writes. Let's explore it next.
  prefs: []
  type: TYPE_NORMAL
- en: RwLock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Mutex is fine for most use cases, for some multi-threaded scenarios, reads
    happen more often than writes from multiple threads. In that case, we can use
    the RwLock type, which also provides shared mutability but can do so at a more
    granular level. RwLock stands for Reader-Writer lock. With `RwLock`, we can have
    many readers at the same but only one writer in a given scope. This is much better
    than a Mutex which agnostic of the kind of access a thread wants. Using RwLock
  prefs: []
  type: TYPE_NORMAL
- en: 'RwLock exposes two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`read`: Gives read access to the thread. There can be many read invocations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`write`: Gives exclusive access to thread for writing data to the wrapped type.
    There can be one write access from an `RwLock` instance to a thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s a sample program that demonstrates using the `RwLock` instead of `Mutex`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: But `RwLock` on some systems such as Linux, suffers from the writer starvation
    problem. It's a situation when readers continually access the shared resource,
    and writer threads never get the chance to access the shared resource.
  prefs: []
  type: TYPE_NORMAL
- en: Communicating through message passing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threads can also communicate with each other through a more high level abstraction
    called message passing. This model of thread communication removes the need to
    use explicit locks by the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard library''s `std::sync::mpsc` module provides a lock-free multi-producer,
    single-subscriber queue, which serves as a shared message queue for threads wanting
    to communicate with one another. The `mpsc` module standard library has two kinds
    of channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '`channel`:Â This is an asynchronous, infinite buffer channel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync_channel`: This is a synchronous, bounded buffer channel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Channels can be used to send data from one thread to another. Let's look at
    asynchronous channels first.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous channels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is an example of a simple producer-consumer system, where the main thread
    produces the values `0, 1, ..., 9` and the spawned thread prints them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We first call the `channel` method. This returns two values, `tx` and `rx`.
    `tx` is the transmitter end, having type `Sender<T>` and `rx` is the receiver
    end having type `Receiver<T>`. Their names are just a convention and you can name
    them anything. Most often, you will see code bases use these names as they are
    concise to write.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we spawn a thread that will receive values from the `rx` side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We use a `while let` loop. This loop will receiveÂ `Err` whenÂ `tx` is dropped.
    The drop happens when `main` returns.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code,Â first, to create the `mpsc` queue, we call the `channel`
    function, which returns to usÂ `Sender<T>` andÂ `Receiver<T>`.
  prefs: []
  type: TYPE_NORMAL
- en: '`Sender<T>` is a `Clone` type, which means it can be handed off to many threads,
    allowing them to send messages into the shared queue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **multi producer, single consumer** (**mpsc**)Â approach provides multiple
    writers but only a single reader. Both of these functions return a pair of generic
    types: a sender and a receiver. The sender can be used to push new things into
    the channel, while receivers can be used to get things from the channel. The sender
    implements the `Clone` trait while the receiver does not.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the default asynchronous channels, the `send` method never blocks. This
    is because the channel buffer is infinite, so there''s always space for more.
    Of course, it''s not really infinite, just conceptually so: your system may run
    out of memory if you send gigabytes to the channel without receiving anything.'
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous channels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Synchronous channels have a bounded buffer and, when it''s full, the `send`
    method blocks until there''s more space in the channel. The usage is otherwise
    quite similar to asynchronous channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The synchronous channel size is `1`, which means that we can't have more than
    one item in the channel. Any send call after the first send will block in such
    a case. However, in the preceding code, we don't get blocks (at least, the long
    ones) as the two sending threads work in the background and the main thread gets
    to receive it without being blocked on the `send` call. For both these channel
    types, the `recv` call returns an `Err` value if the channel is empty.
  prefs: []
  type: TYPE_NORMAL
- en: thread-safety in Rust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we saw how the compiler stops us from sharing the data.
    If a child thread accesses data mutably, it is moved because Rust won't allow
    it to be used in the parent thread as the child thread might deallocate it, leading
    to a dangling pointer dereference in the main thread. Let's explore the idea of
    thread-safety and how Rust's type systems achieves that.
  prefs: []
  type: TYPE_NORMAL
- en: What is thread-safety?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: thread-safety is the property of a type or a piece of code that, when executed
    or accessed by multiple threads, does not lead to unexpected behavior. It refers
    to the idea that data is consistent for reads while being safe from corruption
    when multiple threads write to it.
  prefs: []
  type: TYPE_NORMAL
- en: Rust only protects you from data races. It doesn't aim to protect against deadlocks
    as they are difficult to detect. It instead offloads this to third-party crates
    such as the `parking_lot` crate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rust has a novel approach to protecting against data races. Most of the thread-safety
    bits are already embedded in the `spawn` method''s type signature. Let''s look
    at its type signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: That's a scary-looking type signature. Let's make it less scary by explaining
    what each of the parts mean.
  prefs: []
  type: TYPE_NORMAL
- en: '`spawn` is a generic function over `F` and `T` and takes a parameter,Â `f`,
    and returns a generic type called `JoinHandle<T>`. Following that, the `where`
    clause specifies multiple trait bounds:'
  prefs: []
  type: TYPE_NORMAL
- en: '`F: FnOnce() -> T`: This says that `F` implements a closure that can be called
    only once. In other words, `f` is a closure that takes everything by value and
    moves items referenced from the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`F: Send + ''static`: This means that the closure must be `Send` and must have
    the `''static` lifetime, implying that any type referenced from within the closure
    in its environment must also be Send and must live for the entire duration of
    the program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`T: Send + ''static`: The return type,Â `T`, from the closure must also implement
    the `Send + ''static` trait.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we know,Â `Send` is a marker trait. It is just used as a type-level marker
    that implies that the value is safe to be sent across threads; most types are
    Send. Types that don't implement `Send` are pointers, references, and so on. In
    addition, `Send` is an auto trait or an automatically derived trait whenever applicable.
    Compound data types such as a struct implementÂ `Send` if all of the fields in
    a struct are `Send`.
  prefs: []
  type: TYPE_NORMAL
- en: Traits for thread-safety
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thread-safety is the idea that, if you have data that you want to data from
    multiple threads, any read or write operation on that value does not lead to inconsistent
    results. The problem with updating a value, even with a simple increment operation
    such asÂ `a += 1` is that it roughly translates in to a three-step processâ`load`
    `increment` `store`. Data that can be safely updated is meant to be wrapped in
    thread-safe types such as `Arc` and `Mutex` to ensure that we have data consistency
    in a program.
  prefs: []
  type: TYPE_NORMAL
- en: In Rust, you get compile-time guarantees on types that can be safely used and
    referenced within a thread. These guarantees are implemented as traits, which
    are the `Send` and `Sync` trait.
  prefs: []
  type: TYPE_NORMAL
- en: Send
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Send type is safe to send to multiple threads. This implies that the type
    is a `move` type. Types that aren't Send are pointer types such as `&T`, unless
    `T` is `Sync`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Send` trait has the following type signature in the standard library''s
    `std::marker` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three important things to notice in its definition: first, it''s
    a marker trait without any body or item. Second, it''s prefixed with the `auto`
    keyword as it is implemented implicitly for most types when appropriate. Thirdly,
    it''s an unsafe trait because Rust wants to make the developer sure that they
    opt in explicitly and ensure that their type has thread-safe synchronization built
    in.'
  prefs: []
  type: TYPE_NORMAL
- en: Sync
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Sync` trait has a similar type signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This trait signifies that types that implement this trait are safe to be shared
    between threads. If something is `Sync` then a reference to it in other words,
    `&T` is `Send`. This means that we can pass references to it to many threads.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency using the actor model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another model of concurrency that is quite similar to the message passing model
    is the actor model. The actor model became popular with Erlang, a functional programming
    language popular in the telecom industry, known for its robustness and distributed
    by default nature.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actor model is a conceptual model that implements concurrency at the type
    level using entities called actors. It was first introduced by Carl Eddie Hewitt
    in 1973\. It removes the need for locks and synchronization and provides a cleaner
    way to introduce concurrency in a system. The actor model consists of three things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Actor:** This is a core primitive in the actor model. Each actor consists
    of its address, using which we can send messages to an actor''s and mailbox, which
    is just a queue to store the messages it has received. The queue is generally
    a **First In, First Out** (**FIFO**) queue. The address of an actor is needed
    so that other actors can send messages to it. The supervisor actor can create
    child actors that can create other child actors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Messages:** Actors communicate only via messages. They are processed asynchronously
    by actors. The `actix-web` framework provides a nice wrapper for synchronous operations
    in an asynchronous wrapper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Rust, we have the `actix` crate that implements the actor model. The `actix`
    crate, uses the tokio and futures crate which we'll cover in [Chapter 12](33562886-4278-4ab1-aa0d-de533af4bb99.xhtml),
    *Network Programming in Rust*. The core objects to that crate is the Arbiter type
    which is simply a thread which spawns an event loop underneath and provides a
    handle to the event loop as an `Addr` type. Once created, we can use this handle
    to send messages to the actor.
  prefs: []
  type: TYPE_NORMAL
- en: In `actix`, creation of actor follows a simple step of creating a type, defining
    a message and implementing the handler for the message for the actor type. Once
    that is done, we can create the actor and spawn them into one of the created arbiters.
  prefs: []
  type: TYPE_NORMAL
- en: Each actor runs within an arbiter.
  prefs: []
  type: TYPE_NORMAL
- en: When we create an actor, they don't execute right away. It's when we put these
    actors into arbiter threads, they then start executing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep the code example simple and to show how to setup actors and run them
    in actix, we''ll create a actor that can add two numbers. Let''s create a new
    project by running `cargo new actor_demo` with the following dependencies in `Cargo.toml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `main.rs` contains the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have created an actor named `Adder`. This actor can
    send and receive messages of type `Add`. This is a tuple struct that encapsulates
    two numbers to be added. To allow `Adder` to receive and process `Add` messages,
    we implement the `Handler` trait for `Adder` parameterized over the `Add` message
    type. In the `Handler` implementation, we print the computation being performed
    and return the sum of the given numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Following that, in `main`, we first create a `System` actor by calling its `run`
    method which takes in a closure. Within the closure, we start a `SyncArbiter`
    with `3` threads by calling its `start` method.Â This create 3 actors ready to
    receive messages. It returns a `Addr` type which is a handle to the event loop
    to which we can send messages to the `Adder` actor instance. We then send 5 messages
    to our arbiter address `addr`. As the System::run is an parent event loop that
    runs forever, we spawn a future to stop the System actor after a delay of 1 second.
    We can ignore the details of this part of the code as it is simply to shutdown
    the System actor in an asynchronous way.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that said, let''s take this program for a spin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the `actix` crate, there are other crates in the Rust ecosystem that
    implements various concurrency models suitable for different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Other crates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from `actix`, we have a crate named `rayon` which is a work stealing based
    data parallelism library that makes it dead simple to write concurrent code.
  prefs: []
  type: TYPE_NORMAL
- en: Another notable crate to mention is the `crossbeam` crate which allows one to
    write multi-threaded code that can access data from its parent stack frame and
    are guaranteed to terminate before the parent stack frame goes away.
  prefs: []
  type: TYPE_NORMAL
- en: '`parking_lot` is another crate that provides a faster alternative to concurrency
    primitives present in the standard library. If you have a use case where the standard
    library `Mutex` or `RwLock` is not performant enough, then you can use this crate
    to gain significant speedups.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is quite astonishing that the same ownership principle that prevents memory
    safety violations in single-threaded contexts also works for multithreaded contexts
    in composition with marker traits. Rust has easy and safe ergonomics for integrating
    concurrrency in your application with minimal runtime cost. In this chapter, we
    learned how to use the `threads` API provided by Rust's standard library and got
    to know how copy and move types work in the context of concurrency. We covered
    channels, the atomic reference counting type,Â `Arc`, and how to use `Arc` with
    `Mutex` and also explored the actor model of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll dive into metaprogramming which is all about generating
    code from code.
  prefs: []
  type: TYPE_NORMAL

- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: 'Concurrency and Asynchronous Programming: a Detailed Overview'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并发与异步编程：详细概述
- en: Asynchronous programming is one of those topics many programmers find confusing.
    You come to the point when you think you’ve got it, only to later realize that
    the rabbit hole is much deeper than you thought. If you participate in discussions,
    listen to enough talks, and read about the topic on the internet, you’ll probably
    also come across statements that seem to contradict each other. At least, this
    describes how I felt when I first was introduced to the subject.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 异步编程是许多程序员觉得令人困惑的话题之一。当你认为自己已经掌握了它时，你可能会后来意识到这个兔子洞比你想象的要深得多。如果你参与讨论，听了足够的讲座，并在互联网上阅读有关这个主题的内容，你可能会遇到似乎相互矛盾的说法。至少，这描述了我第一次接触这个主题时的感受。
- en: The cause of this confusion is often a lack of context, or authors assuming
    a specific context without explicitly stating so, combined with terms surrounding
    concurrency and asynchronous programming that are rather poorly defined.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这种混淆的原因通常是缺乏上下文，或者作者在没有明确说明的情况下假设了特定的上下文，再加上围绕并发和异步编程的术语定义得相当糟糕。
- en: 'In this chapter, we’ll be covering a lot of ground, and we’ll divide the content
    into the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖很多内容，并将内容分为以下主要主题：
- en: Async history
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步编程历史
- en: Concurrency and parallelism
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并发与并行
- en: The operating system and the CPU
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统与CPU
- en: Interrupts, firmware, and I/O
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中断、固件和I/O
- en: This chapter is general in nature. It doesn’t specifically focus on **Rust**,
    or any specific programming language for that matter, but it’s the kind of background
    information we need to go through so we know that everyone is on the same page
    going forward. The upside is that this will be useful no matter what programming
    language you use. In my eyes, that fact also makes this one of the most interesting
    chapters in this book.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容较为通用。它并不特别关注**Rust**，或者任何特定的编程语言，但这是我们需要的背景信息，以便我们知道每个人在前进的道路上都在同一页面上。好处是，这将对任何编程语言都很有用。在我看来，这也使得这一章成为本书中最有趣的一章之一。
- en: There’s not a lot of code in this chapter, so we’re off to a soft start. It’s
    a good time to make a cup of tea, relax, and get comfortable, as we’re about start
    this journey together.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章代码不多，所以我们从轻松的开始。这是一个泡一杯茶、放松并让自己舒适的时候，因为我们将一起开始这段旅程。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'All examples will be written in Rust, and you have two alternatives for running
    the examples:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所有示例都将使用Rust编写，并且你有两种运行示例的替代方案：
- en: Write and run the examples we’ll write on the Rust playground
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Rust playground上编写和运行我们将编写的示例
- en: Install Rust on your machine and run the examples locally (recommended)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的机器上安装Rust并本地运行示例（推荐）
- en: The ideal way to read this chapter is to clone the accompanying repository ([https://github.com/PacktPublishing/Asynchronous-Programming-in-Rust/tree/main/ch01/a-assembly-dereference](https://github.com/PacktPublishing/Asynchronous-Programming-in-Rust/tree/main/ch01/a-assembly-dereference))
    and open the `ch01` folder and keep it open while you read the book. There, you’ll
    find all the examples we write in this chapter and even some extra information
    that you might find interesting as well. You can of course also go back to the
    repository later if you don’t have that accessible right now.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章的理想方式是克隆随附的仓库([https://github.com/PacktPublishing/Asynchronous-Programming-in-Rust/tree/main/ch01/a-assembly-dereference](https://github.com/PacktPublishing/Asynchronous-Programming-in-Rust/tree/main/ch01/a-assembly-dereference))，打开`ch01`文件夹，并在阅读本书时保持打开状态。在那里，你可以找到本章中我们编写的所有示例，甚至还有一些你可能觉得有趣的信息。当然，如果你现在无法访问，你当然也可以稍后回到仓库。
- en: An evolutionary journey of multitasking
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多任务处理的进化之旅
- en: In the beginning, computers had one CPU that executed a set of instructions
    written by a programmer one by one. No **operating system** (**OS**), no scheduling,
    no threads, no multitasking. This was how computers worked for a long time. We’re
    talking back when a program was assembled in a deck of punched cards, and you
    got in big trouble if you were so unfortunate that you dropped the deck onto the
    floor.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初，计算机只有一个CPU，它会逐个执行程序员编写的指令集。没有**操作系统**（**OS**），没有调度，没有线程，没有多任务处理。这就是计算机长时间以来的工作方式。我们说的是当程序在穿孔卡片堆栈上汇编的时候，如果你不幸将卡片堆叠掉到地上，那可就麻烦了。
- en: There were operating systems being researched very early and when personal computing
    started to grow in the 80s, operating systems such as DOS were the standard on
    most consumer PCs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪80年代个人计算开始增长时，操作系统如DOS在大多数消费级PC上成为标准。
- en: These operating systems usually yielded control of the entire CPU to the program
    currently executing, and it was up to the programmer to make things work and implement
    any kind of multitasking for their program. This worked fine, but as interactive
    UIs using a mouse and windowed operating systems became the norm, this model simply
    couldn’t work anymore.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作系统通常将整个CPU的控制权交给当前正在执行的程序，程序员需要确保程序正常运行并实现任何类型的多任务处理。这没问题，但随着使用鼠标和窗口式操作系统的交互式用户界面成为常态，这种模式已经无法再继续下去了。
- en: Non-preemptive multitasking
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非抢占式多任务处理
- en: '**Non-preemptive multitasking** was the first method used to be able to keep
    a UI interactive (and running background processes).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**非抢占式多任务处理**是第一个能够保持用户界面交互性（并运行后台进程）的方法。'
- en: This kind of multitasking put the responsibility of letting the OS run other
    tasks, such as responding to input from the mouse or running a background task,
    in the hands of the programmer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多任务处理将操作系统运行其他任务（如响应用户鼠标输入或运行后台任务）的责任交给了程序员。
- en: Typically, the programmer *yielded* control to the OS.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，程序员会将控制权“让出”给操作系统。
- en: Besides offloading a huge responsibility to every programmer writing a program
    for your platform, this method was naturally error-prone. A small mistake in a
    program’s code could halt or crash the entire system.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将巨大的责任转嫁给为你的平台编写程序的每个程序员之外，这种方法自然容易出错。程序代码中的一个小错误可能导致整个系统停止或崩溃。
- en: Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Another popular term for what we call non-preemptive multitasking is **cooperative
    multitasking**. Windows 3.1 used cooperative multitasking and required programmers
    to yield control to the OS by using specific system calls. One badly-behaving
    application could thereby halt the entire system.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的非抢占式多任务处理的另一个流行术语是**协作式多任务处理**。Windows 3.1使用了协作式多任务处理，并要求程序员通过使用特定的系统调用来将控制权让给操作系统。一个表现不佳的应用程序可能会因此使整个系统停止。
- en: Preemptive multitasking
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抢占式多任务处理
- en: While non-preemptive multitasking sounded like a good idea, it turned out to
    create serious problems as well. Letting every program and programmer out there
    be responsible for having a responsive UI in an operating system can ultimately
    lead to a bad user experience, since every bug out there could halt the entire
    system.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然非抢占式多任务处理听起来是个好主意，但它实际上也带来了严重的问题。让每个程序和程序员负责在操作系统中拥有响应式的用户界面，最终可能导致糟糕的用户体验，因为任何一个错误都可能导致整个系统停止。
- en: The solution was to place the responsibility of scheduling the CPU resources
    between the programs that requested it (including the OS itself) in the hands
    of the OS. The OS can stop the execution of a process, do something else, and
    switch back.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是将调度CPU资源之间程序的责任（包括操作系统本身）交给了操作系统。操作系统可以停止一个进程的执行，做其他事情，然后再切换回来。
- en: On such a system, if you write and run a program with a graphical user interface
    on a single-core machine, the OS will stop your program to update the mouse position
    before it switches back to your program to continue. This happens so frequently
    that we don’t usually observe any difference whether the CPU has a lot of work
    or is idle.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的系统中，如果你在一台单核机器上编写并运行一个具有图形用户界面的程序，操作系统会在切换回你的程序继续之前停止你的程序以更新鼠标位置。这种情况发生的频率很高，以至于我们通常无法观察到CPU是否有很多工作或空闲。
- en: The OS is responsible for scheduling tasks and does this by switching contexts
    on the CPU. This process can happen many times each second, not only to keep the
    UI responsive but also to give some time to other background tasks and IO events.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统负责调度任务，这是通过在CPU上切换上下文来实现的。这个过程可以每秒发生多次，不仅为了保持用户界面的响应性，还为了给其他后台任务和I/O事件留出时间。
- en: This is now the prevailing way to design an operating system.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这现在已经成为设计操作系统的主流方式。
- en: Note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Later in this book, we’ll write our own green threads and cover a lot of basic
    knowledge about context switching, threads, stacks, and scheduling that will give
    you more insight into this topic, so stay tuned.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的后面部分，我们将编写自己的绿色线程，并涵盖许多关于上下文切换、线程、堆栈和调度等基础知识，这将使你对这个主题有更深入的了解，所以请保持关注。
- en: Hyper-threading
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超线程技术
- en: As CPUs evolved and added more functionality such as several **arithmetic logic
    units** (**ALUs**) and additional logic units, the CPU manufacturers realized
    that the entire CPU wasn't fully utilized. For example, when an operation only
    required some parts of the CPU, an instruction could be run on the ALU simultaneously.
    This became the start of **hyper-threading**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 随着CPU的发展，增加了更多功能，例如几个**算术逻辑单元**（**ALUs**）和额外的逻辑单元，CPU制造商意识到整个CPU并没有得到充分利用。例如，当一个操作只需要CPU的部分功能时，可以在ALU上同时运行一条指令。这成为了**超线程**的起点。
- en: Your computer today, for example, may have 6 cores and 12 logical cores.. This
    is exactly where hyper-threading comes in. It “simulates” two cores on the same
    core by using unused parts of the CPU to drive progress on thread *2* and simultaneously
    running the code on thread *1*. It does this by using a number of smart tricks
    (such as the one with the ALU).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你今天的电脑可能有6个核心和12个逻辑核心。这正是超线程发挥作用的地方。它通过使用CPU的未使用部分来驱动线程*2*的进度，并同时运行线程*1*上的代码，从而“模拟”同一核心上的两个核心。它是通过使用一些智能技巧（例如与ALU相关的技巧）来做到这一点的。
- en: Now, using hyper-threading, we could actually offload some work on one thread
    while keeping the UI interactive by responding to events in the second thread
    even though we only had one CPU core, thereby utilizing our hardware better.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过使用超线程，我们实际上可以在一个线程上卸载一些工作，同时通过在第二个线程中响应事件来保持用户界面交互性，即使我们只有一个CPU核心，从而更好地利用我们的硬件。
- en: You might wonder about the performance of hyper-threading
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会对超线程的性能感到好奇
- en: It turns out that hyper-threading has been continuously improved since the 90s.
    Since you’re not actually running two CPUs, there will be some operations that
    need to wait for each other to finish. The performance gain of hyper-threading
    compared to multitasking in a single core seems to be somewhere close to 30% but
    it largely depends on the workload.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，自90年代以来，超线程技术一直在不断改进。由于你实际上并没有运行两个CPU，因此将会有一些操作需要等待彼此完成。与单核中的多任务处理相比，超线程的性能提升似乎接近30%，但这在很大程度上取决于工作负载。
- en: Multicore processors
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多核处理器
- en: As most know, the clock frequency of processors has been flat for a long time.
    Processors get faster by improving **caches**, **branch prediction**, and **speculative
    execution**, and by working on the **processing pipelines** of the processors,
    but the gains seem to be diminishing.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如大多数人所知，处理器的时钟频率已经很长时间没有提升了。处理器通过改进**缓存**、**分支预测**和**推测执行**，以及通过优化处理器的**处理流水线**来变快，但收益似乎正在减少。
- en: On the other hand, new processors are so small that they allow us to have many
    on the same chip. Now, most CPUs have many cores and most often, each core will
    also have the ability to perform hyper-threading.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，新的处理器非常小，这使我们能够在同一芯片上拥有许多处理器。现在，大多数CPU都有多个核心，而且通常每个核心也都有执行超线程的能力。
- en: Do you really write synchronous code?
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你真的编写了同步代码吗？
- en: Like many things, this depends on your perspective. From the perspective of
    your process and the code you write, everything will normally happen in the order
    you write it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 就像许多事情一样，这取决于你的视角。从你的进程和编写的代码的角度来看，一切通常都会按照你编写的顺序发生。
- en: From the operating system’s perspective, it might or might not interrupt your
    code, pause it, and run some other code in the meantime before resuming your process.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从操作系统的角度来看，它可能会或可能不会中断你的代码，暂停它，并在恢复你的进程之前在同时运行一些其他代码。
- en: From the perspective of the CPU, it will mostly execute instructions one at
    a time.* It doesn’t care who wrote the code, though, so when a **hardware interrupt**
    happens, it will immediately stop and give control to an interrupt handler. This
    is how the CPU handles concurrency.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从CPU的角度来看，它通常会一次执行一条指令。*尽管它不在乎谁编写了代码，但当发生硬件中断时，它会立即停止并将控制权交给中断处理程序。这就是CPU处理并发的方式。
- en: Note
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*However, modern CPUs can also do a lot of things in parallel. Most CPUs are
    pipelined, meaning that the next instruction is loaded while the current one is
    executing. It might have a branch predictor that tries to figure out what instructions
    to load next.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*然而，现代CPU也可以并行执行很多事情。大多数CPU都是流水线化的，这意味着在当前指令执行的同时，会加载下一条指令。它可能有一个分支预测器，试图确定接下来要加载哪些指令。'
- en: The processor can also reorder instructions by using **out-of-order execution**
    if it believes it makes things faster this way without ‘asking’ or ‘telling’ the
    programmer or the OS, so you might not have any guarantee that A happens before
    B.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: The CPU offloads some work to separate ‘coprocessors’ such as the FPU for floating-point
    calculations, leaving the main CPU ready to do other tasks et cetera.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: As a high-level overview, it’s OK to model the CPU as operating in a synchronous
    manner, but for now, let’s just make a mental note that this is a model with some
    caveats that become especially important when talking about parallelism, synchronization
    primitives (such as mutexes and atomics), and the security of computers and operating
    systems.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency versus parallelism
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Right off the bat, we’ll dive into this subject by defining what **concurrency**
    is. Since it is quite easy to confuse *concurrent* with *parallel*, we will try
    to make a clear distinction between the two from the get-go.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Important
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency is about *dealing* with a lot of things at the same time.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism is about *doing* a lot of things at the same time.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'We call the concept of progressing multiple tasks at the same time *multitasking*.
    There are two ways to multitask. One is by *progressing* tasks concurrently, but
    not at the same time. Another is to progress tasks at the exact same time in parallel.
    *Figure 1**.1* depicts the difference between the two scenarios:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Multitasking two tasks](img/B20892_Figure_01.1.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Multitasking two tasks
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to agree on some definitions:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource**: This is something we need to be able to progress a task. Our
    resources are limited. This could be CPU time or memory.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task**: This is a set of operations that requires some kind of resource to
    progress. A task must consist of several sub-operations.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel**: This is something happening independently at the *exact* same
    time.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concurrent**: These are tasks that are *in progress* at the same time, but
    not necessarily progressing simultaneously.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is an important distinction. If two tasks are running concurrently, but
    are not running in parallel, they must be able to stop and resume their progress.
    We say that a task is *interruptible* if it allows for this kind of concurrency.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The mental model I use
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I firmly believe the main reason we find parallel and concurrent programming
    hard to differentiate stems from how we model events in our everyday life. We
    tend to define these terms loosely, so our intuition is often wrong.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t help that *concurrent* is defined in the dictionary as *operating
    or occurring at the same time*, which doesn’t really help us much when trying
    to describe how it differs from *parallel*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: For me, this first clicked when I started to understand why we want to make
    a distinction between parallel and concurrent in the first place!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: The *why* has everything to do with resource utilization and efficiency.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '*Efficiency is the (often measurable) ability to avoid wasting materials, energy,
    effort, money, and time in doing something or in producing a* *desired result.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '*Parallelism* is increasing the resources we use to solve a task. It has nothing
    to do with *efficiency*.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '*Concurrency* has everything to do with efficiency and resource utilization.
    Concurrency can never make *one single task go faster*. It can only help us utilize
    our resources better and thereby *finish a set of* *tasks faster*.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Let’s draw some parallels to process economics
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In businesses that manufacture goods, we often talk about **LEAN** processes.
    This is pretty easy to compare with why programmers care so much about what we
    can achieve if we handle tasks concurrently.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Let’s pretend we’re running a bar. We only serve Guinness beer and nothing else,
    but we serve our Guinness to perfection. Yes, I know, it’s a little niche, but
    bear with me.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'You are the manager of this bar, and your goal is to run it as efficiently
    as possible. Now, you can think of each bartender as a *CPU core*, and each order
    as a *task*. To manage this bar, you need to know the steps to serve a perfect
    Guinness:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Pour the Guinness draught into a glass tilted at 45 degrees until it’s 3-quarters
    full (15 seconds).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow the surge to settle for 100 seconds.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fill the glass completely to the top (5 seconds).
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serve.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since there is only one thing to order in the bar, customers only need to signal
    using their fingers how many they want to order, so we assume taking new orders
    is instantaneous. To keep things simple, the same goes for payment. In choosing
    how to run this bar, you have a few alternatives.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Alternative 1 – Fully synchronous task execution with one bartender
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You start out with only one bartender (CPU). The bartender takes one order,
    finishes it, and progresses to the next. The line is out the door and going two
    blocks down the street – great! One month later, you’re almost out of business
    and you wonder why.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Well, even though your bartender is very fast at taking new orders, they can
    only serve 30 customers an hour. Remember, they’re waiting for 100 seconds while
    the beer settles and they’re practically just standing there, and they only use
    20 seconds to actually fill the glass. Only after one order is completely finished
    can they progress to the next customer and take their order.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: The result is bad revenue, angry customers, and high costs. That’s not going
    to work.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Alternative 2 – Parallel and synchronous task execution
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, you hire 12 bartenders, and you calculate that you can serve about 360 customers
    an hour. The line is barely going out the door now, and revenue is looking great.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: One month goes by and again, you’re almost out of business. How can that be?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that having 12 bartenders is pretty expensive. Even though revenue
    is high, the costs are even higher. Throwing more resources at the problem doesn’t
    really make the bar more efficient.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Alternative 3 – Asynchronous task execution with one bartender
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, we’re back to square one. Let’s think this through and find a smarter way
    of working instead of throwing more resources at the problem.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: You ask your bartender whether they can start taking new orders while the beer
    settles so that they’re never just standing and waiting while there are customers
    to serve. The opening night comes and...
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Wow! On a busy night where the bartender works non-stop for a few hours, you
    calculate that they now only use just over 20 seconds on an order. You’ve basically
    eliminated all the waiting. Your theoretical throughput is now 240 beers per hour.
    If you add one more bartender, you’ll have higher throughput than you did while
    having 12 bartenders.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: However, you realize that you didn’t actually accomplish 240 beers an hour,
    since orders come somewhat erratically and not evenly spaced over time. Sometimes,
    the bartender is busy with a new order, preventing them from topping up and serving
    beers that are finished almost immediately. In real life, the throughput is only
    180 beers an hour.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Still, two bartenders could serve 360 beers an hour this way, the same amount
    that you served while employing 12 bartenders.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: This is good, but you ask yourself whether you can do even better.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Alternative 4 – Parallel and asynchronous task execution with two bartenders
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What if you hire two bartenders, and ask them to do just what we described
    in Alternative 3, but with one change: you allow them to steal each other’s tasks,
    so *bartender 1* can start pouring and set the beer down to settle, and *bartender
    2* can top it up and serve it if *bartender 1* is busy pouring a new order at
    that time? This way, it is only rarely that both bartenders are busy at the same
    time as one of the beers-in-progress becomes ready to get topped up and served.
    Almost all orders are finished and served in the shortest amount of time possible,
    letting customers leave the bar with their beer faster and giving space to customers
    who want to make a new order.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Now, this way, you can increase throughput even further. You still won’t reach
    the theoretical maximum, but you’ll get very close. On the opening night, you
    realize that the bartenders now process 230 orders an hour each, giving a total
    throughput of 460 beers an hour.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Revenue looks good, customers are happy, costs are kept at a minimum, and you’re
    one happy manager of the weirdest bar on earth (an extremely efficient bar, though).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency is about working smarter. Parallelism is a way of throwing more
    resources at the problem.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency and its relation to I/O
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you might understand from what I’ve written so far, writing async code mostly
    makes sense when you need to be smart to make optimal use of your resources.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you write a program that is working hard to solve a problem, there is
    often no help in concurrency. This is where parallelism comes into play, since
    it gives you a way to throw more resources at the problem if you can split it
    into parts that you can work on in parallel.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following two different use cases for concurrency:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: When performing I/O and you need to wait for some external event to occur
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you need to divide your attention and prevent one task from waiting too
    long
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first is the classic I/O example: you have to wait for a network call,
    a database query, or something else to happen before you can progress a task.
    However, you have many tasks to do so instead of waiting, you continue to work
    elsewhere and either check in regularly to see whether the task is ready to progress,
    or make sure you are notified when that task is ready to progress.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The second is an example that is often the case when having a UI. Let’s pretend
    you only have one core. How do you prevent the whole UI from becoming unresponsive
    while performing other CPU-intensive tasks?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Well, you can stop whatever task you’re doing every 16 ms, run the *update UI*
    task, and then resume whatever you were doing afterward. This way, you will have
    to stop/resume your task 60 times a second, but you will also have a fully responsive
    UI that has a roughly 60 Hz refresh rate.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: What about threads provided by the operating system?
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll cover threads a bit more when we talk about strategies for handling I/O
    later in this book, but I’ll mention them here as well. One challenge when using
    OS threads to understand concurrency is that they appear to be mapped to cores.
    That’s not necessarily a correct mental model to use, even though most operating
    systems will try to map one thread to one core up to the number of threads equal
    to the number of cores.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Once we create more threads than there are cores, the OS will switch between
    our threads and progress each of them concurrently using its scheduler to give
    each thread some time to run. You also must consider the fact that your program
    is not the only one running on the system. Other programs might spawn several
    threads as well, which means there will be many more threads than there are cores
    on the CPU.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, threads can be a means to perform tasks in parallel, but they can
    also be a means to achieve concurrency.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: This brings me to the last part about concurrency. It needs to be defined in
    some sort of reference frame.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right reference frame
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you write code that is perfectly synchronous from your perspective, stop
    for a second and consider how that looks from the operating system perspective.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: The operating system might not run your code from start to end at all. It might
    stop and resume your process many times. The CPU might get interrupted and handle
    some inputs while you think it’s only focused on your task.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'So, synchronous execution is only an illusion. But from the perspective of
    you as a programmer, it’s not, and that is the important takeaway:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '*When we talk about concurrency without providing any other context, we are
    using you as a programmer and your code (your process) as the reference frame.
    If you start pondering concurrency without keeping this in the back of your head,
    it will get confusing* *very fast.*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The reason I’m spending so much time on this is that once you realize the importance
    of having the same definitions and the same reference frame, you’ll start to see
    that some of the things you hear and learn that might seem contradictory really
    are not. You’ll just have to consider the reference frame first.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous versus concurrent
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, you might wonder why we’re spending all this time talking about multitasking,
    concurrency, and parallelism, when the book is about asynchronous programming.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The main reason for this is that all these concepts are closely related to each
    other, and can even have the same (or overlapping) meanings, depending on the
    context they’re used in.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: In an effort to make the definitions as distinct as possible, we’ll define these
    terms more narrowly than you’d normally see. However, just be aware that we can’t
    please everyone and we do this for our own sake of making the subject easier to
    understand. On the other hand, if you fancy heated internet debates, this is a
    good place to start. Just claim someone else’s definition of concurrent is 100
    % wrong or that yours is 100 % correct, and off you go.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '*For the sake of this book, we’ll stick to this definition: asynchronous programming
    is the way a programming language or library abstracts over concurrent operations,
    and how we as users of a language or library use that abstraction to execute*
    *tasks concurrently.*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: The operating system already has an existing abstraction that covers this, called
    **threads**. Using OS threads to handle asynchrony is often referred to **as multithreaded
    programming**. To avoid confusion, we’ll not refer to using OS threads directly
    as asynchronous programming, even though it solves the same problem.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Given that asynchronous programming is now scoped to be about abstractions over
    concurrent or parallel operations in a language or library, it’s also easier to
    understand that it’s just as relevant on embedded systems without an operating
    system as it is for programs that target a complex system with an advanced operating
    system. The definition itself does not imply any specific implementation even
    though we’ll look at a few popular ones throughout this book.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: If this still sounds complicated, I understand. Just sitting and reflecting
    on concurrency is difficult, but if we try to keep these thoughts in the back
    of our heads when we work with async code I promise it will get less and less
    confusing.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: The role of the operating system
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The operating system (OS) stands in the center of everything we do as programmers
    (well, unless you’re writing an operating system or working in the embedded realm),
    so there is no way for us to discuss any kind of fundamentals in programming without
    talking about operating systems in a bit of detail.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency from the operating system’s perspective
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This ties into what I talked about earlier when I said that concurrency needs
    to be talked about within a reference frame, and I explained that the OS might
    stop and start your process at any time.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: What we call synchronous code is, in most cases, code that appears synchronous
    to us as programmers. Neither the OS nor the CPU lives in a fully synchronous
    world.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Operating systems use preemptive multitasking and as long as the operating system
    you’re running is preemptively scheduling processes, you won’t have a guarantee
    that your code runs instruction by instruction without interruption.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: The operating system will make sure that all important processes get some time
    from the CPU to make progress.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: This is not as simple when we’re talking about modern machines with 4, 6, 8,
    or 12 physical cores, since you might actually execute code on one of the CPUs
    uninterrupted if the system is under very little load. The important part here
    is that you can’t know for sure and there is no guarantee that your code will
    be left to run uninterrupted.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Teaming up with the operating system
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you make a web request, you’re not asking the CPU or the network card to
    do something for you – you’re asking the operating system to talk to the network
    card for you.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: There is no way for you as a programmer to make your system optimally efficient
    without playing to the strengths of the operating system. You basically don’t
    have access to the hardware directly. *You must remember that the operating system
    is an abstraction over* *the hardware.*
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: However, this also means that to understand everything from the ground up, you’ll
    also need to know how your operating system handles these tasks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: To be able to work with the operating system, you’ll need to know how you can
    communicate with it, and that’s exactly what we’re going to go through next.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Communicating with the operating system
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Communication with an operating system happens through what we call a **system
    call** (**syscall**). We need to know how to make system calls and understand
    why it’s so important for us when we want to cooperate and communicate with the
    operating system. We also need to understand how the basic abstractions we use
    every day use system calls behind the scenes. We’ll have a detailed walkthrough
    in [*Chapter 3*](B20892_03.xhtml#_idTextAnchor063), so we’ll keep this brief for
    now.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: A system call uses a public API that the operating system provides so that programs
    we write in ‘userland’ can communicate with the OS.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, these calls are abstracted away for us as programmers by the
    language or the runtime we use.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Now, a syscall is an example of something that is unique to the kernel you’re
    communicating with, but the **UNIX** family of kernels has many similarities.
    UNIX systems expose this through **libc**.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Windows, on the other hand, uses its own API, often referred to as **WinAPI**,
    and it can operate radically differently from how the UNIX-based systems operate.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Most often, though, there is a way to achieve the same things. In terms of functionality,
    you might not notice a big difference but as we’ll see later, and especially when
    we dig into how **epoll**, **kqueue**, and **IOCP** work, they can differ a lot
    in how this functionality is implemented.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: However, a syscall is not the only way we interact with our operating system,
    as we’ll see in the following section.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The CPU and the operating system
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Does the CPU cooperate with the* *operating system?*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: If you had asked me this question when I first thought I understood how programs
    work, I would most likely have answered *no*. We run programs on the CPU and we
    can do whatever we want if we know how to do it. Now, first of all, I wouldn’t
    have thought this through, but unless you learn how CPUs and operating systems
    work together, it’s not easy to know for sure.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'What started to make me think I was very wrong was a segment of code that looked
    like what you’re about to see. If you think inline assembly in Rust looks foreign
    and confusing, don’t worry just yet. We’ll go through a proper introduction to
    inline assembly a little later in this book. I’ll make sure to go through each
    of the following lines until you get more comfortable with the syntax:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Repository reference: ch01/ac-assembly-dereference/src/main.rs'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: What you’ve just looked at is a dereference function written in assembly.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: The `mov {0}, [{1}]` line needs some explanation. `{0}` and `{1}` are templates
    that tell the compiler that we’re referring to the registers that `out(reg)` and
    `in(reg)` represent. The number is just an index, so if we had more inputs or
    outputs they would be numbered `{2}`, `{3}`, and so on. Since we only specify
    `reg` and not a specific register, we let the compiler choose what registers it
    wants to use.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The `mov` instruction instructs the CPU to take the first 8 bytes (if we’re
    on a 64-bit machine) it gets when reading the memory location that `{1}` points
    to and place that in the register represented by `{0}`. The `[]` brackets will
    instruct the CPU to treat the data in that register as a memory address, and instead
    of simply copying the memory address itself to `{0}`, it will fetch what’s at
    that memory location and move it over.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '*Anyway, we’re just writing instructions to the CPU here. No standard library,
    no syscall; just raw instructions. There is no way the OS is involved in that
    dereference* *function, right?*'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run this program, you get what you’d expect:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, if you keep the `dereference` function but replace the `main` function
    with a function that creates a pointer to the `99999999999999` address, which
    we know is invalid, we get this function:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, if we run that we get the following results.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the result on Linux:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is the result on Windows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We get a segmentation fault. Not surprising, really, but as you also might notice,
    the error we get is different on different platforms. Surely, the OS is involved
    somehow. Let’s take a look at what’s really happening here.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Down the rabbit hole
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It turns out that there is a great deal of cooperation between the OS and the
    CPU, but maybe not in the way you would naively think.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Many modern CPUs provide some basic infrastructure that operating systems use.
    This infrastructure gives us the security and stability we expect. Actually, most
    advanced CPUs provide a lot more options than operating systems such as Linux,
    BSD, and Windows actually use.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two in particular that I want to address here:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: How the CPU prevents us from accessing memory we’re not supposed to access
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the CPU handles asynchronous events such as I/O
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll cover the first one here and the second in the next section.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: How does the CPU prevent us from accessing memory we’re not supposed to access?
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As I mentioned, modern CPU architectures define some basic concepts by design.
    Some examples of this are as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Virtual memory
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Page table
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Page fault
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exceptions
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privilege level
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exactly how this works will differ depending on the specific CPU, so we’ll treat
    them in general terms here.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Most modern CPUs have a **memory management unit** (**MMU**). This part of the
    CPU is often etched on the same dye, even. The MMU’s job is to translate the virtual
    address we use in our programs to a physical address.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: When the OS starts a process (such as our program), it sets up a page table
    for our process and makes sure a special register on the CPU points to this page
    table.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Now, when we try to dereference `t_ptr` in the preceding code, the address is
    at some point sent for translation to the MMU, which looks it up in the page table
    to translate it to a physical address in the memory where it can fetch the data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: In the first case, it will point to a memory address on our stack that holds
    the value `100`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: When we pass in `99999999999999` and ask it to fetch what’s stored at that address
    (which is what dereferencing does), it looks for the translation in the page table
    but can’t find it.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The CPU then treats this as a page fault.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: At boot, the OS provided the CPU with an **interrupt descriptor table**. This
    table has a predefined format where the OS provides handlers for the predefined
    conditions the CPU can encounter.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Since the OS provided a pointer to a function that handles *page fault*, the
    CPU jumps to that function when we try to dereference `99999999999999` and thereby
    hands over control to the operating system.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The OS then prints a nice message for us, letting us know that we encountered
    what it calls a **segmentation fault**. This message will therefore vary depending
    on the OS you run the code on.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: But can’t we just change the page table in the CPU?
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, this is where the *privilege level* comes in. Most modern operating systems
    operate with two *ring levels*: *ring 0*, the kernel space, and *ring 3*, the
    user space.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Privilege rings](img/B20892_Figure_01.2.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Privilege rings
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Most CPUs have a concept of more rings than what most modern operating systems
    use. This has historical reasons, which is also why *ring 0* and *ring 3* are
    used (and not 1 and 2).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Every entry in the page table has additional information about it. Amongst that
    information is the information about which ring it belongs to. This information
    is set up when your OS boots up.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Code executed in *ring 0* has almost unrestricted access to external devices
    and memory, and is free to change registers that provide security at the hardware
    level.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: The code you write in *ring 3* will typically have extremely restricted access
    to I/O and certain CPU registers (and instructions). Trying to issue an instruction
    or setting a register from *ring 3* to change the page table will be prevented
    by the CPU. The CPU will then treat this as an exception and jump to the handler
    for that exception provided by the OS.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: This is also the reason why you have no other choice than to cooperate with
    the OS and handle I/O tasks through syscalls. The system wouldn’t be very secure
    if this wasn’t the case.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to sum it up: yes, the CPU and the OS cooperate a great deal. Most modern
    desktop CPUs are built with an OS in mind, so they provide the hooks and infrastructure
    that the OS latches onto upon bootup. When the OS spawns a process, it also sets
    its privilege level, making sure that normal processes stay within the borders
    it defines to maintain stability and security.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Interrupts, firmware, and I/O
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re nearing the end of the general CS subjects in this book, and we’ll start
    to dig our way out of the rabbit hole soon.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: This part tries to tie things together and look at how the whole computer works
    as a system to handle I/O and concurrency.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get to it!
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: A simplified overview
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s look at some of the steps where we imagine that we read from a network
    card:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20892_Figure_01.3.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: Remember that we’re simplifying a lot here. This is a rather complex operation
    but we’ll focus on the parts that are of most interest to us and skip a few steps
    along the way.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Our code
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We register a socket. This happens by issuing a *syscall* to the OS. Depending
    on the OS, we either get a *file descriptor* (macOS/Linux) or a *socket* (Windows).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: The next step is that we register our interest in `Read` events on that socket.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – Registering events with the OS
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is handled in one of three ways:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: We tell the operating system that we’re interested in `Read` events but we want
    to wait for it to happen by `yielding` control over our thread to the OS. The
    OS then suspends our thread by storing the register state and switches to some
    other thread
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*From our perspective, this will be blocking our thread until we have data*
    *to read.*'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: We tell the operating system that we’re interested in `Read` events but we just
    want a handle to a task that we can `poll` to check whether the event is ready
    or not.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*The OS will not suspend our thread, so this will not block* *our code.*'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: We tell the operating system that we are probably going to be interested in
    many events, but we want to subscribe to one event queue. When we `poll` this
    queue, it will block our thread until one or more events occur.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*This will block our thread while we wait for events* *to occur.*'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Chapters 3 and 4 will go into detail about the third method, as it’s the most
    used method for modern async frameworks to handle concurrency.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – The network card
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’re skipping some steps here, but I don’t think they’re vital to our understanding.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: On the network card, there is a small microcontroller running specialized firmware.
    We can imagine that this microcontroller is polling in a busy loop, checking whether
    any data is incoming.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: The exact way the network card handles its internals is a little different from
    what I suggest here, and will most likely vary from vendor to vendor. The important
    part is that there is a very simple but specialized CPU running on the network
    card doing work to check whether there are incoming events.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Once the firmware registers incoming data, it issues a *hardware interrupt*.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – Hardware interrupt
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A modern CPU has a set of **interrupt request line** (**IRQs**) for it to handle
    events that occur from external devices. A CPU has a fixed set of interrupt lines.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: A hardware interrupt is an electrical signal that can occur at any time. The
    CPU immediately *interrupts* its normal workflow to handle the interrupt by saving
    the state of its registers and looking up the interrupt handler. The interrupt
    handlers are defined in the **interrupt descriptor** **table** (**IDT**).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – Interrupt handler
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The IDT is a table where the OS (or a driver) registers handlers for different
    interrupts that may occur. Each entry points to a handler function for a specific
    interrupt. The handler function for a network card would typically be registered
    and handled by a *driver* for that card.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The IDT is not stored on the CPU as it might seem in *Figure 1**.3*. It’s located
    in a fixed and known location in the main memory. The CPU only holds a pointer
    to the table in one of its registers.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Step 6 – Writing the data
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a step that might vary a lot depending on the CPU and the firmware on
    the network card. If the network card and the CPU support **direct memory access**
    (**DMA**), which should be the standard on all modern systems today, the network
    card will write data directly to a set of buffers that the OS already has set
    up in the main memory.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: In such a system, the *firmware* on the network card might issue an *interrupt*
    when the data is *written* to memory. DMA is very efficient, since the CPU is
    only notified when the data is already in memory. On older systems, the CPU needed
    to devote resources to handle the data transfer from the network card.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '*The* **direct memory access controller** *(* **DMAC***) is added to the diagram
    since in such a system, it would control the* *access to memory. It’s not part
    of the CPU as indicated in the previous diagram. We’re deep enough in the rabbit
    hole now, and exactly where the different parts of a system are is not really
    important to us right now, so let’s* *move on.*'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Step 7 – The driver
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *driver* would normally handle the communication between the OS and the
    network card. At some point, the buffers are filled and the network card issues
    an interrupt. The CPU then jumps to the handler of that interrupt. The interrupt
    handler for this exact type of interrupt is registered by the driver, so it’s
    actually the driver that handles this event and, in turn, informs the kernel that
    the data is ready to be read.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Step 8 – Reading the data
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Depending on whether we chose method 1, 2, or 3, the OS will do as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Wake our thread
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return **Ready** on the next poll
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wake the thread and return a `Read` event for the handler we registered
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interrupts
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you know by now, there are two kinds of interrupts:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Hardware interrupts
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software interrupts
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are very different in nature.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Hardware interrupts
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hardware interrupts are created by sending an electrical signal through an IRQ.
    These hardware lines signal the CPU directly.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Software interrupts
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These are interrupts issued from software instead of hardware. As in the case
    of a hardware interrupt, the CPU jumps to the IDT and runs the handler for the
    specified interrupt.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Firmware
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Firmware doesn’t get much attention from most of us; however, it’s a crucial
    part of the world we live in. It runs on all kinds of hardware and has all kinds
    of strange and peculiar ways to make the computers we program on work.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Now, the firmware needs a microcontroller to be able to work. Even the CPU has
    firmware that makes it work. That means there are many more small ‘CPUs’ on our
    system than the cores we program against.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Why is this important? Well, you remember that concurrency is all about efficiency,
    right? Since we have many CPUs/microcontrollers already doing work for us on our
    system, one of our concerns is to not replicate or duplicate that work when we
    write code.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: If a network card has firmware that continually checks whether new data has
    arrived, it’s pretty wasteful if we duplicate that by letting our CPU continually
    check whether new data arrives as well. It’s much better if we either check once
    in a while, or even better, get notified when data has arrived.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered a lot of ground, so good job on doing all that legwork.
    We learned a little bit about how CPUs and operating systems have evolved from
    a historical perspective and the difference between non-preemptive and preemptive
    multitasking. We discussed the difference between concurrency and parallelism,
    talked about the role of the operating system, and learned that system calls are
    the primary way for us to interact with the host operating system. You’ve also
    seen how the CPU and the operating system cooperate through an infrastructure
    designed as part of the CPU.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we went through a diagram on what happens when you issue a network call.
    You know there are at least three different ways for us to deal with the fact
    that the I/O call takes some time to execute, and we have to decide which way
    we want to handle that waiting time.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: This covers most of the general background information we need so that we have
    the same definitions and overview before we go on. We’ll go into more detail as
    we progress through the book, and the first topic that we’ll cover in the next
    chapter is how programming languages model asynchronous program flow by looking
    into threads, coroutines and futures.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL

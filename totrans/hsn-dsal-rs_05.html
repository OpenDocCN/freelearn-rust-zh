<html><head></head><body>
        

                            
                    <h1 class="header-title">Robust Trees</h1>
                
            
            
                
<p class="mce-root">Lists are great for storing a bunch of items, but what about looking up specific elements? In the previous chapter, a skip list greatly outperformed a regular linked list when simply finding an item. Why? Because it was utilizing an iteration strategy that resembles that of a balanced tree structure: there, the internal order lets the algorithm strategically skip items. However, that's only the beginning. Many libraries, databases, and search engines are built on trees; in fact, whenever a program is compiled, the compiler creates an abstract syntax tree.</p>
<p class="mce-root">Tree-based data structures incorporate all kinds of smart ideas that we will explore in this chapter, so you can look forward to the following:</p>
<ul>
<li>Implementing and understanding a binary search tree</li>
<li>Learning about self-balancing trees</li>
<li>How prefix or suffix trees work</li>
<li>What a priority queue uses internally</li>
<li>Graphs, the most general tree structure</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Binary search tree</h1>
                
            
            
                
<p>A tree structure is almost like a linked list: each node has branches—in the case of a binary tree, there are two—which represent children of that node. Since these children have children of their own, the node count grows exponentially, building a hierarchical structure that looks like a regular tree turned on its head.</p>
<p>Binary trees are a subset of these structures with only two branches, typically called left and right. However, that does not inherently help the tree's performance. This is why using a <em>binary search tree</em>, where left represents the smaller or equal value to its parent, and right anything that's greater than that parent node, was established!</p>
<p>If that was confusing, don't worry; there will be code. First, some vocabulary though: what would you call the far ends of the tree? Leaves. Cutting off branches? Pruning. The number of branches per node? Branching factor (binary trees have a branching factor of 2).</p>
<p>Great, with that out of the way, the nodes can be shown—although they look a lot like the doubly linked list from the previous chapter:</p>
<pre>type Tree = Option&lt;Box&lt;Node&gt;&gt;;<br/><br/>struct Node {<br/>    pub value: u64,<br/>    left: Tree,<br/>    right: Tree,<br/>}</pre>
<p>Similarly, the tree structure itself is only a pointer to the root node:</p>
<pre>pub struct BinarySearchTree {<br/>    root: Tree,<br/>    pub length: u64,<br/>}</pre>
<p class="mce-root">Yet before you can get comfortable with the new data structure, the product team from the previous chapter is back! You did a great job improving the transaction log and they want to continue that progress and build an <strong>Internet of Things</strong> (<strong>IoT</strong>) device management platform so users can register a device with a numerical name and later search for it. However, the search has to be fast or really fast, which is especially critical since many customers have announced the incorporation of more than 10,000 devices into the new system!</p>
<p>Isn't this a great opportunity to get more experience with a binary search tree?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">IoT device management</h1>
                
            
            
                
<p>Device management in the IoT space is mostly about storing and retrieving specific devices or device twins. These objects typically store addresses, configuration values, encryption keys, or other things for small devices so nobody has to connect manually. Consequently, keeping an inventory is critical!</p>
<p>For now, the product team settled on a numerical "name", to be available faster than the competition, and to keep the requirements short:</p>
<ul>
<li>Store IoT device objects (containing the IP address, numerical name, and type)</li>
<li>Retrieve IoT objects by numerical name</li>
<li>Iterate over IoT objects</li>
</ul>
<p>A great use for a tree: the numerical name can be used to create a tree and search for it nice and quickly. The basic object for storing this IoT device information looks like this:</p>
<div><pre>#[derive(Clone, Debug)]<br/>pub struct IoTDevice {<br/>    pub numerical_id: u64,<br/>    pub address: String,<br/>}</pre></div>
<p>For simplicity, this object will be used in the code directly (adding generics isn't too tricky, but would go beyond the scope of this book):</p>
<pre>type Tree = Option&lt;Box&lt;Node&gt;&gt;;<br/>struct Node {<br/>    pub dev: IoTDevice,<br/>    left: Tree,<br/>    right: Tree,<br/>}</pre>
<p>Starting with this basic implementation, the requisite operations, <kbd>add</kbd> and <kbd>find</kbd>, can be implemented.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">More devices</h1>
                
            
            
                
<p>Unlike lists, trees make a major decision on insert: which side does the new element go to? Starting at the root node, each node's value is compared to the value that is going to be inserted: is this greater than or less than that? Either decision will lead down a different subtree (left or right).</p>
<p>This process is (usually recursively) repeated until the targeted subtree is <kbd>None</kbd>, which is exactly where the new value is inserted—as a leaf of the tree. If this is the first value going into the tree, it becomes the root node. There are some problems with this, and the more experienced programmers will have had a strange feeling already: what happens if you insert numbers in ascending order?</p>
<p>These feelings are justified. Inserting in ascending order (for example, <kbd>1</kbd>, <kbd>2</kbd>, <kbd>3</kbd>, <kbd>4</kbd>) will lead to a tree that is basically a list in disguise! This is also called a (very) unbalanced tree and won't have any of the benefits of other trees:</p>
<pre>  1<br/>/  \ <br/>     2<br/>   /   \<br/>         3<br/>       /   \ <br/>             4 </pre>
<p>During this chapter, we are going to go a lot more things on balancing trees and why that is important in order to achieve high performance. In order to avoid this pitfall associated with binary search trees, the first value to insert should ideally be the median of all elements since it will be used as the root node, as is visible in the following code snippet:</p>
<div><pre>pub fn add(&amp;mut self, device: IoTDevice) {<br/>    self.length += 1;<br/>    let root = mem::replace(&amp;mut self.root, None);<br/>    self.root = self.add_rec(root, device);<br/>}<br/><br/>fn add_rec(&amp;mut self, node: Tree, device: IoTDevice) -&gt; Tree {<br/>    match node {<br/>        Some(mut n) =&gt; {<br/>            if n.dev.numerical_id &lt;= device.numerical_id {<br/>                n.left = self.add_rec(n.left, device);<br/>                Some(n)<br/>            } else {<br/>                n.right = self.add_rec(n.right, device);<br/>                Some(n)<br/>            }<br/>        }<br/>        _ =&gt; Node::new(device),<br/>    }<br/>}</pre></div>
<p>Split into two parts, this code walks the tree recursively to find the appropriate position and attaches the new value as a leaf there. Actually, the insert is not that different from a regular tree walk in search or iteration.</p>
<div><strong>Recursion</strong> is when a function calls itself. Think of the movie Inception—having a dream inside a dream inside a dream. it's the same concept. There are a few implications in programming: the original function is disposed of last since it's only finished after all recursive calls return. This also means that everything lives on the much smaller stack, which may result in a stack overflow when there are too many calls! Typically, recursive algorithms can also be implemented iteratively, but they are much harder to understand—so choose wisely!</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Finding the right one</h1>
                
            
            
                
<p>Having the ability to add devices to the tree, it's even more important to retrieve them again. Just like the skip list in the previous chapter, this retrieval ideally runs in <em>O(log n)</em> time, meaning that the majority of elements are going to be skipped when searching.</p>
<p>Consequently, if the tree is skewed in one direction, the performance approaches <em>O(n)</em> and more elements are looked at, thereby making the search slower. Since a skewed tree is more like a list, the recursive insert algorithm can overflow the stack quickly thanks to the high number of "levels" with only a single item. Otherwise, the recursive algorithm is only called as many times as the tree's height, a considerably lower number in a balanced tree. The algorithm itself resembles the previously shown insert algorithm:</p>
<div><pre>pub fn find(&amp;self, numerical_id: u64) -&gt; Option&lt;IoTDevice&gt; {<br/>    self.find_r(&amp;self.root, numerical_id)<br/>}<br/><br/>fn find_r(&amp;self, node: &amp;Tree, numerical_id: u64) -&gt; Option&lt;IoTDevice&gt; {<br/>    match node {<br/>        Some(n) =&gt; {<br/>            if n.dev.numerical_id == numerical_id {<br/>                Some(n.dev.clone())<br/>            } else if n.dev.numerical_id &lt; numerical_id {<br/>                self.find_r(&amp;n.left, numerical_id)<br/>            } else {<br/>                self.find_r(&amp;n.right, numerical_id)<br/>            }<br/>        }<br/>        _ =&gt; None,<br/>    }<br/>}</pre></div>
<p class="mce-root">Although this snippet's purpose is to find a specific node, there is a close relationship to enumerating every device—something that the users of this service certainly will want to have.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Finding all devices</h1>
                
            
            
                
<p>Walking a tree and executing a callback when visiting each node can be done in three ways:</p>
<ul>
<li>Pre-order, executing the callback <em>before descending</em></li>
<li>In-order, which executes the callback <em>after descending left, but before descending into the right subtree</em></li>
<li>Post-order, where the callback is executed <em>after descending</em></li>
</ul>
<p>Each of these traversal strategies yields a different order of tree elements, with in-order producing a sorted output, while pre- and post-order create a more structurally oriented sorting. For our users, the in-order walk will provide the best experience, since it also lets them reason better regarding the expected outcome, and, if displayed in a list, it's easier to navigate.</p>
<p>While implementing this walk is very easy to do recursively, providing an iterator is more user-friendly (just like the lists in the previous chapter) and it enables a number of added functions, such as <kbd>map()</kbd> and <kbd>filter()</kbd>. However, this implementation has to be iterative, which makes it more complex and removes some of the efficiency of the tree.</p>
<p>Therefore, this tree supports a <kbd>walk()</kbd> function which calls a provided function each time it encounters a node, which can be used to fill a vector for the iterator:</p>
<div><pre>pub fn walk(&amp;self, callback: impl Fn(&amp;IoTDevice) -&gt; ()) {   <br/>    self.walk_in_order(&amp;self.root, &amp;callback);<br/>}<br/><br/>fn walk_in_order(&amp;self, node: &amp;Tree, callback: &amp;impl Fn(&amp;IoTDevice) -&gt; ()) {<br/>    if let Some(n) = node {<br/>        self.walk_in_order(&amp;n.left, callback);<br/>        callback(&amp;n.dev);<br/>        self.walk_in_order(&amp;n.right, callback);<br/>    }<br/>}</pre></div>
<p>An example of how to build a vector using this walk method is shown here:</p>
<div><pre>let my_devices: RefCell&lt;Vec&lt;IoTDevice&gt;&gt; = RefCell::new(vec![]); tree.walk(|n| my_devices.borrow_mut().push(n.clone()));</pre></div>
<p>With this walking ability, all requirements are satisfied for now.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Wrap up</h1>
                
            
            
                
<p>Thanks to their simplicity, binary search trees are beautifully efficient. In fact, the entire tree implementation for this section was done in fewer than 90 lines of Rust code, with functions of about 10 lines each.</p>
<p>A binary tree's efficiency allows for recursion to be used a lot, which typically results in functions that are easier to understand compared to their iterative counterparts. In the ideal case, that is, when a tree is perfectly balanced, a function only has to process <em>log2(n)</em> nodes (<em>n</em> being the total number of nodes)—19 in a tree of 1,000,000 elements!</p>
<p class="mce-root">Unbalanced trees will decrease performance significantly and they are easily created by accident. The most unbalanced tree is created by inserting values that are already sorted, creating a very large difference in search performance:</p>
<pre><strong>test tests::bench_sorted_insert_bst_find ... bench: 16,376 ns/iter (+/- 6,525)</strong><br/><strong>test tests::bench_unsorted_insert_bst_find ... bench: 398 ns/iter (+/- 182)</strong></pre>
<p>These results reflect the differences between a skip list and a doubly linked list from the previous chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Upsides</h1>
                
            
            
                
<p>To recap, a binary search tree has a number of great benefits for its users:</p>
<ul>
<li>Simple implementation</li>
<li>Efficient and fast search</li>
<li>Traversal allows for different orderings</li>
<li>Great for large amounts of unsorted data</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Downsides</h1>
                
            
            
                
<p>By using a binary search tree, its drawbacks become obvious quickly:</p>
<ul>
<li>Worst-case performance is that of a linked list</li>
<li>Unbalanced trees are easy to create by accident</li>
<li>Unbalanced trees cannot be "repaired"</li>
<li>Recursive algorithms can overflow on unbalanced trees</li>
</ul>
<p>Obviously, a lot of the deeper issues result from the tree being unbalanced in some way—for which there is a solution: self-balancing binary search trees.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Red-black tree</h1>
                
            
            
                
<p>With the previous tree structure, there was a major downside: a previously unknown sequence of keys that is inserted into the tree cannot be sorted. Think of how most identifiers are generated; they are typically ascending numbers. Shuffling these numbers won't always work, especially when they are gradually added. Since this leads to an unbalanced tree (the extreme case behaves just like a list), Rudolf Bayer came up with the idea of a special, self-balancing tree: the red-black tree.</p>
<p>This tree is a binary search tree that adds logic to rebalance after inserts. Within this operation, it is crucial to know when to stop "balancing"—which is where the inventor thought to use two colors: red and black.</p>
<p>In literature, the red-black tree is described as a binary search tree that satisfies a set of rules:</p>
<ul>
<li>The root node is always black</li>
<li>Each other node is either red or black</li>
<li>All leaves (often <kbd>null</kbd>/<kbd>NIL</kbd> values) are considered black</li>
<li>A red node can only have black children</li>
<li>Any path from the root to its leaves has the same number of black nodes</li>
</ul>
<p>By enforcing these rules, a tree can be programmatically verified to be balanced. How are these rules doing that? Rules 4 and 5 provide the answer: if each branch has to have the same number of black nodes, neither side can be significantly longer than the other unless there were lots of red nodes.</p>
<p>How many of those can there be? At most, as many as there are black nodes—because they cannot have red children. Thus, one branch cannot significantly exceed the other, making this tree balanced. The code of the validation function illustrates this very well:</p>
<pre><br/>pub fn is_a_valid_red_black_tree(&amp;self) -&gt; bool {<br/>    let result = self.validate(&amp;self.root, Color::Red, 0);<br/>    let red_red = result.0;<br/>    let black_height_min = result.1;<br/>    let black_height_max = result.2;<br/>    red_red == 0 &amp;&amp; black_height_min == black_height_max<br/>}<br/><br/>// red-red violations, min black-height, max-black-height<br/>fn validate(<br/>    &amp;self,<br/>    node: &amp;Tree,<br/>    parent_color: Color,<br/>    black_height: usize,<br/>) -&gt; (usize, usize, usize) {<br/>    if let Some(n) = node {<br/>        let n = n.borrow();<br/>        let red_red = if parent_color == Color::Red &amp;&amp; n.color == Color::Red {<br/>            1<br/>        } else {<br/>            0<br/>        };<br/>        let black_height = black_height + match n.color {<br/>            Color::Black =&gt; 1,<br/>            _ =&gt; 0,<br/>        };<br/>        let l = self.validate(&amp;n.left, n.color.clone(), black_height);<br/>        let r = self.validate(&amp;n.right, n.color.clone(), black_height);<br/>        (red_red + l.0 + r.0, cmp::min(l.1, r.1), cmp::max(l.2, r.2))<br/>    } else {<br/>        (0, black_height, black_height)<br/>    }<br/>}</pre>
<p>Like the binary search tree, each node in a tree has two children, with a key either greater than, equal to, or less than that of the current node. In addition to the key (as in a key-value pair), the nodes store a color that is red on insert, and a pointer back to its parent. Why? This is due to the required rebalancing, which will be described later. First, this can be a typical node:</p>
<pre>type BareTree = Rc&lt;RefCell&lt;Node&gt;&gt;;<br/>type Tree = Option&lt;BareTree&gt;;<br/><br/>struct Node {<br/>    pub color: Color,<br/>    pub key: u32,<br/>    pub parent: Tree,<br/>    left: Tree,<br/>    right: Tree,<br/>}</pre>
<p>Using these nodes, a tree can be created just like a binary search tree. In fact, the insert mechanism is exactly the same except for setting the parent pointer. Newly inserted nodes are always colored red and, once in place, the tree might violate the rules. Only then is it time to find and fix these issues.</p>
<p>After an insert, the tree is in an invalid state that requires a series of steps to restore the red-black tree's properties. This series, comprised of rotation and recolor<em>,</em> starts at the inserted node and goes up the tree until the root node is considered valid. In summary, a red-black tree is a binary search tree that is rotated and recolored until balance is restored.</p>
<p><strong>Recolor</strong> is simply changing the color of a specified node to a specific color, which happens as a final step when doing tree rebalancing. <strong>Rotation</strong> is an operation of a set of three nodes: the current node, its parent, and its grandparent. It is employed to fold list-like chains into trees by rotating either left or right around a specified node. The result is a changed hierarchy, with either the left or right child of the center node on top, and its children adjusted accordingly:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/f45acff7-777a-4242-b534-c2464c606330.png" style="width:17.42em;height:11.25em;"/></p>
<p>Clearly, this example is too simple and it can only happen within the first few inserts. Rotations require recolors after redefining the hierarchy of a set of nodes. To add further complexity, rotations regularly happen in succession:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/bd2d323f-1996-49ea-a6ba-facdae784fe9.png" style="width:22.75em;height:14.00em;"/></p>
<p>The preceding tree has had a node inserted and is now violating rule 4: <em>no red children on a red node</em>. The next step is to determine which steps are required to establish balance. For that, the parent's sibling's color (that is, the uncle's color) is examined. Red means that a simple recoloring of both siblings to black and their parent to red won't invalidate the tree and will fix the condition. This is not the case here (the uncle is <kbd>None</kbd>, which means black), and some rotation is required:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0a1e50d9-be7b-4738-b786-f1c75b648f92.png" style="width:18.08em;height:11.92em;"/></p>
<p>The first move is to align the nodes into a chain of left children (in this case), which is done by rotating around the center node, the insertee's parent:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b3f8638e-86e6-4b37-bf7b-60c6340b3c60.png" style="width:18.33em;height:11.33em;"/></p>
<p>Once the chain is aligned, a right rotation of the third node (grandparent) creates a valid subtree by elevating the middle node (the "youngest" node/insertee), with the former parent and grandparent to the left and right, respectively. Then, the new constellation is recolored and the procedure begins anew, centered around the root of the new subtree (in this example, though, the tree is already valid):</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/f007f46a-f3ea-4374-8775-e444dec4c6f7.png" style="width:24.25em;height:15.33em;"/></p>
<p>These steps can be repeated until the tree is valid and the root is reached (which might be different from what you started off with). This root node is heuristically painted black as well, which cannot violate the rules but shortcuts a potential red-red violation. For code on the fixing operation, see the following subsections.</p>
<p>The product team has even called this time to put emphasis on their new product ideas. The IoT platform is quite popular and customers have been using it a lot—and recognized a major slowdown when they kept adding their sequentially numbered devices. This resulted in angry calls to customer services, which then turned to the product team for help—and now it's time to implement the solution and replace the current tree for device management.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Better IoT device management</h1>
                
            
            
                
<p>The problem that our users face is clear: if a binary search tree encounters sorted data (such as incremental IDs), it can only ever append to one side, creating an unbalanced tree. A red-black tree is able to handle this at the cost of more operations being executed during insert (such as rotating subtrees), which is acceptable for the users.</p>
<p>This tree has similar nodes to the binary search tree, with the addition of a color field and a parent field, the latter of which triggers a wider change compared to the binary search tree. Thanks to the pointer back, the tree nodes cannot exclusively own the pointers to the children and parent (because, who owns this value, the parent or the child?), which requires a well-known pattern in Rust: interior mutability. As discussed in an earlier chapter, <kbd>RefCell</kbd> owns the data's portion of the memory and handles borrow-checking at runtime so that mutable and immutable references can be obtained:</p>
<pre>type BareTree = Rc&lt;RefCell&lt;Node&gt;&gt;;<br/>type Tree = Option&lt;BareTree&gt;;<br/><br/>struct Node {<br/>    pub color: Color,<br/>    pub dev: IoTDevice,<br/>    pub parent: Tree,<br/>    left: Tree,<br/>    right: Tree,<br/>}<br/><br/>impl Node {<br/>    pub fn new(dev: IoTDevice) -&gt; Tree {<br/>        Some(Rc::new(RefCell::new(Node {<br/>            color: Color::Red,<br/>            dev: dev,<br/>            parent: None,<br/>            left: None,<br/>            right: None,<br/>        })))<br/>    }<br/>}</pre>
<p>With that in place, devices can be added.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Even more devices</h1>
                
            
            
                
<p>Once the tree is created, an <kbd>add()</kbd> function lets the user add a device. The tree then proceeds to insert the new key just as if it were a binary search tree—only to check and fix any errors immediately afterward. Where a binary search tree could use a simple <kbd>if</kbd> condition to decide the direction it proceeds in, in the red-black tree, the direction has a larger impact, and nesting <kbd>if</kbd> conditions will result in chaotic, unreadable code.</p>
<p>Thus, let's create <kbd>enum</kbd> first, so any time the direction (example, insert, position of a node relative to another node, and so on) has to be decided, we can rely on that <kbd>enum</kbd>. The same goes for the tree's color:</p>
<pre>#[derive(Clone, Debug, PartialEq)]<br/>enum Color {<br/>    Red,<br/>    Black,<br/>}<br/><br/>#[derive(PartialEq)]<br/>enum RBOperation {<br/>    LeftNode,<br/>    RightNode,<br/>}</pre>
<p>Now, the <kbd>add()</kbd> function can use Rust's match clause to nicely structure the two branches:</p>
<pre>pub fn add(&amp;mut self, device: IoTDevice) {<br/>    self.length += 1;<br/>    let root = mem::replace(&amp;mut self.root, None);<br/>    let new_tree = self.add_r(root, device);<br/>    self.root = self.fix_tree(new_tree.1);<br/>}<br/><br/>fn add_r(&amp;mut self, mut node: Tree, device: IoTDevice) -&gt; (Tree, BareTree) {<br/>    if let Some(n) = node.take() {<br/>        let new: BareTree;<br/>        let current_device = n.borrow().dev.clone();<br/><br/>        match self.check(&amp;current_device, &amp;device) {<br/>            RBOperation::LeftNode =&gt; {<br/>                let left = n.borrow().left.clone();<br/>                let new_tree = self.add_r(left, device);<br/>                new = new_tree.1;<br/>                let new_tree = new_tree.0.unwrap();<br/>                new_tree.borrow_mut().parent = Some(n.clone());<br/>                n.borrow_mut().left = Some(new_tree);<br/>            }<br/><br/>            RBOperation::RightNode =&gt; {<br/>                let right = n.borrow().right.clone();<br/>                let new_tree = self.add_r(right, device);<br/>                new = new_tree.1;<br/>                let new_tree = new_tree.0.unwrap();<br/><br/>                new_tree.borrow_mut().parent = Some(n.clone());<br/>                n.borrow_mut().right = Some(new_tree);<br/>            }<br/>        }<br/>        (Some(n), new)<br/>    } else {<br/>        let new = Node::new(device);<br/>        (new.clone(), new.unwrap())<br/>    }<br/>}</pre>
<p>One of the primary parts of the code is "checking" two devices, that is, comparing them in order to provide a direction that they should be appended to. This comparison is done in a separate function to improve maintainability:</p>
<pre>fn check(&amp;self, a: &amp;IoTDevice, b: &amp;IoTDevice) -&gt; RBOperation {<br/>    if a.numerical_id &lt;= b.numerical_id {<br/>        RBOperation::LeftNode<br/>    } else {<br/>        RBOperation::RightNode<br/>    }<br/>}</pre>
<p>While this tree will append every larger item to the left (which seems unusual), the algorithms don't care; they will work regardless—and, by wrapping this into its own function, change is quick and easy.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Balancing the tree</h1>
                
            
            
                
<p>After the node is added properly, <kbd>fix_tree()</kbd> takes care of restoring the red-black tree's properties—iteratively. While this is nicely descriptive and demonstrative it is long, so let's break it up into parts. Initially, the function determines whether it should stop (or not even start)—which only happens in two cases:</p>
<ul>
<li>When it's already the root node</li>
<li>When the parent of the currently inspected node is red</li>
</ul>
<p>Clearly, the former is the regular exit criterion as well, as the loop optimizes and moves the current pointer (<kbd>n</kbd> as in node) from the bottom toward the root of the tree to stop there:</p>
<pre>fn fix_tree(&amp;mut self, inserted: BareTree) -&gt; Tree {<br/>    let mut not_root = inserted.borrow().parent.is_some();<br/><br/>    let root = if not_root {<br/>        let mut parent_is_red = self.parent_color(&amp;inserted) == Color::Red;<br/>        let mut n = inserted.clone();<br/>        while parent_is_red &amp;&amp; not_root {<br/>            if let Some(uncle) = self.uncle(n.clone()) {</pre>
<p>Once started, the loop immediately goes for the uncle of a particular node (that is, the grandparent's second child) and its color. The uncle node can either be black (or <kbd>None</kbd>) or red, which are the two cases covered next. It is also important to find out <em>which</em> uncle it is, and therefore which node the current pointer points to: a left node or a right node. Let's take a look at the following code snippet:</p>
<pre>           if let Some(uncle) = self.uncle(n.clone()) {<br/>                let which = uncle.1;<br/>                let uncle = uncle.0;<br/><br/>                match which {<br/>                    RBOperation::LeftNode =&gt; {<br/>                        // uncle is on the left<br/>                        // ...<br/><br/>                    RBOperation::RightNode =&gt; {<br/>                        // uncle is on the right<br/>                        // ...</pre>
<p>This information is critical in determining the rotation order in this area of the tree. In fact, the two branches will execute the same steps, but mirrored:</p>
<pre>                        // uncle is on the left<br/>                        let mut parent = n.borrow().parent<br/>                                         .as_ref().unwrap().clone();<br/>                        if uncle.is_some()<br/>                            &amp;&amp; uncle.as_ref().unwrap().borrow()<br/>                               .color == Color::Red<br/>                        {<br/>                            let uncle = uncle.unwrap();<br/>                            parent.borrow_mut().color = Color::Black;<br/>                            uncle.borrow_mut().color = Color::Black;<br/>                            parent.borrow().parent.as_ref()<br/>                              .unwrap().borrow_mut().color =<br/>                                                        Color::Red;<br/><br/>                            n = parent.borrow().parent.as_ref()<br/>                                 .unwrap().clone();<br/>                        } else {<br/>                            if self.check(&amp;parent.borrow().dev, <br/>                                          &amp;n.borrow().dev)<br/>                                            == RBOperation::LeftNode<br/>                            {<br/>                                // do only if it's a right child<br/>                                let tmp = n.borrow().parent.as_ref()<br/>                                          .unwrap().clone();<br/>                                n = tmp;<br/>                                self.rotate(n.clone(), <br/>                                Rotation::Right);<br/>                                parent = n.borrow().parent.as_ref()<br/>                                         .unwrap().clone();<br/>                            }<br/>                            // until here. then for all black uncles<br/>                            parent.borrow_mut().color = Color::Black;<br/>                            parent.borrow().parent.as_ref()<br/>                              .unwrap().borrow_mut().color =<br/>                                                     Color::Red;<br/>                            let grandparent = n<br/>                                .borrow()<br/>                                .parent<br/>                                .as_ref()<br/>                                .unwrap()<br/>                                .borrow()<br/>                                .parent<br/>                                .as_ref()<br/>                                .unwrap()<br/>                                .clone();<br/>                            self.rotate(grandparent, Rotation::Left);<br/>                        }</pre>
<p>This code contains a large amount of <kbd>unwrap()</kbd>, <kbd>clone()</kbd>, and <kbd>borrow()</kbd> instances, a consequence of the interior mutability pattern. In this case, macros could help to reduce the code's verbosity.</p>
<p>Once the operations for one part of the tree finishes, the next iteration is prepared by checking for a red-red violation to see whether the loop needs to continue.</p>
<p>After the main loop exits, the pointer to the current node is moved up the tree to the root node (which is the function's return value, after all) and colored black. Why? This is a shortcut solution that would otherwise result in another iteration requiring many more expensive steps to be executed, and the rules of a red-black tree mandate a black root anyway:</p>
<pre>            not_root = n.borrow().parent.is_some();<br/>            if not_root {<br/>                parent_is_red = self.parent_color(&amp;n) == Color::Red;<br/>            }<br/>        }<br/>        while n.borrow().parent.is_some() {<br/>            let t = n.borrow().parent.as_ref().unwrap().clone();<br/>            n = t;<br/>        }<br/>        Some(n)<br/>    } else {<br/>        Some(inserted)<br/>    };<br/>    root.map(|r| {<br/>        r.borrow_mut().color = Color::Black;<br/>        r<br/>    })</pre>
<p>With that shortcut, a valid tree is returned that can be set as the new root. However, the main purpose of the tree is to find stuff, which is not that different from a regular binary search tree.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Finding the right one, now</h1>
                
            
            
                
<p>This piece of code can almost be reused from the binary search tree. Other than the <kbd>borrow()</kbd> calls (instead of a simple dereference or <kbd>*</kbd> operator) adding some amount of processing time, they provides a consistent search speed. For greater reuse of existing functions, the value to be found is wrapped into a dummy node. This way, no additional interface has to be created for comparing nodes:</p>
<pre>pub fn find(&amp;self, numerical_id: u64) -&gt; Option&lt;IoTDevice&gt; {<br/>    self.find_r(<br/>        &amp;self.root,<br/>        &amp;IoTDevice::new(numerical_id, "".to_owned(), "".to_owned()),<br/>    )<br/>}<br/><br/>fn find_r(&amp;self, node: &amp;Tree, dev: &amp;IoTDevice) -&gt; Option&lt;IoTDevice&gt; {<br/>    match node {<br/>        Some(n) =&gt; {<br/>            let n = n.borrow();<br/>            if n.dev.numerical_id == dev.numerical_id {<br/>                Some(n.dev.clone())<br/>            } else {<br/>                match self.check(&amp;n.dev, &amp;dev) {<br/>                    RBOperation::LeftNode =&gt; self.find_r(&amp;n.left, dev),<br/>                    RBOperation::RightNode =&gt; self.find_r(&amp;n.right, dev),<br/>                }<br/>            }<br/>        }<br/>        _ =&gt; None,<br/>    }<br/>}</pre>
<p>This is, again, a recursive walk of the tree until the specified value is found. Additionally, the "regular" tree walk was also added to the red-black tree variant:</p>
<pre>pub fn walk(&amp;self, callback: impl Fn(&amp;IoTDevice) -&gt; ()) {<br/>    self.walk_in_order(&amp;self.root, &amp;callback);<br/>}<br/><br/>fn walk_in_order(&amp;self, node: &amp;Tree, callback: &amp;impl Fn(&amp;IoTDevice) -&gt; ()) {<br/>    if let Some(n) = node {<br/>        let n = n.borrow();<br/><br/>        self.walk_in_order(&amp;n.left, callback);<br/>        callback(&amp;n.dev);<br/>        self.walk_in_order(&amp;n.right, callback);<br/>    }<br/>}</pre>
<p>With these parts fixed, the platform performs consistently fast!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Wrap up</h1>
                
            
            
                
<p class="mce-root">Red-black trees are great self-balancing binary trees, similar to <strong>AVL</strong> (short for <strong>Adelson-Velsky and L</strong><strong>andis</strong>) trees. Both appeared around the same time, yet AVL trees are considered to be superior thanks to a lower height difference between the branches. Regardless of which tree structure is used, both are significantly faster than their less complex sibling, the binary search tree. Benchmarks using sorted data on insert (100,000 elements in this case) show how significant the difference between a balanced and unbalanced tree is:</p>
<pre><strong>test tests::bench_sorted_insert_bst_find ... bench: 370,185 ns/iter (+/- 265,997)</strong><br/><strong>test tests::bench_sorted_insert_rbt_find ... bench: 900 ns/iter (+/- 423)</strong></pre>
<p>Another variation of a balanced tree is the 2-3-4 tree, a data structure that the red-black tree can be converted into. However, the 2-3-4 tree is, like the B-Tree (coming up later in this chapter), non-binary. Therefore, it is briefly discussed later in this chapter, but we encourage you to find other sources for details.</p>
<p>One major upside to implementing a red-black tree in Rust is the deep understanding of borrowing and ownership that follows the reference juggling when rotating, or "unpacking", a node's grandfather. It is highly recommended as a programming exercise to implement your own version!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Upsides</h1>
                
            
            
                
<p>A red-black tree has a few desirable properties over a regular binary search tree:</p>
<ul>
<li>Balance makes searches consistently fast</li>
<li>Predictable, low-memory usage</li>
<li>Inserts are reasonably fast</li>
<li>Simplicity of a binary tree</li>
<li>Easy to validate</li>
</ul>
<p>However, the data structure has some significant downsides as well, especially when planning to implement it!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Downsides</h1>
                
            
            
                
<p>Speed is great, but can your implementation achieve it? Let's have a look at the downsides of red-black trees:</p>
<ul>
<li>Complex implementation, especially in Rust</li>
<li>Concurrent writes require the entire tree to be locked</li>
<li>Performance is great compared to binary search trees, but other trees perform better at the same complexity</li>
<li>Skip lists (from the previous chapter) perform similarly with better concurrency and simpler implementations</li>
</ul>
<p>In any case, the red-black tree is a great journey into sophisticated binary tree structures. A more exotic binary tree structure is the heap (not to be confused with the portion of main memory).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Heaps</h1>
                
            
            
                
<p>Since binary trees are the most basic forms of trees, there are several variations designed for a specific purpose. Where the red-black tree is an advanced version of the initial tree, the binary heap is a version of the binary tree that does not facilitate search.</p>
<p>In fact, it has a specified purpose: finding the maximum or minimum value of a node. These heaps (min-heap or max-heap) are built in a way that the root node is always the value with the desired property (min or max) so it can be retrieved in constant time—that is, it always takes the same number of operations to fetch. Once fetched, the tree is restored in a way that the next operation works the same. How is this done though?</p>
<p>Heaps work, irrespective of whether they are min-heaps or max-heaps, because a node's children always have the same property as the entire tree. In a max-heap, this means that the root node is the maximum value of the sequence, so it has to be the greatest value of its children (it's the same with min-heaps, just in reverse). While there is no specific order to this (such as the left node being greater than the right node), there is a convention to prefer the right node for max-heaps and the left for min-heaps.</p>
<p>Upon inserting a new node, it is added last and then a place in the tree has to be determined. The strategy to do that is simple: look at the parent node; if it's greater (in a max-heap), swap the two, and repeat until this doesn't work or it becomes the root node. We call this operation <strong>upheap</strong>.</p>
<p>Similarly, this is how removals work. Once removed, the now-empty slot is replaced by a leaf of the tree—which is either the smallest (max-heap) or greatest (min-heap) value. Then, the same comparisons as with the insert are implemented, but in reverse. Comparing and swapping this node with the children restores the heap's properties and is called <strong>downheap</strong>.</p>
<p>If you paid attention to a node's journey, there is one detail that will be obvious to you: the tree is always "filled". This means that each level is fully populated (that is, every node has both children), making it a <strong>complete binary tree</strong> that maintains total order. This is a property that lets us implement this tree in an array (dynamic or not), making jumps cheap. It will all become clear once you see some diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/e2060e60-5458-46bd-b9e7-9634b47f8c98.png" style="width:20.08em;height:12.25em;"/></p>
<p>Commonly, the heap is used to create a priority queue of some kind, thanks to the ability to quickly retrieve the highest- or lowest-valued items. A very basic heap can be implemented in Rust as an array, which will provide everything necessary to make it work, but won't be as convenient as a <kbd>Vec</kbd>.</p>
<p>After the great success of the IoT device platform, an add-on has been planned. The product team is asking for a way to efficiently process messages that come from the devices, so that customers only have to deal with the actual handling of the message and skip the "plumbing" code. Since processing can be executed at (short) intervals, they require a way to order them quickly—ideally so that the device with the most messages can come first.</p>
<p>This sounds like the heap data structure, doesn't it? In fact, it can be a max-heap.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">A huge inbox</h1>
                
            
            
                
<p>Typically, heaps are used as priority queues of all kinds. Queues like that exist in any resource-constrained environment (and everywhere else, probably), but their purpose is to output things in an ordered fashion. By using the number of messages to determine the priority of a message notification, the heap can do the heavy lifting of this feature. Before jumping into the hard stuff, though, here are the bits containing the information:</p>
<div><div><pre>#[derive(Clone, Debug)]<br/>pub struct MessageNotification {<br/>    pub no_messages: u64,<br/>    pub device: IoTDevice,<br/>}</pre></div>
</div>
<p>The idea is to use the number of messages as an indicator of which device to poll first, which is why the device is required. Using this type, the heap does not require any specific node or link types to work:</p>
<pre>pub struct MessageChecker {<br/>    pub length: usize,<br/>    heap: Vec&lt;Box&lt;MessageNotification&gt;&gt;,<br/>}</pre>
<p>There are two interesting points here: the underlying structure is a regular <kbd>Vec&lt;T&gt;</kbd>, which was chosen for its expansion capabilities (Rust's arrays are sized at compile time), and the functionality of <kbd>push</kbd> or <kbd>pop</kbd>.</p>
<p>Another noteworthy modification is that no <kbd>Option</kbd> is needed, which removes a check from the code and makes it easier to read. However, since many of the heap's operations work well with a direct, 1-index-based access, indices have to be translated before hitting <kbd>Vec&lt;T&gt;</kbd>.</p>
<p>So how does data get in?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting messages in</h1>
                
            
            
                
<p>Once a message arrives, it is pushed to the back of the array when the upheap operation "bubbles up" the item until it finds its proper place. In Rust code, this is what that looks like:</p>
<div><pre>pub fn add(&amp;mut self, notification: MessageNotification) {<br/>    self.heap.push(Box::new(notification));<br/>    self.length = self.heap.len();<br/>    if self.length &gt; 1 {<br/>        let mut i = self.length;<br/>        while i / 2 &gt; 0 &amp;&amp; self.has_more_messages(i, i / 2) {<br/>            self.swap(i, i / 2);<br/>            i /= 2;<br/>        }<br/>    }<br/>}</pre></div>
<p>Initially, the new notification lives in a <kbd>Box</kbd> at the back of the <kbd>Vec&lt;T&gt;</kbd>, inserted via <kbd>push()</kbd>. A simple <kbd>while</kbd> loop then bubbles up the new addition by repeatedly swapping it whenever the <kbd>has_more_messages()</kbd> function is true. When is it true? Let's see the code:</p>
<div><pre>fn has_more_messages(&amp;self, pos1: usize, pos2: usize) -&gt; bool {<br/>    let a = &amp;self.heap[pos1 - 1];<br/>    let b = &amp;self.heap[pos2 - 1];<br/>    a.no_messages &gt;= b.no_messages<br/>}</pre></div>
<p>By encapsulating this function, it's easily possible to change the heap into a min-heap should that be required—and the index translations are wrapped away here as well.</p>
<p>Getting data out requires doing this process in reverse in a function called <kbd>pop()</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Taking messages out</h1>
                
            
            
                
<p>Removing the first item in a <kbd>Vec&lt;T&gt;</kbd> is not difficult—in fact, <kbd>Vec&lt;T&gt;</kbd> ships with a <kbd>swap_remove()</kbd> function that does exactly what a heap needs: removing the first element of a <kbd>Vec&lt;T&gt;</kbd> by replacing it with the last element! This makes the code significantly shorter and therefore easier to reason about:</p>
<div><pre>pub fn pop(&amp;mut self) -&gt; Option&lt;MessageNotification&gt; {<br/>    if self.length &gt; 0 {<br/>        let elem = self.heap.swap_remove(0);<br/>        self.length = self.heap.len();<br/>        let mut i = 1;<br/>        while i * 2 &lt; self.length {<br/>            let children = (i * 2, i * 2 + 1);<br/>            i = if self.has_more_messages(children.0, children.1) {<br/>                if self.has_more_messages(children.0, i) {<br/>                    self.swap(i, children.0);<br/>                    children.0<br/>                } else {<br/>                    break;<br/>                }<br/>            } else {<br/>                if self.has_more_messages(children.1, i) {<br/>                self.swap(i, children.1);<br/>                children.1<br/>                } else {<br/>                    break;<br/>                }    <br/>            }<br/>        }<br/>        Some(*elem)<br/>    } else {<br/>        None<br/>    }<br/>}</pre></div>
<p>Obviously, this code is not short though—so what's amiss? The bubbling down. Swapping downward requires to look at the children (which are at the positions <kbd>i * 2</kbd> and <kbd>i * 2 + 1</kbd>) to find out where (or if) the next iteration should proceed.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Wrap up</h1>
                
            
            
                
<p>The heap data structure is surprisingly simple to implement. There are no lengthy unwraps, borrows, or other calls, and the pointer is owned by the <kbd>Vec</kbd> and can easily be swapped. Other than that, the upheap operation is only a <kbd>while</kbd> loop, just like the (slightly more complex) downheap function.</p>
<p>There is another typical use case for a heap though: sorting! Consider a bunch of numbers going into the heap instead of <kbd>MessageNotification</kbd> objects—they would come out sorted. Thanks to the efficiency of the upheap/downheap operations, the worst-case runtime of that sorting algorithm is great—but more on that in <a href="a9ba9f9e-59a2-411f-8998-831fe4e69266.xhtml">Chapter 9</a>, <em>Ordering Things</em>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Upsides</h1>
                
            
            
                
<p>Compact and low-complexity implementation make the binary heap a great candidate for requiring any kind of sorting data structure. Other benefits include the following:</p>
<ul>
<li>An efficient way to sort lists</li>
<li>Works well in concurrent situations</li>
<li>A very efficient way to store a sorted array</li>
</ul>
<p>Yet there are also downsides.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Downsides</h1>
                
            
            
                
<p>Heaps are generally great, but have two caveats that limit their use:</p>
<ul>
<li>Use cases outside of queuing or sorting are rare</li>
<li>There are better ways to sort</li>
</ul>
<p>The binary heap was the last of the binary trees, and the next section will cover another rather exotic variation of a tree: the trie.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Trie</h1>
                
            
            
                
<p>The trie is another interesting data structure—in particular, the way in which it is pronounced! Depending on your mother tongue, intuition might dictate a way, but—according to Wikipedia—the name was selected thanks to Edward Fredkin, who pronounced this type of tree differently, namely like <strong>trie</strong> in re<strong>trie</strong>val. Many English speakers resort to saying something along the lines of "try" though.</p>
<p>With that out of the way, what does the trie actually do for it to deserve a different name? It transpires that using retrieval was not a bad idea: tries store strings.</p>
<p>Imagine having to store the entire vocabulary of this book in a way to find out whether certain words are contained within the book. How can this be done efficiently?</p>
<p>After the previous sections, you should already have an answer, but if you think about strings—they are stored as arrays or lists of <kbd>char</kbd> instances—it would use a good amount of memory. Since each word has to use letters from the English alphabet, can't we use that?</p>
<p>Tries do something similar. They use characters as nodes in a tree where the parent node is the preceding character and all children (limited only by the size of the alphabet) are what follows. A trie storing the strings ABB, ABC, CAACB, CAACA, BBB, and BBA can be seen in the following trie diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/91d88689-25be-4760-b986-aeb4ebe7937f.png" style="width:23.00em;height:18.33em;"/></p>
<p>Storing strings like this enables a very efficient search. You only have to walk through the letters in the key that is to be stored to find out (or store) whether that string is contained in—for example—a set. In fact, if a string can only have a certain size, then the retrieval time is constant and it does not matter whether the trie stores 10 or 10 million words. Typically, this is useful for set data structures or key-value stores with string keys (such as hashes, but more on that later). Just like the binary search tree, this structure has a strong hierarchical memory management (that is, no pointers "back up"), making it a perfect fit for Rust.</p>
<p>Lately, the product team has looked into the user's device keys once again and found that the typical IoT device uses keys that represent a path, and they would often look like <kbd>countryA/cityB/factoryC/machine1/positionX/sensorY</kbd>. Reminded of the trees that worked so well earlier, they thought that you could use those to improve the directory as well. But you already have a better idea!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">More realistic IoT device management</h1>
                
            
            
                
<p>Paths like that tend to have a huge overlap, since there are countless sensors and devices in a single location. Additionally, they are unique thanks to the hierarchical properties and are human-readable in case the sensor needs to be found. A great fit for a trie!</p>
<p>The basis for this trie will be a node type that stores the children, current character, and, if it's a node that concludes a full key, the <kbd>IoTDevice</kbd> object from earlier in this chapter. This is what this looks like in Rust:</p>
<pre>struct Node {<br/>    pub key: char,<br/>    next: HashMap&lt;char, Link&gt;,<br/>    pub value: Option&lt;IoTDevice&gt;,<br/>}</pre>
<p>This time, the children is a different data structure as well: a <kbd>HashMap</kbd>. Maps (also called dictionaries, associative arrays) explicitly store a key alongside a value and the word "hash" hints at the method, which will be discussed in the next chapter. For now, the <kbd>HashMap</kbd> guarantees a single character to be associated with a Node type, leading the way for iteration. On top of that, this data structure allows for a get-or-add type operation, which significantly improves code readability.</p>
<p>Since the number of possible word beginnings is similar, the root is a <kbd>HashMap</kbd> as well, giving the trie multiple roots:</p>
<pre>pub struct BestDeviceRegistry {<br/>    pub length: u64,<br/>    root: HashMap&lt;char, Link&gt;,<br/>}</pre>
<p>In order to fill up these maps with data, a method to add paths is required.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Adding paths</h1>
                
            
            
                
<p>The algorithm for inserting a string into a trie can be described in only a few sentences: go through each character of the word and trace it down the trie. If a node does not yet exist, create it, and add the object with the last entry.</p>
<p>Of course, there are special cases that need to be decided as well: what happens when a string already exists? Overwrite or ignore? In the case of this implementation, the last write will win—that is, it's overwriting whatever existed previously:</p>
<div><pre>pub fn add(&amp;mut self, device: IoTDevice) {<br/>    let p = device.path.clone();<br/>    let mut path = p.chars();<br/>    if let Some(start) = path.next() {<br/>        self.length += 1;<br/>        let mut n = self.root<br/>                .entry(start)<br/>                .or_insert(Node::new(start, None));<br/>        for c in path {<br/>            let tmp = n.next<br/>                    .entry(c)<br/>                    .or_insert(Node::new(c, None));<br/>            n = tmp;<br/>        }<br/>        n.value = Some(device);<br/>    }<br/>}</pre></div>
<p>Another special case is the root node, since it's not a real node but a <kbd>HashMap</kbd> right away. Once a trie is set up, the most important thing is to get stuff out again!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Walking</h1>
                
            
            
                
<p>Add and search work in a very similar manner: follow the links to the characters of the key and return the "value" in the end:</p>
<pre>pub fn find(&amp;mut self, path: &amp;str) -&gt; Option&lt;IoTDevice&gt; {<br/>    let mut path = path.chars();<br/>    if let Some(start) = path.next() {<br/>        self.root.get(&amp;start).map_or(None, |mut n| {<br/>            for c in path {<br/>                match n.next.get(&amp;c) {<br/>                    Some(ref tmp) =&gt; n = tmp,<br/>                    None =&gt; break,<br/>                }<br/>            }    <br/>            n.value.clone()<br/>        })<br/>    } else {<br/>        None<br/>    }<br/>}</pre>
<p>Since the trie does not store strings in any particular order (or even consistently), getting the same data out in a predictable way is tricky! Walking it like a binary tree works well enough, but will only be deterministic with respect to the insertion order, something that should be kept in mind when testing the implementation:</p>
<pre>pub fn walk(&amp;self, callback: impl Fn(&amp;IoTDevice) -&gt; ()) {<br/>    for r in self.root.values() {<br/>        self.walk_r(&amp;r, &amp;callback);<br/>    }<br/>}<br/><br/>fn walk_r(&amp;self, node: &amp;Link, callback: &amp;impl Fn(&amp;IoTDevice) -&gt; ()) {<br/>    for n in node.next.values() {<br/>        self.walk_r(&amp;n, callback);<br/>    }<br/>    if let Some(ref dev) = node.value {<br/>        callback(dev);<br/>    }<br/>}</pre>
<p>As previously mentioned, this walk is called a breadth-first traversal.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Wrap up</h1>
                
            
            
                
<p>The trie data structure is a very efficient way of storing and finding strings by storing common prefixes, and they are often used in practice. One use case is the popular Java search engine Lucene, which uses this structure to store words in the search index, but there are plenty of other examples across different fields. Additionally, the simplicity is great for implementing a custom trie to store entire words or other objects instead of characters.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Upsides</h1>
                
            
            
                
<p>The inherent prefix is great for efficient storage and, apart from that, there are the following benefits:</p>
<ul>
<li>Easy implementation facilitates customizing</li>
<li>Minimal memory requirements for sets of strings</li>
<li>Constant-time retrieval for strings with a known maximum length</li>
<li>Exotic algorithms are available (for example, Burst Sort)</li>
</ul>
<p>While the trie is great, it is also fairly simple, which comes with a number of downsides.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Downsides</h1>
                
            
            
                
<p>Tries can work in a lot of shapes and forms, but can't handle every use case, unfortunately. Other disadvantages include the following:</p>
<ul>
<li>It has a name that's strange to pronounce</li>
<li>There is no deterministic order on walking</li>
<li>There are no duplicate keys</li>
</ul>
<p>This concludes the more exotic tree varieties. Next up is the B-Tree, which is essentially a universal tree!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">B-Tree</h1>
                
            
            
                
<p>As you have noticed, restricting the number of children to 2 (like the binary trees earlier) yields a tree that only lets the algorithm decide whether to go left or right, and it's easily hardcoded. Additionally, storing only a single key-value pair in a node can be seen as a waste of space—after all, the pointers can be a lot larger than the actual payload!</p>
<p>B-Trees generally store multiple keys and values per node, which can make them more space-efficient (the payload-to-pointer ratio is higher). As a tree, each of these (key-value) pairs has children, which hold the values between the nodes they are located at. Therefore, a B-Tree stores triples of key, value, and child, with an additional child pointer to cover any "other" values. The following diagram shows a simple B-Tree. Note the additional pointer to a node holding smaller keys:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/8c99ddbf-3f3d-4424-9b49-6c6aed5f7d5c.png" style="width:16.67em;height:6.33em;"/></p>
<p>As depicted here, a B-Tree can have varying amounts of those key-value pairs (only the keys are visible), but they will have a maximum number of children—defined by the <em>order</em> parameter. Consequently, a binary search tree can be considered an order-2 B-Tree, without the added benefit of being self-balancing.</p>
<p>In order to achieve the self-balancing nature, a B-Tree has certain properties (as defined by Donald Knuth):</p>
<ol>
<li>Each node can only have <em>order</em> children</li>
<li>Each node that is not a leaf node or root has at least <em>order/2</em> children</li>
<li>The root node has at least two children</li>
<li>All nodes hold <em>order - 1</em> keys when they have <em>order</em> children</li>
<li>All leaf nodes appear on the same level</li>
</ol>
<p>How does self-balancing work? It is way simpler than a red-black tree. Firstly, new keys can only be inserted at the leaf level. Secondly, once the new key has found a node, the node is evaluated to the preceding rules—in particular, if there are now more than <em>order - 1</em> keys. If that is the case, the node has to be split, moving the center key to the parent node, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/5bc64dd2-6e9e-48d8-a778-49eef819928d.png" style="width:17.17em;height:11.83em;"/></p>
<p>Next, the children are put in their intended position (especially important if the elevated node had children) and then the process is repeated up the tree until the root node is valid.</p>
<p>This process creates something that is called a <strong>fat tree</strong> (as opposed to a high tree), which means that adding height is only possible through splitting, which doesn't happen very often. In order to work with the nodes, they contain additional information about themselves:</p>
<pre>type Tree = Box&lt;Node&gt;;<br/><br/>#[derive(Clone, PartialEq, Debug)]<br/>enum NodeType {<br/>    Leaf,<br/>    Regular,<br/>}<br/><br/>#[derive(Clone)]<br/>struct Node {<br/>    keys: Vec&lt;Option&lt;(u64, String, Option&lt;Tree&gt;)&gt;&gt;,<br/>    left_child: Option&lt;Tree&gt;,<br/>    pub node_type: NodeType,<br/>}</pre>
<p>In this case, the type of node is determined by a property, <kbd>node_type</kbd>, but the entire node could be wrapped into an enumeration as well. Furthermore, a special variable holding the "left child" has been attached in order to deal with keys lower than what is associated with the triples in the <kbd>keys</kbd> vector.</p>
<p>Like binary trees, the B-Tree exhibits logarithmic runtime complexity on search and insert (<em>O(log2(n))</em>) and, with the the simplified rebalancing, they make for a great choice for database indices. In fact, many SQL databases (such as SQLite and SQL Server) use B-Trees to store those search indices, and B+ Trees to store tables thanks to their smart ways of accessing the disk.</p>
<p>The product team has also heard about this and, since the previous attempts at the IoT device management solution have been a huge success, they thought about replacing the red-black tree with something better! They want to reduce the number of bugs by creating a more simplified version of the original database, so the requirements actually stay the same.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">An IoT database</h1>
                
            
            
                
<p>As in the previous implementation, this tree builds on the <kbd>numerical_id</kbd> property of <kbd>IoTDevice</kbd> as keys, and the device object as value. In code, a node looks very similar to the previous example:</p>
<pre>type Tree = Box&lt;Node&gt;;<br/>type KeyType = u64;<br/><br/>type Data = (Option&lt;IoTDevice&gt;, Option&lt;Tree&gt;);<br/><br/>#[derive(Clone, PartialEq, Debug)]<br/>enum NodeType {<br/>    Leaf,<br/>    Regular,<br/>}<br/><br/>#[derive(Clone, PartialEq)]<br/>enum Direction {<br/>    Left,<br/>    Right(usize),<br/>}<br/><br/>#[derive(Clone)]<br/>struct Node {<br/>    devices: Vec&lt;Option&lt;IoTDevice&gt;&gt;,<br/>    children: Vec&lt;Option&lt;Tree&gt;&gt;,<br/>    left_child: Option&lt;Tree&gt;,<br/>    pub node_type: NodeType,<br/>}</pre>
<p>Instead of triples, this node type uses a synchronized index to find the children associated with a specified key-value pair. These pairs are also created ad hoc by evaluating the <kbd>numerical_id</kbd> property of the contained device, thereby also simplifying the code and eventual updates to the keys. Something that is missing from the node is a parent pointer, which made the entire red-black tree code significantly more complex.</p>
<p>The tree itself is stored as an <kbd>Option</kbd> on a boxed node (aliased as <kbd>Tree</kbd>), along with the <kbd>order</kbd> and <kbd>length</kbd> properties:</p>
<pre>pub struct DeviceDatabase {<br/>    root: Option&lt;Tree&gt;,<br/>    order: usize,<br/>    pub length: u64,<br/>}</pre>
<p>Finally, to check the validity of the tree, here's a <kbd>validate</kbd> method that recursively finds the minimum and maximum leaf height and checks whether the number of children is within bounds (as mentioned in the rules indicated earlier):</p>
<pre>pub fn is_a_valid_btree(&amp;self) -&gt; bool {<br/>    if let Some(tree) = self.root.as_ref() {<br/>        let total = self.validate(tree, 0);<br/>        total.0 &amp;&amp; total.1 == total.2<br/>    } else {<br/>        false // there is no tree<br/>    }<br/>}<br/><br/>fn validate(&amp;self, node: &amp;Tree, level: usize) -&gt; (bool, usize, usize) {<br/>    match node.node_type {<br/>        NodeType::Leaf =&gt; (node.len() &lt;= self.order, level, level),<br/>        NodeType::Regular =&gt; {<br/>            // Root node only requires two children, <br/>            //  every other node at least half the<br/>            // order<br/>            let min_children = if level &gt; 0 { <br/>                self.order / 2usize } else { 2 };<br/>            let key_rules = node.len() &lt;= self.order &amp;&amp; <br/>                 node.len() &gt;= min_children;<br/><br/>            let mut total = (key_rules, usize::max_value(), level);<br/>            for n in node.children.iter().chain(vec![&amp;node.left_child]) {<br/>                if let Some(ref tree) = n {<br/>                    let stats = self.validate(tree, level + 1);<br/>                    total = (<br/>                        total.0 &amp;&amp; stats.0,<br/>                        cmp::min(stats.1, total.1),<br/>                        cmp::max(stats.2, total.2),<br/>                    );<br/>                }<br/>            }<br/>            total<br/>        }<br/>    }<br/>}</pre>
<p>Having established these basic structures, we can move on to how to add new devices to the tree.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Adding stuff</h1>
                
            
            
                
<p class="mce-root">B-Trees add new entries to their leaves, which then bubble up as nodes grow too large. In order to efficiently find a spot, this is done recursively, removing and replacing ownership as needed. Here is the <kbd>add()</kbd> function, which takes care of retrieving ownership of the root node and calling the recursive call with an existing or new node:</p>
<pre>type Data = (Option&lt;IoTDevice&gt;, Option&lt;Tree&gt;);<br/><br/>pub fn add(&amp;mut self, device: IoTDevice) {<br/>    let node = if self.root.is_some() {<br/>        mem::replace(&amp;mut self.root, None).unwrap()<br/>    } else {<br/>        Node::new_leaf()<br/>    };<br/><br/>    let (root, _) = self.add_r(node, device, true);<br/>    self.root = Some(root);<br/>}</pre>
<p>Except in the case of the root node, the <kbd>add_r()</kbd> function (the recursive call) returns two pieces of information: the key it descended into and—in case of a "promotion"—the device and child that are to be added to whichever node it returns to. In principle, this function works as follows:</p>
<ol>
<li>Recursively find the appropriate leaf and perform a sorted insert.</li>
<li>Increment the length if it's not a duplicate.</li>
<li>If the node now has more keys than are allowed: split.</li>
<li>Return the original node and the key with its new value to the caller.</li>
<li>Place the new node where it came from.</li>
<li>Add the promoted key.</li>
<li>Repeat from step 3 until at the root level:</li>
</ol>
<pre style="padding-left: 60px"><br/>fn add_r(&amp;mut self, node: Tree, device: IoTDevice, is_root: bool) -&gt; (Tree, Option&lt;Data&gt;) {<br/>    let mut node = node;<br/>    let id = device.numerical_id;<br/><br/>    match node.node_type {<br/>        NodeType::Leaf =&gt; {                 // 1<br/>            if node.add_key(id, (Some(device), None)) {<br/>                self.length += 1;           // 2<br/>            }<br/>        }<br/>        NodeType::Regular =&gt; {<br/>            let (key, (dev, tree)) = node.remove_key(id).unwrap();<br/>            let new = self.add_r(tree.unwrap(), device, false);<br/>            if dev.is_none() {              // 5<br/>                node.add_left_child(Some(new.0));<br/>            } else {<br/>                node.add_key(key, (dev, Some(new.0)));<br/>            }<br/>                                            // 6<br/>            if let Some(split_result) = new.1 {<br/>                let new_id = &amp;split_result.0.clone().unwrap();<br/>                node.add_key(new_id.numerical_id, split_result);<br/>            }<br/>        }<br/>    }<br/><br/>    if node.len() &gt; self.order {             // 3<br/>        let (new_parent, sibling) = node.split();<br/><br/>        // Check if the root node is "full" and add a new level<br/>        if is_root {<br/>            let mut parent = Node::new_regular();<br/>            // Add the former root to the left<br/>            parent.add_left_child(Some(node));<br/>            // Add the new right part as well<br/>            parent.add_key(new_parent.numerical_id, <br/>                           (Some(new_parent), Some(sibling)));<br/>            (parent, None)<br/>        } else {<br/>                                            // 4<br/>            (node, Some((Some(new_parent), Some(sibling))))<br/>        }<br/>    } else {<br/>        (node, None)<br/>    }<br/>}</pre>
<p>Since the root node is a special case where a new level is added to the tree, this has to be taken care of where the last split is happening—in the <kbd>add_r()</kbd> function. This is as simple as creating a new non-leaf node and adding the former root to the left and its sibling to the right, placing the new parent on top as the root node.</p>
<p>In this implementation, a lot of the heavy lifting is done by the node's implementation of several functions, including <kbd>split()</kbd>. While this is complex, it encapsulates the inner workings of the tree—something that should not be exposed too much so as to facilitate change:</p>
<pre>pub fn split(&amp;mut self) -&gt; (IoTDevice, Tree) {<br/>    let mut sibling = Node::new(self.node_type.clone());<br/><br/>    let no_of_devices = self.devices.len();<br/>    let split_at = no_of_devices / 2usize;<br/><br/>    let dev = self.devices.remove(split_at);<br/>    let node = self.children.remove(split_at);<br/><br/>    for _ in split_at..self.devices.len() {<br/>        let device = self.devices.pop().unwrap();<br/>        let child = self.children.pop().unwrap();<br/>        sibling.add_key(device.as_ref().unwrap()<br/>                        .numerical_id, (device, child));<br/>    }<br/><br/>    sibling.add_left_child(node);<br/>    (dev.unwrap(), sibling)<br/>}</pre>
<p>As described previously, splitting yields a new sibling to the original node and a new parent to both of them. The sibling will receive the upper half of the keys, the original node remains with the lower half, and the one in the center becomes the new parent.</p>
<p>Having added several devices, let's talk about how to get them back out.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Searching for stuff</h1>
                
            
            
                
<p>A B-Tree's search works just the way binary tree searches do: recursively checking each node for the path to follow. In B-Trees, this becomes very convenient since it can be done in a loop, in this case, by the <kbd>get_device()</kbd> function:</p>
<pre>pub fn get_device(&amp;self, key: KeyType) -&gt; Option&lt;&amp;IoTDevice&gt; {<br/>    let mut result = None;<br/>    for d in self.devices.iter() {<br/>        if let Some(device) = d {<br/>            if device.numerical_id == key {<br/>                result = Some(device);<br/>                break;<br/>            }<br/>        }<br/>    }<br/>    result<br/>}</pre>
<p>This function is implemented at the node structure and does a regular linear search for the key itself. If it is unable to find that key, the <kbd>find_r()</kbd> function has to decide whether to continue, which it does by evaluating the node type. Since leaf nodes don't have any children, not finding the desired key will end the search, returning <kbd>None</kbd>. Regular nodes allow the search to continue on a deeper level of the tree:</p>
<pre>pub fn find(&amp;self, id: KeyType) -&gt; Option&lt;IoTDevice&gt; {<br/>    match self.root.as_ref() {<br/>        Some(tree) =&gt; self.find_r(tree, id),<br/>        _ =&gt; None,<br/>    }<br/>}<br/><br/>fn find_r(&amp;self, node: &amp;Tree, id: KeyType) -&gt; Option&lt;IoTDevice&gt; {<br/>    match node.get_device(id) {<br/>        Some(device) =&gt; Some(device.clone()),<br/>        None if node.node_type != NodeType::Leaf =&gt; {<br/>            if let Some(tree) = node.get_child(id) {<br/>                self.find_r(tree, id)<br/>            } else {<br/>                None<br/>            }<br/>        }<br/>        _ =&gt; None,<br/>    }<br/>}</pre>
<p>Another method for finding something within the tree's values is walking the tree.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Walking the tree</h1>
                
            
            
                
<p>Similarly to the binary trees earlier in this chapter, walking can be done with different strategies, even if there are many more branches to walk. The following code shows an in-order tree walking algorithm, where the callback is executed between the left child and before descending into the child that is currently looked at:</p>
<pre>pub fn walk(&amp;self, callback: impl Fn(&amp;IoTDevice) -&gt; ()) {<br/>    if let Some(ref root) = self.root {<br/>        self.walk_in_order(root, &amp;callback);<br/>    }<br/>}<br/><br/>fn walk_in_order(&amp;self, node: &amp;Tree, callback: &amp;impl Fn(&amp;IoTDevice) -&gt; ()) {<br/>    if let Some(ref left) = node.left_child {<br/>        self.walk_in_order(left, callback);<br/>    }<br/><br/>    for i in 0..node.devices.len() {<br/>        if let Some(ref k) = node.devices[i] {<br/>            callback(k);<br/>        }<br/><br/>        if let Some(ref c) = node.children[i] {<br/>            self.walk_in_order(&amp;c, callback);<br/>        }<br/>    }<br/>}</pre>
<p>Thanks to the internal sorting, this walk retrieves the keys in an ascending order.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Wrap up</h1>
                
            
            
                
<p>B-Trees are awesome. They are widely used in real-world applications, their implementation in Rust is not all that complex, and they maintain a great performance regardless of insertion order. Furthermore, the tree's order can dramatically improve performance by decreasing the tree's height. It is recommended to estimate the number of key-value pairs beforehand and adjust the order accordingly.</p>
<p>As a benchmark, let's evaluate the trees by inserting 100,000 unsorted, unique elements, and retrieving them using <kbd>find()</kbd>. Dot size represents the variance, while the values shown along the <em>y</em> axis are nanoseconds:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/320a2f24-c980-4899-a5cb-27c43debf797.png" style="width:46.00em;height:55.08em;"/></p>
<p>The chart output of Unsorted find ()</p>
<p>Other than that, it performs at the level of other trees, with vastly fewer lines of code and less code complexity, both of which impact readability and maintainability for other developers.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Upsides</h1>
                
            
            
                
<p>This type of tree achieves great performance with the order parameter set accordingly:</p>
<ul>
<li>Less complex to implement than other self-balancing trees</li>
<li>Widely used in database technology</li>
<li>Predictable performance thanks to self-balancing</li>
<li>Range queries are possible</li>
<li>Variants that minimize disk access (B+ Tree)</li>
</ul>
<p>The tree's downsides are few.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Downsides</h1>
                
            
            
                
<p>Absolute performance depends significantly on the tree's order; other than that, this tree does not have many downsides.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Graphs</h1>
                
            
            
                
<p>In their most generic form, trees are graphs—directed, acyclic graphs. A general graph can be described as a collection of connected nodes, sometimes referred to as vertices, with certain properties such as whether cycles are allowed. The connections between those also have their own name: edges. These edges can have certain properties as well, in particular, weights and directions (like one-way streets).</p>
<p>By enforcing these constraints, a model can be built that, just like trees, reflects a certain reality very well. There is one particular thing that is typically represented as a weighted graph: the internet. While, nowadays, this might be an oversimplification, with various versions of the Internet Protocol (IPv4 and IPv6) and <strong>Network Address Translation</strong> (<strong>NAT</strong>) technologies hiding large numbers of participants online, in its earlier days, the internet could be drawn as a collection of routers, computers, and servers (nodes) interconnected with links (edges) defined by speed and latency (weights).</p>
<p>The following diagram shows a random, undirected, unweighted graph:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/d334d74d-a7fe-4d1b-9ae7-afeb995e1afd.png" style="width:15.50em;height:17.83em;"/></p>
<p>Other than humans, who can typically see and follow a reasonably efficient path through this mesh of interconnected nodes, computers require specific instructions to find anything in there! This called for new algorithms that allow for dealing with this complexity—which is especially tricky once the number of nodes in the mesh exceeds the number of nodes that can be looked at in time. This led to the development of many routing algorithms, techniques to finding cycles and segmenting the network, or popular NP-hard problems, such as the traveling salesman problem or the graph-coloring problem. The traveling salesman problem is defined as follows.</p>
<p>Find the optimal (shortest) path between cities without visiting one twice. On the left are some cities in Europe; on the right, two possible solutions (dotted versus solid lines):</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/6ab202ba-48ca-4742-a8eb-2c52e0348ebe.png" style="width:41.00em;height:16.75em;"/></p>
<p>Today, there are many examples of graphs, the most obvious being a social graph (in social networks), but also as part of TensorFlow's deep learning API, state machines, and the rise of graph databases that offer a generic query language to traverse graphs. Even some less obvious use cases can be found, such as storing genetic sequences (nodes being the small parts of the DNA)!</p>
<p>To get out of theoretical constructs, how would you represent a graph in a program <em>efficiently</em>? As a node structure with a list of outbound vertices? How would you find a particular node then? A tricky problem! Graphs also have the habit of growing quite large, as anyone who ever wanted to serialize object graphs to JSON can testify: they run out of memory quite easily.</p>
<p>The best way to work with this data structure is surprisingly simple: a matrix. This matrix can either be sparse (that is, a list of lists with varying sizes), called an <strong>adjacency list</strong>, or a full-blown matrix (adjacency matrix). Especially for a matrix, the size is typically the number of nodes on either side and the weights (or Boolean values representing "connected" or "not connected") at each crossing. Many implementations will also keep the "real" nodes in its own list, using the indices as IDs. The following diagram shows how to display a graph as a matrix:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/30d9f986-73f7-43d2-95b7-86c0f0d2fd54.png" style="width:33.58em;height:15.42em;"/></p>
<p>Rust provides many great tools for implementing really complex graph structures: enumerations and pattern-matching provide ways to operate on types of nodes and edges with low overhead, while iterators and functional approaches remove the need for verbose loops. Let's look at a generic graph structure in Rust:</p>
<pre class="language-">struct ASimpleGraph {
    adjacency_list: Vec&lt;Vec&lt;usize&gt;&gt;,
}</pre>
<p>This adjacency list can store nodes and whether they are connected, making this a finite, undirected, unweighted graph—great for storing simple relationships between objects. Already, a data structure such as this has the ability to implement sophisticated routing algorithms or run out of resources on a backtracking algorithm. In an adjacency list, each index in the list represents the origin of an edge and the contained elements (also lists) are any outbound edges. To traverse the graph, start at an origin index and find the next index by searching its edges. Then repeat until arriving at the destination node!</p>
<p>When the product team heard of this amazing data structure—and they are now well aware of your abilities—they came up with a new product: the literal Internet of Things (it's a working title). Their idea is to provide customers with a way to model complex sensor placements that would have distance built in! Customers can then go and evaluate all sensors that are within a certain range of each other, find single points of failure, or plan a route to inspect them quickly.</p>
<p>To summarize, customers should be able to do the following:</p>
<ul>
<li>Create or add a list of nodes</li>
<li>Connect nodes with their physical distance to each other</li>
<li>Find the shortest path between two nodes with respect to the distance provided</li>
<li>Retrieve a list of neighbors of a specified node, up to a certain degree</li>
</ul>
<p>Great idea, right? A great fit for graphs as well.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The literal Internet of Things</h1>
                
            
            
                
<p>In order to get a head start on these requirements, the decision for a graph representation has to be made: list or matrix? Both work well, but for explanatory reasons, the examples will go with an adjacency list built on top of a vector of vectors:</p>
<pre class="language-">pub struct InternetOfThings {<br/>    adjacency_list: Vec&lt;Vec&lt;Edge&gt;&gt;,<br/>    nodes: Vec&lt;KeyType&gt;,<br/>}</pre>
<p>As previously mentioned, it makes sense to keep the actual values, identifiers, or even entire objects in their own list and simply work with indices of the <kbd>usize</kbd> type. The edge structure in this example could be represented as a tuple just as well, but it's way more readable this way:</p>
<pre class="language-">#[derive(Clone, Debug)]<br/>struct Edge {<br/>   weight: u32,<br/>    node: usize,<br/>}</pre>
<p>Having those two structures in place, adding nodes (or... things) to the graph can be done with only a few lines:</p>
<pre class="language-">fn get_node_index(&amp;self, node: KeyType) -&gt; Option&lt;usize&gt; {<br/>    self.nodes.iter().position(|n| n == &amp;node)<br/>}<br/><br/>pub fn set_edges(&amp;mut self, from: KeyType, edges: Vec&lt;(u32, KeyType)&gt;) {<br/>    let edges: Vec&lt;Edge&gt; = edges.into_iter().filter_map(|e| {<br/>        if let Some(to) = self.get_node_index(e.1) {<br/>            Some(Edge { weight: e.0, node: to }) <br/>            } else {<br/>                None<br/>            }}).collect();<br/>    match self.nodes.iter().position(|n| n == &amp;from) {<br/>        Some(i) =&gt; self.adjacency_list[i] = edges,<br/>        None =&gt; {<br/>            self.nodes.push(from);<br/>            self.adjacency_list.push(edges)<br/>        }<br/>    }<br/>}</pre>
<p>Within that function, there is a crucial check that's made: every edge has to connect to a valid node, otherwise it will not be added to the graph. To achieve this, the code looks up the IDs provided in the <kbd>edges</kbd> parameter in its internal node storage to find the index it's at, something that is done by the <kbd>position()</kbd> function of Rust's iterator trait. It returns the position of when the provided predicate returns true! Similarly, the <kbd>filter_map()</kbd> function of the iterator will only include elements that evaluate to <kbd>Some()</kbd> (as opposed to <kbd>None</kbd>) in its result set. Therefore, the nodes have to have a setter that also initializes the adjacency list:</p>
<pre class="language-">pub fn set_nodes(&amp;mut self, nodes: Vec&lt;KeyType&gt;) {<br/>    self.nodes = nodes;<br/>    self.adjacency_list = vec![vec![]; self.nodes.len()]<br/>}</pre>
<p>Once that's done, the graph is ready to use. How about we go looking for neighbors first?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Neighborhood search</h1>
                
            
            
                
<p>Neighborhood search is a very trivial algorithm: starting from the node provided, follow every edge and return what you find. In our case, the degree of the relationship is important.</p>
<p>Just like for the tree algorithms shown previously, recursion is a great choice for solving this problem. While an iterative solution will often be more memory-efficient (no stack overflows), recursion is way more descriptive once you get the hang of it. Additionally, some compilers (and partly <kbd>rustc</kbd>, but not guaranteed) will expand the recursion into a loop, providing the best of both worlds (look for tail call optimization)! Obviously, the most important thing is to have a projected growth in mind; 100,000 recursive calls are likely to fill up the stack.</p>
<p>However, the function to run the neighborhood is implemented two-fold. First, the public-facing function takes care of validating input data and sees whether the node actually exists:</p>
<pre class="language-">pub fn connected(&amp;self, from: KeyType, degree: usize) -&gt; Option&lt;HashSet&lt;KeyType&gt;&gt; {<br/>    self.nodes.iter().position(|n| n == &amp;from).map(|i| {<br/>        self.connected_r(i, degree).into_iter().map(|n| <br/>        self.nodes[n].clone()).collect()<br/>    })<br/>}</pre>
<p>With that out of the way, the recursive call can create a list of all its neighbors and run the same call on each of them. Returning a set of nodes eliminates the duplicates as well:</p>
<pre>fn connected_r(&amp;self, from: usize, degree: usize) -&gt; HashSet&lt;usize&gt; {<br/>    if degree &gt; 0 {<br/>        self.adjacency_list[from]<br/>            .iter()<br/>            .flat_map(|e| {<br/>                let mut set = self.connected_r(e.node, degree - 1);<br/>                set.insert(e.node);<br/>                set<br/>            }).collect()<br/>    } else {<br/>        HashSet::new()<br/>    }<br/>}</pre>
<div><p>Since the recursive call returns the internal representation (that is, indices), the outer function translates those back into data the user can understand. This function can serve as a basis for other features, such as intersecting the neighborhoods of two nodes, and vicinity search. Or, to make it more real, on a sensor outage, the company can check whether there is a common device that's responsible (intersection), or if other close-by sensors are reporting similar measurements to rule out malfunctions (neighborhood search). Now, let's move on to something more complex: finding the shortest path.</p>
</div>


            

            
        
    

        

                            
                    <h1 class="header-title">The shortest path</h1>
                
            
            
                
<p>This algorithm has its roots in early networking: routers had to decide where to forward packets to, without having any knowledge of what's beyond. They simply had to make the best decision without having perfect information! Edsger Dijkstra, one of the pioneers of computer science, then came up with a graph-routing algorithm that has been named after him: Dijkstra's algorithm.</p>
<p>The algorithm works iteratively and goes over each node to add up their weights, thereby finding the distance (or cost) of reaching this node. It will then continue at the node with the lowest cost, which makes this algorithm a "greedy" algorithm. This continues until the desired node is reached or there are no more nodes to evaluate.</p>
<p>Algorithms that immediately converge toward what's best right now (<strong>local optimum</strong>) in order to find the best overall solution (<strong>global optimum</strong>) are called <strong>greedy algorithms</strong>. This, of course, is tricky, since the path to a global optimum might require the acceptance of an increased cost! There is no guaranteed way to finding the global optimum, so it's about reducing the probability of getting stuck in a local optimum. A well-known greedy algorithm in 2018 is stochastic gradient descent, which is used to train neural networks.</p>
<p>In code, this is what that looks like:</p>
<pre class="language-">pub fn shortest_path(&amp;self, from: KeyType, to: KeyType) -&gt; Option&lt;(u32, Vec&lt;KeyType&gt;)&gt; {<br/>    let mut src = None;<br/>    let mut dest = None;<br/><br/>    for (i, n) in self.nodes.iter().enumerate() {<br/>        if n == &amp;from {<br/>            src = Some(i);<br/>        }<br/>        if n == &amp;to {<br/>            dest = Some(i);<br/>        }<br/>        if src.is_some() &amp;&amp; dest.is_some() {<br/>            break;<br/>        }<br/>    }<br/>    if src.is_some() &amp;&amp; dest.is_some() {<br/>        let (src, dest) = (src.unwrap(), dest.unwrap());<br/><br/>        let mut distance: Vec&lt;TentativeWeight&gt; =<br/>            vec![TentativeWeight::Infinite; self.nodes.len()];<br/>        distance[src] = TentativeWeight::Number(0);<br/><br/>        let mut open: Vec&lt;usize&gt; = <br/>                  (0..self.nodes.len()).into_iter().collect();<br/>        let mut parent = vec![None; self.nodes.len()];<br/>        let mut found = false;<br/>        while !open.is_empty() {<br/>            let u = min_index(&amp;distance, &amp;open);<br/>            let u = open.remove(u);<br/><br/>            if u == dest {<br/>                found = true;<br/>                break;<br/>            }<br/><br/>            let dist = distance[u].clone();<br/><br/>            for e in &amp;self.adjacency_list[u] {<br/>                let new_distance = match dist {<br/>                    TentativeWeight::Number(n) =&gt; <br/>                       TentativeWeight::Number(n + e.weight),<br/>                    _ =&gt; TentativeWeight::Infinite,<br/>                };<br/>                <br/>                let old_distance = distance[e.node].clone();<br/><br/>                if new_distance &lt; old_distance {<br/>                    distance[e.node] = new_distance;<br/>                    parent[e.node] = Some(u);<br/>                }<br/>            }<br/>        }<br/>        if found {<br/>            let mut path = vec![];<br/>            let mut p = parent[dest].unwrap();<br/>            path.push(self.nodes[dest].clone());<br/>            while p != src {<br/>                path.push(self.nodes[p].clone());<br/>                p = parent[p].unwrap();<br/>            }<br/>            path.push(self.nodes[src].clone());<br/><br/>            path.reverse();<br/>            let cost = match distance[dest] {<br/>                TentativeWeight::Number(n) =&gt; n,<br/>                _ =&gt; 0,<br/>            };<br/>            Some((cost, path))<br/>        } else {<br/>            None<br/>        }<br/>    } else {<br/>        None<br/>    }<br/>}</pre>
<p>Since this is a long one, let's break it down. This is boiler-plate code to ensure that both source and destination nodes are nodes in the graph:</p>
<pre>pub fn shortest_path(&amp;self, from: KeyType, to: KeyType) -&gt; Option&lt;(u32, Vec&lt;KeyType&gt;)&gt; {<br/>    let mut src = None;<br/>    let mut dest = None;<br/><br/>    for (i, n) in self.nodes.iter().enumerate() {<br/>        if n == &amp;from {<br/>            src = Some(i);<br/>        }<br/>        if n == &amp;to {<br/>            dest = Some(i);<br/>        }<br/>        if src.is_some() &amp;&amp; dest.is_some() {<br/>            break;<br/>        }<br/>    }<br/>    if src.is_some() &amp;&amp; dest.is_some() {<br/>        let (src, dest) = (src.unwrap(), dest.unwrap());</pre>
<p>Then, each node gets a tentative weight assigned, which is infinite in the beginning, except for the origin node, which has zero cost to reach. The "open" list, which contains all the nodes yet to be processed, is conveniently created using Rust's range—as it corresponds to the indices we are working with.</p>
<p>The parent array keeps track of each node's parent once the lower cost is established, which provides a way to trace back the best possible path!</p>
<pre>        let mut distance: Vec&lt;TentativeWeight&gt; =<br/>            vec![TentativeWeight::Infinite; self.nodes.len()];<br/>        distance[src] = TentativeWeight::Number(0);<br/><br/>        let mut open: Vec&lt;usize&gt; = <br/>                     (0..self.nodes.len()).into_iter().collect();<br/>        let mut parent = vec![None; self.nodes.len()];<br/>        let mut found = false;</pre>
<p>Now, let's plunge into the path-finding. The helper function, <kbd>min_index()</kbd>, takes the current distances and returns the index of the node that is easiest (as in lowest distance) to reach next. This node will then be removed from the open list. Here's a good point at which to also stop if the destination has been reached. For more thoughts on this, see the preceding information box on greedy algorithms. Setting <kbd>found</kbd> to <kbd>true</kbd> will help distinguish between no result and early stopping.</p>
<p>For each edge of this node, the new distance is computed and, if lower, inserted into a distance list (as seen from the source node). There are a lot of clones going on as well, which is due to ensuring not borrowing while updating the vector. With <kbd>u64</kbd> (or <kbd>u32</kbd>) types, this should not create a large overhead (pointers are typically that large too), but for other types, this can be a performance pitfall:</p>
<pre class="language-">        while !open.is_empty() {<br/>            let u = min_index(&amp;distance, &amp;open);<br/>            let u = open.remove(u);<br/><br/>            if u == dest {<br/>                found = true;<br/>                break;<br/>            }<br/><br/>            let dist = distance[u].clone();<br/><br/>            for e in &amp;self.adjacency_list[u] {<br/>                let new_distance = match dist {<br/>                    TentativeWeight::Number(n) =&gt; <br/>                        TentativeWeight::Number(n + e.weight),<br/>                    _ =&gt; TentativeWeight::Infinite,<br/>                };<br/>                <br/>                let old_distance = distance[e.node].clone();<br/><br/>                if new_distance &lt; old_distance {<br/>                    distance[e.node] = new_distance;<br/>                    parent[e.node] = Some(u);<br/>                }<br/>            }<br/>        }</pre>
<p>After this loop exits, there is a distance array and a parent array to be prepared for returning to the caller. First, trace back the path from the destination to the origin node in the parent array, which leads to the reverse optimal path between the two nodes:</p>
<div><pre>        if found {<br/>            let mut path = vec![];<br/>            let mut p = parent[dest].unwrap();  <br/>            path.push(self.nodes[dest].clone());<br/>            while p != src {<br/>                path.push(self.nodes[p].clone());<br/>                p = parent[p].unwrap();<br/>            }<br/>            path.push(self.nodes[src].clone());<br/>            path.reverse();<br/>            let cost = match distance[dest] {     <br/>                TentativeWeight::Number(n) =&gt; n,<br/>                _ =&gt; 0,<br/>            };<br/>            Some((cost, path))<br/>        } else {<br/>            None<br/>        }<br/>    } else {<br/>        None<br/>    }<br/>}</pre></div>
<p>By strictly following the node with the lowest distance, Dijkstra's algorithm achieves a great runtime when stopping early, and runtime can even be improved by using more efficient data structures (such as a heap) to fetch the next node efficiently.</p>
<p>Modern approaches to shortest paths in a graph typically use the <em>A*</em> (pronounced "a star") algorithm. While it operates on the same principles, it is also a bit more complex and would therefore go beyond the scope of this book.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Wrap up</h1>
                
            
            
                
<p>A graph is surprisingly straightforward to implement: clear ownership in adjacency lists or matrices makes them almost effortless to work with! On top of that, there are two additional aspects that weren't yet covered in this implementation: an enumeration with an implementation, and using regular operations (here: comparison) with this implementation.</p>
<p>This shows how conforming to standard interfaces provides great ways to interface with the standard library or well-known operations in addition to the flexibility enumerations provide. With a few lines of code, infinity can be represented and worked with in a readable way. It was also a step toward more algorithmic aspects, which will be covered later in the book. For now, let's focus on graphs again.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Upsides</h1>
                
            
            
                
<p>Graph structures are unique and there are rarely other ways of achieving the same outcome. Working in this environment enables you to focus deeply on relationships and think about problems differently. Following are some upsides of using graphs:</p>
<ul>
<li>Are amazing in modeling relationships</li>
<li>Efficient retrieval of dependencies of a specific node</li>
<li>Simplify complex abstractions</li>
<li>Enable certain problems to be solved at all</li>
</ul>
<p>Whether you choose a matrix or list representation is often a subjective choice and, for example, while the matrix provides easy deletes, a list stores edges more efficiently in the first place. It's all a trade-off.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Downsides</h1>
                
            
            
                
<p>This leads us to the downsides of this particular data structure:</p>
<ul>
<li>Unable to solve certain problems efficiently (for example, a list of all nodes that have a certain property)</li>
<li>More resource-inefficient</li>
</ul>
<ul>
<li>Unsolved problems exist (for example, the traveling salesman problem with a high number of cities)</li>
<li>Typically requires a problem to be reconsidered</li>
</ul>
<p>With this, we can conclude this chapter about trees and their relatives after a summary.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter went deep into trees, starting off with the simplest form: the binary search tree. This tree prepares the inserted data for search by creating a left and a right branch which hold smaller or greater values. A search algorithm can therefore just pick the direction based on the current node and the value coming in, thereby skipping a majority of the other nodes.</p>
<p>The regular binary search tree has a major drawback, however: it can become unbalanced. Red-black trees provide a solution for that: by rotating subtrees, a balanced tree structure is maintained and search performance is guaranteed.</p>
<p>Heaps are a more exotic use of the tree structure. With their primary use as a priority queue, they efficiently produce the lowest or highest number of an array in constant time. The upheap and downheap operations repair the structure upon insert or removal so that the root is again the lowest (min-heap) or highest (max-heap) number.</p>
<p>Another very exotic structure is the trie. They are specialized in holding strings and very efficiently find the data associated with a certain string by combining the characters as nodes with words "branching off" as required.</p>
<p class="mce-root">To go up in the generalization level, B-Trees are a generic form of a tree. They hold several values, with the ranges between them leading to a child node. Similar to red-black trees, they are balanced, and adding nodes only happens at the leaves where they may be "promoted" to a higher level. Typically, these are used in database indices.</p>
<p>Last but not least, the most generic form of a tree: the graph. Graphs are a flexible way to express constrained relationships, such as no cycles, and directionality. Typically, each node has weighted connections (edges) that provide some notion of cost of transitioning between the nodes.</p>
<p>With some of the essential data structures covered, the next chapter will explore sets and maps (sometimes called dictionaries). In fact, some of those have already been used in this chapter, so the next chapter will focus on implementing our own.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Questions</h1>
                
            
            
                
<ul>
<li>How does a binary search tree skip several nodes when searching?</li>
<li>What are self-balancing trees?</li>
<li>Why is balance in a tree important?</li>
<li>Is a heap a binary tree?</li>
<li>What are good use cases for tries?</li>
<li>What is a B-Tree?</li>
<li>What are the fundamental components of a graph?</li>
</ul>


            

            
        
    </body></html>
<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Random and Combinatorial</h1>
                </header>
            
            <article>
                
<p class="mce-root">While sorting and searching are two very fundamental problems in computer science, they are far from the only ones. In fact, those problems have been thoroughly solved by people who deeply specialize in such things. In today's world, it is more likely that a solution to a real-world problem involves generating random numbers, the best possible combination of several items (combinatorics) , "rolling up" several time periods into single numbers, and visualizing the results. Random number generation algorithms and solving combinatorial problems efficiently have become very important. Especially for the latter, the implementation will be specific to the solution, but there are fundamental approaches that remain. In this chapter, we will discuss a few of these fundamental approaches and learn about the following:</p>
<ul>
<li>Implementing backtracking algorithms</li>
<li>Utilizing dynamic programming techniques</li>
<li>How a pseudo-random number generator works</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pseudo-random numbers</h1>
                </header>
            
            <article>
                
<p>In the last few years, random number generation has seen an interesting rise in popularity, yet many developers simply accept the generator provided by whatever technology they use. However, good random numbers are critical for many applications, such as encryption and security (or the lack thereof; see 2010's Sony PlayStation 3 security incident that prompted a famous XKCD—<a href="https://xkcd.com/221/">https://xkcd.com/221/</a>), simulation, games, statistics, and biology.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As a basic principle: the more random a sequence is, the better. The reason for this is obvious. If any number in a sequence of random numbers is statistically dependent on one of the others, it becomes a pattern that can be predicted, and there is no such thing as predictable randomness. Thus, the numbers in a random sequence have to be statistically independent to qualify as good random numbers.</p>
<p>To get these random numbers, either a pseudo-random number generator or a true random number generator can be used (or you can buy a book—<a href="https://www.rand.org/pubs/monograph_reports/MR1418.html">https://www.rand.org/pubs/monograph_reports/MR1418.html</a>). Since computers are deterministic machines, the latter is impossible without an external influence, which is why there have actually been (unsuccessful) devices to try and achieve truly random numbers. <strong>Pseudo-random number generators</strong> (<strong>PRNGs</strong>), on the other hand, are deterministic, but start off using fairly random input (mouse pointer movements, network traffic, and so on) and periodically produce numbers based on that seed.</p>
<p>PRNGs also enjoy a speed advantage (since there is no physical interaction required, such as measuring atmospheric noise) and the output is often good enough for many applications. In fact, if the seed is very close to random, PRNGs do a great job, as can be seen in modern cryptography.</p>
<p>There are a range of institutions researching PRNGs and their effectiveness at producing cryptographically saved random numbers, for example, Germany's BSI provides an in-depth analysis paper (<a href="https://bit.ly/2AOIcBl">https://bit.ly/2AOIcBl</a>). This is a fascinating topic with a close relationship to IT security. For non-security researchers, however, there is a simple way to appraise the quality of a random number generator at a glance: visual inspection. When randomly deciding whether to plot each single pixel in a scatter plot, there should not be any visible pattern.</p>
<p><span>The following graph is of Python's</span> <kbd>numpy.random</kbd> <span>random generator, which was created to provide the same number from the same seed:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2c0d09b7-25d1-4729-ab5f-928e2671e060.png" style="width:56.33em;height:54.58em;"/></p>
<p>It fares well enough for statistical work and some simulations, but should not be relied upon for cryptographic work.</p>
<p>Regardless of the type of work, a bad random generator should never look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f2e87459-49e0-44b1-b2b6-a443b57bd0bd.png" style="width:49.33em;height:47.83em;"/></p>
<p>As the pattern indicates, there are systemic errors that can be found in this random generator! Unfortunately, this is not unheard of, even in widely used technologies such as PHP on Windows (<a href="https://boallen.com/random-numbers.html">https://boallen.com/random-numbers.html</a>).</p>
<p>Thanks to the seed, PRNGs can create reproducible as well as close-to-random numbers, which comes in handy for simulations or simply drawing a random sample for data science purposes. One very old and well researched method is the <strong>linear congruential generator</strong>, or <strong>LCG</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LCG</h1>
                </header>
            
            <article>
                
<p>The LCG is one of the oldest ways of generating a pseudo-random number sequence. It follows a simple, recursive formula:</p>
<p><img class="fm-editor-equation" src="assets/2b3715dd-f677-4fd7-bdcb-e69c831c3e76.png" style="width:18.83em;height:1.75em;"/></p>
<p><em>X</em> denotes the random number (or, more precisely, the <em>n<sup>th</sup></em> random number in the sequence). It is based on its predecessor multiplied by a factor, <em>a</em>, and offset by a constant, <em>c</em>. The modulo operator makes sure that there is no overflow. What's the first <em>X</em>? The seed! So a random number sequence will start with the seed, providing determinism if needed.</p>
<p>These parameter settings are subject to significant testing; in fact, many library and compiler developers have different settings. The Wikipedia page provides an overview (<a href="https://en.wikipedia.org/wiki/Linear_congruential_generator#Parameters_in_common_use">https://en.wikipedia.org/wiki/Linear_congruential_generator</a>):</p>
<pre>pub struct LCG {<br/>    xn: f32,<br/>    m: f32,<br/>    c: f32,<br/>    a: f32,<br/>}<br/><br/>impl LCG {<br/>    fn seeded(seed: u32) -&gt; LCG {<br/>        LCG {<br/>            xn: seed as f32,<br/>            m: 2e31,<br/>            a: 171f32,<br/>            c: 8f32,<br/>        }<br/>    }<br/><br/>    fn new(seed: f32, m: f32, a: f32, c: f32) -&gt; LCG {<br/>        LCG {<br/>            xn: seed,<br/>            m: m,<br/>            a: a,<br/>            c: c,<br/>        }<br/>    }<br/><br/>    fn next_f32(&amp;mut self) -&gt; f32 {<br/>        self.xn = (self.a * self.xn + self.c) % self.m;<br/>        self.xn / self.m<br/>    }<br/>}</pre>
<p>This parameter setting, while chosen at random, does not look terrible:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/31d8bf3a-5b17-41b4-9e07-42c761314588.png" style="width:42.67em;height:41.25em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The bitmap that was generated as a bad example previously also used the LCG, but with another random parameter setting:</p>
<pre>impl LCG {<br/>    fn seeded(seed: u32) -&gt; LCG {<br/>        LCG {<br/>            xn: seed as f32,<br/>            m: 181f32,<br/>            a: 167f32,<br/>            c: 0f32,<br/>        }<br/>    }<br/>...<br/>}</pre>
<p>Since the result is obviously bad, this goes to show how important the parameters are here. Typically, these are not settings you should adjust (or you'd know about them). Similarly, two scientists came up with a particular set of magic numbers that allow for a better random number generator: the Wichmann-Hill PRNG.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Wichmann-Hill</h1>
                </header>
            
            <article>
                
<p>An extended approach to the LCG was taken by Brian Wichmann and David Hill when they invented their random number generator. It is based on the LCG, but uses three of them modified and combined by (magic) prime numbers.</p>
<p>These numbers, when added together, produce a sequence that is <span>6,</span><span>953,</span><span>607,</span><span>871,</span><span>644 (or 6.95 * 10<sup>12</sup>) numbers long, which means that calling the PRNG after this number of calls will make it start over:</span></p>
<pre>const S1_MOD: f32 = 30269f32;<br/>const S2_MOD: f32 = 30307f32;<br/>const S3_MOD: f32 = 30323f32;<br/><br/>pub struct WichmannHillRng {<br/>    s1: f32,<br/>    s2: f32,<br/>    s3: f32,<br/>}<br/><br/>impl WichmannHillRng {<br/>    fn new(s1: f32, s2: f32, s3: f32) -&gt; WichmannHillRng {<br/>        WichmannHillRng {<br/>            s1: s1,<br/>            s2: s2,<br/>            s3: s3,<br/>        }<br/>    }<br/><br/>    pub fn seeded(seed: u32) -&gt; WichmannHillRng {<br/>        let t = seed;<br/>        let s1 = (t % 29999) as f32;<br/>        let s2 = (t % 29347) as f32;<br/>        let s3 = (t % 29097) as f32;<br/>        WichmannHillRng::new(s1, s2, s3)<br/>    }<br/><br/>    pub fn next_f32(&amp;mut self) -&gt; f32 {<br/>        self.s1 = (171f32 * self.s1) % S1_MOD;<br/>        self.s2 = (172f32 * self.s2) % S2_MOD;<br/>        self.s3 = (170f32 * self.s3) % S3_MOD;<br/>        (self.s1 / S1_MOD + self.s2 / S2_MOD + self.s3 / S3_MOD) % 1f32<br/>    }<br/>}</pre>
<p>The generator does well, as the visual inspection shows. In fact, the Wichmann-Hill generator was used in various technologies and applications in the past, so this is not surprising.</p>
<p>Here is the visual analysis:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1bf57847-cdea-4317-bd4a-30959cb3e01c.png" style="width:48.92em;height:47.42em;"/></p>
<p>Clearly, implementing every variation of the random generator is not efficient for every project. Luckily, there is an excellent crate on <a href="https://crates.io/">https://crates.io/</a> <span><span>called <kbd>rand</kbd>.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The rand crate</h1>
                </header>
            
            <article>
                
<p>When talking about random number generators, there is an excellent crate that cannot be skipped: <kbd>rand</kbd>. Since Rust's standard library does not include a random function, this crate provides that, and more.</p>
<p>In particular, there are several implementations that come with the <kbd>rand</kbd> crate, ranging from regular PRNGs, to an interface to the OS number generator (<kbd>/dev/random</kbd> on Unix-like systems), including a compatible interface for other targets, such as web assembly!</p>
<p>The features are impossible to describe in this chapter, so more information on these can be found in their own book (<a href="https://rust-random.github.io/book/">https://rust-random.github.io/book/</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Back to front</h1>
                </header>
            
            <article>
                
<p>There are types of problems that humans can solve a lot easier than computers. These are typically somewhat spatial in nature (for example, a traveling salesman, knapsack problem) and rely on patterns, both of which are domains humans are great at. Another name for this class of problems is optimization problems, with solutions that minimize or maximize a particular aspect (for example, a minimum distance or maximum value). A subset of this class is constraint satisfaction problems, where a solution has to conform to a set of rules while minimizing or maximizing another attribute.</p>
<p>The brute force approach that's used to create these solutions is an algorithmic class called backtracking, in which many small choices are recursively added together to form a solution. Fundamentally, this search for the optimal solution can run to find all possible combinations (<em>exhaustive</em> search) or stop early. Why recursion? What makes it better suited than regular loops?</p>
<p>A typical constraint satisfaction problem requires incrementally adding items to a set of existing items and then evaluating their quality. A backtracking algorithm is such that it can backtrack once it encounters a bad solution early on so that it can skip at the best possible time. This is much clearer when talking about an example, so here are two famous problems that can be solved with regular backtracking algorithms: the 0-1 knapsack problem, and the N queens problem.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Packing bags or the 0-1 knapsack problem</h1>
                </header>
            
            <article>
                
<p>The knapsack problem is very real: any time you fly with a cheap airline with cabin baggage only, things get complicated. Do I really need <em>this</em>? I could just leave my DSLR at home and use my phone for pictures, right?</p>
<p>These are statements that express the potential value of an item and the considerations regarding its weight (or volume on these flights), and we typically want to bring the most valuable (to us) items on a trip. While this smells like an algorithmic problem, it's far from simple. Let's start with the goal:</p>
<p><em><span>Given n items (with weights and values), find the subset of items providing the highest value without exceeding the knapsack's capacity, W.</span></em></p>
<p>Derived from this, the way to implement the solution can be constructed as follows: as an exhaustive search algorithm, every possible solution can be the best solution. However, this will only become clear once all solutions are evaluated. Thus, let's generate every possible solution first and then worry about the best one.</p>
<p>For any recursive scenario, it's important to worry about the exit condition first: when should the recursion stop and what will it return? In the case of the knapsack problem, the stopping condition is built around the current weight:</p>
<ul>
<li>The weight exceeds the capacity</li>
<li>The current weight is at capacity</li>
<li>There are no items left</li>
</ul>
<p>If the capacity is already exceeded, the algorithm returns the data type's minimum value and "backtracks" on this execution branch. However, if the weight is exactly the same as the capacity, or there are no more items left, a neutral value is returned.</p>
<p>What does the return value indicate, then? It's the total value of the items and, since this is a search for maximum value, the return value of the two possibilities are compared:</p>
<ul>
<li>Including the item</li>
<li>Excluding the item</li>
</ul>
<p>Thus, we'll take the maximum of the return values of a recursive call either with or without the current item, thereby excluding any combination that exceeds the capacity <span>provided</span>:</p>
<pre>pub trait Backtracking {<br/>    fn fill(&amp;self, items: Vec&lt;&amp;Item&gt;) -&gt; u64;<br/>    fn fill_r(&amp;self, remaining: &amp;[&amp;Item], current_weight: usize) -&gt; i64;<br/>}</pre>
<p class="mce-root"/>
<p>A note on architecture: since this example is going to be improved using dynamic programming (refer to the following code), a nice way to structure this is to create and implement a trait for either technique:</p>
<pre><br/>#[derive(Debug, PartialEq)]<br/>pub struct Item {<br/>    pub weight: u32,<br/>    pub value: u32,<br/>}<br/><br/>pub struct Knapsack {<br/>    capacity: usize,<br/>}<br/><br/>impl Knapsack {<br/>    pub fn new(capacity: usize) -&gt; Knapsack {<br/>        Knapsack { capacity: capacity }<br/>    }<br/>}<br/><br/>impl Backtracking for Knapsack {<br/><br/>    fn fill(&amp;self, items: Vec&lt;&amp;Item&gt;) -&gt; u64 {<br/>        let value = self.fill_r(&amp;items, 0);<br/>        if value &lt; 0 {<br/>            0<br/>        } else {<br/>            value as u64<br/>        }<br/>    }<br/><br/>    fn fill_r(&amp;self, remaining: &amp;[&amp;Item], current_weight: usize)<br/>     -&gt; i64 {<br/>        let w = current_weight;<br/><br/>        if w &gt; self.capacity {<br/>            return i64::min_value();<br/>        }<br/><br/>        if remaining.len() &gt; 0 &amp;&amp; w &lt; self.capacity {<br/>            let include = remaining[0].value as i64<br/>                + self.fill_r(&amp;remaining[1..], current_weight <br/>                + remaining[0].weight as usize);<br/>            let exclude = self.fill_r(&amp;remaining[1..], current_weight);<br/>            if include &gt;= exclude {<br/>                include<br/>            } else {<br/>                exclude<br/>            }<br/>        } else {<br/>            0<br/>        }<br/>    }<br/><br/>}</pre>
<p>One question about the runtime complexity of this algorithm <span>remains</span>—and it's not very clear cut this time. Some people suggest that it's <em>O(2<sup>n</sup>)</em>, but there are two main growth factors: the capacity, as well as the number of available items. In this book, the graphs will focus on the number of items to be added to the bag, which exercises (pseudo) polynomial complexity (greater than <em>O(n²)</em>). Regardless, you should know that this is an expensive problem to solve using backtracking.</p>
<p>Another popular example in universities for backtracking is the 8 queens problem (or, in its general form, the N queens problem).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">N queens</h1>
                </header>
            
            <article>
                
<p>The N queens <span>chess</span> problem (the generalized version of the 8 queens problem/puzzle) is defined as follows:</p>
<p><em>On a chessboard with N by N squares, place N queens so that they cannot attack each other.</em></p>
<p>As a first step, it's important to understand the ways a queen can move in chess, which is luckily straightforward: they can move in a straight line up, down, left, right, and diagonally, as demonstrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0ede9858-d92b-47a9-ab9c-387780b662ba.png" style="width:18.33em;height:18.33em;"/></p>
<p>With this known, the rest is very similar to the preceding knapsack problem, but with a few more possibilities caused by various placement options. There are a number of strategies to tackle that:</p>
<ul>
<li>Each cell individually, which would result in a large number of recursive calls quickly</li>
<li>Each row (or column) individually, and iterate over the cells within</li>
</ul>
<p>The latter is clearly the preferred method, since a 10 by 10 board results in 100 recursive calls for each individual cell (including allocations, for example) and thereby quickly results in a stack overflow. Hence, the second option (by row) is the best trade-off, since each row/column has to have at least one queen placed and it rules out any other queen placements there:</p>
<pre>pub struct ChessBoard {<br/>    board: Vec&lt;Vec&lt;bool&gt;&gt;,<br/>    n: usize,<br/>}<br/><br/>impl ChessBoard {<br/>    pub fn new(n: usize) -&gt; ChessBoard {<br/>        ChessBoard {<br/>            n: n,<br/>            board: vec![vec![false; n]; n],<br/>        }<br/>    }<br/><br/>    pub fn place_queens(&amp;mut self) -&gt; bool {<br/>        self.place_queens_r(0)<br/>    }<br/><br/>    pub fn place_queens_r(&amp;mut self, column: usize) -&gt; bool {<br/>        if column &lt; self.n {<br/>            for r in 0..self.n {<br/>                if self.is_valid(r, column) {<br/>                    self.board[r][column] = true;<br/>                    if self.place_queens_r(column + 1) {<br/>                        return true;<br/>                    }<br/><br/>                    self.board[r][column] = false;<br/>                }<br/>            }<br/>            false<br/>        }<br/>        else {<br/>            true<br/>        }<br/>    }<br/><br/>    fn is_valid(&amp;self, row: usize, col: usize) -&gt; bool {<br/>        for i in 0..self.n {<br/>            if self.board[i][col] {<br/>                return false;<br/>            }<br/>            if self.board[row][i] {<br/>                return false;<br/>            }<br/>        }<br/>        let mut i = 0;<br/>        let (mut left_lower, mut left_upper, mut right_lower, <br/>             mut right_upper) =<br/>            (true, true, true, true);<br/><br/>        while left_lower || left_upper || right_lower || right_upper {<br/>            if left_upper &amp;&amp; self.board[row - i][col - i] {<br/>                return false;<br/>            }<br/>            if left_lower &amp;&amp; self.board[row + i][col - i] {<br/>                return false;<br/>            }<br/>            if right_lower &amp;&amp; self.board[row + i][col + i] {<br/>                return false;<br/>            }<br/>            if right_upper &amp;&amp; self.board[row - i][col + i] {<br/>                return false;<br/>            }<br/>            i += 1;<br/>            left_upper = row as i64 - i as i64 &gt;= 0 <br/>                         &amp;&amp; col as i64 - i as i64 &gt;= 0;<br/>            left_lower = row + i &lt; self.n &amp;&amp; col as i64 - i <br/>                         as i64 &gt;= 0;<br/><br/>            right_lower = row + i &lt; self.n &amp;&amp; col + i &lt; self.n;<br/>            right_upper = row as i64 - i as i64 &gt;= 0 <br/>                          &amp;&amp; col + i &lt; self.n;<br/>        }<br/>        true<br/>    }<br/>// ...<br/>}</pre>
<p>The strategy is simple: for each cell in a row, check whether a valid queen can be placed under the current conditions. Then, descend deeper into the recursion and end it as soon as a valid setting has been found. The result looks as follows (<em>n = 4</em>):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/120777b3-5cad-42d7-b860-1edf3089a16d.png" style="width:17.83em;height:17.75em;"/></p>
<p>However, the computational complexity of this algorithm grows exponentially (<em>O(2<sup>n</sup>)</em>), which means that for large <em>n</em>, it will not finish in any reasonable amount of time:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7e11657e-47e3-4caa-a2a7-32d896a6ddeb.png" style="width:60.25em;height:38.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The output graph for N queens problems</div>
<p>While this particular problem is probably more like a teaching problem, this approach can certainly be applied to other (similar) use cases, especially in the spatial domain.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advanced problem solving</h1>
                </header>
            
            <article>
                
<p>Backtracking calculates and finds the best overall solution to a particular problem. However, as described in <a href="6ab96dc6-b8f5-4c03-88a3-f4a345f8cc9b.xhtml">Chapter 8</a>, <em>Algorithm Evaluation</em>, there are problems that have a really large computational complexity, which leads to a really long running time. Since this is unlikely to be solved by simply making computers faster, smarter approaches are required.</p>
<p>With several strategies and techniques available, the choice is yours to find an approach that best solves your problem. The position of Rust in this space can be critical, thanks to its great speed and memory efficiency, so keeping an eye on solutions for complex problems might pay off in the future (in the author's opinion).</p>
<p class="mce-root"/>
<p>First up is a surprising programming technique that is aimed at improving the complexities of backtracking algorithms: dynamic programming.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dynamic programming</h1>
                </header>
            
            <article>
                
<p>The concept of dynamic programming is one of these techniques that you thought had a different name: caching. The fundamental idea is to save relevant temporary results to a cache and use this precomputed result instead of recalculating something over and over again!</p>
<p>This means that a problem and a potential solution have to be examined to find relevant sub-problems, so any result can be cached. The main upside of this approach is that it finds the globally best solution possible, but at the price of a potentially high runtime complexity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The knapsack problem improved</h1>
                </header>
            
            <article>
                
<p>As an example, let's examine the recursive calls of the knapsack solver. For brevity, this knapsack is to be filled using a list of three items where the weight is uniformly one and has a capacity of two. Since the backtracking algorithm walks through the list of items in order (and tries either to include or exclude a particular item), the knapsack solver can be seen as a function <em>K</em> that maps any items that are remaining as well as capacity remaining to a particular value:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/935f3d69-113b-4aee-87fd-1b6e2c40c5c3.png" style="width:39.33em;height:20.42em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>Therefore, at the same level, the same input parameter leads to the same value and this is easy to cache. In the preceding diagram, the nodes marked by the rectangle are calculated at least twice. This example was taken from the GeeksforGeeks' article (<a href="https://www.geeksforgeeks.org/0-1-knapsack-problem-dp-10/">https://www.geeksforgeeks.org/0-1-knapsack-problem-dp-10/</a>) regarding the 0-1 knapsack problem.</span></p>
<p>Before anything else, we can now implement a different trait to the backtracking:</p>
<pre>pub trait DynamicProgramming {<br/>    fn fill(&amp;self, items: Vec&lt;&amp;Item&gt;) -&gt; u64;<br/>}</pre>
<p>Implementation then follows and, as a function with two input parameters, each combination of input parameters can be saved in a two-dimensional array, which reduces the runtime complexity to walking this matrix, leading to a <em>O(n * W)</em> runtime complexity:</p>
<pre><br/>impl DynamicProgramming for Knapsack {<br/>    <br/>    fn fill(&amp;self, items: Vec&lt;&amp;Item&gt;) -&gt; u64 {<br/>        let mut cache = vec![vec![0u64; self.capacity + 1]; <br/>                        items.len() + 1];<br/>        for i in 1..items.len() + 1 {<br/>            for w in 1..self.capacity + 1 {<br/>                if items[i -1].weight as usize &lt;= w {<br/>                    let prev_weight = <br/>                        w - (items[i - 1].weight as usize);<br/>                    cache[i][w] = max(<br/>                        items[i - 1].value as u64 <br/>                        + cache[i - 1][prev_weight],<br/>                        cache[i - 1][w],<br/>                    );<br/>                } else {<br/>                    cache[i][w] = cache[i - 1][w]<br/>                }<br/>            }<br/>        }<br/>        cache[items.len()][self.capacity]<br/>    }<br/>}<br/><br/></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The code went from a recursive call chain to constructing a matrix where the maximum value for a particular combination is just a lookup, which seriously improves the absolute and relative runtime (20 items take 41,902 +/- 10,014 ns when using backtracking and 607 +/- 138 ns for dynamic programming):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d895dca2-11eb-4ad4-adb7-df59e03c4ce4.png" style="width:60.00em;height:37.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The output graph for Knapsack problems</div>
<p>In relative terms, the runtime complexity improved significantly:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a5934e0b-59c8-4831-9b69-d836b2795ddf.png" style="width:58.33em;height:36.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The runtime complexity graph comparison between dynamic programming and backtracking</div>
<p>Employing this strategy (or similar) to problems that allow for that kind of optimization permits far higher input parameters and therefore enable it to solve real-world problems! Imagine an airline trying to work out the most valuable cargo it can bring, but it's limited to 40 different items at once.</p>
<p>Since there are many harder problems (for example, a problem class called NP-hard problems), people came up with ways to find good solutions as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Metaheuristic approaches</h1>
                </header>
            
            <article>
                
<p>Dynamic programming is great for constraint satisfaction problems. However, better solutions can be found using something akin to systematic guessing, or metaheuristics. These problem-agnostic solution generators can be classified in several ways, for instance, whether they are population-based, inspired by nature, and searching globally or locally.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Whichever optimization algorithm is chosen, it will treat the problem like a search problem, trying to find the best possible solution within the solutions <span>provided</span>. Absent of any guarantees to find the best solution possible, it will typically find a good enough solution. Thanks to the expensive runtimes of NP-hard problems, a wide variety of ways can lead to a better solution than a more specific solution.</p>
<p class="mce-root">Popular metaheuristics include the following:</p>
<ul>
<li>Simulated annealing</li>
<li>Genetic algorithms</li>
<li>Particle swarm optimization</li>
<li>Ant colony optimization</li>
<li>Tabu search</li>
</ul>
<p>Rust's ecosystem features several crates that implement these metaheuristic strategies. The progress of some of these crates can be tracked on <a href="http://www.arewelearningyet.com/metaheuristics/">http://www.arewelearningyet.com/metaheuristics/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example metaheuristic – genetic algorithms</h1>
                </header>
            
            <article>
                
<p>Examples include the traveling salesman problem, where a tour of the shortest path connecting <em>n</em> cities has to be found. With a <em>O(n!)</em> runtime complexity, only 20 cities prove to be computationally very expensive, but it can be solved well enough for a very large <em>n</em> by starting off with a random order of cities (tour), and then repeatedly recombining or randomly changing (mutating) several of these tours only to select the best ones and restarting the process with these.</p>
<p>Using the <kbd>rsgenetic</kbd> crate (<a href="https://crates.io/crates/rsgenetic">https://crates.io/crates/rsgenetic</a>), implementing the solution becomes a matter of implementing the <kbd>TspTour</kbd> trait, which requires a <kbd>fitness()</kbd> function to be supplied so that a solution can be evaluated, the <kbd>crossover()</kbd> function to recombine two parents into a new offspring tour, and the <kbd>mutate()</kbd> function to apply random changes to a tour:</p>
<pre>impl Phenotype&lt;TourFitness&gt; for TspTour {<br/>    ///<br/>    /// The Euclidean distance of an entire tour.<br/>    ///<br/>    fn fitness(&amp;self) -&gt; TourFitness {<br/>        let tour_cities: Vec&lt;&amp;City&gt; = self.tour.iter().map(|t| <br/>                                      &amp;self.cities[*t]).collect();<br/>        let mut fitness = 0f32;<br/>        for i in 1..tour_cities.len() {<br/>            fitness += distance(tour_cities[i], tour_cities[i - 1]);<br/>        }<br/>        -(fitness.round() as i32)<br/>    }<br/><br/>    ///<br/>    /// Implements the crossover for a TSP tour using PMX<br/>    ///<br/>    fn crossover(&amp;self, other: &amp;TspTour) -&gt; TspTour {<br/>        // ...<br/><br/>        TspTour {<br/>            tour: offspring,<br/>            cities: self.cities.clone(),<br/>            rng_cell: self.rng_cell.clone(),<br/>        }<br/>    }<br/><br/>    ///<br/>    /// Mutates the solution by swapping neighbors at a chance<br/>    ///<br/>    fn mutate(&amp;self) -&gt; TspTour {<br/>        let mut rng = self.rng_cell.borrow_mut();<br/>        if rng.gen::&lt;f32&gt;() &lt; MUTPROB {<br/>            let mut mutated: Tour = self.tour.clone();<br/>            for i in 0..mutated.len() {<br/>                if rng.gen::&lt;f32&gt;() &lt; INDPB {<br/>                    let mut swap_idx = rng.gen_range(0, <br/>                                       mutated.len() - 2);<br/>                    if swap_idx &gt;= i {<br/>                        swap_idx += 1;<br/>                    }<br/>                    let tmp = mutated[i];<br/>                    mutated[i] = mutated[swap_idx];<br/>                    mutated[swap_idx] = tmp;<br/>                }<br/>            }<br/>            TspTour {<br/>                tour: mutated,<br/>                cities: self.cities.clone(),<br/>                rng_cell: self.rng_cell.clone(),<br/>            }<br/>        } else {<br/>            self.clone()<br/>        }<br/>    }<br/>}</pre>
<p>Once these are implemented, the framework allows you to set a selector to select the best <em>n</em> solutions in each generation to create the next generation's population. These steps are repeated until the fitness values stagnate (converge) and the highest fitness in the last generation can be considered a good solution for the problem.</p>
<p>Over several generations, a solution like this one can be found:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f26b4e7a-419a-4c98-85e5-d9c1b69a9816.png" style="width:37.33em;height:36.92em;"/></p>
<p>A more in-depth look at solving this problem in JavaScript, as well as in Rust (and Wasm), can be found on my blog at <a href="https://blog.x5ff.xyz/blog/azure-functions-wasm-rust-ai/#">https://blog.x5ff.xyz</a>. A similar approach can be taken to arrange a highly valuable combination of items in a knapsack, which is left for you to find out.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Other than regular data structures and sorting, as well as searching methods, there are several other problems that arise. This chapter talks about a small subset of those: generating random numbers and solving constraint satisfaction problems.</p>
<p>Random number generation is useful in lots of ways: encryption, gaming, gambling, simulations, data science—all require good random numbers. Good? There are two important types: pseudo-random numbers and "real" random numbers. While the latter has to be taken from the physical world (computers are deterministic), the former can be implemented with the LCG or the Wichmann-Hill generator (which combines LCGs using magic numbers).</p>
<p>Constraint satisfaction problems are problems that find the best combination that conform to a set of constraints. A technique called backtracking builds a state of the current permutation by using recursion to generate all combinations, but tracking back on those that do not satisfy the required constraints. Both the 8 queens (or N queens) problem and the 0-1 knapsack problem are examples of backtracking algorithms that exhibit expensive runtime behavior.</p>
<p>Advanced techniques such as dynamic programming or metaheuristics (that return good enough solutions) can lead to a significant improvement in solving these challenges quicker (or for larger sizes). Rust, as a fast and efficient language, can play a significant role in these techniques in the future.</p>
<p>In the next chapter, we will look into algorithms that the Rust standard library provides.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ul>
<li>What is the difference between PRNGs and RNGs?</li>
<li>What crate provides random number generators in Rust?</li>
<li>How can backtracking solve combinatorial problems?</li>
<li>What is dynamic programming?</li>
<li>How are metaheuristics a problem-agnostic approach to solving hard problems?</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p><span>Here is some additional reference material that you may refer to regarding what has been covered in this chapter:</span></p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Random_number_generator_attack">https://en.wikipedia.org/wiki/Random_number_generator_attack</a></li>
<li><a href="https://blog.x5ff.xyz">https://blog.x5ff.xyz</a></li>
<li><a href="https://en.wikipedia.org/wiki/Metaheuristic">https://en.wikipedia.org/wiki/Metaheuristic</a></li>
</ul>


            </article>

            
        </section>
    </body></html>
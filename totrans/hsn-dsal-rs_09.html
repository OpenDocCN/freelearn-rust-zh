<html><head></head><body>
        

                            
                    <h1 class="header-title">Ordering Things</h1>
                
            
            
                
<p><em>Tidy house, tidy mind</em> is a saying that, as in its German variation, implies that order plays an important part in our lives. Anyone who wants to maximize efficiency has to rely on order, or risk the occasional time-consuming search through the chaos that has slowly unfolded. Having things in a particular order is great; it's the process of getting there that is expensive.</p>
<p>This often does not feel like a good use of our time, or simply may not be worth it. While a computer does not exactly feel, the time required to sort things is of a similar cost. Minimizing this time is the goal of inventing new algorithms and improving their efficiency, which is necessary for a task as common as sorting. A call to <kbd>mycollection.sort()</kbd> is not expected to take seconds (or minutes or even hours), so this is also a matter of usability. In this chapter, we will explore several solutions for that, so you can look forward to learning about the following:</p>
<ul>
<li>Implementing and analyzing sorting algorithms</li>
<li>Knowing more about (in)famous sorting strategies</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">From chaos to order</h1>
                
            
            
                
<p>There are many sorting algorithms (and their individual variations), each with their individual characteristics. Since it is impossible to cover every algorithm in a single chapter, and considering their limited usefulness, this chapter covers a selected few.</p>
<p>The selection should show the different strategies that are common in sorting a collection of items, many of which have been implemented in various libraries across different languages. Since many of you will never implement any sorting algorithms for productive use, this section is supposed to familiarize you with what's behind the scenes when a call to <kbd>mycollection.sort()</kbd> is issued, and why this could take a surprising amount of time.</p>
<p>Sorting algorithms fall into a group on each of these properties:</p>
<ul>
<li><strong>Stable</strong>: Maintains a relative order when comparing equal values</li>
<li><strong>Hybrid</strong>: Combines two or more sorting approaches (for example, by collection length)</li>
<li><strong>In-place</strong>: Uses indices instead of full copies for passing collections around</li>
</ul>
<p>While stable and hybrid algorithms are more complex and, in many cases, at a higher level (because they combine various approaches), in-place sorting is common and reduces the space and amount of copying an algorithm has to do.</p>
<p>We have touched on a very basic sorting algorithm already: <strong>insertion sort</strong>. It is the exact algorithm most real life things are done with: when adding a new book to a bookshelf, most people will pick up the book, look at the property to order by (such as the author's last name), and find the spot in their current collection, starting from the letter <em>A</em>. This is a very efficient approach and is used to build a new collection with minimal overhead, but it does not warrant its own section.</p>
<p>Let's start off with an absolute classic that is always a part of any university's curriculum because of its simplicity: bubble sort.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Bubble sort</h1>
                
            
            
                
<p>Bubble sort is the infamous algorithm that university students often learn as their first sorting algorithm. In terms of performance and runtime complexity, it is certainly among the worst ways to sort a collection, but it's great for teaching.</p>
<p>The principle is simple: walk through an array, scanning two elements and bringing them into the correct order by swapping. Repeat these steps until no swaps occur. The following diagram shows this process on the example array <kbd>[8, 9, 7, 6]</kbd>, where a total of four swaps establishes the order of <kbd>[6, 7, 8, 9]</kbd> by repeatedly comparing two succeeding elements:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/49af9e15-7037-4151-87d3-8a79f6380e00.png" style="width:10.42em;height:44.58em;"/></p>
<p>This diagram also shows an interesting (and name-giving) property of the algorithm: the "bubbling up" of elements to their intended position. The number <kbd>6</kbd> in the diagram travels, swap by swap, from the last position to the first position in the collection.</p>
<p>When this is transformed into Rust code, the simplicity remains: two nested loops iterate over the collection, whereas the outer loop could just as well run till infinity, since the inner portion does all the comparing and swapping.</p>
<p>Bubble sort is, infamously, a short snippet of code:</p>
<pre>pub fn bubble_sort&lt;T: PartialOrd + Clone&gt;(collection: &amp;[T]) -&gt; Vec&lt;T&gt; {<br/>    let mut result: Vec&lt;T&gt; = collection.into();<br/>    for _ in 0..result.len() {<br/>        let mut swaps = 0;<br/>        for i in 1..result.len() {<br/>            if result[i - 1] &gt; result[i] {<br/>                result.swap(i - 1, i);<br/>                swaps += 1;<br/>            }<br/>        }<br/>        if swaps == 0 {<br/>            break;<br/>        }<br/>    }<br/>    result<br/>}</pre>
<p>For easier handling, the algorithm creates a copy of the input array (using the <kbd>Into&lt;T&gt;</kbd> trait's <kbd>into()</kbd> method) and swaps around elements using the <kbd>swap()</kbd> method provided by <kbd>Vec&lt;T&gt;</kbd>.</p>
<p>The nested loops already hint toward the (worst case) runtime complexity: <em>O(n²)</em>. However, thanks to the early stopping when there are no swaps in a run, a partially ordered collection will be sorted surprisingly quickly. In fact, the best case scenario is really fast with bubble sort, since it's basically a single run-through (in other words, <em>O(n)</em> in this case).</p>
<p>The following chart shows three cases: sorting an already sorted collection (ascending numbers and descending numbers), as well as sorting a randomly shuffled array of distinct numbers:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/6d516145-94c2-4c8b-8794-612388aa77f5.png" style="width:61.42em;height:38.58em;"/></p>
<p>The output graph comparison between Bubble sort ascending, descending, and randomly sorted arrays</p>
<p>The algorithm will produce an ascending sequence, yet the shuffled collection shows a worse absolute runtime than the traditional worst case: a collection sorted in descending order. In any case, the exponential nature of these runtimes shows why bubble sort is not fit for real-world use.</p>
<p>Shell sort is sometimes dubbed as an optimized version of bubble sort!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Shell sort</h1>
                
            
            
                
<p>Bubble sort always compares an element to the neighboring element, but is this important? Many would say that it depends on the pre-existing order of the unsorted collection: are these future neighbors far apart or close together?</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Donald Shell, the inventor of shell sort, must have had a similar idea and used a "gap" between elements to make further jumps with the swapping approach adopted by bubble sort. By utilizing a specified strategy to choose those gaps, the runtime can change dramatically. Shell's original strategy is to start with half the collection's length and, by halving the gap size until zero, a runtime of <em>O(n²)</em> is achieved. Other strategies include choosing numbers based on some form of calculation of the current iteration <em>k</em> (for example, <em>2<sup>k</sup> - 1</em>), or empirically collected gaps (<a href="http://sun.aei.polsl.pl/~mciura/publikacje/shellsort.pdf">http://sun.aei.polsl.pl/~mciura/publikacje/shellsort.pdf</a>), which do not have a fixed runtime complexity yet!<br/></p>
<p>The following diagram explains some of the workings of shell sort. First, the initial gap is chosen, which is <kbd>n / 2</kbd> in the original paper. Starting at that gap (<kbd>2</kbd>, in this particular example), the element is saved and compared to the element <em>at the other end of the gap</em>, in other words, the current index minus the gap:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c0ef36fb-3d18-4a9a-9dc4-22eacb87ec54.png" style="width:21.92em;height:31.00em;"/></p>
<p>If the element at the other end of the gap is greater, it replaces the origin. Then, the process walks toward index zero with gap-sized steps, so the question becomes: what is going to fill that hole (<kbd>7</kbd> is overwritten by <kbd>8</kbd>, so the hole is where <kbd>8</kbd> was)—the original element, or element "gap" steps before it?</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In this example, it's <kbd>7</kbd>, since there is no preceding element. In longer collections, a lot more moving around can occur before the original element is inserted. After this insertion process has finished for index 2, it's repeated for index 3, moving from the gap toward the end of the collection. Following that, the gap size is reduced (in our case, by half) and the insertion steps are repeated until the collection is in order (and the gap size is zero).</p>
<p>Words, and even an image, make it surprisingly hard to understand what is going on. Code, however, shows the workings nicely:</p>
<pre>pub fn shell_sort&lt;T: PartialOrd + Clone&gt;(collection: &amp;[T]) -&gt; Vec&lt;T&gt; {<br/>    let n = collection.len();<br/>    let mut gap = n / 2;<br/>    let mut result: Vec&lt;T&gt; = collection.into();<br/><br/>    while gap &gt; 0 {<br/>        for i in gap..n {<br/>            let temp = result[i].clone();<br/><br/>            let mut j = i;<br/>            while j &gt;= gap &amp;&amp; result[j - gap] &gt; temp {<br/>                result[j] = result[j - gap].clone();<br/>                j -= gap;<br/>            }<br/>            result[j] = temp;<br/>        }<br/>        gap /= 2;<br/>    }<br/>    result<br/>}</pre>
<p>This snippet shows the value of shell sort: with the correct gap strategy, it can achieve results that are similar to more sophisticated sorting algorithms, but it is a lot shorter to implement and understand. Because of this, it can be a good choice for embedded use cases, where no library and only limited space is available.</p>
<p>The actual performance on the test sets is good:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/13270bdd-7cb8-4119-ab99-f66fe233620e.png" style="width:51.08em;height:31.58em;"/></p>
<p>The output graph comparison between shell sort ascending, descending, and randomly sorted arrays</p>
<p>Even with the original gap strategy that is said to produce <em>O(n²)</em> runtimes, the random set produces something more akin to linear behavior. Definitely a solid performance, but can it compare to heap sort?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Heap sort</h1>
                
            
            
                
<p>Ordering numbers was already a topic that we covered earlier in this book (<a href="84f203ac-a9f6-498b-90ff-e069c41aaca0.xhtml"/><a href="84f203ac-a9f6-498b-90ff-e069c41aaca0.xhtml">Chapter 5</a>, <em>Robust Trees</em>) while discussing trees: with heaps. A heap is a tree-like data structure with the highest (max-heap) or lowest number (min-heap) at the root that maintains order when inserting or removing elements. Hence, a sorting mechanism could be as simple as inserting everything into a heap and retrieving it again!</p>
<p class="mce-root"/>
<p>Since a (binary) heap has a known runtime complexity of <em>O(log n)</em>, and the entire array has to be inserted, the estimated runtime complexity will be <em>O(n log n)</em>, among the best sorting performances in sorting. The following diagram shows the binary heap in tree notation on the right, and the array implementation on the left:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/87a58184-896d-489c-8fe7-5d4ce6ea28e3.png" style="width:17.75em;height:27.75em;"/></p>
<p class="mce-root">In the Rust standard library, there is a <kbd>BinaryHeap</kbd> structure available, which makes the implementation quick and easy:</p>
<pre>pub fn heap_sort&lt;T: PartialOrd + Clone + Ord&gt;(collection: &amp;[T]) -&gt; Vec&lt;T&gt; {<br/>    let mut heap = BinaryHeap::new();<br/>    for c in collection {<br/>        heap.push(c.clone());<br/>    }<br/>    heap.into_sorted_vec()<br/>}</pre>
<p>The fact that a heap is used to do the sorting will generate fairly uniform outcomes, making it a great choice for unordered collections, but an inferior choice for presorted ones. This is due to the fact that a heap is filled and emptied, regardless of the pre-existing ordering. Plotting the different cases shows almost no difference:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/2ba32ddb-8ca6-4a97-b5ce-4e9e58789cf4.png" style="width:59.58em;height:37.17em;"/></p>
<p>The output graph comparison between heap sort ascending, descending, and randomly sorted arrays</p>
<p>A very different strategy, called <em>divide and conquer</em>, is employed by an entire group of algorithms. This group is what we are going to explore now, starting with merge sort.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Merge sort</h1>
                
            
            
                
<p>One fundamental strategy in battle, as well as in sorting collections, is to divide and conquer. Merge sort does exactly that, by splitting the collection in half recursively until only a single element remains. The merging operation can then put these single elements together in the correct order with the benefit of working with presorted collections.</p>
<p>What this does is reduce the problem size (in other words, the number of elements in the collection) to more manageable chunks that come presorted for easier comparison, resulting in a worst case runtime complexity of <em>O(n log n)</em>. The following diagram shows the split and merge process (note that comparing and ordering only starts at the merge step):</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/8471c780-90ab-4e95-98a8-0162ddaff0f3.png" style="width:16.67em;height:20.33em;"/></p>
<p>There are various implementations of this principle: bottom up, top down, using blocks, and other variations. In fact, as of 2018, Rust's default sorting algorithm is Timsort, a stable, hybrid algorithm that combines insertion sort (up until a certain size) with merge sort.</p>
<p>Implementing a vanilla merge sort in Rust is, again, a great place to use recursion. First, the left half is evaluated, then the right half of a sequence, and only then does merging begin, first by comparing the two sorted results (left and right) and picking elements from either side. Once one of these runs out of elements, the rest is simply appended since the elements are obviously larger. This result is returned to the caller, repeating the merging on a higher level until the original caller is reached.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Here's the Rust code for a typical merge sort implementation:</p>
<pre><br/>pub fn merge_sort&lt;T: PartialOrd + Clone + Debug&gt;(collection: &amp;[T]) -&gt; Vec&lt;T&gt; {<br/>    if collection.len() &gt; 1 {<br/>        let (l, r) = collection.split_at(collection.len() / 2);<br/>        let sorted_l = merge_sort(l);<br/>        let sorted_r = merge_sort(r);<br/>        let mut result: Vec&lt;T&gt; = collection.into();<br/>        let (mut i, mut j) = (0, 0);<br/>        let mut k = 0;<br/>        while i &lt; sorted_l.len() &amp;&amp; j &lt; sorted_r.len() {<br/>            if sorted_l[i] &lt;= sorted_r[j] {<br/>                result[k] = sorted_l[i].clone();<br/>                i += 1;<br/>            } else {<br/>                result[k] = sorted_r[j].clone();<br/>                j += 1;<br/>            }<br/>            k += 1;<br/>        }<br/><br/>        while i &lt; sorted_l.len() {<br/>            result[k] = sorted_l[i].clone();<br/>            k += 1;<br/>            i += 1;<br/>        }<br/><br/>        while j &lt; sorted_r.len() {<br/>            result[k] = sorted_r[j].clone();<br/>            k += 1;<br/>            j += 1;<br/>        }<br/><br/>        result<br/>    } else {<br/>        collection.to_vec()<br/>    }<br/>}</pre>
<p class="mce-root"/>
<p>This behavior also pays off, creating a quasi-linear runtime complexity, as shown in the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/24ed5a69-8014-4793-a25b-93bf72a593a0.png" style="width:61.92em;height:39.08em;"/></p>
<p>The output graph comparison between Quicksort asc, desc, and random</p>
<p>Another divide-and-conquer-type algorithm is Quicksort. It's a very interesting way to sort a list for a variety of reasons.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Quicksort</h1>
                
            
            
                
<p>This algorithm significantly outperformed merge sort in best case scenarios and was quickly adopted as Unix's default sorting algorithm, as well as in Java's reference implementation. By using a similar strategy to merge sort, Quicksort achieves faster average and best case speeds. Unfortunately, the worst case complexity is just as bad as bubble sort: <em>O(n²)</em>. How so? you might ask.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Quicksort operates, sometimes recursively, on parts of the full collection, and swaps elements around to establish an order. Hence, the critical question becomes: how do we choose these parts? This choosing bit is called the partitioning scheme and typically includes the swapping as well, not just choosing a split index. The choice is made by picking a pivot element, the value of which is what everything is compared with.</p>
<p>Everything less than the pivot value goes to one side, and everything greater goes to the other—by swapping. Once the algorithm detects a nice ascending (on the one side) and descending (from the other side) order, the split can be made where the two sequences intersect. Then, the entire process starts anew with each of the partitions.</p>
<p>The following illustration shows the picking and ordering of the elements based on the previous example collection. While the partitions in this example are only length one versus the rest, the same process would apply if these were longer sequences as well:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/71ad8529-8a9f-4779-bdf2-7b9ac0cf55f6.png" style="width:27.33em;height:36.00em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The partitioning scheme used here is called the Hoare scheme, named after the inventor of Quicksort, Sir Anthony Hoare, in 1959. There are other schemes (Lomuto seems to be the most popular alternative) that may provide better performance by trading off various other aspects, such as memory efficiency or the number of swaps. Whatever the partition scheme, picking a pivot value plays a major role in performance as well and the more equal parts it produces (like the median), the better the value is. Potential strategies include the following:</p>
<ul>
<li>Choosing the median</li>
<li>Choosing the arithmetic mean</li>
<li>Picking an element (random, first, or last, as chosen here)</li>
</ul>
<p>In Rust code, Quicksort is implemented in three functions:</p>
<ul>
<li>The public API to provide a usable interface</li>
<li>A wrapped recursive function that takes a low and high index to sort in-between</li>
<li>The partition function implementing the Hoare partition scheme</li>
</ul>
<p>This implementation can be considered in-place since it operates on the same vector that was provided in the beginning, swapping elements based on their indices. Here is the code:</p>
<pre>fn partition&lt;T: PartialOrd + Clone + Debug&gt;(<br/>    collection: &amp;mut [T],<br/>    low: usize,<br/>    high: usize,<br/>) -&gt; usize {<br/>    let pivot = collection[high].clone();<br/>    let (mut i, mut j) = (low as i64 - 1, high as i64 + 1);<br/><br/>    loop {<br/>        'lower: loop {<br/>            i += 1;<br/>            if i &gt; j || collection[i as usize] &gt;= pivot {<br/>                break 'lower;<br/>            }<br/>        }<br/><br/>        'upper: loop {<br/>            j -= 1;<br/>            if i &gt; j || collection[j as usize] &lt;= pivot {<br/>                break 'upper;<br/>            }<br/>        }<br/><br/>        if i &gt; j {<br/>            return j as usize;<br/>        }<br/>        collection.swap(i as usize, j as usize);<br/>    }<br/>}<br/><br/>fn quick_sort_r&lt;T: PartialOrd + Clone + Debug&gt;(collection: &amp;mut [T], low: usize, high: usize) {<br/>    if low &lt; high {<br/>        let pivot = partition(collection, low, high);<br/>        quick_sort_r(collection, low, pivot);<br/>        quick_sort_r(collection, pivot + 1, high);<br/>    }<br/>}<br/><br/>pub fn quick_sort&lt;T: PartialOrd + Clone + Debug&gt;(collection: &amp;[T]) -&gt; Vec&lt;T&gt; {<br/>    let mut result = collection.to_vec();<br/>    quick_sort_r(&amp;mut result, 0, collection.len() - 1);<br/>    result<br/>}</pre>
<p>Another new aspect in this implementation is the use of loop labels, which allow for better structure and readability. This is due to Hoare's use of a do-until type loop, a syntax that is not available in Rust, but that required the algorithm to avoid an infinite loop.</p>
<p>The <kbd>break</kbd>/<kbd>continue</kbd> instructions are relatives of the infamous go-to instruction, so they should only be used sparingly and with great care for the purpose of readability. Loop labels provide a tool to achieve that. They allow a reader to track exactly which loop is being exited or continued. The syntax leans slightly on that of the lifetimes: <kbd>'mylabel: loop { break 'mylabel; }</kbd>.</p>
<p>Quicksort's performance characteristics are definitely interesting. The rare worst case behavior or <em>O(n²)</em> has triggered many optimizations over the decades since its invention, the latest of which is called Dual-Pivot Quicksort from 2009, which has been adopted in Oracle's library for Java 7. Refer to the <em>Further reading</em> section for a more detailed explanation.</p>
<p>Running the original Quicksort on the previous dataset, the worst case and best case behaviors are clearly visible. The performance on the descending and (curiously) the ascending datasets is clearly <em>O(n²)</em>, while the randomized array is quickly processed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0ef3c141-c993-4c43-957c-2fbef567911d.png" style="width:56.50em;height:35.75em;"/></p>
<p>The output graph comparison between Quicksort ascensding, descending and randomly sorted arrays</p>
<p>This behavior speaks for the Quicksort's strong sides, which are more "real-world" type scenarios, where the worst case rarely appears. In current libraries around various programming languages though, sorting is done in a hybrid fashion, which means that these generic algorithms are used according to their strengths. This approach is called <strong>Introsort</strong> (from introspective sort) and, in C++'s <kbd>std::sort</kbd>, relies on Quicksort up to a certain point. Rust's standard library, however, uses Timsort.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Putting things in order is a very fundamental problem that has been solved in many different ways, varying in aspects such as worst-case runtime complexity, memory required, the relative order of equal elements (stability), as well as overall strategies. A few fundamental approaches were presented in this chapter.</p>
<p><strong>Bubble sort</strong> is one of the simplest algorithms to implement, but it comes at a high runtime cost, with a worst-case behavior of <em>O(n²)</em>. This is due to the fact that it simply swaps elements based on a nested loop, which makes elements "bubble up" to either end of the collection.</p>
<p><strong>Shell sort</strong> can be seen as an improved version of bubble sort, with a major upside: it does not start off by swapping neighbors. Instead, there is a gap that elements are compared and swapped across, covering a greater distance. This gap size changes with every round that shows worst-case runtime complexities of <em>O(n²)</em> for the original scheme to <em>O(n log n)</em> in the fastest variant. In fact, the runtime complexity of some empirically derived gaps cannot even be measured reliably!</p>
<p><strong>Heap sort</strong> makes use of a data structure's property to create a sorted collection. The heap, as presented earlier, retains the largest (or smallest) element at its root, returning it at every <kbd>pop()</kbd>. Heap sort therefore simply inserts the entire collection into a heap, only to retrieve it one by one in a sorted fashion. This leads to a runtime complexity of <em>O(n log n)</em>.</p>
<p>Tree-based strategies are also found in <strong>merge sort</strong>, a divide-and-conquer approach. This algorithm recursively splits the collection in half to sort the subset before working on the entire collection. This work is done when returning from the recursive calls when the resulting sub-collections have to be merged, hence the name. Typically, this will exercise a runtime complexity of <em>O(n log n)</em>.</p>
<p><strong>Quicksort</strong> also uses a divide-and-conquer approach, but instead of simply breaking the collection in half every time, it works with a pivot value, where the other values are swapped before looking at each sub-collection. This results in a worst-case behavior of <em>O(n²)</em>, but Quicksort is often used for its frequent average complexity of <em>O(n log n)</em>.</p>
<p>Nowadays, standard libraries use hybrid approaches such as Timsort, Introsort, or pattern-defeating Quicksort to get the best absolute and relative runtime performance. Rust's standard library provides either a stable sorting function for slices (<kbd>slice::sort()</kbd> versus <kbd>slice::sort_unstable()</kbd>) based on merge sort, and an unstable sorting function based on the pattern-defeating Quicksort.</p>
<p>This chapter aimed to be the basis for the next chapter, which will cover how to find a specific element, something that typically requires a sorted collection!</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Questions</h1>
                
            
            
                
<ul>
<li>Why is sorting an important aspect of programming?</li>
<li>What makes values bubble up in bubble sort?</li>
<li>Why is shell sort useful?</li>
<li>Can heap sort outperform bubble sort in its best case scenario?</li>
<li>What do merge sort and Quicksort have in common?</li>
<li>What are hybrid sorting algorithms?</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Further reading</h1>
                
            
            
                
<p>Here is some additional reference material that you may refer to regarding what has been covered in this chapter:</p>
<ul>
<li><em>Dual-Pivot Quicksort</em> (<a href="https://web.archive.org/web/20151002230717/http://iaroslavski.narod.ru/quicksort/DualPivotQuicksort.pdf">https://web.archive.org/web/20151002230717/http://iaroslavski.narod.ru/quicksort/DualPivotQuicksort.pdf</a>)</li>
<li>C++ sorting explained (<a href="https://medium.com/@lucianoalmeida1/exploring-some-standard-libraries-sorting-functions-dd633f838182">https://medium.com/@lucianoalmeida1/exploring-some-standard-libraries-sorting-functions-dd633f838182</a>)</li>
<li>Wikipedia on Introsort (<a href="https://en.wikipedia.org/wiki/Introsort">https://en.wikipedia.org/wiki/Introsort</a>)</li>
<li>Wikipedia on Timsort (<a href="https://en.wikipedia.org/wiki/Timsort">https://en.wikipedia.org/wiki/Timsort</a>)</li>
<li>Pattern defeating Quicksort (<a href="https://github.com/orlp/pdqsort">https://github.com/orlp/pdqsort</a>)</li>
</ul>


            

            
        
    </body></html>
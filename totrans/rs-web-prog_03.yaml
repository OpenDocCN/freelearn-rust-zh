- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Handling HTTP Requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have structured our to-do module in a flexible, scalable, and reusable
    manner. However, this can only get us so far in terms of web programming. We want
    our to-do module to reach multiple people quickly without the user having to install
    Rust on their own computers. We can do this with a web framework. Rust has plenty
    to offer. Initially, we will build our main server in the **Actix Web** framework.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we will be building the views of the server in a modular fashion;
    we can slot our to-do module into our web application with minimal effort. It
    must be noted that the Actix Web framework defines views using `async` functions.
    Because of this, we will also cover asynchronous programming to get a better understanding
    of how the Actix Web framework works.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Actix Web framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launching a basic Actix Web server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding closures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding asynchronous programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding `async` and `await` with web programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing views using the Actix Web framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we move toward building web apps in Rust, we are going to have to start
    relying on third-party packages to do some of the heavy lifting for us. Rust manages
    dependencies through a package manager called **Cargo**. To use Cargo, we are
    going to have to install Rust on our computer from the following URL: [https://www.rust-lang.org/tools/install](https://www.rust-lang.org/tools/install).'
  prefs: []
  type: TYPE_NORMAL
- en: This installation delivers the Rust programming language and Cargo. You can
    find all the code files on GitHub at [https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter03](https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter03).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Actix Web framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of writing, Actix Web is the most popular Rust web framework as
    can be seen from the activity on the GitHub page. You might be tempted to jump
    into another framework that looks more ergonomic, such as Rocket, or one that
    is faster and more lightweight, such as Hyper. We will be covering these frameworks
    later in this book over various different chapters; however, we must remember
    that we are trying to get our heads around web programming in Rust first. Considering
    that we are new to Rust and web programming, Actix Web is a great start. It is
    not too low-level that we will get caught up with just trying to get a server
    to handle a range of views, database connections, and authentication. It is also
    popular, stable, and has a lot of documentation. This will facilitate a pleasant
    programming experience when trying to go beyond the book and develop your own
    web application. It is advised that you get comfortable with Actix Web before
    moving on to other web frameworks. This is not to say that Actix Web is the best
    and that all other frameworks are terrible; it is just to facilitate a smooth
    learning and development experience. With this in mind, we can now move on to
    the first section, where we set up a basic web server.
  prefs: []
  type: TYPE_NORMAL
- en: Launching a basic Actix Web server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building with Cargo is straightforward. All we need to do is navigate to a
    directory where we want to build our project and run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command builds a basic Cargo Rust project. When we explore this
    application, we get the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now define our Actix Web dependency in our `Cargo.toml` file with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result of the preceding code, we can now move on to building the web application.
    For now, we will put it all in our `src/main.rs` file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can see that we import the required structs and traits
    from the `actix_web` crate. We can see that we have used several different ways
    to define a view. We defined a view by building a function. This takes in an `HttpRequest`
    struct. It then gets `name` from the request and then returns a variable that
    can implement the `Responder` trait from the `actix_web` crate. The `Responder`
    trait converts our type into an HTTP response. We assign this `greet` function
    that we have created for our application server as the route view, with the `.route("/",
    web::get().to(greet))` command. We can also see that we can pass in the name from
    the URL to our `greet` function with the `.route("/{name}", web::get().to(greet))`
    command. Finally, we pass a closure into the final route. With our configuration,
    let’s run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following printout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see in the preceding output that, right now, there is no logging. This
    is expected, and we will configure logging later. Now that our server is running,
    for each of the following URL inputs, we should expect the corresponding outputs
    in the browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://127.0.0.1:8080/`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Hello World!`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http://127.0.0.1:8080/maxwell`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Hello maxwell!`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http://127.0.0.1:8080/say/hello`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Hello Again!`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preceding code in the `src/main.rs` file, we can see that there is some
    new syntax that we have not come across before. We have decorated our `main` function
    with the `#[actix_web::main]` macro. This marks our `async` `main` function as
    the Actix Web system entry point. With this, we can see that our functions are
    `async` and that we are using closures to build our server. We will go through
    both concepts in the next couple of sections. In the next section, we will investigate
    closures to truly understand what is happening.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding closures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Closures are, essentially, functions, but they are also anonymous, meaning
    that they do not have names. This means that closures can be passed around into
    functions and structs. However, before we delve into passing closures around,
    let us explore closures by defining a basic closure in a blank Rust program (you
    can use the Rust playground if you prefer) with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code will give us the following printout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding output, we can see that our closure behaves like a function.
    However, instead of using curly brackets to define the inputs, we use pipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might have noticed in the preceding closure that we have not defined the
    data type for the `string_input` parameter; however, the code still runs. This
    is different from a function that needs to have the parameter data types defined.
    This is because functions are part of an explicit interface that is exposed to
    users. A function could be called anywhere in the code if the code can access
    the function. Closures, on the other hand, have a short lifetime and are only
    relevant to the scope that they are in. Because of this, the compiler can infer
    the type being passed into the closure from the use of the closure in the scope.
    Because we are passing in `&str` when we call the closure, the compiler knows
    that the `string_input` type is `&str`. While this is convenient, we need to know
    that closures are not generic. This means that a closure has a concrete type.
    For instance, after defining our closure, let’s try and run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The error occurs because the first call to our closure tells the compiler that
    we are expecting `&str`, so the second call breaks the compilation process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scopes do not just affect closures. Closures adhere to the same scope rules
    that variables do. For instance, let’s say we were going to try and run the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It would refuse to compile because when we try and call the closure, it is not
    in the scope of the call. Considering this, you would be right to assume that
    other scope rules apply to closures. For instance, if we tried to run the following
    code, what do you think would happen?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If you thought that we would get the following output, you would be right:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Unlike functions, closures can access variables in their own scope. So, to try
    and describe closures in a simplistic way that we can understand, they are kind
    of like dynamic variables in a scope that we call to perform a computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can take ownership of the outside variables used in the closure by utilizing
    `move`, as seen with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Because `move` is utilized in the closure defined here, the `another_str` variable
    cannot be used after `test_closure` is declared because `test_closure` took ownership
    of `another_str`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also pass closures into a function; however, it must be noted that we
    can also pass functions into other functions. We can achieve passing functions
    into other functions with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can see that we define a closure that doubles an
    integer that is passed in and returned. We then pass this into our `add_doubles`
    function with the notation of `fn(i32)-> i32`, which is known as a function pointer.
    When it comes to closures, we can implement one of the following traits:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Fn`: Immutably borrows variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FnMut`: Mutably borrows variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FnOnce`: Takes ownership of variables so it can only be called once'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can pass a closure that has one of the preceding traits implemented into
    our `add_doubles` function with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the `closure` function parameter has the `Box<dyn Fn(i32)
    -> i32>` signature. This means that the `add_doubles` function is accepting closures
    that have implemented the `Fn` trait that accepted `i32`, and returned `i32`.
    The `Box` struct is a smart pointer where we have put the closure on the heap
    because we do not know the closure’s size at compile time. You can also see that
    we have utilized `move` when defining the closure. This is because we are using
    the `one` variable, which is outside the closure. The `one` variable may not live
    long enough; therefore, the closure takes ownership of it because we used `move`
    when defining the closure.
  prefs: []
  type: TYPE_NORMAL
- en: 'With what we have covered about closures in mind, we can have another look
    at the `main` function in our server application with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can see that we are running our `HttpServer` after
    constructing it using the `HttpServer::new` function. Knowing what we know now,
    we can see that we have passed in a closure that returns the `App` struct. Based
    on what we know about closures, we can be more confident with what we do with
    this code. We can essentially do what we like within the closure if it returns
    the `App` struct. With this in mind, we can get some more information about the
    process with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can see that we have added a `print` statement to
    tell us that the closure is firing. We also added another function called `workers`.
    This means we can define how many workers are being used to create our server.
    We also print out that the server factory is firing in our closure. Running the
    preceding code gives us the following printout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The preceding result tells us that the closure was fired three times. Altering
    the number of workers shows us that there is a direct relationship between this
    and the number of times the closure is fired. If the `workers` function is left
    out, then the closure is fired in relation to the number of cores your system
    has. We will explore how these workers fit into the server process in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the nuances around the building of the `App` struct,
    it is time to look at the main change in the structure of a program, asynchronous
    programming.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding asynchronous programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until this chapter, we have been writing code in a sequential manner. This
    is good enough for standard scripts. However, in web development, asynchronous
    programming is important, as there are multiple requests to servers, and API calls
    introduce idle time. In some other languages, such as Python, we can build web
    servers without touching any asynchronous concepts. While asynchronous concepts
    are utilized in these web frameworks, the implementation is defined under the
    hood. This is also true for the Rust framework Rocket. However, as we have seen,
    it is directly implemented in Actix Web.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to utilizing asynchronous code, there are two main concepts we
    must understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Processes**: A process is a program that is being executed. It has its own
    memory stack, registers for variables, and code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`main` program. However, threads do not share the stack.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is demonstrated in the following classic diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Relationship between threads and processes [source: Cburnett
    (2007) (https://commons.wikimedia.org/wiki/File:Multithreaded_process.svg), CC
    BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0/deed.en)]](img/Figure_3.1_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1 – Relationship between threads and processes [source: Cburnett (2007)
    (https://commons.wikimedia.org/wiki/File:Multithreaded_process.svg), CC BY-SA
    3.0 (https://creativecommons.org/licenses/by-sa/3.0/deed.en)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand what threads are and what relation they have to our
    code on a high-level basis, we can play with a toy example to understand how to
    utilize threads in our code and see the effects of these threads firsthand. A
    classic example is to build a basic function that merely sleeps, blocking time.
    This can simulate a time-expensive function such as a network request. We can
    run it sequentially with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code will give us the following printout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding output, we can see that our time-expensive functions run in
    the order that we expect them to. It also takes just over 6 seconds to run the
    entire program, which makes sense since we are running three expensive functions
    that sleep at 2 seconds each. Our expensive function also returns the value `2`.
    When we add the results of all three expensive functions together, we are going
    to get a result of  the value `6`, which is what we have. We speed up our program
    to roughly 2 seconds for the entire program, by spinning up three threads at the
    same time and waiting for them to complete before moving on. Waiting for the threads
    to complete before moving on is called *joining*. So, before we start spinning
    off threads, we must import the `join` handler with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now spin up threads in our `main` function with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code gives us the following printout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the whole process took just over 2 seconds to run. This is because
    all three threads are running concurrently. Note also that thread three is fired
    before thread two. Do not worry if you get a sequence of `1`, `2`, and `3`. Threads
    finish in an indeterminate order. The scheduling is deterministic; however, there
    are thousands of events happening under the hood that require the CPU to do something.
    As a result, the exact time slices that each thread gets are never the same. These
    tiny changes add up. Because of this, we cannot guarantee that the threads will
    finish in a determinate order.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at how we spin off threads, we can see that we pass a closure into
    our thread. If we try and just pass the `do_something` function through the thread,
    we get an error complaining that the compiler expected an `FnOnce<()>` closure
    and found `i8` instead. This is because a standard closure implements the `FnOnce<()>`
    public trait, whereas our `do_something` function simply returns `i8`. When `FnOnce<()>`
    is implemented, the closure can only be called once. This means that when we create
    a thread, we can ensure that the closure can only be called once, and then when
    it returns, the thread ends. As our `do_something` function is the final line
    of the closure, `i8` is returned. However, it has to be noted that just because
    the `FnOnce<()>` trait is implemented, it does not mean that we cannot call it
    multiple times. This trait only gets called if the context requires it. This means
    that if we were to call the closure outside of the thread context, we could call
    it multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note also that we directly unwrap our results. From what we know, we can deduce
    that the `join` function on the `JoinHandle` struct returns `Result`, which we
    also know can be `Err` or `Ok`. We know it is going to be okay to unwrap the result
    directly because we are merely sleeping and then returning an integer. We also
    printed out the results, which were indeed integers. However, our error is not
    what you would expect. The full `Result` type we get is `Result<i8, Box<dyn Any
    + Send>>`. We already know what `Box` is; however, `dyn Any + Send` seems new.
    `dyn` is a keyword that we use to indicate what type of trait is being used. `Any`
    and `Send` are two traits that must be implemented. The `Any` trait is for dynamic
    typing, meaning that the data type can be anything. The `Send` trait means that
    it is safe to be moved from one thread to another. The  `Send` trait also means
    that it is safe to copy from one thread to another. So, what we are sending has
    implemented the `Copy` trait as what we are sending can be sent between threads.
    Now that we understand this, we can handle the results of the threads by merely
    matching the `Result` outcome, and then downcasting the error into a string to
    get the error message with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code enables us to manage the results of threads gracefully. Now,
    there is nothing stopping you from logging failures of threads or spinning up
    new threads based on the outcomes of previous threads. Thus, we can see how powerful
    the `Result` struct is. There is more we can do with threads, such as give them
    names or pass data between them with channels. However, the focus of this book
    is web programming, not advanced concurrency design patterns and concepts. However,
    further reading on the subject is provided at the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We now understand how to spin up threads in Rust, what they return, and how
    to handle them. With this information, we can move on to the next section about
    understanding the `async` and `await` syntax, as this is what will be used in
    our Actix Web server.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding async and await
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `async` and `await` syntax manages the same concepts covered in the previous
    section; however, there are some nuances. Instead of simply spawning off threads,
    we create **futures** and then manipulate them as and when needed.
  prefs: []
  type: TYPE_NORMAL
- en: In computer science, a future is an unprocessed computation. This is where the
    result is not yet available, but when we call or wait, the future will be populated
    with the result of the computation. Another way of describing this is that a future
    is a way of expressing a value that is not yet ready. As a result, a future is
    not exactly a thread. In fact, threads can use futures to maximize their potential.
    For instance, let us say that we have several network connections. We could have
    an individual thread for each network connection. This is better than sequentially
    processing all connections, as a slow network connection would prevent other faster
    connections from being processed down the line until it itself is processed, resulting
    in a slower processing time overall. However, spinning up threads for every network
    connection is not free. Instead, we can have a future for each network connection.
    These network connections can be processed by a thread from a thread pool when
    the future is ready. Therefore, we can see why futures are used in web programming,
    as there are a lot of concurrent connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Futures can also be referred to as *promises*, *delays*, or *deferred*. To
    explore futures, we will create a new Cargo project and utilize the futures created
    in the `Cargo.toml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding crate installed, we can import what we need in our `main.rs`
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can define futures by merely using the `async` syntax. The `block_on` function
    will block the program until the future we defined has been executed. We can now
    define the `do_something` function with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `do_something` function essentially does what the code says it does, which
    is print out what number it is, sleep for 2 seconds, and then return an integer.
    However, if we were to directly call it, we would not get `i8`. Instead, calling
    the `do_something` function directly will give us `Future<Output = i8>`. We can
    run our future and time it in the main function with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code will give us the following printout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what is expected. However, let’s see what happens if we enter an extra
    `sleep` function before we call the `block_on` function with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following printout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we can see that our future does not execute until we apply an executor
    using the `block_on` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be a bit laborious, as we may just want a future that we can execute
    later in the same function. We can do this with the `async`/`await` syntax. For
    instance, we can call the `do_something` function and block the code until it
    is finished using the `await` syntax inside the `main` function, with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: What the `async` block does is return a future. Inside this block, we call the
    `do_something` function blocking the `async` block until the `do_something` function
    is resolved, by using the `await` expression. We then apply the `block_on` function
    on the `future_two` future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at our preceding code block, this might seem a little excessive, as
    it can be done with just two lines of code that call the `do_something` function
    and pass it to the `block_on` function. In this case, it is excessive, but it
    can give us more flexibility on how we call futures. For instance, we can call
    the `do_something` function twice and add them together as a return with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding the preceding code to our `main` function will give us the following
    printout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Whilst the preceding output is the result that we are expecting, we know that
    these futures will run sequentially and that the total time for this block of
    code will be just above 4 seconds. Maybe we can speed this up by using `join`.
    We have seen `join` speed up threads by running them at the same time. It does
    make sense that it will also work to speed up our futures. First, we must import
    the `join` macro with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now utilize `join` for our futures and time the implementation with
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can see that the `join` macro returns a tuple of
    the results and that we unpack the tuple to give us the same result. However,
    if we do run the code, we can see that although we get the result that we want,
    our future execution does not speed up and is still stuck at just above 4 seconds.
    This is because a future is not being run using an `async` task. We will have
    to use `async` tasks to speed up the execution of our futures. We can achieve
    this by carrying out the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the futures needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Put them into a vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loop through the vector, spinning off tasks for each future in the vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join the `async` tasks and sum the vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This can be visually mapped out with the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – The steps to running multiple futures at once](img/Figure_3.2_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – The steps to running multiple futures at once
  prefs: []
  type: TYPE_NORMAL
- en: 'To join all our futures at the same time, we will have to use another crate
    to create our own asynchronous `join` function by using the `async_std` crate.
    We define this crate in the `Cargo.toml` file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the `async_std` crate, we can import what we need to carry
    out the approach laid out in *Figure 3**.2*, by importing what we need at the
    top of the `main.rs` file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `main` function, we can now define our future with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that we define our futures (*1*), and then we add them to
    our vector (*2*). We then loop through our futures in our vector using the `into_iter`
    function. We then spawn a thread on each future using `async_std::task::spawn`.
    This is similar to `std::task::spawn`. So, why bother with all this extra headache?
    We could just loop through the vector and spawn a thread for each task. The difference
    here is that the `async_std::task::spawn` function is spinning off an `async`
    task in the same thread. Therefore, we are concurrently running both futures in
    the same thread! We then join all the handles, `await` for these tasks to finish,
    and then return the sum of all these threads. Now that we have defined our `async_outcome`
    future, we can run and time it with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Running our additional code will give the following additional printout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: It’s working! We have managed to get two `async` tasks running at the same time
    in the same thread, resulting in both futures being executed in just over 2 seconds!
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, spawning threads and `async` tasks in Rust is straightforward.
    However, we must note that passing variables into threads and `async` tasks is
    not. Rust’s borrowing mechanism ensures memory safety. We must go through extra
    steps when passing data into a thread. Further discussion on the general concepts
    behind sharing data between threads is not conducive to our web project. However,
    we can briefly signpost what types allow us to share data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::sync::Arc`: This type enables threads to reference outside data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`std::sync::Mutex`: This type enables threads to mutate outside data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Inside the thread here, we dereference the result of the lock, unwrap it, and
    mutate it. It must be noted that the shared state can only be accessed once the
    lock is held.
  prefs: []
  type: TYPE_NORMAL
- en: We have now covered enough of async programming to return to our web programming.
    Concurrency is a subject that can be covered in an entire book, one of which is
    referenced in the *Further reading* section. For now, we must get back to exploring
    Rust in web development to see how our knowledge of Rust async programming affects
    how we understand the Actix Web server.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring async and await with web programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Knowing what we know about async programming, we can now see the `main` function
    in our web application in a different light, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We know that our `greet` function is an `async` function and thus a future.
    We can also see that the closure we pass into the `/say/hello` view also utilizes
    the `async` syntax. We can also see that the `HttpServer::new` function utilized
    the `await` syntax in `async fn main()`. Therefore, we can deduce that our `HttpServer::new`
    function is an executor. However, if we were to remove the `#[actix_web::main]`
    macro, we would get the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: This is because our `main` function, which is our entry point, would return
    a future as opposed to running our program. `#[actix_web::main]` is a runtime
    implementation and enables everything to be run on the current thread. The `#[actix_web::main]`
    macro marks the `async` function (which, in this case, is the `main` function)
    to be executed by the Actix system.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At the risk of getting into the weeds here, the Actix crate runs concurrent
    computation based on the actor model. This is where an actor is a computation.
    Actors can send and receive messages to and from each other. Actors can alter
    their own state, but they can only affect other actors through messages, which
    removes the need for lock-based synchronization (the mutex we covered is lock-based).
    Further exploration of this model will not help us to develop basic web apps.
    However, the Actix crate does have good documentation on coding concurrent systems
    with Actix at [https://actix.rs/book/actix](https://actix.rs/book/actix).
  prefs: []
  type: TYPE_NORMAL
- en: We have covered a lot here. Do not feel stressed if you do not feel like you
    have retained all of it. We’ve briefly covered a range of topics around asynchronous
    programming. We do not need to understand it inside out to start building applications
    based on the Actix Web framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may also feel like we have been excessive in what we have covered. For
    instance, we could have spun up a server and used the `async` syntax when needed
    to merely punch out views without really knowing what is going on. Not understanding
    what is going on but knowing where to put `async` would not have slowed us down
    when building our toy application. However, this whistle-stop tour is invaluable
    when it comes to debugging and designing applications. To establish this, we can
    look at an example in the wild. We can look at this smart *Stack Overflow* solution
    to running multiple servers in one file: [https://stackoverflow.com/questions/59642576/run-multiple-actix-app-on-different-ports](https://stackoverflow.com/questions/59642576/run-multiple-actix-app-on-different-ports).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in the *Stack Overflow* solution involves basically running two servers
    at one runtime. First, they define the views with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the views are defined, the two servers are defined in the `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'I have not added any notation to this code, but it should not intimidate you.
    We can confidently deduce that `s1` and `s2` are futures that the `run` function
    returns. We then join these two futures together and `await` for them to finish.
    There is also a slight difference between our code and the code in the *Stack
    Overflow* solution. Our solution utilizes `await?` and then returns `Ok` with
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'This is because a `?` operator is essentially a `try` match. `join(s1, s2).await?`
    expands roughly to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Whereas `join(s1, s2).await.unwrap()` expands roughly to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Because of the `?` operator, the person providing the solution has to insert
    `Ok` at the end because the `main` function returns `Result`, and this was taken
    away by implementing the `?` operator.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in the wild solution, *Stack Overflow* has demonstrated the importance
    of covering async programming. We can look at code in the wild and work out what
    is going on and how the posters on *Stack Overflow* managed to achieve what they
    did. This can also mean that we can get creative ourselves. There is nothing stopping
    us from creating three servers and running them in the `main` function. This is
    where Rust really shines. Taking the time to learn Rust gives us the ability to
    safely dive into low-level territory and have more fine-grain control over what
    we do. You will find this is true in any field of programming done with Rust.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one more concept that we should investigate before trying to build
    our application, and this is `main` function to be a future. If we look at the
    **Tokio** crate, we can see that it is an asynchronous runtime for the Rust programming
    language by providing the building blocks needed to write network applications.
    The workings of Tokio are complex; however, if we look at the Tokio documentation
    on speeding up the runtime, we can add diagrams like the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Speeding up the Tokio runtime [source: Tokio Documentation (2019)
    (https://tokio.rs/blog/2019-10-scheduler)]](img/Figure_3.3_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3 – Speeding up the Tokio runtime [source: Tokio Documentation (2019)
    ([https://tokio.rs/blog/2019-10-scheduler](https://tokio.rs/blog/2019-10-scheduler))]'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding figure, we can see that there are tasks queued up and processors
    processing them. We processed our tasks earlier, so this should look familiar.
    Considering this, it might not be too shocking to know that we can use Tokio instead
    of the Actix Web macro to run our server. To do this, we define our Tokio dependency
    in the `Cargo.toml` file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding code, we can now switch our macro in the `main.rs` file
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Running the preceding code will give us the same outcome as running a server.
    There might be some inconsistencies when using Tokio instead of our Actix runtime
    macro. While this is an interesting result that demonstrates how we can confidently
    configure our server, we will use the Actix runtime macro for the rest of the
    book when it comes to developing the to-do application in Actix. We will revisit
    Tokio in [*Chapter 14*](B18722_14.xhtml#_idTextAnchor279), *Exploring the Tokio
    Framework*.
  prefs: []
  type: TYPE_NORMAL
- en: We have now covered enough of server configuration and how the server processes
    requests to be productive. We can now move on to defining our views and how they
    are handled in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Managing views using the Actix Web framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have defined all our views in the `main.rs` file. This is fine for
    small projects; however, as our project grows, this will not scale well. Finding
    the right views can be hard, and updating them can lead to mistakes. It also makes
    it harder to remove modules from or insert them into your web application. Also,
    if we have all the views being defined on one page, this can lead to a lot of
    merge conflicts if a bigger team is working on the application, as they will all
    want to alter the same file if they are altering the definitions of views. Because
    of this, it is better to keep the logic of a set of views contained in a module.
    We can explore this by building a module that handles authentication. We will
    not be building the logic around authentication in this chapter, but it is a nice
    straightforward example to use when exploring how to manage the structure of a
    views module. Before we write any code, our web application should have the following
    file layout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The code inside each file can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`main.rs`: The entry point for the server where the server is defined'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`views/auth/login.rs`: The code defining the view for logging in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`views/auth/logout.rs`: The code defining the view for logging out'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`views/auth/mod.rs`: The factory that defines the views for `auth`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`views/mod.rs`: The factory that defines all the views for the whole app'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, let us start off our entry point with a basic web server with no extras
    in the `main.rs` file, with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'This preceding code is straightforward and there should be no surprises. We
    will alter the code later, and we can move on to defining the views. For this
    chapter, we just want to return a string saying what the view is. We will know
    that our application structure works. We can define our basic login view in the
    `views/auth/login.rs` file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it will not be surprising that the logout view in the `views/auth/logout.rs`
    file takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our views have been defined, all we need to do is define the factories
    in the `mod.rs` files to enable our server to serve them. Our factories give the
    data flow of our app, taking the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – The data flow of our application](img/Figure_3.4_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – The data flow of our application
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see in *Figure 3**.4* that chaining factories gives us a lot of flexibility.
    If we wanted to remove all the `auth` views from our application, we would be
    able to do this by simply removing one line of code in our main view factory.
    We can also reuse our modules. For instance, if we were to use the `auth` module
    on multiple servers, we could merely have a git submodule for the `auth` views
    module and use it on other servers. We can build our `auth` module factory view
    in the `views/auth/mod.rs` file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can see that we pass in a mutable reference of a
    `ServiceConfig` struct. This enables us to define things such as views on the
    server in different fields. The documentation on this struct states that it is
    to allow bigger applications to split up a configuration into different files.
    We then apply a service to the `ServiceConfig` struct. The service enables us
    to define a block of views that all get populated with the prefix defined in the
    scope. We also state that we are using `get` methods, for now, to make it easily
    accessible in the browser. We can now plug the `auth` views factory into the `main`
    views factory in the `views/mod.rs` file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we have been able to chop our entire views modules with
    just one line of code. We can also chain the modules as much as we like. For instance,
    if we wanted to have submodules within the `auth` views module, we could, and
    we merely feed the factories of those `auth` submodules into the `auth` factory.
    We can also define multiple services in a factory. Our `main.rs` file remains
    pretty much the same with the addition of a `configure` function, as seen with
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'When we call the `configure` function on the `App` struct, we pass the views
    factory into the `configure` function, which will pass the `config` struct into
    our factory function for us. As the `configure` function returns `Self`, meaning
    the `App` struct, we can return the result at the end of the closure. We can now
    run our server, resulting in the following outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – The login view](img/Figure_3.5_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – The login view
  prefs: []
  type: TYPE_NORMAL
- en: We can see that our application with the expected prefix works! With this, we
    have covered all the basics to handle HTTP requests with confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the basics of threading, futures, and `async` functions.
    As a result, we were able to look at a multi-server solution in the wild and understand
    confidently what was going on. With this, we built on the concepts we learned
    in the previous chapter to build modules that define views. In addition, we chained
    factories to enable our views to be constructed on the fly and added to a server.
    With this chained factory mechanism, we can slot entire view modules in and out
    of a configuration when the server is being built.
  prefs: []
  type: TYPE_NORMAL
- en: We also built a utility struct that defines a path, standardizing the definition
    of a URL for a set of views. In future chapters, we will use this approach to
    build authentication, JSON serialization, and frontend modules. With what we’ve
    covered, we’ll be able to build views that extract and return data from the user
    in a range of different ways in the next chapter. With this modular understanding,
    we have a strong foundation that enables us to build real-world web projects in
    Rust where logic is isolated and can be configured, and where code can be added
    in a manageable way.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will work on processing requests and responses. We will
    learn how to pass params, bodies, headers, and forms to views and process them
    by returning JSON. We will be using these new methods with the to-do module we
    built in the previous chapter to enable our interaction with to-do items through
    server views.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What parameter is passed into the `HttpServer::new` function and what does the
    parameter return?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is a closure different from a function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between a process and a thread?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between an `async` function and a normal one?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between `await` and `join`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the advantage of chaining factories?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A closure is passed into the `HttpServer::new` function. The `HttpServer::new`
    function has to return the `App` struct so that the `bind` and `run` functions
    can be acted on them after the `HttpServer::new` function has fired.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A closure can interact with variables outside of its scope.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A process is a program that is being executed with its own memory stack, registers,
    and variables, whereas a thread is a lightweight process that is managed independently
    but shares data with other threads and the main program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A normal function executes as soon as it is called, whereas an `async` function
    is a promise and must be executed with a blocking function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`await` blocks a program to wait for a future to be executed; however, the
    `join` function can run multiple threads or futures concurrently. `await` can
    also be executed on a `join` function.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chaining factories gives us flexibility on how individual modules are constructed
    and orchestrated. A factory inside a module focuses on how the module is constructed,
    and the factory outside the module focuses on how the different modules are orchestrated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Hands-On Concurrency with Rust* (2018) by *Brian Troutwine*, *Packt Publishing*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

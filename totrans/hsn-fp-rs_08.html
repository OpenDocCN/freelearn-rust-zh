<html><head></head><body>
        

                            
                    <h1 class="header-title">Implementing Concurrency</h1>
                
            
            
                
<p class="mce-root">Concurrency is the act of doing two things at the same time. On a single-core processor, this means <strong>multitasking</strong>. When multitasking, an operating system will switch between running processes to give each of them a share of time to use the processor. On a multi-core processor, concurrent processes can run simultaneously.</p>
<p>In this chapter, we will look at different models of concurrency. Some of these tools are relevant, others are used more for educational purposes. Here, we recommend and explain the thread model of concurrency. Further, we will explain how functional design patterns can make it easier to develop programs that use concurrency effectively.</p>
<p>Learning outcomes will include the following:</p>
<ul>
<li>Recognizing and applying subprocess concurrency appropriately</li>
<li>Understanding the nix fork concurrency model and its benefits</li>
<li>Recognizing and applying thread concurrency appropriately</li>
<li>Understanding Rust primitive <kbd>Send</kbd> and <kbd>Sync</kbd> traits</li>
<li>Recognizing and applying the actor design pattern</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>A recent version of Rust is necessary to run the examples provided:</p>
<p><a href="https://www.rust-lang.org/en-US/install.html">https://www.rust-lang.org/en-US/install.html</a></p>
<p>This chapter's code is also available on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Hands-On-Functional-Programming-in-RUST">https://github.com/PacktPublishing/Hands-On-Functional-Programming-in-RUST</a></p>
<p>Specific installation and build instructions are also included in each chapter's <kbd>README.md</kbd> file.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using subprocess concurrency</h1>
                
            
            
                
<p>A subprocess is a command that is started from within another process. As a simple example of this, let's create a parent process with three children. <kbd>process_a</kbd> will be the parent. Consider the following code snippet:</p>
<pre class="p1" style="padding-left: 30px">use std::process::Command;<br/>use std::env::current_exe;<br/><br/>fn main() {<br/>   let path = current_exe()<br/>             .expect("could not find current executable");<br/>   let path = path.with_file_name("process_b");<br/><br/>   let mut children = Vec::new();<br/>   for _ in 0..3 {<br/>      children.push(<br/>         Command::new(path.as_os_str())<br/>                 .spawn()<br/>                 .expect("failed to execute process")<br/>      );<br/>   }<br/>   for mut c in children {<br/>      c.wait()<br/>       .expect("failed to wait on child");<br/>   }<br/>}</pre>
<p>The child process, <kbd>process_b</kbd>, runs a loop and prints its own process ID. This is shown as follows:</p>
<pre class="p1" style="padding-left: 30px">use std::{thread,time};<br/>use std::process;<br/>fn main() {<br/>   let t = time::Duration::from_millis(1000);<br/>   loop {<br/>      println!("process b #{}", process::id());<br/>      thread::sleep(t);<br/>   }<br/>}</pre>
<p>If you run <kbd>process_a</kbd>, then you will see output from the three <kbd>process_b</kbd> processes:</p>
<pre class="p1" style="padding-left: 30px">process b #54061<br/>process b #54060<br/>process b #54059<br/>process b #54061<br/>process b #54059<br/>process b #54060</pre>
<p>If you inspect the process tree starting at <kbd>process_a</kbd>, then you will find that three <kbd>process_b</kbd> processes are attached as children, as shown in the following code:</p>
<pre class="p1" style="padding-left: 30px">$ ps -a | grep process_a<br/>54058 ttys001    0:00.00 process_a<br/>55093 ttys004    0:00.00 grep process_a<br/>$ pstree 54058<br/>54058 process_a<br/>&gt;   54059 process_b<br/>&gt;   54060 process_b<br/>&gt;   54061 process_b</pre>
<p>The preceding commands to inspect the process tree require a Unix-like Command Prompt. The subprocess module itself, though, is more or less platform-independent.</p>
<p>Subprocess concurrency is useful if you want to run and manage other projects or utilities. A good example of subprocess concurrency done right is the <kbd>cron</kbd> utility. <kbd>cron</kbd> accepts a configuration file that specifies different commands to be run, and a schedule of when to run them. <kbd>cron</kbd> continues to run in the background and at the appropriate time starts each configured process according to schedule.</p>
<p>Subprocess concurrency is not well suited for parallel computation in general. No resources will be shared between parent and child processes when using the <kbd>subprocess::Command</kbd> interface. Also, information cannot be shared easily between these processes.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding nix fork concurrency</h1>
                
            
            
                
<p>Before threads were introduced as a standard for POSIX operating systems in 1995, the best option available for concurrency was <kbd>fork</kbd>. On these operating systems, <kbd>fork</kbd> was a fairly primitive command that allowed programs to create copies of themselves as child processes. The name <kbd>fork</kbd> comes from the idea of taking one process and splitting it into two.</p>
<p><kbd>fork</kbd> is not platform-independent, specifically it is not available on Windows, and we recommend using threads instead. However, for educational purposes, it is helpful to introduce some of the concepts from <kbd>fork</kbd> because they are also relevant to threaded programming.</p>
<p>The following code is a translation of the preceding <kbd>process_a</kbd>, <kbd>process_b</kbd> example to use <kbd>fork</kbd>:</p>
<pre class="p1">extern crate nix;<br/>use nix::unistd::{fork,ForkResult};<br/>use std::{thread,time};<br/>use std::process;<br/><br/>fn main() {<br/>   let mut children = Vec::new();<br/>   for _ in 0..3 {<br/>      match fork().expect("fork failed") {<br/>         ForkResult::Parent{ child: pid } =&gt; { children.push(pid); }<br/>         ForkResult::Child =&gt; {<br/>            let t = time::Duration::from_millis(1000);<br/>            loop {<br/>               println!("child process #{}", process::id());<br/>               thread::sleep(t);<br/>            }<br/>         }<br/>      }<br/>   }<br/>   let t = time::Duration::from_millis(1000);<br/>   loop {<br/>      println!("parent process #{}", process::id());<br/>      thread::sleep(t);<br/>   }<br/>}</pre>
<p>In this example, the parent-child relationship is very similar to our first example. We have three children running and one parent managing them.</p>
<p>It should be noted that forked processes share memory initially. Only when either process modifies its memory, will the operating system then perform an operation called <strong>copy-on-write</strong>, duplicating the memory. This behavior is a first step into shared memory between running processes.</p>
<p>To demonstrate copy-on-write, let's allocate 200 MB of memory and fork 500 processes. Without copy-on-write, this would be 100 GB and would crash most personal computers. Consider the following code:</p>
<pre class="p1" style="padding-left: 30px">extern crate nix;<br/>use nix::unistd::{fork};<br/>use std::{thread,time};<br/><br/>fn main() {<br/>   let mut big_data: Vec&lt;u8&gt; = Vec::with_capacity(200000000);<br/>   big_data.push(1);<br/>   big_data.push(2);<br/>   big_data.push(3);<br/>   //Both sides of the fork, will continue to fork<br/>   //This is called a fork bomb<br/>   for _ in 0..9 {<br/>      fork().expect("fork failed");<br/>   }<br/>   //2^9 = 512<br/><br/>   let t = time::Duration::from_millis(1000);<br/>   loop {<br/>      //copy on write, not on read<br/>      big_data[2];<br/>      thread::sleep(t);<br/>   }<br/>}</pre>
<p>Many resources from the parent process also remain available and safe to use from the child process. This is very useful for server applications that listen on a socket in the parent process and poll for incoming connections in the child process. This simple trick permits server applications to distribute work across worker processes:</p>
<pre class="p1" style="padding-left: 30px">extern crate nix;<br/>use nix::unistd::{fork,ForkResult};<br/>use std::{thread,time};<br/>use std::process;<br/>use std::io::prelude::*;<br/>use std::net::TcpListener;<br/><br/>fn serve(listener: TcpListener) -&gt; ! {<br/>   for stream in listener.incoming() {<br/>      let mut buffer = [0; 2048];<br/>      let mut tcp = stream.unwrap();<br/>      tcp.read(&amp;mut buffer).expect("tcp read failed");<br/>      let response = format!("respond from #{}\n", process::id());<br/>      tcp.write(response.as_bytes()).expect("tcp write failed");<br/>   }<br/>   panic!("unreachable");<br/>}<br/><br/>fn main() {<br/>   let listener = TcpListener::bind("127.0.0.1:8888").unwrap();<br/>   let mut children = Vec::new();<br/>   for _ in 0..3 {<br/>      match fork().expect("fork failed") {<br/>         ForkResult::Parent{ child: pid } =&gt; { children.push(pid); }<br/>         ForkResult::Child =&gt; { serve(listener) }<br/>      }<br/>   }<br/><br/>   let t = time::Duration::from_millis(1000);<br/>   loop {<br/>      thread::sleep(t);<br/>   }<br/>}</pre>
<p>In this example, we start listening for connections on port <kbd>8888</kbd>. Then, after forking three times, we start serving responses with our worker process. Sending requests to the server, we can confirm that separate processes are indeed competing to serve requests. Consider the following code:</p>
<pre class="p1" style="padding-left: 30px">$ curl 'http://localhost:8888/'<br/>respond from #59485<br/>$ curl 'http://localhost:8888/'<br/>respond from #59486<br/>$ curl 'http://localhost:8888/'<br/>respond from #59487<br/>$ curl 'http://localhost:8888/'<br/>respond from #59485<br/>$ curl 'http://localhost:8888/'<br/>respond from #59486</pre>
<p class="p1">All three workers served at least one response. Combining the first strategy of memory sharing with this new concept of built-in load balancing, forked processes effectively solve several common problems where concurrency is desired.</p>
<p>However, the fork concurrency model is very rigid. Both of these tricks require planning the application to strategically fork after resources are allocated. Fork does not help at all after the processes have been split. In POSIX, there have been additional standards created to address this problem. Sending information over channels or sharing memory are a common pattern, much like in Rust. However, none of these solutions have proved as practical as threads.</p>
<p>Threads implicitly permit inter-process messaging and memory sharing. The risk of threads is that sharing messages or memory may not be thread-safe and may lead to memory corruption. Rust is built from the ground up to make threaded programming safer.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using thread concurrency</h1>
                
            
            
                
<p>Rust threads have the following features:</p>
<ul>
<li>Share memory</li>
<li>Share resources, such as files or sockets</li>
<li>Tend to be thread-safe</li>
<li>Support inter-thread messaging</li>
<li>Are platform-independent</li>
</ul>
<p>For the preceding reasons, we suggest that Rust threads are better suited to most concurrency use cases than subprocesses. If you want to distribute computation, circumvent a blocking operation, or otherwise utilize concurrency for your application—use threads.</p>
<p>To show the thread pattern, we can re-implement the preceding examples. Here are three children threads:</p>
<pre class="p1" style="padding-left: 30px">use std::{thread,time};<br/>use std::process;<br/>extern crate thread_id;<br/><br/>fn main() {<br/>   for _ in 0..3 {<br/>      thread::spawn(|| {<br/>         let t = time::Duration::from_millis(1000);<br/>         loop {<br/>            println!("child thread #{}:{}", process::id(), <br/>       thread_id::get());<br/>            thread::sleep(t);<br/>         }<br/>      });<br/>   }<br/>   let t = time::Duration::from_millis(1000);<br/>   loop {<br/>      println!("parent thread #{}:{}", process::id(), <br/>      thread_id::get());<br/>      thread::sleep(t);<br/>   }<br/>}</pre>
<p>Here, we spawn three threads and let them run. We print the process ID, but we must also print the thread ID because threads share the same process ID. Here is the output demonstrating this:</p>
<pre class="p1" style="padding-left: 30px">parent thread #59804:140735902303104<br/>child thread #59804:123145412530176<br/>child thread #59804:123145410420736<br/>child thread #59804:123145408311296<br/>parent thread #59804:140735902303104<br/>child thread #59804:123145410420736<br/>child thread #59804:123145408311296</pre>
<p>The next example to port is the 500 processes and shared memory. In a threaded program, sharing might look something like the following code snippet:</p>
<pre class="p1" style="padding-left: 30px">use std::{thread,time};<br/>use std::sync::{Mutex, Arc};<br/><br/>fn main() {<br/>   let mut big_data: Vec&lt;u8&gt; = Vec::with_capacity(200000000);<br/>   big_data.push(1);<br/>   big_data.push(2);<br/>   big_data.push(3);<br/>   let big_data = Arc::new(Mutex::new(big_data));<br/>   for _ in 0..512 {<br/>      let big_data = Arc::clone(&amp;big_data);<br/>      thread::spawn(move || {<br/>         let t = time::Duration::from_millis(1000);<br/>         loop {<br/>            let d = big_data.lock().unwrap();<br/>            (*d)[2];<br/>            thread::sleep(t);<br/>         }<br/>      });<br/>   }<br/>   let t = time::Duration::from_millis(1000);<br/>   loop {<br/>      thread::sleep(t);<br/>   }<br/>}</pre>
<p>The process starts 500 threads, all sharing the same memory. Also, thanks to the lock, we could modify this memory safely if we wanted.</p>
<p>Let's try the server example, as shown in the following code:</p>
<pre class="p1" style="padding-left: 30px">use std::{thread,time};<br/>use std::process;<br/>extern crate thread_id;<br/>use std::io::prelude::*;<br/>use std::net::{TcpListener,TcpStream};<br/>use std::sync::{Arc,Mutex};<br/><br/>fn serve(incoming: Arc&lt;Mutex&lt;Vec&lt;TcpStream&gt;&gt;&gt;) {<br/>   let t = time::Duration::from_millis(10);<br/>   loop {<br/>      {<br/>         let mut incoming = incoming.lock().unwrap();<br/>         for stream in incoming.iter() {<br/>            let mut buffer = [0; 2048];<br/>            let mut tcp = stream;<br/>            tcp.read(&amp;mut buffer).expect("tcp read failed");<br/>            let response = format!("respond from #{}:{}\n", <br/>              process::id(), thread_id::get());<br/>            tcp.write(response.as_bytes()).expect("tcp write failed");<br/>         }<br/>         incoming.clear();<br/>      }<br/>      thread::sleep(t);<br/>   }<br/>}<br/><br/>fn main() {<br/>   let listener = TcpListener::bind("127.0.0.1:8888").unwrap();<br/>   let incoming = Vec::new();<br/>   let incoming = Arc::new(Mutex::new(incoming));<br/>   for _ in 0..3 {<br/>      let incoming = Arc::clone(&amp;incoming);<br/>      thread::spawn(move || {<br/>         serve(incoming);<br/>      });<br/>   }<br/><br/>   for stream in listener.incoming() {<br/>      let mut incoming = incoming.lock().unwrap();<br/>      (*incoming).push(stream.unwrap());<br/>   }<br/>}</pre>
<p>Here, three worker processes scrape a queue of requests that get served down from the parent process. All three children and the parent need to read and mutate the request queue. To mutate the request queue, each thread must lock the data. There is a dance here that the children and parent do to avoid holding the lock for too long. If one thread monopolizes the locked resource, then all other processes wanting to use the data must wait.</p>
<p>The trade-off of locking and waiting is called <strong>contention</strong>. In the worst case scenario, two threads can each hold a lock while waiting for the other thread to release the lock it holds. This is called <strong>deadlock</strong>.</p>
<p>Contention is a difficult problem associated with mutable shared state. For the preceding server case, it would have been better to send messages to children threads. Message passing does not create locks.</p>
<p>Here is a lock-free server:</p>
<pre class="p1">use std::{thread,time};<br/>use std::process;<br/>use std::io::prelude::*;<br/>extern crate thread_id;<br/>use std::net::{TcpListener,TcpStream};<br/>use std::sync::mpsc::{channel,Receiver};<br/>use std::collections::VecDeque;<br/><br/>fn serve(receiver: Receiver&lt;TcpStream&gt;) {<br/>   let t = time::Duration::from_millis(10);<br/>   loop {<br/>      let mut tcp = receiver.recv().unwrap();<br/>      let mut buffer = [0; 2048];<br/>      tcp.read(&amp;mut buffer).expect("tcp read failed");<br/>      let response = format!("respond from #{}:{}\n", process::id(), <br/>             thread_id::get());<br/>      tcp.write(response.as_bytes()).expect("tcp write failed");<br/>      thread::sleep(t);<br/>   }<br/>}<br/><br/>fn main() {<br/>   let listener = TcpListener::bind("127.0.0.1:8888").unwrap();<br/>   let mut channels = VecDeque::new();<br/>   for _ in 0..3 {<br/>      let (sender, receiver) = channel();<br/>      channels.push_back(sender);<br/>      thread::spawn(move || {<br/>         serve(receiver);<br/>      });<br/>   }<br/>   for stream in listener.incoming() {<br/>      let round_robin = channels.pop_front().unwrap();<br/>      round_robin.send(stream.unwrap()).unwrap();<br/>      channels.push_back(round_robin);<br/>   }<br/>}</pre>
<p>Channels work much better in this situation. This multi-threaded server has load balancing controlled from the parent process and does not suffer from lock contention.</p>
<p>Channels are not strictly better than shared state. For example, legitimately contentious resources are good to handle with locks. Consider the following code snippet:</p>
<pre class="p1" style="padding-left: 30px">use std::{thread,time};<br/>extern crate rand;<br/>use std::sync::{Arc,Mutex};<br/>#[macro_use] extern crate lazy_static;<br/>lazy_static! {<br/>   static ref NEURAL_NET_WEIGHTS: Vec&lt;Arc&lt;Mutex&lt;Vec&lt;f64&gt;&gt;&gt;&gt; = {<br/>      let mut nn = Vec::with_capacity(10000);<br/>      for _ in 0..10000 {<br/>         let mut mm = Vec::with_capacity(100);<br/>         for _ in 0..100 {<br/>            mm.push(rand::random::&lt;f64&gt;());<br/>         }<br/>         let mm = Arc::new(Mutex::new(mm));<br/>         nn.push(mm);<br/>      }<br/>      nn<br/>   };<br/>}<br/><br/>fn train() {<br/>   let t = time::Duration::from_millis(100);<br/>   loop {<br/>      for _ in 0..100 {<br/>         let update_position = rand::random::&lt;u64&gt;() % 1000000;<br/>         let update_column = update_position / 10000;<br/>         let update_row = update_position % 100;<br/>         let update_value = rand::random::&lt;f64&gt;();<br/>         let mut update_column = NEURAL_NET_WEIGHTS[update_column as usize].lock().unwrap();<br/>         update_column[update_row as usize] = update_value;<br/>      }<br/>      thread::sleep(t);<br/>   }<br/>}<br/><br/>fn main() {<br/>   let t = time::Duration::from_millis(1000);<br/>   for _ in 0..500 {<br/>      thread::spawn(train);<br/>   }<br/>   loop {<br/>      thread::sleep(t);<br/>   }<br/>}</pre>
<p>Here, we have a large mutable data structure (a neural network) that is broken into rows and columns. Each column has a thread-safe lock. Row data is all associated with the same lock. This pattern is useful for data and computation-heavy programs. Neural network training is a good example of where this technique may be relevant. Unfortunately, the code does not implement an actual neural network, but it does demonstrate how lock concurrency could be used to do so.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding Send and Sync traits</h1>
                
            
            
                
<p>In the previous neural network example, we used a static data structure that was shared between threads without being wrapped in a counter or lock. It contained locks, but why was the outer data structure permitted to be shared?</p>
<p>To answer this question, let's first review the rules of ownership:</p>
<ul>
<li>Each value in Rust has a variable that's called its <strong>owner</strong></li>
<li>There can only be one owner at a time</li>
<li>When the owner goes out of scope, the value will be dropped</li>
</ul>
<p>With these rules in mind, let's try to share a variable across threads, as follows:</p>
<pre class="p1" style="padding-left: 30px">use std::thread;<br/><br/>fn main() {<br/>   let a = vec![1, 2, 3];<br/><br/>   thread::spawn(|| {<br/>      println!("a = {:?}", a);<br/>   });<br/>}</pre>
<p>If we try to compile this, then we will get an error complaining of the following:</p>
<pre><strong>closure may outlive the current function, but it borrows `a`, which is owned by the current function</strong></pre>
<p>This error indicates the following:</p>
<ul>
<li>Referencing variable <kbd>a</kbd> from inside the closure is okay</li>
<li>The closure lives longer than variable <kbd>a</kbd></li>
</ul>
<p>Closures sent to threads must have a static lifetime. Variable <kbd>a</kbd> is a local variable, and thus will go out of scope before the static closure.</p>
<p>To fix this error, it is common to move the variable <kbd>a</kbd> into the closure. Thus, <kbd>a</kbd> will inherit the same lifetime as the closure:</p>
<pre class="p1" style="padding-left: 30px">use std::thread;<br/><br/>fn main() {<br/>   let a = vec![1, 2, 3];<br/><br/>   thread::spawn(move || {<br/>      println!("a = {:?}", a);<br/>   });<br/>}</pre>
<p>This program will compile and run. Ownership of the variable <kbd>a</kbd> is transferred to the closure and therefore lifetime issues are avoided. It should be noted that transferring ownership of a variable implies that the original variable is no longer valid. This is caused by ownership rule number <kbd>2</kbd>—there can only by one owner at a time.</p>
<p>If we try to share the variable again, we get an error:</p>
<pre class="p1" style="padding-left: 30px">use std::thread;<br/><br/>fn main() {<br/>   let a = vec![1, 2, 3];<br/><br/>   thread::spawn(move || {<br/>      println!("a = {:?}", a);<br/>   });<br/><br/>   thread::spawn(move || {<br/>      println!("a = {:?}", a);<br/>   });<br/>}</pre>
<p>Compiling this gives us this error message:</p>
<pre class="p1">$ rustc t.rs<br/>error[E0382]: capture of moved value: `a`<br/>  --&gt; t.rs:11:28<br/>   |<br/>6  |    thread::spawn(move || {<br/>   |                  ------- value moved (into closure) here<br/>...<br/>11 |       println!("a = {:?}", a);<br/>   |                            ^ value captured here after move<br/>   |<br/>   = note: move occurs because `a` has type `std::vec::Vec&lt;i32&gt;`, which does not implement the `Copy` trait<br/><br/>error: aborting due to previous error<br/><br/>For more information about this error, try `rustc --explain E0382`.</pre>
<p>This compiler error is a bit complicated. It says the following:</p>
<ul>
<li><strong>Capture of moved value</strong>: <kbd>a</kbd></li>
<li>Value moved (into closure) here</li>
<li>Value captured here after move</li>
<li>Note—move occurs because <kbd>a</kbd> does not implement the <kbd>Copy</kbd> trait</li>
</ul>
<p>Part four of the error tells us that if <kbd>a</kbd> implements the <kbd>Copy</kbd> trait, then we would not have this error. However, that would be implicitly copying the variable for us, meaning we would not be sharing data. So, that suggestion is not useful for us.</p>
<p>The main problem is part one—capture of moved value <kbd>a</kbd>:</p>
<ol>
<li>First we move the variable <kbd>a</kbd> into the first closure. We needed to do this to avoid the lifetime problem and to use the variable. Using a variable in a closure is called a <strong>capture</strong>.</li>
<li>Next we use variable <kbd>a</kbd> in the second closure. This is the <kbd>value captured after move</kbd>.</li>
</ol>
<p>So our problem is that moving variable <kbd>a</kbd> invalidates it for further use. A much simpler example of this problem would be as follows:</p>
<pre style="padding-left: 30px">fn main() {<br/>   let a = vec![1, 2, 3];<br/>   let b = a;<br/>}</pre>
<p>By moving ownership of the value in <kbd>a</kbd> into <kbd>b</kbd>, we invalidate the original variable.</p>
<p>So what do we do? Are we stuck?</p>
<p>In the neural network example, we used a shared data structure, so clearly there must be a way. If there is a way, hopefully there is also a rule to make sense of the problem. To fully understand thread-safety rules in Rust, you must understand three concepts—scope, <kbd>Send</kbd>, and <kbd>Sync</kbd>.</p>
<p>First, let's address scope. Scope for threads means that variables used must be allowed to capture the variables that they used. Variables can be captured by value, by reference, or by mutable reference.</p>
<p>Our first example, not using <kbd>move</kbd>, almost worked. The only problem was that the lifetime of the variable we used went out of scope too soon. All thread closures must have static lifetimes, and therefore variables that they capture must also have static lifetimes. Adjusting for this, we can create a simple two-thread program that captures our variable, <kbd>A</kbd>, by reference and therefore does not move the variable:</p>
<pre class="p1" style="padding-left: 30px">use std::thread;<br/><br/>fn main() {<br/>   static A: [u8; 100] = [22; 100];<br/><br/>   thread::spawn(|| {<br/>      A[3];<br/>   });<br/><br/>   thread::spawn(|| {<br/>      A[3]<br/>   });<br/>}</pre>
<p>Reading from static variables is safe. Mutating static variables is unsafe. Static variables are also disallowed from allocating heap memory directly, so they can be difficult to work with.</p>
<p>Using the <kbd>lazy_static</kbd> crate is a good way to create static variables with types that have memory allocation and need initialization:</p>
<pre class="p1">use std::thread;<br/>#[macro_use] extern crate lazy_static;<br/><br/>lazy_static! {<br/>   static ref A: Vec&lt;u32&gt; = {<br/>      vec![1, 2, 3]<br/>   };<br/>}<br/><br/>fn main() {<br/>   thread::spawn(|| {<br/>      A[1];<br/>   });<br/><br/>   thread::spawn(|| {<br/>      A[2];<br/>   });<br/>}</pre>
<p>A second way to fix scope problems is to use a reference counter, such as <kbd>Arc</kbd>. Here, we use <kbd>Arc</kbd> instead of <kbd>Rc</kbd> because <kbd>Arc</kbd> is thread-safe and <kbd>Rc</kbd> is not. Consider the following code:</p>
<pre class="p1" style="padding-left: 30px">use std::thread;<br/>use std::sync::{Arc};<br/><br/>fn main() {<br/>   let a = Arc::new(vec![1, 2, 3]);<br/>   {<br/>      let a = Arc::clone(&amp;a);<br/>      thread::spawn(move || {<br/>         a[1];<br/>      });<br/>   }<br/><br/>   {<br/>      let a = Arc::clone(&amp;a);<br/>      thread::spawn(move || {<br/>         a[1];<br/>      });<br/>   }<br/>}</pre>
<p>The reference counter moves the reference into the closure. However, the internal data is shared, so it is then possible to reference common data.</p>
<p>If shared data should be mutated, then a <kbd>Mutex</kbd> lock can allow thread-safe locking. Another useful lock is the <kbd>std::sync::RwLock</kbd>. This is shown as follows:</p>
<pre class="p1" style="padding-left: 30px">use std::thread;<br/>use std::sync::{Arc,Mutex};<br/><br/>fn main() {<br/>   let a = Arc::new(Mutex::new(vec![1, 2, 3]));<br/>   {<br/>      let a = Arc::clone(&amp;a);<br/>      thread::spawn(move || {<br/>         let mut a = a.lock().unwrap();<br/>         (*a)[1] = 2;<br/>      });<br/>   }<br/>   {<br/>      let a = Arc::clone(&amp;a);<br/>      thread::spawn(move || {<br/>         let mut a = a.lock().unwrap();<br/>         (*a)[1] = 3;<br/>      });<br/>   }<br/>}</pre>
<p>So why is mutation allowed after the lock, but not before? The answer is <kbd>Send</kbd> and <kbd>Sync</kbd>.</p>
<p><kbd>Send</kbd> and <kbd>Sync</kbd> are marker traits. A marker trait does not implement any functionality; however, it indicates that a type has some property. These two properties tell the compiler what behavior should be allowed with regards to sharing data between threads.</p>
<p>These are the rules regarding thread data sharing:</p>
<ul>
<li>A type is <kbd>Send</kbd> if it is safe to send it to another thread</li>
<li>A type is <kbd>Sync</kbd> if it is safe to share between multiple threads</li>
</ul>
<p>To make mutable data that can be shared across threads, whatever data type, you use must implement <kbd>Sync</kbd>. The standard Rust library has some thread-safe concurrency primitives, such as <kbd>Mutex</kbd>, for this purpose. If you don't like the options available, then you can search for another crate or make something yourself.</p>
<p>To implement <kbd>Sync</kbd> for a type, just implement the trait with no body:</p>
<pre class="p1" style="padding-left: 30px">use std::thread;<br/><br/>struct MyBox(u8);<br/>unsafe impl Send for MyBox {}<br/>unsafe impl Sync for MyBox {}<br/><br/>static A: MyBox = MyBox(22);<br/><br/>fn main() {<br/>   thread::spawn(move || {<br/>      A.0<br/>   });<br/>   thread::spawn(move || {<br/>      A.0<br/>   });<br/>}</pre>
<p>Be warned—incorrectly implementing <kbd>Send</kbd> or <kbd>Sync</kbd> can cause undefined behavior. The traits are always unsafe to implement. Thankfully, both of these marker traits are generally derived by the compiler, so you will very rarely need to manually derive them.</p>
<p>With these various rules in mind, we can see how Rust prevents many common threading bugs. Foremost, the ownership system prevents a lot of problems. Then, to allow some inter-thread communication, we find that channels and locks can help to safely implement most concurrency models.</p>
<p>This was a lot of trial and error but, in summary, we learned that <kbd>thread</kbd>, <kbd>move</kbd>, <kbd>channel</kbd>,  <kbd>Arc</kbd>, and <kbd>Mutex</kbd> will get us through most problems.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using functional design for concurrency</h1>
                
            
            
                
<p>Concurrency forces the programmer to be more careful about information sharing. This difficulty coincidentally encourages good functional programming practices, such as immutable data and pure functions; when computation is not context-sensitive, it tends to also be thread-safe.</p>
<p>Functional programming sounds great for concurrency, but are there downsides?</p>
<p>In one example of good intentions with bad effects, during development of a functional language called <strong>Haskell</strong>, the development team (<a href="https://www.infoq.com/interviews/armstrong-peyton-jones-erlang-haskell">https://www.infoq.com/interviews/armstrong-peyton-jones-erlang-haskell</a>) wanted to make programs run faster using concurrency. Due to a unique trait of the Haskell language, it was possible to run all expressions and sub-expressions in new threads. The development team thought this sounded great and tested it out.</p>
<p>The result was that more time was spent spawning new threads than doing any computation. The idea still had merit, but it turned out that implementing concurrency automatically would be difficult. There are many trade-offs in concurrent programming. Letting the programmer make decisions regarding these trade-offs is the current state-of-the-art.</p>
<p>So, from functional programming, what patterns have proven useful?</p>
<p>There are many patterns for concurrent programming, but here we will introduce a few primitives:</p>
<ul>
<li><strong>Actors</strong>: Threads and patterns of behavior</li>
<li><strong>Supervisors</strong>: Monitor and manage actors</li>
<li><strong>Routers</strong>: Send messages between actors</li>
<li><strong>Monads</strong>: Composable units of behavior</li>
</ul>
<p>First, let's look at actors in the following code:</p>
<pre class="p1" style="padding-left: 30px">use std::thread;<br/>use std::sync::mpsc::{channel};<br/>use std::time;<br/><br/>fn main() {<br/>   let (pinginsend,pinginrecv) = channel();<br/>   let (pingoutsend,pingoutrecv) = channel();<br/>   let mut ping = 1;<br/>   thread::spawn(move || {<br/>      let t = time::Duration::from_millis(1000);<br/>      loop {<br/>         let n = pinginrecv.recv().unwrap();<br/>         ping += n;<br/>         println!("ping {}", ping);<br/>         thread::sleep(t);<br/>         pingoutsend.send(ping).unwrap();<br/>      }<br/>   });<br/><br/>   let (ponginsend,ponginrecv) = channel();<br/>   let (pongoutsend,pongoutrecv) = channel();<br/>   let mut pong = 2;<br/>   thread::spawn(move || {<br/>      let t = time::Duration::from_millis(1000);<br/>      loop {<br/>         let n = ponginrecv.recv().unwrap();<br/>         pong += n;<br/>         println!("pong {}", pong);<br/>         thread::sleep(t);<br/>         pongoutsend.send(pong).unwrap();<br/>      }<br/>   });<br/><br/>   let mut d = 3;<br/>   loop {<br/>      pinginsend.send(d).unwrap();<br/>      d = pingoutrecv.recv().unwrap();<br/>      ponginsend.send(d).unwrap();<br/>      d = pongoutrecv.recv().unwrap();<br/>   }<br/>}</pre>
<p>Here we have two threads sending messages back and forth. Is this really much different than any of the previous examples?</p>
<p>There is a fairly common saying in functional programming that "<em>a closure is a poor man's object, and an object is a</em> <em>poor man's closure</em>".</p>
<p>According to object-oriented programming, objects have a type, fields, and methods. The closures we define hold their own mutable state, like fields of on an object. The ping and pong closures have slightly different types. The behavior inside the closure could be thought of as a single nameless method on the closure object. There are similarities here between object and closure.</p>
<p>However, it would be much nicer to use a normal object. The problem with attempting this is that the thread boundary gets in the way. Threads do not expose methods, only message passing. As a compromise, we could wrap the message passing into the form of methods. This would hide all of the channel management and would make programming with concurrent objects much nicer. We call this pattern the actor model.</p>
<p>An actor is very similar to an OOP object with the additional property that it lives in its own thread. Messages are sent to the actor, the actor processes the messages, and maybe sends out messages of its own. The actor model is like a busy city of people living and working doing different jobs but interacting and exchanging with one another according to their own schedules.</p>
<p>There are crates that attempt to provide elegant concurrent actor behavior, but we won't endorse any specifically. For the time being, please just squint your eyes and continue to pretend that closures are similar to objects.</p>
<p>In the next example, let's wrap these actors into functions so that they can be created more easily:</p>
<pre class="p1" style="padding-left: 30px">use std::thread;<br/>use std::sync::mpsc::{channel,Sender,Receiver};<br/>use std::time;<br/>extern crate rand;<br/><br/>fn new_ping() -&gt; (Sender&lt;u64&gt;, Receiver&lt;u64&gt;) {<br/>   let (pinginsend,pinginrecv) = channel();<br/>   let (pingoutsend,pingoutrecv) = channel();<br/>   let mut ping = 1;<br/>   thread::spawn(move || {<br/>      let t = time::Duration::from_millis(1000);<br/>      loop {<br/>         let n = pinginrecv.recv().unwrap();<br/>         ping += n;<br/>         println!("ping {}", ping);<br/>         thread::sleep(t);<br/>         pingoutsend.send(ping).unwrap();<br/>      }<br/>   });<br/>   (pinginsend, pingoutrecv)<br/>}<br/><br/>fn new_pong() -&gt; (Sender&lt;u64&gt;, Receiver&lt;u64&gt;) {<br/>   let (ponginsend,ponginrecv) = channel();<br/>   let (pongoutsend,pongoutrecv) = channel();<br/>   let mut pong = 2;<br/>   thread::spawn(move || {<br/>      let t = time::Duration::from_millis(1000);<br/>      loop {<br/>         let n = ponginrecv.recv().unwrap();<br/>         pong += n;<br/>         println!("pong {}", pong);<br/>         thread::sleep(t);<br/>         pongoutsend.send(pong).unwrap();<br/>      }<br/>   });<br/>   (ponginsend, pongoutrecv)<br/>}</pre>
<p>To run the example, we will create three of each type of actor and store the channels in a vector, as shown in the following code:</p>
<pre style="padding-left: 30px">fn main() {<br/>   let pings = vec![new_ping(), new_ping(), new_ping()];<br/>   let pongs = vec![new_pong(), new_pong(), new_pong()];<br/>   loop {<br/>      let mut d = 3;<br/><br/>      let (ref pingin,ref pingout) = pings[(rand::random::&lt;u64&gt;() % 3) as usize];<br/>      pingin.send(d).unwrap();<br/>      d = pingout.recv().unwrap();<br/><br/>      let (ref pongin,ref pongout) = pongs[(rand::random::&lt;u64&gt;() % 3) as usize];<br/>      pongin.send(d).unwrap();<br/>      pongout.recv().unwrap();<br/>   }<br/>}</pre>
<p>Now, we have actors and a really basic supervisor for each actor group. The supervisor here is just a vector to keep track of communication channels for each actor. A good supervisor should periodically check the health of each actor, kill bad actors, and resupply the stock of good actors.</p>
<p>The last actor-based primitive that we will mention is routing. Routing is the method equivalent of object-oriented programming. OOP method calls were originally called <strong>message passing</strong>. The actor model is very object-oriented and accordingly we still call methods by actually passing messages around. We are still using the poor man's objects (closures), so our routing will probably look like a glorified <kbd>if</kbd> statement.</p>
<p>To start our actor router, we will define two data types—addresses and messages. Addresses should define all possible destinations and routing behaviors for messages. Messages should correspond to all possible method calls from all actors. Here is our extended ping pong application:</p>
<pre class="p1" style="padding-left: 30px">use std::thread;<br/>use std::sync::mpsc::{channel,Sender,Receiver};<br/>use std::time;<br/>extern crate rand;<br/><br/>enum Address {<br/>   Ping,<br/>   Pong<br/>}<br/><br/>enum Message {<br/>   PingPlus(u64),<br/>   PongPlus(u64),<br/>}</pre>
<p>Then we define our actors. They now need to match against the new <kbd>Message</kbd> type, and outgoing messages should have an <kbd>Address</kbd> in addition to a <kbd>Message</kbd>. Despite the changes, the code remains very similar to before:</p>
<pre class="p1">fn new_ping() -&gt; (Sender&lt;Message&gt;, Receiver&lt;(Address,Message)&gt;) {<br/>   let (pinginsend,pinginrecv) = channel();<br/>   let (pingoutsend,pingoutrecv) = channel();<br/>   let mut ping = 1;<br/>   thread::spawn(move || {<br/>      let t = time::Duration::from_millis(1000);<br/>      loop {<br/>         let msg = pinginrecv.recv().unwrap();<br/>         match msg {<br/>            Message::PingPlus(n) =&gt; { ping += n; },<br/>            _ =&gt; panic!("Unexpected message")<br/>         }<br/>         println!("ping {}", ping);<br/>         thread::sleep(t);<br/>         pingoutsend.send((<br/>            Address::Pong,<br/>            Message::PongPlus(ping)<br/>         )).unwrap();<br/>         pingoutsend.send((<br/>            Address::Pong,<br/>            Message::PongPlus(ping)<br/>         )).unwrap();<br/>      }<br/>   });<br/>   (pinginsend, pingoutrecv)<br/>}<br/><br/>fn new_pong() -&gt; (Sender&lt;Message&gt;, Receiver&lt;(Address,Message)&gt;) {<br/>   let (ponginsend,ponginrecv) = channel();<br/>   let (pongoutsend,pongoutrecv) = channel();<br/>   let mut pong = 1;<br/>   thread::spawn(move || {<br/>      let t = time::Duration::from_millis(1000);<br/>      loop {<br/>         let msg = ponginrecv.recv().unwrap();<br/>         match msg {<br/>            Message::PongPlus(n) =&gt; { pong += n; },<br/>            _ =&gt; panic!("Unexpected message")<br/>         }<br/>         println!("pong {}", pong);<br/>         thread::sleep(t);<br/>         pongoutsend.send((<br/>            Address::Ping,<br/>            Message::PingPlus(pong)<br/>         )).unwrap();<br/>         pongoutsend.send((<br/>            Address::Ping,<br/>            Message::PingPlus(pong)<br/>         )).unwrap();<br/>      }<br/>   });<br/>   (ponginsend, pongoutrecv)<br/>}</pre>
<p>Each ping pong process loops to consume one message and send two more across. The last component for the program is initialization and routing:</p>
<pre class="p1" style="padding-left: 30px">fn main() {<br/>   let pings = vec![new_ping(), new_ping(), new_ping()];<br/>   let pongs = vec![new_pong(), new_pong(), new_pong()];<br/><br/>   //Start the action<br/>   pings[0].0.send(Message::PingPlus(1)).unwrap();<br/><br/>   //This thread will be the router<br/>   //This is a busy wait and otherwise bad code<br/>   //select! would be much better, but it is still experimental<br/>   //https://doc.rust-lang.org/std/macro.select.html<br/>   let t = time::Duration::from_millis(10);<br/>   loop {<br/>      let mut mail = Vec::new();<br/><br/>      for (_,r) in pings.iter() {<br/>         for (addr,msg) in r.try_iter() {<br/>            mail.push((addr,msg));<br/>         }<br/>      }<br/>      for (_,r) in pongs.iter() {<br/>         for (addr,msg) in r.try_iter() {<br/>            mail.push((addr,msg));<br/>         }<br/>      }<br/><br/>      for (addr,msg) in mail.into_iter() {<br/>         match addr {<br/>            Address::Ping =&gt; {<br/>               let (ref s,_) = pings[(rand::random::&lt;u32&gt;() as usize) % pings.len()];<br/>               s.send(msg).unwrap();<br/>            },<br/>            Address::Pong =&gt; {<br/>               let (ref s,_) = pongs[(rand::random::&lt;u32&gt;() as usize) % pongs.len()];<br/>               s.send(msg).unwrap();<br/>            }<br/>         }<br/>      }<br/>      thread::sleep(t);<br/>   }<br/>}</pre>
<p>After initializing the different actors, the main thread starts acting as the router. The router is a single thread with the sole responsibility of finding destinations, then moving, copying, cloning, and otherwise distributing messages to the recipient threads. This is not a complex solution, but it is effective, and uses only the typesafe, thread-safe, platform-independent primitives that we have introduced so far.</p>
<p>In a more complex example, the routing <kbd>Address</kbd> will typically have the following:</p>
<ul>
<li>An actor role</li>
<li>A method name</li>
<li>Argument type signatures</li>
</ul>
<p>The message would then be the arguments according to the preceding type signature. Sending a message from an actor is as simple as sending your <kbd>(Address,Message)</kbd> to the router. The router at this time should be regularly checking each channel for new routing requests. When it sees the new message, it will pick an actor that satisfies the <kbd>Address</kbd> condition and send the message to that actor's inbox.</p>
<p>Watching the output, each ping pong action doubles the number of messages received. If each thread didn't do so much sleeping, then the program could get out of hand quickly. Messaging noise is one risk of overusing the actor model.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we introduced the primitives of concurrent computation. Subprocesses, forked processes, and threads are the basic building blocks of all concurrent applications. In Rust threads, there are additional concerns that are introduced by the language to encourage type and thread safety.</p>
<p>In several examples, we built a concurrent web server using fork or threads. Later, while exploring thread behavior, we looked closely at what data can be shared between threads and how information can be sent between threads safely.</p>
<p>In the design pattern section, we introduced the actor design pattern. This popular technique combines some elements of object-oriented programming with other concepts from functional programming. The result is a programming tool designed specifically for complex resilient concurrency.</p>
<p>In the next chapter, we will explore performance, debugging, and metaprogramming. Performance can be hard to measure or compare, but we will try to introduce habits that are strictly good for performance. To help debugging, we will look at proactive and reactive techniques to solve issues. Proactive debugging is a set of techniques, such as proper error handling, that either prevents bugs or makes them easier to document and resolve. Reactive techniques are useful for difficult bugs that don't have an obvious cause. Finally, metaprogramming can do lots of complicated work behind the scenes to make ugly code look nicer.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Questions</h1>
                
            
            
                
<ol>
<li>What is a subprocess?</li>
<li>Why is fork called fork?</li>
<li>Is fork still useful?</li>
<li>When were threads standardized?</li>
<li>Why is <kbd>move</kbd> sometimes needed for thread closures?</li>
<li>What is the difference between <kbd>Send</kbd> and <kbd>Sync</kbd> traits?</li>
<li>Why are we allowed to lock and then mutate <kbd>Mutex</kbd> without an unsafe block?</li>
</ol>


            

            
        
    </body></html>
- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rust is an open source project with a large number of contributors from all
    over the world. As with any such project, there are often multiple solutions to
    the same problems. The crate ecosystem makes this easier, since people can publish
    multiple crates that propose multiple ways of solving the same set of problems.
    This approach fosters a healthy sense of competition in true open source spirit. In
    this book, we have covered a number of different topics in the ecosystem. This
    chapter will be an addendum to those topics. We will discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: Coroutine and generator-based approaches to concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The async/await abstraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing using Pest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miscellaneous utilities in the ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to coroutines and generators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We looked into the Tokio ecosystem earlier. We saw how it is very common to
    chain futures in Tokio, yielding a larger task that can then be scheduled as necessary.
    In practice, the following often looks like the pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, our function takes in a URL and recursively downloads raw HTML. It then
    parses and collects links in the document. Our task is run in an event loop. Arguably,
    the control flow here is harder to follow, due to all the callbacks and how they
    interact. This becomes more complex with larger tasks and more conditional branches.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of coroutines helps us to better reason about non-linearity in these
    examples. A coroutine (also known as a **generator**) is a generalization of a
    function that can suspend or resume itself at will, yielding control to another
    entity. Since they do not require preemption by a supersizing entity, they are
    often easier to work with. Note that we always assume a cooperative model where
    a coroutine yields when it is waiting for  a computation to finish or I/O, and
    we ignore cases where a coroutine might be malicious in some other way. When a
    coroutine starts execution again, it resumes from the point where it left off,
    providing a form of continuity. They generally have multiple entry and exit points.
  prefs: []
  type: TYPE_NORMAL
- en: Having set that up, a subroutine (function) becomes approximately a special
    case of coroutine, which has exactly one entry and exit point, and cannot be externally
    suspended. Computer science literature also distinguishes between generators and
    coroutines, arguing that the former cannot control where execution continues after
    they are suspended, while the later can. However, in our current context, we will
    use generator and coroutines interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coroutines can be of two broad types: stackless and stackful. Stackless coroutines
    do not maintain a stack when they are suspended. As a result, they cannot resume
    at arbitrary locations. Stackful coroutines, on the other hand, always maintain
    a small stack. This enables them to suspend and resume from any arbitrary point
    in execution. They always preserve the complete state when they suspend. Thus,
    from a caller''s point of view, they behave like any regular function that can
    run independent of current execution. In practice, stackful coroutines are generally
    more resource heavy but easier to work with (for the caller). Note that all coroutines
    are resource light compared to threads and processes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the recent past, Rust has been experimenting with a generator implementation
    in the standard library. These are located in `std::ops`, and like all new features,
    this is behind multiple feature gates: `generators` and `generator_trait`. There
    are a few parts to this implementation. Firstly, there is a new yield keyword
    for yielding from a generator. Generators are defined by overloading the closure
    syntax. Secondly, there are a few items defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since these are generators (not coroutines), they can have only one `yield`
    and only one `return`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Generator` trait has two types: one for the `yield` case and one for the
    `return` case. The `resume` function resumes execution from the last point. The
    return value for resume is `GeneratorState`, which is an enum, and looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: There are two variants; `Yielded` represents the variant for the `yield` statement,
    and `Complete` represents the variant for the `return` statement. Also, the `Yielded`
    variant represents that the generator can continue again from the last yield statement.
    `Complete` represents that the generator has finished executing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of using this to generate the Collatz sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Since these are in the standard library, we do not need external dependencies.
    We start with the feature gates and imports. In our main function, we define a
    generator using the closure syntax. Notice how it captures the variable called
    `input` from the enclosing scope. Instead of returning the current position at
    the sequence, we `yield` it. And when we are done, we `return` from the generator.
    Now we need to call `resume` on the generator to actually run it. We do that in
    an infinite loop, since we do not know in advance how many times we will need
    to iterate. In that, we `match` on the `resume` call; in both arms, we print out
    the value that we have. Additionally, in the `Complete` arm, we need to `break`
    away from the loop.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we did not use the implicit return syntax here; rather, we did an
    explicit `return end;`. This was not necessary, but this makes the code a little
    bit easier to read in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what it produces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Generators are only available in nightly Rust, for now. Their syntax is expected
    to change over time, possibly by a huge amount.
  prefs: []
  type: TYPE_NORMAL
- en: How May handles coroutines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Rust ecosystem has a number of coroutine implementations, while the core
    team is working on in-language implementations. Of these, one of the widely used
    is called *May*, which is a standalone stackful coroutine library based on generators.
    May strives to be user-friendly enough so that a function can be asynchronously
    invoked using a simple macro that takes in the function. In feature parity with
    the Go programming language, this macro is called `go!`. Let's look at a small
    example of using May.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use our good ol'' friend, Collatz sequence, for this example; this
    will show us multiple ways of achieving the same goal. Let''s start with setting
    up our project using Cargo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the main file. There are two examples here; one uses the generator
    crate to yield numbers in the collatz sequence, acting as a coroutine. The other
    one is a regular function, which is being run as a coroutine using the `go!` macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let's start with the `collatz_generator` function that takes in an input to
    start from and returns an iterator of type `u64`. To be able to specify this,
    we will need to activate the `conservative_impl_trait` feature. We create a scoped
    generator using `Gn::new_scoped` from the generator crate. It takes in a closure
    that actually does the computation. We yield the current value using the `yield_`
    function and signal the end of the computation using the `done!` macro.
  prefs: []
  type: TYPE_NORMAL
- en: Our second example is a regular function that returns a vector of numbers in
    the Collatz sequence. It collects intermediate results in a vector and finally
    returns it once the sequence reaches `1`. In our main function, we parse and sanitize
    input as we always do. We then call our non-generator function asynchronously
    in a coroutine using the `go!` macro. However, `collatz_generator` returns an
    iterator, which we can iterate over in a loop while printing out the numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'As one would expect, this is how the output looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'May also includes an asynchronous network stack implementation (like Tokio),
    and over the last few months, it has gathered a mini ecosystem of a few dependent
    crates. Apart from the generator and coroutine implementations, there is an HTTP
    library based on those, an RPC library, and a crate that supports actor-based
    programming. Let''s look at an example where we write our hyper HTTP using May.
    Here is what a Cargo config looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'At the time of writing, `may_minihttp` is not published in `crates.io` yet,
    so we will need to use the repository to build. Here is the main file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This time, our code is much shorter than with Hyper. This is because May nicely
    abstracts away a lot of things, while letting us have a similar set of functionalities.
    Like the `Service` trait earlier, the `HttpService` trait is what defines a server.
    Functionality is defined by the `call` function. There are some minor differences
    in function calls and how responses are constructed. Another advantage here is
    that this does not expose futures and works with regular `Result`. Arguably, this
    model is easier to understand and follow. In the `main` function, we set the number
    of I/O workers to the number of cores we have. We then start the server on port
    `3000` and wait for it to exit. According to some rough benchmarks on the GitHub
    page, the `may` based HTTP server is slightly faster than a Tokio implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what we see upon running the server. In this particular run, it got
    two `GET` requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Our client side is just `curl`, and we see `done` being printed for each request.
    Note that since our server and the client are on the same physical box, we can
    use `127.0.0.1` as the server''s address. If that is not the case, the actual
    address should be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Awaiting the future
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, we saw how tasks composed of multiple futures are often
    difficult to write and debug. One attempt at remedying this is using a crate that
    wraps futures and associated error handling, yielding a more linear flow of code.
    This crate is called *futures-await* and is under active development.
  prefs: []
  type: TYPE_NORMAL
- en: 'This crate provides two primary mechanisms of dealing with futures:'
  prefs: []
  type: TYPE_NORMAL
- en: The `#[async]` attribute that can be applied to functions, marking them as asynchronous.
    These functions must return the `Result` of their computation as a future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `await!` macro that can be used with async functions to consume the future,
    returning a `Result`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given these constructions, our earlier example download will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This is arguably easier to read than the example with futures. Internally, the
    compiler translates this to code which looks like our earlier example. Additionally,
    since each of the steps returns a `Result`, we can use the `?` operator to nicely
    bubble up errors. The final task can then be run in an event loop, like always.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a more concrete example, rewriting our hyper server project
    using this crate. In this case, our Cargo setup looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is our code, as shown in the following code snippet. Notice that we have
    not used types from the futures crate like last time. Instead, we have used types
    re-exported from futures-await, which are wrapped versions of those types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `async_block!` macro takes in a closure and converts that to an `async`
    function. Thus, our server here is an `async` function. We also use an asynchronous
    `for` loop (a `for` loop marked by `#[async]`) to asynchronously iterate over
    the stream of connections. The rest of the code is exactly the same as last time.
    Running the server is simple; we will use Cargo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'On the client side, we can use curl:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: At the time of writing, running this example will produce a warning about using
    *bind_connection*. Since there is no clear deprecation timeline for this API,
    we will ignore the warning for now.
  prefs: []
  type: TYPE_NORMAL
- en: Data parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data parallelism is a way of speeding up computation by making data a central
    entity. This is in contrast to the coroutine and thread-based parallelism that
    we have seen so far. In those cases, we first determine tasks that can be run
    independently. We then distribute available data to those tasks as needed. This
    approach is often called **task parallelism**. Our topic of discussion in this
    section is data parallelism. In this case, we need to figure out what parts of
    the input data can be used independently; then multiple tasks can be assigned
    to individual parts. This also conforms to the divide and conquer approach, one
    strong example being `mergesort`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Rust ecosystem has a library called **Rayon** that provides simple APIs
    for writing data parallel code. Let''s look at a simple example of using Rayon
    for a binary search on a given slice. We start with setting up our project using
    `cargo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the Cargo configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In our code, we have two implementations of the binary search function, both
    being recursive. The naive implementation is called `binary_search_recursive` and
    does not do any data parallelism. The other version, called `binary_search_rayon`,
    computes the two cases in parallel. Both functions take in a slice of type `T`
    that implements a number of traits. They also take an element of the same type.
    The functions will look for the given element in the slice and return `true` if
    it exists, and `false` otherwise. Let''s look at the code now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In both cases, the first thing to do is to sort the input slice, since a binary
    search requires input to be sorted. `binary_search_recursive` is straightforward;
    we compute the middle point, and if the element there is the one we want, we return
    a `true`. We also include a case checking if there is only one element left in
    the slice and if that is the element we want. We return `true` or `false`, accordingly.
    This case forms the base case for our recursion. We can then check if our desired
    element is less than or greater than the current mid-point. We recursively search
    either side, based on that check.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Rayon case is mostly the same, the only difference being how we recurse.
    We use a `scope` to spawn the two cases in parallel and collect their results.
    The scope takes in a closure and invokes that in reference to the named scope, `s`.
    The scope also ensures that each of the tasks is completed before it exits. We
    collect the results from each half in two variables, and finally, we return a
    logical OR of those, since we care about finding the element in either of the
    two halves. Here is how a sample run looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Rayon also provides a parallel iterator, an iterator that has the same semantics
    as the one in the standard library, but where elements might be accessed in parallel.
    This construct is useful in situations where each unit of data can be processed
    completely independently, without any form of synchronization between each processing
    task. Let''s look at an example of how to use these iterators, starting with the
    project setup using Cargo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we will compare the performance of a regular iterator to that
    of a parallel iterator using Rayon. For that, we will need to use the `rustc-test`
    crate. Here is how the Cargo setup looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the code, as shown in the following code snippet. We have two functions
    doing the exact same thing. Both of them receive a vector of integers, they then
    iterate over that vector and filter out even integers. Finally, they return squares
    of odd integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We start with importing everything from Rayon. In `filter_parallel`, we use
    `par_iter` to get a parallel iterator. `filter_sequential` is the same, the only
    difference being that it uses the `iter` function to get a regular iterator. In
    our main function, we create two sequences and pass those to our functions while
    printing the outputs. Here is what we should see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Not surprisingly, both return the same result. The most important part of this
    example is the benchmark. For this to work, we will need to activate the test
    feature using `#![feature(test)]` and declare a new test module. In that, we import
    everything from the top-level module, which is the main file in this case. We
    also import `test::Bencher`, which will be used for running the benchmarks. The
    benchmarks are defined by the `#[bench]` attributes, which are applied to functions
    that take in an object which is a mutable reference to a `Bencher` type. We pass
    the functions that we need to benchmark to the bencher, which takes care of running
    those and printing results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benchmarks can be run using Cargo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This output shows the two functions and how much time it took to execute an
    iteration of each. The number in the bracket indicates the confidence interval
    for the given measurement. While the confidence interval for the parallel version
    is larger than the non-parallel one, it does perform 58 times more iterations
    than the non-parallel one. Thus, the parallel version is considerably faster.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing using Pest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We studied different parsing techniques in [Chapter 4](part0053.html#1IHDQ0-e803f047c8b7448c90887daa96419287)*,* *Data
    serialization, De-Serialization, and Parsing*. We looked at using parser combinators
    using Nom, building a large parser from smaller parts. There is a completely different
    way of solving the same problem of parsing textual data, using **Parsing Expression
    Grammar (PEG)**. A PEG is a formal grammar that defines how a parser should behave.
    Thus, it includes a finite set of rules, from basic tokens to more complex structures.
    A library that can take in such grammar to produce a functional parser is Pest.
    Let''s look at an example of rewriting our HTTP parsing example from [Chapter
    4](part0053.html#1IHDQ0-e803f047c8b7448c90887daa96419287),* Data Serialization,
    De-Serialization, and Parsing*, using Pest. Start with the Cargo project set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Like always, we will need to declare dependency on Pest components like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to define our grammar, which is a linear collection of parsing
    rules. Like we did previously, we are interested in parsing `HTTP GET` or `POST`
    requests. Here is what the grammar looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The first step is to define literal rules that are to be matched verbatim. These
    correspond to the leaf parsers in Nom. We define literals for newline, carriage
    return, space, the two request strings, the separator, and the fixed string for
    the HTTP version. We also define `request` as the logical OR of the two request
    literals. A list of characters is the logical OR of all lowercase letters and
    all uppercase letters. At this point, we have all we need to define the final
    rule. That is given by `ident_list` and consists of the request, followed by a
    single space, then a separator; then we indicate that our parser should accept
    one or more characters using `*`. The next valid input is again a separator, followed
    by a single space, the version string, a carriage return, and finally, a newline.
    Note that consecutive inputs are separated by the `~` character. The leading `_`
    in front indicates that this is a silent rule and should only be used at the top
    level, as we will see shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main file looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The code is simple; the library provides one basic trait, called `Parser`. This
    can be custom derived for unit structures to produce a functional parser based
    on the grammar file, as input using an attribute called `grammar`. Notably, this
    library uses custom derivatives and custom attributes very efficiently to provide
    a nicer user experience. In our case, the unit structure is called `RequestParser`,
    which implements the `parse` method. In our main function, we call the that method,
    passing in the rule from which parsing should start (in our case, that happens
    to be the final top-level rule, called `ident_list`) and a string to parse. Errors
    are handled by aborting, since there is not much point in continuing if parsing
    failed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having set this structure up, we attempt to parse two strings. The first one
    is a normal HTTP request. The `parse` method returns an iterator over the stream
    of parsed tokens. We loop over them and print out the name of the rule that token
    matched with, the span in the input that has the token, and the literal text in
    that token. Later, we attempt to parse a string which does not have a valid HTTP
    request. Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The first thing to notice is that parsing a wrong HTTP request failed. The error
    message is nice and clear, explaining exactly where it failed to parse. The correct
    request did parse successfully and printed out the tokens and all required details
    to further process those.
  prefs: []
  type: TYPE_NORMAL
- en: Miscellaneous utilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In C and C++, a common workflow is to define a set of bits as flags. They are
    generally defined at powers of two, so the first flag will have the decimal value
    of one, the second one will have two, and so on. This helps in performing logical
    combinations of those flags. The Rust ecosystem has a crate to facilitate the
    same workflow. Let''s look at an example of using the bitflags crate for working
    with flags. Let''s start with initializing an empty project using Cargo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We will set up our project manifest to add `bitflags` as a dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'When all of that is ready, our main file will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We import our dependencies and then use the `bitflags!` macro to define a number
    of flags, as mentioned before, we set their values in powers of two. We also demonstrate
    attaching additional properties to the `bitflags` using the trait system. For
    this, we have a custom trait called `Format` that prints a given input as a decimal.
    The conversion is achieved using the `bits()` method that returns all the bits
    in the given input. The next step is to implement our trait for the `Flags` structure.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have done that, we move on to the `main` function; in there, we construct
    a logical OR of two given flags. We use the `decimal` method to print out representations
    of the bitflags and ensure they are equal. Finally, we use the `all` function
    to display a human readable form of the flags. Here, the `contains` function returns
    `true`, since the flag `X` is indeed in the logical OR of `X` and `Y`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what we should see upon running this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The values of individual flags should always be an integer type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful utility for network programming is the `url` crate. This crate
    provides a number of functionalities to parse parts of URLs, from links to web
    pages to relative addresses. Let''s look at a very simple example, starting with
    the project setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The Cargo manifest should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the main file. In this relatively short example, we are parsing
    a GitLab URL to extract a few important pieces of information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This example URL contains a fragment, pointing to a one-line number in a file.
    The scheme is set to git, and there is a username and password set for HTTP based
    authentication. The URL crate provides a method call `parse` that takes in a string
    and returns a struct that has all required information. We can subsequently call
    individual methods on that variable to print out relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what this code outputs, matching our expectation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This last chapter was a mix of a number of topics which we did not consider
    to be mainstream enough for other chapters. But we should remember that, in a
    large ecosystem like Rust has, things evolve very quickly. So some ideas which
    may not be mainstream today, might just be adopted in the community tomorrow.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, Rust is a wonderful language with enormous potential. We earnestly
    hope that this book helped the reader get a sense of how to harness its power
    for network programming.
  prefs: []
  type: TYPE_NORMAL

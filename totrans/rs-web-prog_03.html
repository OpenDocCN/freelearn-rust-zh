<html><head></head><body>
		<div id="_idContainer035">
			<h1 id="_idParaDest-60" class="chapter-number"><a id="_idTextAnchor059"/>3</h1>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor060"/>Handling HTTP Requests </h1>
			<p>So far, we have structured our to-do module in a flexible, scalable, and reusable manner. However, this can only get us so far in terms of web programming. We want our to-do module to reach multiple people quickly without the user having to install Rust on their own computers. We can do this with a web framework. Rust has plenty to offer. Initially, we will build our main server in the <strong class="bold">Actix Web</strong> framework. </p>
			<p>To achieve this, we will be building the views of the server in a modular fashion; we can slot our to-do module into our web application with minimal effort. It must be noted that the Actix Web framework defines views using <strong class="source-inline">async</strong> functions. Because of this, we will also cover asynchronous programming to get a better understanding of how the Actix Web framework works. </p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Introducing the Actix Web framework </li>
				<li>Launching a basic Actix <span class="No-Break">Web server</span></li>
				<li><span class="No-Break">Understanding closures</span></li>
				<li>Understanding <span class="No-Break">asynchronous programming</span></li>
				<li>Understanding <strong class="source-inline">async</strong> and <strong class="source-inline">await</strong> with web programming </li>
				<li>Managing views using the Actix Web framework </li>
			</ul>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor061"/>Technical requirements</h1>
			<p>As we move toward building web apps in Rust, we are going to have to start relying on third-party packages to do some of the heavy lifting for us. Rust manages dependencies through a package manager called <strong class="bold">Cargo</strong>. To use Cargo, we are going to have to install Rust on our computer from the following <span class="No-Break">URL: </span><a href="https://www.rust-lang.org/tools/install"><span class="No-Break">https://www.rust-lang.org/tools/install</span></a><span class="No-Break">.</span></p>
			<p>This installation delivers the Rust programming language and Cargo. You can find all the code files on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter03"><span class="No-Break">https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter03</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor062"/>Introducing the Actix Web framework</h1>
			<p>At the time of writing, Actix Web is the most popular Rust web framework as can be seen from the <a id="_idIndexMarker266"/>activity on the GitHub page. You might be tempted to jump into another framework that looks more ergonomic, such as Rocket, or one that is faster and more lightweight, such as Hyper. We will be covering these frameworks later in this book over various different chapters; however, we must remember that we are trying to get our heads around web programming in Rust first. Considering that we are new to Rust and web programming, Actix Web is a great start. It is not too low-level that we will get caught up with just trying to get a server to handle a range of views, database connections, and authentication. It is also popular, stable, and has a lot of documentation. This will facilitate a pleasant programming experience when trying to go beyond the book and develop your own web application. It is advised that you get comfortable with Actix Web before moving on to other web frameworks. This is not to say that Actix Web is the best and that all other frameworks are terrible; it is just to facilitate a smooth learning and development experience. With this in mind, we can now move on to the first section, where we set up a basic web server. </p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor063"/>Launching a basic Actix Web server</h1>
			<p>Building with Cargo is <a id="_idIndexMarker267"/>straightforward. All we need to do is navigate to a directory where we want to build our project and run the <span class="No-Break">following command:</span></p>
			<pre class="console">
cargo new web_app</pre>
			<p>The preceding command builds a basic Cargo Rust project. When we explore this application, we get the <span class="No-Break">following structure:</span></p>
			<pre class="source-code">
└── web_app
    ├── Cargo.toml
    └── src
         └── main.rs</pre>
			<p>We can now define our Actix Web dependency in our <strong class="source-inline">Cargo.toml</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
[dependencies]
actix-web = "4.0.1"</pre>
			<p>As a result of <a id="_idIndexMarker268"/>the preceding code, we can now move on to building the web application. For now, we will put it all in our <strong class="source-inline">src/main.rs</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
use actix_web::{web, App, HttpServer, Responder, 
                HttpRequest};
async fn greet(req: HttpRequest) -&gt; impl Responder {
    let name = 
        req.match_info().get("name").unwrap_or("World");
    format!("Hello {}!", name)
}
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new(|| {
        App::new()
        .route("/", web::get().to(greet))
        .route("/{name}", web::get().to(greet))
        .route("/say/hello", web::get().to(|| 
                    async { "Hello Again!" }))
    })
    .bind("127.0.0.1:8080")?
    .run()
    .await
}</pre>
			<p>In the preceding code, we can see that we import the required structs and traits from the <strong class="source-inline">actix_web</strong> crate. We can see that we have used several different ways to define a view. We defined a view by building a function. This takes in an <strong class="source-inline">HttpRequest</strong> struct. It then gets <strong class="source-inline">name</strong> from the request and then returns a variable that can implement the <strong class="source-inline">Responder</strong> trait from the <strong class="source-inline">actix_web</strong> crate. The <strong class="source-inline">Responder</strong> trait converts our type into an HTTP response. We assign this <strong class="source-inline">greet</strong> function that we have created for our application server as the route view, with the <strong class="source-inline">.route("/", web::get().to(greet))</strong> command. We can also see that we can pass in the name from the URL to our <strong class="source-inline">greet</strong> function with the <strong class="source-inline">.route("/{name}", web::get().to(greet))</strong> command. Finally, we pass a closure into the final route. With our configuration, let’s run the <span class="No-Break">following command:</span></p>
			<pre class="console">
cargo run</pre>
			<p>We will get the <span class="No-Break">following printout:</span></p>
			<pre class="console">
Finished dev [unoptimized + debuginfo] target(s) in 0.21s
 Running `target/debug/web_app`</pre>
			<p>We can see in the <a id="_idIndexMarker269"/>preceding output that, right now, there is no logging. This is expected, and we will configure logging later. Now that our server is running, for each of the following URL inputs, we should expect the corresponding outputs in <span class="No-Break">the browser:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Input</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">http://127.0.0.1:8080/</strong></span></li>
				<li><strong class="bold">Output</strong>: <span class="No-Break"><strong class="source-inline">Hello World!</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Input</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">http://127.0.0.1:8080/maxwell</strong></span></li>
				<li><strong class="bold">Output</strong>: <span class="No-Break"><strong class="source-inline">Hello maxwell!</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Input</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">http://127.0.0.1:8080/say/hello</strong></span></li>
				<li><strong class="bold">Output</strong>: <span class="No-Break"><strong class="source-inline">Hello Again!</strong></span></li>
			</ul>
			<p>In the preceding code in the <strong class="source-inline">src/main.rs</strong> file, we can see that there is some new syntax that we have not come across before. We have decorated our <strong class="source-inline">main</strong> function with the <strong class="source-inline">#[actix_web::main]</strong> macro. This marks our <strong class="source-inline">async</strong> <strong class="source-inline">main</strong> function as the Actix Web system entry point. With this, we can see that our functions are <strong class="source-inline">async</strong> and that we are using <a id="_idIndexMarker270"/>closures to build our server. We will go through both concepts in the next couple of sections. In the next section, we will investigate closures to truly understand what is happening. </p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor064"/>Understanding closures</h1>
			<p>Closures are, essentially, functions, but they are also anonymous, meaning that they do not have names. This <a id="_idIndexMarker271"/>means that closures can be passed around into functions and structs. However, before we delve into passing closures around, let us explore closures by defining a basic closure in a blank Rust program (you can use the Rust playground if you prefer) with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
fn main() {
    let test_closure = |string_input| {
        println!("{}", string_input);
    };
    test_closure("test");
}</pre>
			<p>Running the preceding code will give us the <span class="No-Break">following printout:</span></p>
			<pre class="console">
test</pre>
			<p>In the preceding output, we can see that our closure behaves like a function. However, instead of using curly brackets to define the inputs, we use pipes. </p>
			<p>You might have noticed in the preceding closure that we have not defined the data type for the <strong class="source-inline">string_input</strong> parameter; however, the code still runs. This is different from a function that needs to have the parameter data types defined. This is because functions are part of an explicit interface that is exposed to users. A function could be called anywhere in the code if the code can access the function. Closures, on the other hand, have a short lifetime and are only relevant to the scope that they are in. Because of this, the compiler can infer the type being passed into the closure from the use of the closure in the scope. Because we are passing in <strong class="source-inline">&amp;str</strong> when we call the closure, the compiler knows that the <strong class="source-inline">string_input</strong> type is <strong class="source-inline">&amp;str</strong>. While this is convenient, we need to know that closures are not generic. This means that a closure has a concrete type. For instance, after defining our closure, let’s try and run the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
    test_closure("test");
    test_closure(23);</pre>
			<p>We will get the <span class="No-Break">following error:</span></p>
			<pre class="console">
7 |     test_closure(23);
  |                  ^^ expected `&amp;str`, found integer</pre>
			<p>The error occurs because the first call to our closure tells the compiler that we are expecting <strong class="source-inline">&amp;str</strong>, so the second call breaks the compilation process. </p>
			<p>Scopes do not just <a id="_idIndexMarker272"/>affect closures. Closures adhere to the same scope rules that variables do. For instance, let’s say we were going to try and run the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
fn main() {
    {
        let test_closure = |string_input| {
            println!("{}", string_input);
            };
    }
    test_closure("test");
}</pre>
			<p>It would refuse to compile because when we try and call the closure, it is not in the scope of the call. Considering this, you would be right to assume that other scope rules apply to closures. For instance, if we tried to run the following code, what do you think would happen? </p>
			<pre class="source-code">
fn main() {
    let another_str = "case";
    let test_closure = |string_input| {
        println!("{} {}", string_input, another_str);
    };
    test_closure("test");
}</pre>
			<p>If you thought that we would get the following output, you would <span class="No-Break">be right:</span></p>
			<pre class="console">
test case</pre>
			<p>Unlike functions, closures can access variables in their own scope. So, to try and describe closures in <a id="_idIndexMarker273"/>a simplistic way that we can understand, they are kind of like dynamic variables in a scope that we call to perform a computation. </p>
			<p>We can take ownership of the outside variables used in the closure by utilizing <strong class="source-inline">move</strong>, as seen with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
let test_closure = move |string_input| {
    println!("{} {}", string_input, another_str);
};</pre>
			<p>Because <strong class="source-inline">move</strong> is utilized in the closure defined here, the <strong class="source-inline">another_str</strong> variable cannot be used after <strong class="source-inline">test_closure</strong> is declared because <strong class="source-inline">test_closure</strong> took ownership of <strong class="source-inline">another_str</strong>. </p>
			<p>We can also pass closures into a function; however, it must be noted that we can also pass functions into other functions. We can achieve passing functions into other functions with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
fn add_doubles(closure: fn(i32) -&gt; i32, 
               one: i32, two: i32) -&gt; i32 {
    return closure(one) + closure(two)
}
fn main() {
    let closure = |int_input| {
        return int_input * 2
    };
    let outcome = add_doubles(closure, 2, 3);
    println!("{}", outcome);
}</pre>
			<p>In the preceding code, we can see that we define a closure that doubles an integer that is passed in and returned. We then pass this into our <strong class="source-inline">add_doubles</strong> function with the notation of <strong class="source-inline">fn(i32)-&gt; i32</strong>, which is known as a function pointer. When it comes to closures, we can implement one of the <span class="No-Break">following traits:</span></p>
			<ul>
				<li><strong class="source-inline">Fn</strong>: Immutably <span class="No-Break">borrows variables</span></li>
				<li><strong class="source-inline">FnMut</strong>: Mutably borrows variables </li>
				<li><strong class="source-inline">FnOnce</strong>: Takes ownership of variables so it can only be <span class="No-Break">called once</span></li>
			</ul>
			<p>We can pass a closure that <a id="_idIndexMarker274"/>has one of the preceding traits implemented into our <strong class="source-inline">add_doubles</strong> function with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
fn add_doubles(closure: Box&lt;dyn Fn(i32) -&gt; i32&gt;, 
               one: i32, two: i32) -&gt; i32 {
    return closure(one) + closure(two)
}
fn main() {
    let one = 2;
    let closure = move |int_input| {
        return int_input * one
    };
    let outcome = add_doubles(Box::new(closure), 2, 3);
    println!("{}", outcome);
}</pre>
			<p>Here, we can see that the <strong class="source-inline">closure</strong> function parameter has the <strong class="source-inline">Box&lt;dyn Fn(i32) -&gt; i32&gt;</strong> signature. This means that the <strong class="source-inline">add_doubles</strong> function is accepting closures that have implemented the <strong class="source-inline">Fn</strong> trait that accepted <strong class="source-inline">i32</strong>, and returned <strong class="source-inline">i32</strong>. The <strong class="source-inline">Box</strong> struct is a smart pointer where we have put the closure on the heap because we do not know the closure’s size at compile time. You can also see that we have utilized <strong class="source-inline">move</strong> when defining the <a id="_idIndexMarker275"/>closure. This is because we are using the <strong class="source-inline">one</strong> variable, which is outside the closure. The <strong class="source-inline">one</strong> variable may not live long enough; therefore, the closure takes ownership of it because we used <strong class="source-inline">move</strong> when defining <span class="No-Break">the closure.</span></p>
			<p>With what we have covered about closures in mind, we can have another look at the <strong class="source-inline">main</strong> function in our server application with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new(|| {
        App::new()
        .route("/", web::get().to(greet))
        .route("/{name}", web::get().to(greet))
        .route("/say/hello", web::get().to(
        || async { "Hello Again!" }))
    })
    .bind("127.0.0.1:8080")?
    .run()
    .await
}</pre>
			<p>In the preceding code, we can see that we are running our <strong class="source-inline">HttpServer</strong> after constructing it using the <strong class="source-inline">HttpServer::new</strong> function. Knowing what we know now, we can see that we have passed in a closure that returns the <strong class="source-inline">App</strong> struct. Based on what we know about closures, we <a id="_idIndexMarker276"/>can be more confident with what we do with this code. We can essentially do what we like within the closure if it returns the <strong class="source-inline">App</strong> struct. With this in mind, we can get some more information about the process with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new(|| {
        println!("http server factory is firing");
        App::new()
        .route("/", web::get().to(greet))
        .route("/{name}", web::get().to(greet))
        .route("/say/hello", web::get().to(
               || async { "Hello Again!" }))
    })
    .bind("127.0.0.1:8080")?
    .workers(3)
    .run()
    .await
}</pre>
			<p>In the preceding code, we can see that we have added a <strong class="source-inline">print</strong> statement to tell us that the closure is firing. We also added another function called <strong class="source-inline">workers</strong>. This means we can define how many workers are being used to create our server. We also print out that the server factory is firing in our closure. Running the preceding code gives us the <span class="No-Break">following printout:</span></p>
			<pre class="console">
    Finished dev [unoptimized + debuginfo] target(s) in 
    2.45s
     Running `target/debug/web_app`
http server factory is firing
http server factory is firing
http server factory is firing</pre>
			<p>The preceding result tells us that the closure was fired three times. Altering the number of workers shows us that there is a direct relationship between this and the number of times the closure is fired. If the <strong class="source-inline">workers</strong> function is left out, then the closure is fired in relation to the <a id="_idIndexMarker277"/>number of cores your system has. We will explore how these workers fit into the server process in the next section. </p>
			<p>Now that we understand the nuances around the building of the <strong class="source-inline">App</strong> struct, it is time to look at the main change in the structure of a program, <span class="No-Break">asynchronous programming.</span></p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor065"/>Understanding asynchronous programming</h1>
			<p>Up until this chapter, we have been writing code in a sequential manner. This is good enough for <a id="_idIndexMarker278"/>standard scripts. However, in web development, asynchronous programming is important, as there are multiple requests to servers, and API calls introduce idle time. In some other languages, such as Python, we can build web servers without touching any asynchronous concepts. While asynchronous concepts are utilized in these web frameworks, the implementation is defined under the hood. This is also true for the Rust framework Rocket. However, as we have seen, it is directly implemented in Actix Web. </p>
			<p>When it comes to utilizing asynchronous code, there are two main concepts we must understand: </p>
			<ul>
				<li><strong class="bold">Processes</strong>: A process is a <a id="_idIndexMarker279"/>program that is being executed. It has its own memory stack, registers for variables, and code. </li>
				<li><strong class="bold">Threads</strong>: A thread is a lightweight <a id="_idIndexMarker280"/>process that is managed independently by a scheduler. However, it does share data, code, and the heap with other threads and the <strong class="source-inline">main</strong> program. However, threads do not share the stack. </li>
			</ul>
			<p>This is demonstrated in the following <span class="No-Break">classic diagram:</span></p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/Figure_3.1_B18722.jpg" alt="Figure 3.1 – Relationship between threads and processes [source: Cburnett (2007) (https://commons.wikimedia.org/wiki/File:Multithreaded_process.svg), CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0/deed.en)]"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Relationship between threads and processes [source: Cburnett (2007) (https://commons.wikimedia.org/wiki/File:Multithreaded_process.svg), CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0/deed.en)]</p>
			<p>Now that we understand what threads are and what relation they have to our code on a high-level basis, we can play with a toy example to understand how to utilize threads in our code and <a id="_idIndexMarker281"/>see the effects of these threads firsthand. A classic example is to build a basic function that merely sleeps, blocking time. This can simulate a time-expensive function such as a network request. We can run it sequentially with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
use std::{thread, time};
fn do_something(number: i8) -&gt; i8 {
    println!("number {} is running", number);
    let two_seconds = time::Duration::new(2, 0);
    thread::sleep(two_seconds);
    return 2
}
fn main() {
    let now = time::Instant::now();
    let one: i8 = do_something(1);
    let two: i8 = do_something(2);
    let three: i8 = do_something(3);
    println!("time elapsed {:?}", now.elapsed());
    println!("result {}", one + two + three);
}</pre>
			<p>Running the preceding code will give us the <span class="No-Break">following printout:</span></p>
			<pre class="console">
number 1 is running
number 2 is running
number 3 is running
time elapsed 6.0109845s
result 6</pre>
			<p>In the preceding output, we can see that our time-expensive functions run in the order that we expect them to. It also takes just over 6 seconds to run the entire program, which makes sense since we are running three expensive functions that sleep at 2 seconds each. Our <a id="_idIndexMarker282"/>expensive function also returns the value <strong class="source-inline">2</strong>. When we add the results of all three expensive functions together, we are going to get a result of  the value <strong class="source-inline">6</strong>, which is what we have. We speed up our program to roughly 2 seconds for the entire program, by spinning up three threads at the same time and waiting for them to complete before moving on. Waiting for the threads to complete before moving on is called <em class="italic">joining</em>. So, before we start spinning off threads, we must import the <strong class="source-inline">join</strong> handler with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
use std::thread::JoinHandle;</pre>
			<p>We can now spin up threads in our <strong class="source-inline">main</strong> function with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
let now = time::Instant::now();
let thread_one: JoinHandle&lt;i8&gt; = thread::spawn(
    || do_something(1));
let thread_two: JoinHandle&lt;i8&gt; = thread::spawn(
    || do_something(2));
let thread_three: JoinHandle&lt;i8&gt; = thread::spawn(
    || do_something(3));
let result_one = thread_one.join();
let result_two = thread_two.join();
let result_three = thread_three.join();
println!("time elapsed {:?}", now.elapsed());
println!("result {}", result_one.unwrap() +
          result_two.unwrap() + result_three.unwrap());</pre>
			<p>Running the <a id="_idIndexMarker283"/>preceding code gives us the <span class="No-Break">following printout:</span></p>
			<pre class="console">
number 1 is running
number 3 is running
number 2 is running
time elapsed 2.002991041s
result 6</pre>
			<p>As we can see, the whole process took just over 2 seconds to run. This is because all three threads are running concurrently. Note also that thread three is fired before thread two. Do not worry if you get a sequence of <strong class="source-inline">1</strong>, <strong class="source-inline">2</strong>, and <strong class="source-inline">3</strong>. Threads finish in an indeterminate order. The scheduling is deterministic; however, there are thousands of events happening under the hood that require the CPU to do something. As a result, the exact time slices that each thread gets are never the same. These tiny changes add up. Because of this, we cannot guarantee that the threads will finish in a <span class="No-Break">determinate order.</span></p>
			<p>Looking back at how we spin off threads, we can see that we pass a closure into our thread. If we try and just pass the <strong class="source-inline">do_something</strong> function through the thread, we get an error complaining that the compiler expected an <strong class="source-inline">FnOnce&lt;()&gt;</strong> closure and found <strong class="source-inline">i8</strong> instead. This is because a standard closure implements the <strong class="source-inline">FnOnce&lt;()&gt;</strong> public trait, whereas our <strong class="source-inline">do_something</strong> function simply returns <strong class="source-inline">i8</strong>. When <strong class="source-inline">FnOnce&lt;()&gt;</strong> is implemented, the closure can only be called once. This means that when we create a thread, we can ensure that the closure can only be called once, and then when it returns, the thread ends. As our <strong class="source-inline">do_something</strong> function is the final line of the closure, <strong class="source-inline">i8</strong> is returned. However, it has to be noted that just because the <strong class="source-inline">FnOnce&lt;()&gt;</strong> trait is implemented, it does not mean that we cannot call it multiple times. This trait only <a id="_idIndexMarker284"/>gets called if the context requires it. This means that if we were to call the closure outside of the thread context, we could call it <span class="No-Break">multiple times.</span></p>
			<p>Note also that we directly unwrap our results. From what we know, we can deduce that the <strong class="source-inline">join</strong> function on the <strong class="source-inline">JoinHandle</strong> struct returns <strong class="source-inline">Result</strong>, which we also know can be <strong class="source-inline">Err</strong> or <strong class="source-inline">Ok</strong>. We know it is going to be okay to unwrap the result directly because we are merely sleeping and then returning an integer. We also printed out the results, which were indeed integers. However, our error is not what you would expect. The full <strong class="source-inline">Result</strong> type we get is <strong class="source-inline">Result&lt;i8, Box&lt;dyn Any + Send&gt;&gt;</strong>. We already know what <strong class="source-inline">Box</strong> is; however, <strong class="source-inline">dyn Any + Send</strong> seems new. <strong class="source-inline">dyn</strong> is a keyword that we use to indicate what type of trait is being used. <strong class="source-inline">Any</strong> and <strong class="source-inline">Send</strong> are two traits that must be implemented. The <strong class="source-inline">Any</strong> trait is for dynamic typing, meaning that the data type can be anything. The <strong class="source-inline">Send</strong> trait means that it is safe to be moved from one thread to another. The  <strong class="source-inline">Send</strong> trait also means that it is safe to copy from one thread to another. So, what we are sending has implemented the <strong class="source-inline">Copy</strong> trait as what we are sending can be sent between threads. Now that we understand this, we can handle the results of the threads by merely matching the <strong class="source-inline">Result</strong> outcome, and then downcasting the error into a string to get the error message with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
match thread_result {
    Ok(result) =&gt; {
        println!("the result for {} is {}", 
                  result, name);
    }
    Err(result) =&gt; {
    if let Some(string) = result.downcast_ref::&lt;String&gt;() {
        println!("the error for {} is: {}", name, string);
    } else {
        println!("there error for {} does not have a 
                  message", name);
        }
    }
}</pre>
			<p>The preceding code enables us to manage the results of threads gracefully. Now, there is nothing stopping you from logging failures of threads or spinning up new threads based on the outcomes of previous threads. Thus, we can see how powerful the <strong class="source-inline">Result</strong> struct is. There is more we can do with threads, such as give them names or pass data between them <a id="_idIndexMarker285"/>with channels. However, the focus of this book is web programming, not advanced concurrency design patterns and concepts. However, further reading on the subject is provided at the end of the chapter. </p>
			<p>We now understand how to spin up threads in Rust, what they return, and how to handle them. With this information, we can move on to the next section about understanding the <strong class="source-inline">async</strong> and <strong class="source-inline">await</strong> syntax, as this is what will be used in our Actix Web server.  </p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor066"/>Understanding async and await</h1>
			<p>The <strong class="source-inline">async</strong> and <strong class="source-inline">await</strong> syntax <a id="_idIndexMarker286"/>manages the same concepts covered in the previous <a id="_idIndexMarker287"/>section; however, there are some nuances. Instead of simply spawning off threads, we create <strong class="bold">futures</strong> and then manipulate them as and when needed. </p>
			<p>In computer science, a future is an unprocessed computation. This is where the result is not yet available, but when we call or wait, the future will be populated with the result of the computation. Another way of describing this is that a future is a way of expressing a value that is not yet ready. As a result, a future is not exactly a thread. In fact, threads can use futures to maximize their potential. For instance, let us say that we have several network connections. We could have an individual thread for each network connection. This is better than sequentially processing all connections, as a slow network connection would prevent other faster connections from being processed down the line until it itself is processed, resulting in a slower processing time overall. However, spinning up threads for every network connection is not free. Instead, we can have a future for each network connection. These network connections can be processed by a thread from a thread pool when the future is ready. Therefore, we can see why futures are used in web programming, as there are a lot of concurrent connections. </p>
			<p>Futures can also be referred to as <em class="italic">promises</em>, <em class="italic">delays</em>, or <em class="italic">deferred</em>. To explore futures, we will create a new Cargo project and utilize the futures created in the <span class="No-Break"><strong class="source-inline">Cargo.toml</strong></span><span class="No-Break"> file:</span></p>
			<pre class="source-code">
[dependencies]
futures = "0.3.21"</pre>
			<p>With the preceding crate installed, we can import what we need in our <strong class="source-inline">main.rs</strong> using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
use futures::executor::block_on;
use std::{thread, time};</pre>
			<p>We can define futures by merely using the <strong class="source-inline">async</strong> syntax. The <strong class="source-inline">block_on</strong> function will block the program until the future we defined has been executed. We can now define the <strong class="source-inline">do_something</strong> function with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
async fn do_something(number: i8) -&gt; i8 {
    println!("number {} is running", number);
    let two_seconds = time::Duration::new(2, 0);
    thread::sleep(two_seconds);
    return 2
}</pre>
			<p>The <strong class="source-inline">do_something</strong> function essentially does what the code says it does, which is print out what number it is, sleep for 2 seconds, and then return an integer. However, if we were to <a id="_idIndexMarker288"/>directly call it, we would not get <strong class="source-inline">i8</strong>. Instead, calling the <strong class="source-inline">do_something</strong> function <a id="_idIndexMarker289"/>directly will give us <strong class="source-inline">Future&lt;Output = i8&gt;</strong>. We can run our future and time it in the main function with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
fn main() {
    let now = time::Instant::now();
    let future_one = do_something(1);
    let outcome = block_on(future_one);
    println!("time elapsed {:?}", now.elapsed());
    println!("Here is the outcome: {}", outcome);
}</pre>
			<p>Running the preceding code will give us the <span class="No-Break">following printout:</span></p>
			<pre class="console">
number 1 is running
time elapsed 2.00018789s
Here is the outcome: 2</pre>
			<p>This is what is expected. However, let’s see what happens if we enter an extra <strong class="source-inline">sleep</strong> function before we call the <strong class="source-inline">block_on</strong> function with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
fn main() {
    let now = time::Instant::now();
    let future_one = do_something(1);
    let two_seconds = time::Duration::new(2, 0);
    thread::sleep(two_seconds);
    let outcome = block_on(future_one);
    println!("time elapsed {:?}", now.elapsed());
    println!("Here is the outcome: {}", outcome);
}</pre>
			<p>We will get the <span class="No-Break">following printout:</span></p>
			<pre class="console">
number 1 is running
time elapsed 4.000269667s
Here is the outcome: 2</pre>
			<p>Thus, we can see that our future does not execute until we apply an executor using the <strong class="source-inline">block_on</strong> function. </p>
			<p>This can be a bit <a id="_idIndexMarker290"/>laborious, as we may just want a future that we can execute later in <a id="_idIndexMarker291"/>the same function. We can do this with the <strong class="source-inline">async</strong>/<strong class="source-inline">await</strong> syntax. For instance, we can call the <strong class="source-inline">do_something</strong> function and block the code until it is finished using the <strong class="source-inline">await</strong> syntax inside the <strong class="source-inline">main</strong> function, with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
let future_two = async {
    return do_something(2).await
};
let future_two = block_on(future_two);
println!("Here is the outcome: {:?}", future_two);</pre>
			<p>What the <strong class="source-inline">async</strong> block does is return a future. Inside this block, we call the <strong class="source-inline">do_something</strong> function blocking the <strong class="source-inline">async</strong> block until the <strong class="source-inline">do_something</strong> function is resolved, by using the <strong class="source-inline">await</strong> expression. We then apply the <strong class="source-inline">block_on</strong> function on the <strong class="source-inline">future_two</strong> future. </p>
			<p>Looking at our preceding code block, this might seem a little excessive, as it can be done with just two lines of code that call the <strong class="source-inline">do_something</strong> function and pass it to the <strong class="source-inline">block_on</strong> function. In this case, it is excessive, but it can give us more flexibility on how we call futures. For instance, we can call the <strong class="source-inline">do_something</strong> function twice and add them together as a return with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
let future_three = async {
    let outcome_one = do_something(2).await;
    let outcome_two = do_something(3).await;
    return outcome_one + outcome_two
};
let future_outcome = block_on(future_three);
println!("Here is the outcome: {:?}", future_outcome);</pre>
			<p>Adding the preceding code to our <strong class="source-inline">main</strong> function will give us the <span class="No-Break">following printout:</span></p>
			<pre class="console">
number 2 is running
number 3 is running
Here is the outcome: 4</pre>
			<p>Whilst the preceding output is the result that we are expecting, we know that these futures will run sequentially <a id="_idIndexMarker292"/>and that the total time for this block of code will be just <a id="_idIndexMarker293"/>above 4 seconds. Maybe we can speed this up by using <strong class="source-inline">join</strong>. We have seen <strong class="source-inline">join</strong> speed up threads by running them at the same time. It does make sense that it will also work to speed up our futures. First, we must import the <strong class="source-inline">join</strong> macro with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
use futures::join</pre>
			<p>We can now utilize <strong class="source-inline">join</strong> for our futures and time the implementation with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
let future_four = async {
    let outcome_one = do_something(2);
    let outcome_two = do_something(3);
    let results = join!(outcome_one, outcome_two);
    return results.0 + results.1
};
let now = time::Instant::now();
let result = block_on(future_four);
println!("time elapsed {:?}", now.elapsed());
println!("here is the result: {:?}", result);</pre>
			<p>In the preceding code, we can see that the <strong class="source-inline">join</strong> macro returns a tuple of the results and that we unpack the tuple to give us the same result. However, if we do run the code, we can see <a id="_idIndexMarker294"/>that although we get the result that we want, our future execution does <a id="_idIndexMarker295"/>not speed up and is still stuck at just above 4 seconds. This is because a future is not being run using an <strong class="source-inline">async</strong> task. We will have to use <strong class="source-inline">async</strong> tasks to speed up the execution of our futures. We can achieve this by carrying out the following steps: </p>
			<ol>
				<li>Create the futures needed. </li>
				<li>Put them into a vector. </li>
				<li>Loop through the vector, spinning off tasks for each future in <span class="No-Break">the vector.</span></li>
				<li>Join the <strong class="source-inline">async</strong> tasks and sum the vector. </li>
			</ol>
			<p>This can be visually mapped out with the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/Figure_3.2_B18722.jpg" alt="Figure 3.2 – The steps to running multiple futures at once"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – The steps to running multiple futures at once</p>
			<p>To join all our futures at the same time, we will have to use another crate to create our own asynchronous <strong class="source-inline">join</strong> function by using the <strong class="source-inline">async_std</strong> crate. We define this crate in the <strong class="source-inline">Cargo.toml</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
async-std = "1.11.0"</pre>
			<p>Now that we <a id="_idIndexMarker296"/>have the <strong class="source-inline">async_std</strong> crate, we can import what we need to carry out the <a id="_idIndexMarker297"/>approach laid out in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.2</em>, by importing what we need at the top of the <strong class="source-inline">main.rs</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
use std::vec::Vec;
use async_std;
use futures::future::join_all;</pre>
			<p>In the <strong class="source-inline">main</strong> function, we can now define our future with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
let async_outcome = async {
    // 1.
    let mut futures_vec = Vec::new();
    let future_four = do_something(4);
    let future_five = do_something(5);
    // 2.
    futures_vec.push(future_four);
    futures_vec.push(future_five);
    // 3. 
    let handles = futures_vec.into_iter().map(
    async_std::task::spawn).collect::&lt;Vec&lt;_&gt;&gt;();
    // 4.
    let results = join_all(handles).await;
    return results.into_iter().sum::&lt;i8&gt;();
};</pre>
			<p>Here, we can see that we define our futures (<em class="italic">1</em>), and then we add them to our vector (<em class="italic">2</em>). We then loop <a id="_idIndexMarker298"/>through our futures in our vector using the <strong class="source-inline">into_iter</strong> function. We then <a id="_idIndexMarker299"/>spawn a thread on each future using <strong class="source-inline">async_std::task::spawn</strong>. This is similar to <strong class="source-inline">std::task::spawn</strong>. So, why bother with all this extra headache? We could just loop through the vector and spawn a thread for each task. The difference here is that the <strong class="source-inline">async_std::task::spawn</strong> function is spinning off an <strong class="source-inline">async</strong> task in the same thread. Therefore, we are concurrently running both futures in the same thread! We then join all the handles, <strong class="source-inline">await</strong> for these tasks to finish, and then return the sum of all these threads. Now that we have defined our <strong class="source-inline">async_outcome</strong> future, we can run and time it with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
let now = time::Instant::now();
let result = block_on(async_outcome);
println!("time elapsed for join vec {:?}", now.elapsed());
println!("Here is the result: {:?}", result);</pre>
			<p>Running our additional code will give the following <span class="No-Break">additional printout:</span></p>
			<pre class="console">
number 4 is running
number 5 is running
time elapsed for join vec 2.007713458s
Here is the result: 4</pre>
			<p>It’s working! We have managed to get two <strong class="source-inline">async</strong> tasks running at the same time in the same thread, resulting in both futures being executed in just over <span class="No-Break">2 seconds!</span></p>
			<p>As we can see, spawning threads and <strong class="source-inline">async</strong> tasks in Rust is straightforward. However, we must note that passing<a id="_idIndexMarker300"/> variables into threads and <strong class="source-inline">async</strong> tasks is not. Rust’s borrowing <a id="_idIndexMarker301"/>mechanism ensures memory safety. We must go through extra steps when passing data into a thread. Further discussion on the general concepts behind sharing data between threads is not conducive to our web project. However, we can briefly signpost what types allow us to <span class="No-Break">share data:</span></p>
			<ul>
				<li><strong class="source-inline">std::sync::Arc</strong>: This type enables threads to reference <span class="No-Break">outside data:</span><pre class="source-code">
use std::sync::Arc;</pre><pre class="source-code">
use std::thread;</pre><pre class="source-code">
let names = Arc::new(vec!["dave", "chloe", "simon"]);</pre><pre class="source-code">
let reference_data = Arc::clone(&amp;names);</pre><pre class="source-code">
    </pre><pre class="source-code">
let new_thread = thread::spawn(move || {</pre><pre class="source-code">
    println!("{}", reference_data[1]);</pre><pre class="source-code">
});</pre></li>
				<li><strong class="source-inline">std::sync::Mutex</strong>: This type enables threads to mutate <span class="No-Break">outside data:</span><pre class="source-code">
use std::sync::Mutex;</pre><pre class="source-code">
use std::thread;</pre><pre class="source-code">
let count = Mutex::new(0);</pre><pre class="source-code">
    </pre><pre class="source-code">
let new_thread = thread::spawn(move || {</pre><pre class="source-code">
     count.lock().unwrap() += 1;</pre><pre class="source-code">
});</pre></li>
			</ul>
			<p>Inside the thread <a id="_idIndexMarker302"/>here, we dereference the result of the lock, unwrap it, and <a id="_idIndexMarker303"/>mutate it. It must be noted that the shared state can only be accessed once the lock is held. </p>
			<p>We have now covered enough of async programming to return to our web programming. Concurrency is a subject that can be covered in an entire book, one of which is referenced in the <em class="italic">Further reading</em> section. For now, we must get back to exploring Rust in web development to see how our knowledge of Rust async programming affects how we understand the Actix Web server. </p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor067"/>Exploring async and await with web programming</h1>
			<p>Knowing what <a id="_idIndexMarker304"/>we know about async <a id="_idIndexMarker305"/>programming, we can now see the <strong class="source-inline">main</strong> function <a id="_idIndexMarker306"/>in our web <a id="_idIndexMarker307"/>application in a different light, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new( || {
        App::new()
        .route("/", web::get().to(greet))
        .route("/{name}", web::get().to(greet))
        .route("/say/hello", web::get().to(|| 
               async { "Hello Again!" }))
    })
    .bind("127.0.0.1:8080")?
    .workers(3)
    .run()
    .await
}</pre>
			<p>We know that our <strong class="source-inline">greet</strong> function is an <strong class="source-inline">async</strong> function and thus a future. We can also see that the <a id="_idIndexMarker308"/>closure we pass into the <strong class="source-inline">/say/hello</strong> view <a id="_idIndexMarker309"/>also utilizes <a id="_idIndexMarker310"/>the <strong class="source-inline">async</strong> syntax. We can also <a id="_idIndexMarker311"/>see that the <strong class="source-inline">HttpServer::new</strong> function utilized the <strong class="source-inline">await</strong> syntax in <strong class="source-inline">async fn main()</strong>. Therefore, we can deduce that our <strong class="source-inline">HttpServer::new</strong> function is an executor. However, if we were to remove the <strong class="source-inline">#[actix_web::main]</strong> macro, we would get the <span class="No-Break">following error:</span></p>
			<pre class="console">
`main` function is not allowed to be `async`</pre>
			<p>This is because our <strong class="source-inline">main</strong> function, which is our entry point, would return a future as opposed to running our program. <strong class="source-inline">#[actix_web::main]</strong> is a runtime implementation and enables everything to be run on the current thread. The <strong class="source-inline">#[actix_web::main]</strong> macro marks the <strong class="source-inline">async</strong> function (which, in this case, is the <strong class="source-inline">main</strong> function) to be executed by the <span class="No-Break">Actix system.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">At the risk of getting into the weeds here, the Actix crate runs concurrent computation based on the actor model. This is where an actor is a computation. Actors can send and receive messages to and from each other. Actors can alter their own state, but they can only affect other actors through messages, which removes the need for lock-based synchronization (the mutex we covered is lock-based). Further exploration of this model will not help us to develop basic web apps. However, the Actix crate does have good documentation on coding concurrent systems with Actix <span class="No-Break">at </span><a href="https://actix.rs/book/actix"><span class="No-Break">https://actix.rs/book/actix</span></a><span class="No-Break">.</span></p>
			<p>We have covered a lot here. Do not feel stressed if you do not feel like you have retained all of it. We’ve briefly covered a range of topics around asynchronous programming. We do not need to understand it inside out to start building applications based on the Actix <span class="No-Break">Web framework.</span></p>
			<p>You may also feel like we have been excessive in what we have covered. For instance, we could <a id="_idIndexMarker312"/>have spun up a server and used <a id="_idIndexMarker313"/>the <strong class="source-inline">async</strong> syntax when needed <a id="_idIndexMarker314"/>to merely punch out views without <a id="_idIndexMarker315"/>really knowing what is going on. Not understanding what is going on but knowing where to put <strong class="source-inline">async</strong> would not have slowed us down when building our toy application. However, this whistle-stop tour is invaluable when it comes to debugging and designing applications. To establish this, we can look at an example in the wild. We can look at this smart <em class="italic">Stack Overflow</em> solution <a id="_idIndexMarker316"/>to running multiple servers in one file: <a href="https://stackoverflow.com/questions/59642576/run-multiple-actix-app-on-different-ports">https://stackoverflow.com/questions/59642576/run-multiple-actix-app-on-different-ports</a>. </p>
			<p>The code in the <em class="italic">Stack Overflow</em> solution involves basically running two servers at one runtime. First, they <a id="_idIndexMarker317"/>define the views with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
use actix_web::{web, App, HttpServer, Responder};
use futures::future;
async fn utils_one() -&gt; impl Responder {
    "Utils one reached\n"
}
async fn health() -&gt; impl Responder {
    "All good\n"
}</pre>
			<p>Once the <a id="_idIndexMarker318"/>views are defined, the two <a id="_idIndexMarker319"/>servers are <a id="_idIndexMarker320"/>defined in <a id="_idIndexMarker321"/>the <span class="No-Break"><strong class="source-inline">main</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
#[actix_rt::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    let s1 = HttpServer::new(move || {
            App::new().service(web::scope("/utils").route(
            "/one", web::get().to(utils_one)))
        })
        .bind("0.0.0.0:3006")?
        .run();
    let s2 = HttpServer::new(move || {
            App::new().service(web::resource(
            "/health").route(web::get().to(health)))
        })
        .bind("0.0.0.0:8080")?
        .run();
    future::try_join(s1, s2).await?;
    Ok(())
}</pre>
			<p>I have not added <a id="_idIndexMarker322"/>any notation to this code, but it <a id="_idIndexMarker323"/>should not intimidate you. We <a id="_idIndexMarker324"/>can confidently deduce <a id="_idIndexMarker325"/>that <strong class="source-inline">s1</strong> and <strong class="source-inline">s2</strong> are futures that the <strong class="source-inline">run</strong> function returns. We then join these two futures together and <strong class="source-inline">await</strong> for them to finish. There is also a slight difference between our code and the code in the <em class="italic">Stack Overflow</em> solution. Our solution utilizes <strong class="source-inline">await?</strong> and then returns <strong class="source-inline">Ok</strong> with the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
    future::try_join(s1, s2).await?;
    Ok(())
}</pre>
			<p>This is <a id="_idIndexMarker326"/>because a <strong class="source-inline">?</strong> operator is essentially <a id="_idIndexMarker327"/>a <strong class="source-inline">try</strong> match. <strong class="source-inline">join(s1, s2).await?</strong> expands roughly to the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
match join(s1, s2).await {
    Ok(v) =&gt; v,
    Err(e) =&gt; return Err(e.into()),
}</pre>
			<p>Whereas <strong class="source-inline">join(s1, s2).await.unwrap()</strong> expands roughly to the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
match join(s1, s2).await {
    Ok(v) =&gt; v,
    Err(e) =&gt; panic!("unwrap resulted in {}", e),
}</pre>
			<p>Because of the <strong class="source-inline">?</strong> operator, the person providing the solution has to insert <strong class="source-inline">Ok</strong> at the end because the <strong class="source-inline">main</strong> function returns <strong class="source-inline">Result</strong>, and this was taken away by implementing the <strong class="source-inline">?</strong> operator.  </p>
			<p>Thus, in the wild solution, <em class="italic">Stack Overflow</em> has demonstrated the importance of covering async <a id="_idIndexMarker328"/>programming. We can look at code in the wild and work out what is going on and how the posters on <em class="italic">Stack Overflow</em> managed to achieve what they did. This can also mean that we can get creative ourselves. There is nothing stopping us from creating three servers and running them in the <strong class="source-inline">main</strong> function. This is where Rust really shines. Taking the time to learn Rust gives us the ability to safely dive into low-level territory and have more fine-grain control over what we do. You will find this is true in any field of programming done with Rust. </p>
			<p>There is one more concept that we should investigate before trying to build our application, and this is <strong class="bold">runtimes</strong>. We know that we must have the Actix Web macro to enable the <strong class="source-inline">main</strong> function to be a future. If we <a id="_idIndexMarker329"/>look at the <strong class="bold">Tokio</strong> crate, we can see that it is an <a id="_idIndexMarker330"/>asynchronous runtime for the Rust programming <a id="_idIndexMarker331"/>language by providing the <a id="_idIndexMarker332"/>building blocks needed to write <a id="_idIndexMarker333"/>network applications. The workings of Tokio are complex; however, if we look at the Tokio documentation on speeding up the runtime, we can add diagrams like the <span class="No-Break">following one:</span></p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/Figure_3.3_B18722.jpg" alt="Figure 3.3 – Speeding up the Tokio runtime [source: Tokio Documentation (2019) (https://tokio.rs/blog/2019-10-scheduler)]"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Speeding up the Tokio runtime [source: Tokio Documentation (2019) (<a href="https://tokio.rs/blog/2019-10-scheduler">https://tokio.rs/blog/2019-10-scheduler</a>)]</p>
			<p>In the preceding figure, we can see that there are tasks queued up and processors processing them. We processed our tasks earlier, so this should look familiar. Considering <a id="_idIndexMarker334"/>this, it might not be too shocking to <a id="_idIndexMarker335"/>know that we can use Tokio instead <a id="_idIndexMarker336"/>of the Actix Web macro to run <a id="_idIndexMarker337"/>our server. To do this, we define our Tokio dependency in the <strong class="source-inline">Cargo.toml</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
tokio = { version = "1.17.0", features = ["full"] }</pre>
			<p>With the preceding code, we can now switch our macro in the <strong class="source-inline">main.rs</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
#[tokio::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new( || {
        App::new()
        .route("/", web::get().to(greet))
        .route("/{name}", web::get().to(greet))
    })
    .bind("127.0.0.1:8080")?
    .bind("127.0.0.1:8081")?
    .workers(3)
    .run()
    .await
}</pre>
			<p>Running the preceding code will give us the same outcome as running a server. There might be some inconsistencies when using Tokio instead of our Actix runtime macro. While this is an interesting result that demonstrates how we can confidently configure our server, we will use the Actix runtime macro for the rest of the book when it comes to developing the to-do application in Actix. We will revisit Tokio in <a href="B18722_14.xhtml#_idTextAnchor279"><span class="No-Break"><em class="italic">Chapter 14</em></span></a>, <em class="italic">Exploring the Tokio Framework</em>. </p>
			<p>We have now <a id="_idIndexMarker338"/>covered enough of server configuration <a id="_idIndexMarker339"/>and how the server processes <a id="_idIndexMarker340"/>requests to be productive. We can <a id="_idIndexMarker341"/>now move on to defining our views and how they are handled in the next section. </p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor068"/>Managing views using the Actix Web framework</h1>
			<p>So far, we have defined all our views in the <strong class="source-inline">main.rs</strong> file. This is fine for small projects; however, as <a id="_idIndexMarker342"/>our project grows, this will not scale <a id="_idIndexMarker343"/>well. Finding the right views can be hard, and updating them can lead to mistakes. It also makes it harder to remove modules from or insert them into your web application. Also, if we have all the views being defined on one page, this can lead to a lot of merge conflicts if a bigger team is working on the application, as they will all want to alter the same file if they are altering the definitions of views. Because of this, it is better to keep the logic of a set of views contained in a module. We can explore this by building a module that handles authentication. We will not be building the logic around authentication in this chapter, but it is a nice straightforward example to use when exploring how to manage the structure of a views module. Before we write any code, our web application should have the following <span class="No-Break">file layout:</span></p>
			<pre class="source-code">
├── main.rs
└── views
    ├── auth
    │   ├── login.rs
    │   ├── logout.rs
    │   └── mod.rs
    ├── mod.rs</pre>
			<p>The code inside each file can be described <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">main.rs</strong>: The entry point for the server where the server is defined </li>
				<li><strong class="source-inline">views/auth/login.rs</strong>: The code defining the view for <span class="No-Break">logging in</span></li>
				<li><strong class="source-inline">views/auth/logout.rs</strong>: The code defining the view for <span class="No-Break">logging out</span></li>
				<li><strong class="source-inline">views/auth/mod.rs</strong>: The factory that defines the views <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">auth</strong></span></li>
				<li><strong class="source-inline">views/mod.rs</strong>: The factory that defines all the views for the <span class="No-Break">whole app</span></li>
			</ul>
			<p>First, let us start off our entry point with a basic web server with no extras in the <strong class="source-inline">main.rs</strong> file, with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
use actix_web::{App, HttpServer};
mod views;
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new(|| {
        let app = App::new();
        return app
    })
        .bind("127.0.0.1:8000")?
        .run()
        .await
}</pre>
			<p>This preceding <a id="_idIndexMarker344"/>code is straightforward <a id="_idIndexMarker345"/>and there should be no surprises. We will alter the code later, and we can move on to defining the views. For this chapter, we just want to return a string saying what the view is. We will know that our application structure works. We can define our basic login view in the <strong class="source-inline">views/auth/login.rs</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
pub async fn login() -&gt; String {
    format!("Login view")
}</pre>
			<p>Now, it will not be surprising that the logout view in the <strong class="source-inline">views/auth/logout.rs</strong> file takes the <span class="No-Break">following form:</span></p>
			<pre class="source-code">
pub async fn logout() -&gt; String {
    format!("Logout view")
}</pre>
			<p>Now that our views have been defined, all we need to do is define the factories in the <strong class="source-inline">mod.rs</strong> files to enable our server to serve them. Our factories give the data flow of our app, taking the <span class="No-Break">following form:</span></p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/Figure_3.4_B18722.jpg" alt="Figure 3.4 – The data flow of our application"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – The data flow of our application</p>
			<p>We can see in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.4</em> that chaining factories gives us a lot of flexibility. If we wanted to remove all the <strong class="source-inline">auth</strong> views from our application, we would be able to do this by simply removing <a id="_idIndexMarker346"/>one line of code in our main view factory. We <a id="_idIndexMarker347"/>can also reuse our modules. For instance, if we were to use the <strong class="source-inline">auth</strong> module on multiple servers, we could merely have a git submodule for the <strong class="source-inline">auth</strong> views module and use it on other servers. We can build our <strong class="source-inline">auth</strong> module factory view in the <strong class="source-inline">views/auth/mod.rs</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
mod login;
mod logout;
use actix_web::web::{ServiceConfig, get, scope};
pub fn auth_views_factory(app: &amp;mut ServiceConfig) {
    app.service(scope("v1/auth").route("login", 
                get().to(login::login)).route("logout", 
                get().to(logout::logout))
    );
}</pre>
			<p>In the preceding code, we can see that we pass in a mutable reference of a <strong class="source-inline">ServiceConfig</strong> struct. This enables us to define things such as views on the server in different fields. The documentation on this struct states that it is to allow bigger applications to split <a id="_idIndexMarker348"/>up a configuration into <a id="_idIndexMarker349"/>different files. We then apply a service to the <strong class="source-inline">ServiceConfig</strong> struct. The service enables us to define a block of views that all get populated with the prefix defined in the scope. We also state that we are using <strong class="source-inline">get</strong> methods, for now, to make it easily accessible in the browser. We can now plug the <strong class="source-inline">auth</strong> views factory into the <strong class="source-inline">main</strong> views factory in the <strong class="source-inline">views/mod.rs</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
mod auth;
use auth::auth_views_factory;
use actix_web::web::ServiceConfig;
pub fn views_factory(app: &amp;mut ServiceConfig) {
    auth_views_factory(app);
}</pre>
			<p>In the preceding code, we have been able to chop our entire views modules with just one line of code. We can also chain the modules as much as we like. For instance, if we wanted to have submodules within the <strong class="source-inline">auth</strong> views module, we could, and we merely feed the factories of those <strong class="source-inline">auth</strong> submodules into the <strong class="source-inline">auth</strong> factory. We can also define multiple services in a factory. Our <strong class="source-inline">main.rs</strong> file remains pretty much the same with the addition of a <strong class="source-inline">configure</strong> function, as seen with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
use actix_web::{App, HttpServer};
mod views;
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new(|| {
        let app = 
            App::new().configure(views::views_factory);
        return app
    })
        .bind("127.0.0.1:8000")?
        .run()
        .await
}</pre>
			<p>When <a id="_idIndexMarker350"/>we call the <strong class="source-inline">configure</strong> function <a id="_idIndexMarker351"/>on the <strong class="source-inline">App</strong> struct, we pass the views factory into the <strong class="source-inline">configure</strong> function, which will pass the <strong class="source-inline">config</strong> struct into our factory function for us. As the <strong class="source-inline">configure</strong> function returns <strong class="source-inline">Self</strong>, meaning the <strong class="source-inline">App</strong> struct, we can return the result at the end of the closure. We can now run our server, resulting in the following outcome: </p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/Figure_3.5_B18722.jpg" alt="Figure 3.5 – The login view"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – The login view</p>
			<p>We can see that <a id="_idIndexMarker352"/>our application with the expected prefix <a id="_idIndexMarker353"/>works! With this, we have covered all the basics to handle HTTP requests with confidence. </p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>Summary</h1>
			<p>In this chapter, we covered the basics of threading, futures, and <strong class="source-inline">async</strong> functions. As a result, we were able to look at a multi-server solution in the wild and understand confidently what was going on. With this, we built on the concepts we learned in the previous chapter to build modules that define views. In addition, we chained factories to enable our views to be constructed on the fly and added to a server. With this chained factory mechanism, we can slot entire view modules in and out of a configuration when the server is being built. </p>
			<p>We also built a utility struct that defines a path, standardizing the definition of a URL for a set of views. In future chapters, we will use this approach to build authentication, JSON serialization, and frontend modules. With what we’ve covered, we’ll be able to build views that extract and return data from the user in a range of different ways in the next chapter. With this modular understanding, we have a strong foundation that enables us to build real-world web projects in Rust where logic is isolated and can be configured, and where code can be added in a manageable way. </p>
			<p>In the next chapter, we will work on processing requests and responses. We will learn how to pass params, bodies, headers, and forms to views and process them by returning JSON. We will be using these new methods with the to-do module we built in the previous chapter to enable our interaction with to-do items through <span class="No-Break">server views.</span></p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor070"/>Questions</h1>
			<ol>
				<li value="1">What parameter is passed into the <strong class="source-inline">HttpServer::new</strong> function and what does the <span class="No-Break">parameter return?</span></li>
				<li>How is a closure different from <span class="No-Break">a function?</span></li>
				<li>What is the difference between a process and <span class="No-Break">a thread?</span></li>
				<li>What is the difference between an <strong class="source-inline">async</strong> function and a <span class="No-Break">normal one?</span></li>
				<li>What is the difference between <strong class="source-inline">await</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">join</strong></span><span class="No-Break">?</span></li>
				<li>What is the advantage of <span class="No-Break">chaining factories?</span></li>
			</ol>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/>Answers</h1>
			<ol>
				<li value="1">A closure is passed into the <strong class="source-inline">HttpServer::new</strong> function. The <strong class="source-inline">HttpServer::new</strong> function has to return the <strong class="source-inline">App</strong> struct so that the <strong class="source-inline">bind</strong> and <strong class="source-inline">run</strong> functions can be acted on them after the <strong class="source-inline">HttpServer::new</strong> function <span class="No-Break">has fired.</span></li>
				<li>A closure can interact with variables outside of <span class="No-Break">its scope.</span></li>
				<li>A process is a program that is being executed with its own memory stack, registers, and variables, whereas a thread is a lightweight process that is managed independently but shares data with other threads and the <span class="No-Break">main program.</span></li>
				<li>A normal function executes as soon as it is called, whereas an <strong class="source-inline">async</strong> function is a promise and must be executed with a <span class="No-Break">blocking function.</span></li>
				<li><strong class="source-inline">await</strong> blocks a program to wait for a future to be executed; however, the <strong class="source-inline">join</strong> function can run multiple threads or futures concurrently. <strong class="source-inline">await</strong> can also be executed on a <strong class="source-inline">join</strong> function. </li>
				<li>Chaining factories gives us flexibility on how individual modules are constructed and orchestrated. A factory inside a module focuses on how the module is constructed, and the factory outside the module focuses on how the different modules <span class="No-Break">are orchestrated.</span></li>
			</ol>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/>Further reading</h1>
			<ul>
				<li><em class="italic">Hands-On Concurrency with Rust</em> (2018) by <em class="italic">Brian Troutwine</em>, <span class="No-Break"><em class="italic">Packt Publishing</em></span></li>
			</ul>
		</div>
	</body></html>
<html><head></head><body>
		<div><h1 id="_idParaDest-60" class="chapter-number"><a id="_idTextAnchor059"/>3</h1>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor060"/>Handling HTTP Requests </h1>
			<p>So far, we have structured our to-do module in a flexible, scalable, and reusable manner. However, this can only get us so far in terms of web programming. We want our to-do module to reach multiple people quickly without the user having to install Rust on their own computers. We can do this with a web framework. Rust has plenty to offer. Initially, we will build our main server in the <strong class="bold">Actix Web</strong> framework. </p>
			<p>To achieve this, we will be building the views of the server in a modular fashion; we can slot our to-do module into our web application with minimal effort. It must be noted that the Actix Web framework defines views using <code>async</code> functions. Because of this, we will also cover asynchronous programming to get a better understanding of how the Actix Web framework works. </p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Introducing the Actix Web framework </li>
				<li>Launching a basic Actix Web server</li>
				<li>Understanding closures</li>
				<li>Understanding asynchronous programming</li>
				<li>Understanding <code>async</code> and <code>await</code> with web programming </li>
				<li>Managing views using the Actix Web framework </li>
			</ul>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor061"/>Technical requirements</h1>
			<p>As we move toward building web apps in Rust, we are going to have to start relying on third-party packages to do some of the heavy lifting for us. Rust manages dependencies through a package manager called <strong class="bold">Cargo</strong>. To use Cargo, we are going to have to install Rust on our computer from the following URL: <a href="https://www.rust-lang.org/tools/install">https://www.rust-lang.org/tools/install</a>.</p>
			<p>This installation delivers the Rust programming language and Cargo. You can find all the code files on GitHub at <a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter03">https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter03</a>.</p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor062"/>Introducing the Actix Web framework</h1>
			<p>At the time of writing, Actix Web is the most popular Rust web framework as can be seen from the <a id="_idIndexMarker266"/>activity on the GitHub page. You might be tempted to jump into another framework that looks more ergonomic, such as Rocket, or one that is faster and more lightweight, such as Hyper. We will be covering these frameworks later in this book over various different chapters; however, we must remember that we are trying to get our heads around web programming in Rust first. Considering that we are new to Rust and web programming, Actix Web is a great start. It is not too low-level that we will get caught up with just trying to get a server to handle a range of views, database connections, and authentication. It is also popular, stable, and has a lot of documentation. This will facilitate a pleasant programming experience when trying to go beyond the book and develop your own web application. It is advised that you get comfortable with Actix Web before moving on to other web frameworks. This is not to say that Actix Web is the best and that all other frameworks are terrible; it is just to facilitate a smooth learning and development experience. With this in mind, we can now move on to the first section, where we set up a basic web server. </p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor063"/>Launching a basic Actix Web server</h1>
			<p>Building with Cargo is <a id="_idIndexMarker267"/>straightforward. All we need to do is navigate to a directory where we want to build our project and run the following command:</p>
			<pre class="console">
cargo new web_app</pre>
			<p>The preceding command builds a basic Cargo Rust project. When we explore this application, we get the following structure:</p>
			<pre class="source-code">
└── web_app
    ├── Cargo.toml
    └── src
         └── main.rs</pre>
			<p>We can now define our Actix Web dependency in our <code>Cargo.toml</code> file with the following code:</p>
			<pre class="source-code">
[dependencies]
actix-web = "4.0.1"</pre>
			<p>As a result of <a id="_idIndexMarker268"/>the preceding code, we can now move on to building the web application. For now, we will put it all in our <code>src/main.rs</code> file with the following code:</p>
			<pre class="source-code">
use actix_web::{web, App, HttpServer, Responder, 
                HttpRequest};
async fn greet(req: HttpRequest) -&gt; impl Responder {
    let name = 
        req.match_info().get("name").unwrap_or("World");
    format!("Hello {}!", name)
}
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new(|| {
        App::new()
        .route("/", web::get().to(greet))
        .route("/{name}", web::get().to(greet))
        .route("/say/hello", web::get().to(|| 
                    async { "Hello Again!" }))
    })
    .bind("127.0.0.1:8080")?
    .run()
    .await
}</pre>
			<p>In the preceding code, we can see that we import the required structs and traits from the <code>actix_web</code> crate. We can see that we have used several different ways to define a view. We defined a view by building a function. This takes in an <code>HttpRequest</code> struct. It then gets <code>name</code> from the request and then returns a variable that can implement the <code>Responder</code> trait from the <code>actix_web</code> crate. The <code>Responder</code> trait converts our type into an HTTP response. We assign this <code>greet</code> function that we have created for our application server as the route view, with the <code>.route("/", web::get().to(greet))</code> command. We can also see that we can pass in the name from the URL to our <code>greet</code> function with the <code>.route("/{name}", web::get().to(greet))</code> command. Finally, we pass a closure into the final route. With our configuration, let’s run the following command:</p>
			<pre class="console">
cargo run</pre>
			<p>We will get the following printout:</p>
			<pre class="console">
Finished dev [unoptimized + debuginfo] target(s) in 0.21s
 Running `target/debug/web_app`</pre>
			<p>We can see in the <a id="_idIndexMarker269"/>preceding output that, right now, there is no logging. This is expected, and we will configure logging later. Now that our server is running, for each of the following URL inputs, we should expect the corresponding outputs in the browser:</p>
			<ul>
				<li><code>http://127.0.0.1:8080/</code></li>
				<li><code>Hello World!</code></li>
				<li><code>http://127.0.0.1:8080/maxwell</code></li>
				<li><code>Hello maxwell!</code></li>
				<li><code>http://127.0.0.1:8080/say/hello</code></li>
				<li><code>Hello Again!</code></li>
			</ul>
			<p>In the preceding code in the <code>src/main.rs</code> file, we can see that there is some new syntax that we have not come across before. We have decorated our <code>main</code> function with the <code>#[actix_web::main]</code> macro. This marks our <code>async</code> <code>main</code> function as the Actix Web system entry point. With this, we can see that our functions are <code>async</code> and that we are using <a id="_idIndexMarker270"/>closures to build our server. We will go through both concepts in the next couple of sections. In the next section, we will investigate closures to truly understand what is happening. </p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor064"/>Understanding closures</h1>
			<p>Closures are, essentially, functions, but they are also anonymous, meaning that they do not have names. This <a id="_idIndexMarker271"/>means that closures can be passed around into functions and structs. However, before we delve into passing closures around, let us explore closures by defining a basic closure in a blank Rust program (you can use the Rust playground if you prefer) with the following code:</p>
			<pre class="source-code">
fn main() {
    let test_closure = |string_input| {
        println!("{}", string_input);
    };
    test_closure("test");
}</pre>
			<p>Running the preceding code will give us the following printout:</p>
			<pre class="console">
test</pre>
			<p>In the preceding output, we can see that our closure behaves like a function. However, instead of using curly brackets to define the inputs, we use pipes. </p>
			<p>You might have noticed in the preceding closure that we have not defined the data type for the <code>string_input</code> parameter; however, the code still runs. This is different from a function that needs to have the parameter data types defined. This is because functions are part of an explicit interface that is exposed to users. A function could be called anywhere in the code if the code can access the function. Closures, on the other hand, have a short lifetime and are only relevant to the scope that they are in. Because of this, the compiler can infer the type being passed into the closure from the use of the closure in the scope. Because we are passing in <code>&amp;str</code> when we call the closure, the compiler knows that the <code>string_input</code> type is <code>&amp;str</code>. While this is convenient, we need to know that closures are not generic. This means that a closure has a concrete type. For instance, after defining our closure, let’s try and run the following code:</p>
			<pre class="source-code">
    test_closure("test");
    test_closure(23);</pre>
			<p>We will get the following error:</p>
			<pre class="console">
7 |     test_closure(23);
  |                  ^^ expected `&amp;str`, found integer</pre>
			<p>The error occurs because the first call to our closure tells the compiler that we are expecting <code>&amp;str</code>, so the second call breaks the compilation process. </p>
			<p>Scopes do not just <a id="_idIndexMarker272"/>affect closures. Closures adhere to the same scope rules that variables do. For instance, let’s say we were going to try and run the following code:</p>
			<pre class="source-code">
fn main() {
    {
        let test_closure = |string_input| {
            println!("{}", string_input);
            };
    }
    test_closure("test");
}</pre>
			<p>It would refuse to compile because when we try and call the closure, it is not in the scope of the call. Considering this, you would be right to assume that other scope rules apply to closures. For instance, if we tried to run the following code, what do you think would happen? </p>
			<pre class="source-code">
fn main() {
    let another_str = "case";
    let test_closure = |string_input| {
        println!("{} {}", string_input, another_str);
    };
    test_closure("test");
}</pre>
			<p>If you thought that we would get the following output, you would be right:</p>
			<pre class="console">
test case</pre>
			<p>Unlike functions, closures can access variables in their own scope. So, to try and describe closures in <a id="_idIndexMarker273"/>a simplistic way that we can understand, they are kind of like dynamic variables in a scope that we call to perform a computation. </p>
			<p>We can take ownership of the outside variables used in the closure by utilizing <code>move</code>, as seen with the following code:</p>
			<pre class="source-code">
let test_closure = move |string_input| {
    println!("{} {}", string_input, another_str);
};</pre>
			<p>Because <code>move</code> is utilized in the closure defined here, the <code>another_str</code> variable cannot be used after <code>test_closure</code> is declared because <code>test_closure</code> took ownership of <code>another_str</code>. </p>
			<p>We can also pass closures into a function; however, it must be noted that we can also pass functions into other functions. We can achieve passing functions into other functions with the following code:</p>
			<pre class="source-code">
fn add_doubles(closure: fn(i32) -&gt; i32, 
               one: i32, two: i32) -&gt; i32 {
    return closure(one) + closure(two)
}
fn main() {
    let closure = |int_input| {
        return int_input * 2
    };
    let outcome = add_doubles(closure, 2, 3);
    println!("{}", outcome);
}</pre>
			<p>In the preceding code, we can see that we define a closure that doubles an integer that is passed in and returned. We then pass this into our <code>add_doubles</code> function with the notation of <code>fn(i32)-&gt; i32</code>, which is known as a function pointer. When it comes to closures, we can implement one of the following traits:</p>
			<ul>
				<li><code>Fn</code>: Immutably borrows variables</li>
				<li><code>FnMut</code>: Mutably borrows variables </li>
				<li><code>FnOnce</code>: Takes ownership of variables so it can only be called once</li>
			</ul>
			<p>We can pass a closure that <a id="_idIndexMarker274"/>has one of the preceding traits implemented into our <code>add_doubles</code> function with the following code:</p>
			<pre class="source-code">
fn add_doubles(closure: Box&lt;dyn Fn(i32) -&gt; i32&gt;, 
               one: i32, two: i32) -&gt; i32 {
    return closure(one) + closure(two)
}
fn main() {
    let one = 2;
    let closure = move |int_input| {
        return int_input * one
    };
    let outcome = add_doubles(Box::new(closure), 2, 3);
    println!("{}", outcome);
}</pre>
			<p>Here, we can see that the <code>closure</code> function parameter has the <code>Box&lt;dyn Fn(i32) -&gt; i32&gt;</code> signature. This means that the <code>add_doubles</code> function is accepting closures that have implemented the <code>Fn</code> trait that accepted <code>i32</code>, and returned <code>i32</code>. The <code>Box</code> struct is a smart pointer where we have put the closure on the heap because we do not know the closure’s size at compile time. You can also see that we have utilized <code>move</code> when defining the <a id="_idIndexMarker275"/>closure. This is because we are using the <code>one</code> variable, which is outside the closure. The <code>one</code> variable may not live long enough; therefore, the closure takes ownership of it because we used <code>move</code> when defining the closure.</p>
			<p>With what we have covered about closures in mind, we can have another look at the <code>main</code> function in our server application with the following code:</p>
			<pre class="source-code">
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new(|| {
        App::new()
        .route("/", web::get().to(greet))
        .route("/{name}", web::get().to(greet))
        .route("/say/hello", web::get().to(
        || async { "Hello Again!" }))
    })
    .bind("127.0.0.1:8080")?
    .run()
    .await
}</pre>
			<p>In the preceding code, we can see that we are running our <code>HttpServer</code> after constructing it using the <code>HttpServer::new</code> function. Knowing what we know now, we can see that we have passed in a closure that returns the <code>App</code> struct. Based on what we know about closures, we <a id="_idIndexMarker276"/>can be more confident with what we do with this code. We can essentially do what we like within the closure if it returns the <code>App</code> struct. With this in mind, we can get some more information about the process with the following code:</p>
			<pre class="source-code">
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new(|| {
        println!("http server factory is firing");
        App::new()
        .route("/", web::get().to(greet))
        .route("/{name}", web::get().to(greet))
        .route("/say/hello", web::get().to(
               || async { "Hello Again!" }))
    })
    .bind("127.0.0.1:8080")?
    .workers(3)
    .run()
    .await
}</pre>
			<p>In the preceding code, we can see that we have added a <code>print</code> statement to tell us that the closure is firing. We also added another function called <code>workers</code>. This means we can define how many workers are being used to create our server. We also print out that the server factory is firing in our closure. Running the preceding code gives us the following printout:</p>
			<pre class="console">
    Finished dev [unoptimized + debuginfo] target(s) in 
    2.45s
     Running `target/debug/web_app`
http server factory is firing
http server factory is firing
http server factory is firing</pre>
			<p>The preceding result tells us that the closure was fired three times. Altering the number of workers shows us that there is a direct relationship between this and the number of times the closure is fired. If the <code>workers</code> function is left out, then the closure is fired in relation to the <a id="_idIndexMarker277"/>number of cores your system has. We will explore how these workers fit into the server process in the next section. </p>
			<p>Now that we understand the nuances around the building of the <code>App</code> struct, it is time to look at the main change in the structure of a program, asynchronous programming.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor065"/>Understanding asynchronous programming</h1>
			<p>Up until this chapter, we have been writing code in a sequential manner. This is good enough for <a id="_idIndexMarker278"/>standard scripts. However, in web development, asynchronous programming is important, as there are multiple requests to servers, and API calls introduce idle time. In some other languages, such as Python, we can build web servers without touching any asynchronous concepts. While asynchronous concepts are utilized in these web frameworks, the implementation is defined under the hood. This is also true for the Rust framework Rocket. However, as we have seen, it is directly implemented in Actix Web. </p>
			<p>When it comes to utilizing asynchronous code, there are two main concepts we must understand: </p>
			<ul>
				<li><strong class="bold">Processes</strong>: A process is a <a id="_idIndexMarker279"/>program that is being executed. It has its own memory stack, registers for variables, and code. </li>
				<li><code>main</code> program. However, threads do not share the stack. </li>
			</ul>
			<p>This is demonstrated in the following classic diagram:</p>
			<div><div><img src="img/Figure_3.1_B18722.jpg" alt="Figure 3.1 – Relationship between threads and processes [source: Cburnett (2007) (https://commons.wikimedia.org/wiki/File:Multithreaded_process.svg), CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0/deed.en)]"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Relationship between threads and processes [source: Cburnett (2007) (https://commons.wikimedia.org/wiki/File:Multithreaded_process.svg), CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0/deed.en)]</p>
			<p>Now that we understand what threads are and what relation they have to our code on a high-level basis, we can play with a toy example to understand how to utilize threads in our code and <a id="_idIndexMarker281"/>see the effects of these threads firsthand. A classic example is to build a basic function that merely sleeps, blocking time. This can simulate a time-expensive function such as a network request. We can run it sequentially with the following code:</p>
			<pre class="source-code">
use std::{thread, time};
fn do_something(number: i8) -&gt; i8 {
    println!("number {} is running", number);
    let two_seconds = time::Duration::new(2, 0);
    thread::sleep(two_seconds);
    return 2
}
fn main() {
    let now = time::Instant::now();
    let one: i8 = do_something(1);
    let two: i8 = do_something(2);
    let three: i8 = do_something(3);
    println!("time elapsed {:?}", now.elapsed());
    println!("result {}", one + two + three);
}</pre>
			<p>Running the preceding code will give us the following printout:</p>
			<pre class="console">
number 1 is running
number 2 is running
number 3 is running
time elapsed 6.0109845s
result 6</pre>
			<p>In the preceding output, we can see that our time-expensive functions run in the order that we expect them to. It also takes just over 6 seconds to run the entire program, which makes sense since we are running three expensive functions that sleep at 2 seconds each. Our <a id="_idIndexMarker282"/>expensive function also returns the value <code>2</code>. When we add the results of all three expensive functions together, we are going to get a result of  the value <code>6</code>, which is what we have. We speed up our program to roughly 2 seconds for the entire program, by spinning up three threads at the same time and waiting for them to complete before moving on. Waiting for the threads to complete before moving on is called <em class="italic">joining</em>. So, before we start spinning off threads, we must import the <code>join</code> handler with the following code:</p>
			<pre class="source-code">
use std::thread::JoinHandle;</pre>
			<p>We can now spin up threads in our <code>main</code> function with the following code:</p>
			<pre class="source-code">
let now = time::Instant::now();
let thread_one: JoinHandle&lt;i8&gt; = thread::spawn(
    || do_something(1));
let thread_two: JoinHandle&lt;i8&gt; = thread::spawn(
    || do_something(2));
let thread_three: JoinHandle&lt;i8&gt; = thread::spawn(
    || do_something(3));
let result_one = thread_one.join();
let result_two = thread_two.join();
let result_three = thread_three.join();
println!("time elapsed {:?}", now.elapsed());
println!("result {}", result_one.unwrap() +
          result_two.unwrap() + result_three.unwrap());</pre>
			<p>Running the <a id="_idIndexMarker283"/>preceding code gives us the following printout:</p>
			<pre class="console">
number 1 is running
number 3 is running
number 2 is running
time elapsed 2.002991041s
result 6</pre>
			<p>As we can see, the whole process took just over 2 seconds to run. This is because all three threads are running concurrently. Note also that thread three is fired before thread two. Do not worry if you get a sequence of <code>1</code>, <code>2</code>, and <code>3</code>. Threads finish in an indeterminate order. The scheduling is deterministic; however, there are thousands of events happening under the hood that require the CPU to do something. As a result, the exact time slices that each thread gets are never the same. These tiny changes add up. Because of this, we cannot guarantee that the threads will finish in a determinate order.</p>
			<p>Looking back at how we spin off threads, we can see that we pass a closure into our thread. If we try and just pass the <code>do_something</code> function through the thread, we get an error complaining that the compiler expected an <code>FnOnce&lt;()&gt;</code> closure and found <code>i8</code> instead. This is because a standard closure implements the <code>FnOnce&lt;()&gt;</code> public trait, whereas our <code>do_something</code> function simply returns <code>i8</code>. When <code>FnOnce&lt;()&gt;</code> is implemented, the closure can only be called once. This means that when we create a thread, we can ensure that the closure can only be called once, and then when it returns, the thread ends. As our <code>do_something</code> function is the final line of the closure, <code>i8</code> is returned. However, it has to be noted that just because the <code>FnOnce&lt;()&gt;</code> trait is implemented, it does not mean that we cannot call it multiple times. This trait only <a id="_idIndexMarker284"/>gets called if the context requires it. This means that if we were to call the closure outside of the thread context, we could call it multiple times.</p>
			<p>Note also that we directly unwrap our results. From what we know, we can deduce that the <code>join</code> function on the <code>JoinHandle</code> struct returns <code>Result</code>, which we also know can be <code>Err</code> or <code>Ok</code>. We know it is going to be okay to unwrap the result directly because we are merely sleeping and then returning an integer. We also printed out the results, which were indeed integers. However, our error is not what you would expect. The full <code>Result</code> type we get is <code>Result&lt;i8, Box&lt;dyn Any + Send&gt;&gt;</code>. We already know what <code>Box</code> is; however, <code>dyn Any + Send</code> seems new. <code>dyn</code> is a keyword that we use to indicate what type of trait is being used. <code>Any</code> and <code>Send</code> are two traits that must be implemented. The <code>Any</code> trait is for dynamic typing, meaning that the data type can be anything. The <code>Send</code> trait means that it is safe to be moved from one thread to another. The  <code>Send</code> trait also means that it is safe to copy from one thread to another. So, what we are sending has implemented the <code>Copy</code> trait as what we are sending can be sent between threads. Now that we understand this, we can handle the results of the threads by merely matching the <code>Result</code> outcome, and then downcasting the error into a string to get the error message with the following code:</p>
			<pre class="source-code">
match thread_result {
    Ok(result) =&gt; {
        println!("the result for {} is {}", 
                  result, name);
    }
    Err(result) =&gt; {
    if let Some(string) = result.downcast_ref::&lt;String&gt;() {
        println!("the error for {} is: {}", name, string);
    } else {
        println!("there error for {} does not have a 
                  message", name);
        }
    }
}</pre>
			<p>The preceding code enables us to manage the results of threads gracefully. Now, there is nothing stopping you from logging failures of threads or spinning up new threads based on the outcomes of previous threads. Thus, we can see how powerful the <code>Result</code> struct is. There is more we can do with threads, such as give them names or pass data between them <a id="_idIndexMarker285"/>with channels. However, the focus of this book is web programming, not advanced concurrency design patterns and concepts. However, further reading on the subject is provided at the end of the chapter. </p>
			<p>We now understand how to spin up threads in Rust, what they return, and how to handle them. With this information, we can move on to the next section about understanding the <code>async</code> and <code>await</code> syntax, as this is what will be used in our Actix Web server.  </p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor066"/>Understanding async and await</h1>
			<p>The <code>async</code> and <code>await</code> syntax <a id="_idIndexMarker286"/>manages the same concepts covered in the previous <a id="_idIndexMarker287"/>section; however, there are some nuances. Instead of simply spawning off threads, we create <strong class="bold">futures</strong> and then manipulate them as and when needed. </p>
			<p>In computer science, a future is an unprocessed computation. This is where the result is not yet available, but when we call or wait, the future will be populated with the result of the computation. Another way of describing this is that a future is a way of expressing a value that is not yet ready. As a result, a future is not exactly a thread. In fact, threads can use futures to maximize their potential. For instance, let us say that we have several network connections. We could have an individual thread for each network connection. This is better than sequentially processing all connections, as a slow network connection would prevent other faster connections from being processed down the line until it itself is processed, resulting in a slower processing time overall. However, spinning up threads for every network connection is not free. Instead, we can have a future for each network connection. These network connections can be processed by a thread from a thread pool when the future is ready. Therefore, we can see why futures are used in web programming, as there are a lot of concurrent connections. </p>
			<p>Futures can also be referred to as <em class="italic">promises</em>, <em class="italic">delays</em>, or <em class="italic">deferred</em>. To explore futures, we will create a new Cargo project and utilize the futures created in the <code>Cargo.toml</code> file:</p>
			<pre class="source-code">
[dependencies]
futures = "0.3.21"</pre>
			<p>With the preceding crate installed, we can import what we need in our <code>main.rs</code> using the following code:</p>
			<pre class="source-code">
use futures::executor::block_on;
use std::{thread, time};</pre>
			<p>We can define futures by merely using the <code>async</code> syntax. The <code>block_on</code> function will block the program until the future we defined has been executed. We can now define the <code>do_something</code> function with the following code:</p>
			<pre class="source-code">
async fn do_something(number: i8) -&gt; i8 {
    println!("number {} is running", number);
    let two_seconds = time::Duration::new(2, 0);
    thread::sleep(two_seconds);
    return 2
}</pre>
			<p>The <code>do_something</code> function essentially does what the code says it does, which is print out what number it is, sleep for 2 seconds, and then return an integer. However, if we were to <a id="_idIndexMarker288"/>directly call it, we would not get <code>i8</code>. Instead, calling the <code>do_something</code> function <a id="_idIndexMarker289"/>directly will give us <code>Future&lt;Output = i8&gt;</code>. We can run our future and time it in the main function with the following code:</p>
			<pre class="source-code">
fn main() {
    let now = time::Instant::now();
    let future_one = do_something(1);
    let outcome = block_on(future_one);
    println!("time elapsed {:?}", now.elapsed());
    println!("Here is the outcome: {}", outcome);
}</pre>
			<p>Running the preceding code will give us the following printout:</p>
			<pre class="console">
number 1 is running
time elapsed 2.00018789s
Here is the outcome: 2</pre>
			<p>This is what is expected. However, let’s see what happens if we enter an extra <code>sleep</code> function before we call the <code>block_on</code> function with the following code:</p>
			<pre class="source-code">
fn main() {
    let now = time::Instant::now();
    let future_one = do_something(1);
    let two_seconds = time::Duration::new(2, 0);
    thread::sleep(two_seconds);
    let outcome = block_on(future_one);
    println!("time elapsed {:?}", now.elapsed());
    println!("Here is the outcome: {}", outcome);
}</pre>
			<p>We will get the following printout:</p>
			<pre class="console">
number 1 is running
time elapsed 4.000269667s
Here is the outcome: 2</pre>
			<p>Thus, we can see that our future does not execute until we apply an executor using the <code>block_on</code> function. </p>
			<p>This can be a bit <a id="_idIndexMarker290"/>laborious, as we may just want a future that we can execute later in <a id="_idIndexMarker291"/>the same function. We can do this with the <code>async</code>/<code>await</code> syntax. For instance, we can call the <code>do_something</code> function and block the code until it is finished using the <code>await</code> syntax inside the <code>main</code> function, with the following code:</p>
			<pre class="source-code">
let future_two = async {
    return do_something(2).await
};
let future_two = block_on(future_two);
println!("Here is the outcome: {:?}", future_two);</pre>
			<p>What the <code>async</code> block does is return a future. Inside this block, we call the <code>do_something</code> function blocking the <code>async</code> block until the <code>do_something</code> function is resolved, by using the <code>await</code> expression. We then apply the <code>block_on</code> function on the <code>future_two</code> future. </p>
			<p>Looking at our preceding code block, this might seem a little excessive, as it can be done with just two lines of code that call the <code>do_something</code> function and pass it to the <code>block_on</code> function. In this case, it is excessive, but it can give us more flexibility on how we call futures. For instance, we can call the <code>do_something</code> function twice and add them together as a return with the following code:</p>
			<pre class="source-code">
let future_three = async {
    let outcome_one = do_something(2).await;
    let outcome_two = do_something(3).await;
    return outcome_one + outcome_two
};
let future_outcome = block_on(future_three);
println!("Here is the outcome: {:?}", future_outcome);</pre>
			<p>Adding the preceding code to our <code>main</code> function will give us the following printout:</p>
			<pre class="console">
number 2 is running
number 3 is running
Here is the outcome: 4</pre>
			<p>Whilst the preceding output is the result that we are expecting, we know that these futures will run sequentially <a id="_idIndexMarker292"/>and that the total time for this block of code will be just <a id="_idIndexMarker293"/>above 4 seconds. Maybe we can speed this up by using <code>join</code>. We have seen <code>join</code> speed up threads by running them at the same time. It does make sense that it will also work to speed up our futures. First, we must import the <code>join</code> macro with the following code:</p>
			<pre class="source-code">
use futures::join</pre>
			<p>We can now utilize <code>join</code> for our futures and time the implementation with the following code:</p>
			<pre class="source-code">
let future_four = async {
    let outcome_one = do_something(2);
    let outcome_two = do_something(3);
    let results = join!(outcome_one, outcome_two);
    return results.0 + results.1
};
let now = time::Instant::now();
let result = block_on(future_four);
println!("time elapsed {:?}", now.elapsed());
println!("here is the result: {:?}", result);</pre>
			<p>In the preceding code, we can see that the <code>join</code> macro returns a tuple of the results and that we unpack the tuple to give us the same result. However, if we do run the code, we can see <a id="_idIndexMarker294"/>that although we get the result that we want, our future execution does <a id="_idIndexMarker295"/>not speed up and is still stuck at just above 4 seconds. This is because a future is not being run using an <code>async</code> task. We will have to use <code>async</code> tasks to speed up the execution of our futures. We can achieve this by carrying out the following steps: </p>
			<ol>
				<li>Create the futures needed. </li>
				<li>Put them into a vector. </li>
				<li>Loop through the vector, spinning off tasks for each future in the vector.</li>
				<li>Join the <code>async</code> tasks and sum the vector. </li>
			</ol>
			<p>This can be visually mapped out with the following figure:</p>
			<div><div><img src="img/Figure_3.2_B18722.jpg" alt="Figure 3.2 – The steps to running multiple futures at once"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – The steps to running multiple futures at once</p>
			<p>To join all our futures at the same time, we will have to use another crate to create our own asynchronous <code>join</code> function by using the <code>async_std</code> crate. We define this crate in the <code>Cargo.toml</code> file with the following code:</p>
			<pre class="source-code">
async-std = "1.11.0"</pre>
			<p>Now that we <a id="_idIndexMarker296"/>have the <code>async_std</code> crate, we can import what we need to carry out the <a id="_idIndexMarker297"/>approach laid out in <em class="italic">Figure 3</em><em class="italic">.2</em>, by importing what we need at the top of the <code>main.rs</code> file with the following code:</p>
			<pre class="source-code">
use std::vec::Vec;
use async_std;
use futures::future::join_all;</pre>
			<p>In the <code>main</code> function, we can now define our future with the following code:</p>
			<pre class="source-code">
let async_outcome = async {
    // 1.
    let mut futures_vec = Vec::new();
    let future_four = do_something(4);
    let future_five = do_something(5);
    // 2.
    futures_vec.push(future_four);
    futures_vec.push(future_five);
    // 3. 
    let handles = futures_vec.into_iter().map(
    async_std::task::spawn).collect::&lt;Vec&lt;_&gt;&gt;();
    // 4.
    let results = join_all(handles).await;
    return results.into_iter().sum::&lt;i8&gt;();
};</pre>
			<p>Here, we can see that we define our futures (<em class="italic">1</em>), and then we add them to our vector (<em class="italic">2</em>). We then loop <a id="_idIndexMarker298"/>through our futures in our vector using the <code>into_iter</code> function. We then <a id="_idIndexMarker299"/>spawn a thread on each future using <code>async_std::task::spawn</code>. This is similar to <code>std::task::spawn</code>. So, why bother with all this extra headache? We could just loop through the vector and spawn a thread for each task. The difference here is that the <code>async_std::task::spawn</code> function is spinning off an <code>async</code> task in the same thread. Therefore, we are concurrently running both futures in the same thread! We then join all the handles, <code>await</code> for these tasks to finish, and then return the sum of all these threads. Now that we have defined our <code>async_outcome</code> future, we can run and time it with the following code:</p>
			<pre class="source-code">
let now = time::Instant::now();
let result = block_on(async_outcome);
println!("time elapsed for join vec {:?}", now.elapsed());
println!("Here is the result: {:?}", result);</pre>
			<p>Running our additional code will give the following additional printout:</p>
			<pre class="console">
number 4 is running
number 5 is running
time elapsed for join vec 2.007713458s
Here is the result: 4</pre>
			<p>It’s working! We have managed to get two <code>async</code> tasks running at the same time in the same thread, resulting in both futures being executed in just over 2 seconds!</p>
			<p>As we can see, spawning threads and <code>async</code> tasks in Rust is straightforward. However, we must note that passing<a id="_idIndexMarker300"/> variables into threads and <code>async</code> tasks is not. Rust’s borrowing <a id="_idIndexMarker301"/>mechanism ensures memory safety. We must go through extra steps when passing data into a thread. Further discussion on the general concepts behind sharing data between threads is not conducive to our web project. However, we can briefly signpost what types allow us to share data:</p>
			<ul>
				<li><code>std::sync::Arc</code>: This type enables threads to reference outside data:<pre class="source-code">
use std::sync::Arc;</pre><pre class="source-code">
use std::thread;</pre><pre class="source-code">
let names = Arc::new(vec!["dave", "chloe", "simon"]);</pre><pre class="source-code">
let reference_data = Arc::clone(&amp;names);</pre><pre class="source-code">
    </pre><pre class="source-code">
let new_thread = thread::spawn(move || {</pre><pre class="source-code">
    println!("{}", reference_data[1]);</pre><pre class="source-code">
});</pre></li>
				<li><code>std::sync::Mutex</code>: This type enables threads to mutate outside data:<pre class="source-code">
use std::sync::Mutex;</pre><pre class="source-code">
use std::thread;</pre><pre class="source-code">
let count = Mutex::new(0);</pre><pre class="source-code">
    </pre><pre class="source-code">
let new_thread = thread::spawn(move || {</pre><pre class="source-code">
     count.lock().unwrap() += 1;</pre><pre class="source-code">
});</pre></li>
			</ul>
			<p>Inside the thread <a id="_idIndexMarker302"/>here, we dereference the result of the lock, unwrap it, and <a id="_idIndexMarker303"/>mutate it. It must be noted that the shared state can only be accessed once the lock is held. </p>
			<p>We have now covered enough of async programming to return to our web programming. Concurrency is a subject that can be covered in an entire book, one of which is referenced in the <em class="italic">Further reading</em> section. For now, we must get back to exploring Rust in web development to see how our knowledge of Rust async programming affects how we understand the Actix Web server. </p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor067"/>Exploring async and await with web programming</h1>
			<p>Knowing what <a id="_idIndexMarker304"/>we know about async <a id="_idIndexMarker305"/>programming, we can now see the <code>main</code> function <a id="_idIndexMarker306"/>in our web <a id="_idIndexMarker307"/>application in a different light, as follows:</p>
			<pre class="source-code">
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new( || {
        App::new()
        .route("/", web::get().to(greet))
        .route("/{name}", web::get().to(greet))
        .route("/say/hello", web::get().to(|| 
               async { "Hello Again!" }))
    })
    .bind("127.0.0.1:8080")?
    .workers(3)
    .run()
    .await
}</pre>
			<p>We know that our <code>greet</code> function is an <code>async</code> function and thus a future. We can also see that the <a id="_idIndexMarker308"/>closure we pass into the <code>/say/hello</code> view <a id="_idIndexMarker309"/>also utilizes <a id="_idIndexMarker310"/>the <code>async</code> syntax. We can also <a id="_idIndexMarker311"/>see that the <code>HttpServer::new</code> function utilized the <code>await</code> syntax in <code>async fn main()</code>. Therefore, we can deduce that our <code>HttpServer::new</code> function is an executor. However, if we were to remove the <code>#[actix_web::main]</code> macro, we would get the following error:</p>
			<pre class="console">
`main` function is not allowed to be `async`</pre>
			<p>This is because our <code>main</code> function, which is our entry point, would return a future as opposed to running our program. <code>#[actix_web::main]</code> is a runtime implementation and enables everything to be run on the current thread. The <code>#[actix_web::main]</code> macro marks the <code>async</code> function (which, in this case, is the <code>main</code> function) to be executed by the Actix system.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">At the risk of getting into the weeds here, the Actix crate runs concurrent computation based on the actor model. This is where an actor is a computation. Actors can send and receive messages to and from each other. Actors can alter their own state, but they can only affect other actors through messages, which removes the need for lock-based synchronization (the mutex we covered is lock-based). Further exploration of this model will not help us to develop basic web apps. However, the Actix crate does have good documentation on coding concurrent systems with Actix at <a href="https://actix.rs/book/actix">https://actix.rs/book/actix</a>.</p>
			<p>We have covered a lot here. Do not feel stressed if you do not feel like you have retained all of it. We’ve briefly covered a range of topics around asynchronous programming. We do not need to understand it inside out to start building applications based on the Actix Web framework.</p>
			<p>You may also feel like we have been excessive in what we have covered. For instance, we could <a id="_idIndexMarker312"/>have spun up a server and used <a id="_idIndexMarker313"/>the <code>async</code> syntax when needed <a id="_idIndexMarker314"/>to merely punch out views without <a id="_idIndexMarker315"/>really knowing what is going on. Not understanding what is going on but knowing where to put <code>async</code> would not have slowed us down when building our toy application. However, this whistle-stop tour is invaluable when it comes to debugging and designing applications. To establish this, we can look at an example in the wild. We can look at this smart <em class="italic">Stack Overflow</em> solution <a id="_idIndexMarker316"/>to running multiple servers in one file: <a href="https://stackoverflow.com/questions/59642576/run-multiple-actix-app-on-different-ports">https://stackoverflow.com/questions/59642576/run-multiple-actix-app-on-different-ports</a>. </p>
			<p>The code in the <em class="italic">Stack Overflow</em> solution involves basically running two servers at one runtime. First, they <a id="_idIndexMarker317"/>define the views with the following code:</p>
			<pre class="source-code">
use actix_web::{web, App, HttpServer, Responder};
use futures::future;
async fn utils_one() -&gt; impl Responder {
    "Utils one reached\n"
}
async fn health() -&gt; impl Responder {
    "All good\n"
}</pre>
			<p>Once the <a id="_idIndexMarker318"/>views are defined, the two <a id="_idIndexMarker319"/>servers are <a id="_idIndexMarker320"/>defined in <a id="_idIndexMarker321"/>the <code>main</code> function:</p>
			<pre class="source-code">
#[actix_rt::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    let s1 = HttpServer::new(move || {
            App::new().service(web::scope("/utils").route(
            "/one", web::get().to(utils_one)))
        })
        .bind("0.0.0.0:3006")?
        .run();
    let s2 = HttpServer::new(move || {
            App::new().service(web::resource(
            "/health").route(web::get().to(health)))
        })
        .bind("0.0.0.0:8080")?
        .run();
    future::try_join(s1, s2).await?;
    Ok(())
}</pre>
			<p>I have not added <a id="_idIndexMarker322"/>any notation to this code, but it <a id="_idIndexMarker323"/>should not intimidate you. We <a id="_idIndexMarker324"/>can confidently deduce <a id="_idIndexMarker325"/>that <code>s1</code> and <code>s2</code> are futures that the <code>run</code> function returns. We then join these two futures together and <code>await</code> for them to finish. There is also a slight difference between our code and the code in the <em class="italic">Stack Overflow</em> solution. Our solution utilizes <code>await?</code> and then returns <code>Ok</code> with the following code snippet:</p>
			<pre class="source-code">
    future::try_join(s1, s2).await?;
    Ok(())
}</pre>
			<p>This is <a id="_idIndexMarker326"/>because a <code>?</code> operator is essentially <a id="_idIndexMarker327"/>a <code>try</code> match. <code>join(s1, s2).await?</code> expands roughly to the following code:</p>
			<pre class="source-code">
match join(s1, s2).await {
    Ok(v) =&gt; v,
    Err(e) =&gt; return Err(e.into()),
}</pre>
			<p>Whereas <code>join(s1, s2).await.unwrap()</code> expands roughly to the following code:</p>
			<pre class="source-code">
match join(s1, s2).await {
    Ok(v) =&gt; v,
    Err(e) =&gt; panic!("unwrap resulted in {}", e),
}</pre>
			<p>Because of the <code>?</code> operator, the person providing the solution has to insert <code>Ok</code> at the end because the <code>main</code> function returns <code>Result</code>, and this was taken away by implementing the <code>?</code> operator.  </p>
			<p>Thus, in the wild solution, <em class="italic">Stack Overflow</em> has demonstrated the importance of covering async <a id="_idIndexMarker328"/>programming. We can look at code in the wild and work out what is going on and how the posters on <em class="italic">Stack Overflow</em> managed to achieve what they did. This can also mean that we can get creative ourselves. There is nothing stopping us from creating three servers and running them in the <code>main</code> function. This is where Rust really shines. Taking the time to learn Rust gives us the ability to safely dive into low-level territory and have more fine-grain control over what we do. You will find this is true in any field of programming done with Rust. </p>
			<p>There is one more concept that we should investigate before trying to build our application, and this is <code>main</code> function to be a future. If we <a id="_idIndexMarker329"/>look at the <strong class="bold">Tokio</strong> crate, we can see that it is an <a id="_idIndexMarker330"/>asynchronous runtime for the Rust programming <a id="_idIndexMarker331"/>language by providing the <a id="_idIndexMarker332"/>building blocks needed to write <a id="_idIndexMarker333"/>network applications. The workings of Tokio are complex; however, if we look at the Tokio documentation on speeding up the runtime, we can add diagrams like the following one:</p>
			<div><div><img src="img/Figure_3.3_B18722.jpg" alt="Figure 3.3 – Speeding up the Tokio runtime [source: Tokio Documentation (2019) (https://tokio.rs/blog/2019-10-scheduler)]"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Speeding up the Tokio runtime [source: Tokio Documentation (2019) (<a href="https://tokio.rs/blog/2019-10-scheduler">https://tokio.rs/blog/2019-10-scheduler</a>)]</p>
			<p>In the preceding figure, we can see that there are tasks queued up and processors processing them. We processed our tasks earlier, so this should look familiar. Considering <a id="_idIndexMarker334"/>this, it might not be too shocking to <a id="_idIndexMarker335"/>know that we can use Tokio instead <a id="_idIndexMarker336"/>of the Actix Web macro to run <a id="_idIndexMarker337"/>our server. To do this, we define our Tokio dependency in the <code>Cargo.toml</code> file with the following code:</p>
			<pre class="source-code">
tokio = { version = "1.17.0", features = ["full"] }</pre>
			<p>With the preceding code, we can now switch our macro in the <code>main.rs</code> file with the following code:</p>
			<pre class="source-code">
#[tokio::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new( || {
        App::new()
        .route("/", web::get().to(greet))
        .route("/{name}", web::get().to(greet))
    })
    .bind("127.0.0.1:8080")?
    .bind("127.0.0.1:8081")?
    .workers(3)
    .run()
    .await
}</pre>
			<p>Running the preceding code will give us the same outcome as running a server. There might be some inconsistencies when using Tokio instead of our Actix runtime macro. While this is an interesting result that demonstrates how we can confidently configure our server, we will use the Actix runtime macro for the rest of the book when it comes to developing the to-do application in Actix. We will revisit Tokio in <a href="B18722_14.xhtml#_idTextAnchor279"><em class="italic">Chapter 14</em></a>, <em class="italic">Exploring the Tokio Framework</em>. </p>
			<p>We have now <a id="_idIndexMarker338"/>covered enough of server configuration <a id="_idIndexMarker339"/>and how the server processes <a id="_idIndexMarker340"/>requests to be productive. We can <a id="_idIndexMarker341"/>now move on to defining our views and how they are handled in the next section. </p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor068"/>Managing views using the Actix Web framework</h1>
			<p>So far, we have defined all our views in the <code>main.rs</code> file. This is fine for small projects; however, as <a id="_idIndexMarker342"/>our project grows, this will not scale <a id="_idIndexMarker343"/>well. Finding the right views can be hard, and updating them can lead to mistakes. It also makes it harder to remove modules from or insert them into your web application. Also, if we have all the views being defined on one page, this can lead to a lot of merge conflicts if a bigger team is working on the application, as they will all want to alter the same file if they are altering the definitions of views. Because of this, it is better to keep the logic of a set of views contained in a module. We can explore this by building a module that handles authentication. We will not be building the logic around authentication in this chapter, but it is a nice straightforward example to use when exploring how to manage the structure of a views module. Before we write any code, our web application should have the following file layout:</p>
			<pre class="source-code">
├── main.rs
└── views
    ├── auth
    │   ├── login.rs
    │   ├── logout.rs
    │   └── mod.rs
    ├── mod.rs</pre>
			<p>The code inside each file can be described as follows:</p>
			<ul>
				<li><code>main.rs</code>: The entry point for the server where the server is defined </li>
				<li><code>views/auth/login.rs</code>: The code defining the view for logging in</li>
				<li><code>views/auth/logout.rs</code>: The code defining the view for logging out</li>
				<li><code>views/auth/mod.rs</code>: The factory that defines the views for <code>auth</code></li>
				<li><code>views/mod.rs</code>: The factory that defines all the views for the whole app</li>
			</ul>
			<p>First, let us start off our entry point with a basic web server with no extras in the <code>main.rs</code> file, with the following code:</p>
			<pre class="source-code">
use actix_web::{App, HttpServer};
mod views;
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new(|| {
        let app = App::new();
        return app
    })
        .bind("127.0.0.1:8000")?
        .run()
        .await
}</pre>
			<p>This preceding <a id="_idIndexMarker344"/>code is straightforward <a id="_idIndexMarker345"/>and there should be no surprises. We will alter the code later, and we can move on to defining the views. For this chapter, we just want to return a string saying what the view is. We will know that our application structure works. We can define our basic login view in the <code>views/auth/login.rs</code> file with the following code:</p>
			<pre class="source-code">
pub async fn login() -&gt; String {
    format!("Login view")
}</pre>
			<p>Now, it will not be surprising that the logout view in the <code>views/auth/logout.rs</code> file takes the following form:</p>
			<pre class="source-code">
pub async fn logout() -&gt; String {
    format!("Logout view")
}</pre>
			<p>Now that our views have been defined, all we need to do is define the factories in the <code>mod.rs</code> files to enable our server to serve them. Our factories give the data flow of our app, taking the following form:</p>
			<div><div><img src="img/Figure_3.4_B18722.jpg" alt="Figure 3.4 – The data flow of our application"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – The data flow of our application</p>
			<p>We can see in <em class="italic">Figure 3</em><em class="italic">.4</em> that chaining factories gives us a lot of flexibility. If we wanted to remove all the <code>auth</code> views from our application, we would be able to do this by simply removing <a id="_idIndexMarker346"/>one line of code in our main view factory. We <a id="_idIndexMarker347"/>can also reuse our modules. For instance, if we were to use the <code>auth</code> module on multiple servers, we could merely have a git submodule for the <code>auth</code> views module and use it on other servers. We can build our <code>auth</code> module factory view in the <code>views/auth/mod.rs</code> file with the following code:</p>
			<pre class="source-code">
mod login;
mod logout;
use actix_web::web::{ServiceConfig, get, scope};
pub fn auth_views_factory(app: &amp;mut ServiceConfig) {
    app.service(scope("v1/auth").route("login", 
                get().to(login::login)).route("logout", 
                get().to(logout::logout))
    );
}</pre>
			<p>In the preceding code, we can see that we pass in a mutable reference of a <code>ServiceConfig</code> struct. This enables us to define things such as views on the server in different fields. The documentation on this struct states that it is to allow bigger applications to split <a id="_idIndexMarker348"/>up a configuration into <a id="_idIndexMarker349"/>different files. We then apply a service to the <code>ServiceConfig</code> struct. The service enables us to define a block of views that all get populated with the prefix defined in the scope. We also state that we are using <code>get</code> methods, for now, to make it easily accessible in the browser. We can now plug the <code>auth</code> views factory into the <code>main</code> views factory in the <code>views/mod.rs</code> file with the following code:</p>
			<pre class="source-code">
mod auth;
use auth::auth_views_factory;
use actix_web::web::ServiceConfig;
pub fn views_factory(app: &amp;mut ServiceConfig) {
    auth_views_factory(app);
}</pre>
			<p>In the preceding code, we have been able to chop our entire views modules with just one line of code. We can also chain the modules as much as we like. For instance, if we wanted to have submodules within the <code>auth</code> views module, we could, and we merely feed the factories of those <code>auth</code> submodules into the <code>auth</code> factory. We can also define multiple services in a factory. Our <code>main.rs</code> file remains pretty much the same with the addition of a <code>configure</code> function, as seen with the following code:</p>
			<pre class="source-code">
use actix_web::{App, HttpServer};
mod views;
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new(|| {
        let app = 
            App::new().configure(views::views_factory);
        return app
    })
        .bind("127.0.0.1:8000")?
        .run()
        .await
}</pre>
			<p>When <a id="_idIndexMarker350"/>we call the <code>configure</code> function <a id="_idIndexMarker351"/>on the <code>App</code> struct, we pass the views factory into the <code>configure</code> function, which will pass the <code>config</code> struct into our factory function for us. As the <code>configure</code> function returns <code>Self</code>, meaning the <code>App</code> struct, we can return the result at the end of the closure. We can now run our server, resulting in the following outcome: </p>
			<div><div><img src="img/Figure_3.5_B18722.jpg" alt="Figure 3.5 – The login view"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – The login view</p>
			<p>We can see that <a id="_idIndexMarker352"/>our application with the expected prefix <a id="_idIndexMarker353"/>works! With this, we have covered all the basics to handle HTTP requests with confidence. </p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>Summary</h1>
			<p>In this chapter, we covered the basics of threading, futures, and <code>async</code> functions. As a result, we were able to look at a multi-server solution in the wild and understand confidently what was going on. With this, we built on the concepts we learned in the previous chapter to build modules that define views. In addition, we chained factories to enable our views to be constructed on the fly and added to a server. With this chained factory mechanism, we can slot entire view modules in and out of a configuration when the server is being built. </p>
			<p>We also built a utility struct that defines a path, standardizing the definition of a URL for a set of views. In future chapters, we will use this approach to build authentication, JSON serialization, and frontend modules. With what we’ve covered, we’ll be able to build views that extract and return data from the user in a range of different ways in the next chapter. With this modular understanding, we have a strong foundation that enables us to build real-world web projects in Rust where logic is isolated and can be configured, and where code can be added in a manageable way. </p>
			<p>In the next chapter, we will work on processing requests and responses. We will learn how to pass params, bodies, headers, and forms to views and process them by returning JSON. We will be using these new methods with the to-do module we built in the previous chapter to enable our interaction with to-do items through server views.</p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor070"/>Questions</h1>
			<ol>
				<li value="1">What parameter is passed into the <code>HttpServer::new</code> function and what does the parameter return?</li>
				<li>How is a closure different from a function?</li>
				<li>What is the difference between a process and a thread?</li>
				<li>What is the difference between an <code>async</code> function and a normal one?</li>
				<li>What is the difference between <code>await</code> and <code>join</code>?</li>
				<li>What is the advantage of chaining factories?</li>
			</ol>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/>Answers</h1>
			<ol>
				<li value="1">A closure is passed into the <code>HttpServer::new</code> function. The <code>HttpServer::new</code> function has to return the <code>App</code> struct so that the <code>bind</code> and <code>run</code> functions can be acted on them after the <code>HttpServer::new</code> function has fired.</li>
				<li>A closure can interact with variables outside of its scope.</li>
				<li>A process is a program that is being executed with its own memory stack, registers, and variables, whereas a thread is a lightweight process that is managed independently but shares data with other threads and the main program.</li>
				<li>A normal function executes as soon as it is called, whereas an <code>async</code> function is a promise and must be executed with a blocking function.</li>
				<li><code>await</code> blocks a program to wait for a future to be executed; however, the <code>join</code> function can run multiple threads or futures concurrently. <code>await</code> can also be executed on a <code>join</code> function. </li>
				<li>Chaining factories gives us flexibility on how individual modules are constructed and orchestrated. A factory inside a module focuses on how the module is constructed, and the factory outside the module focuses on how the different modules are orchestrated.</li>
			</ol>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/>Further reading</h1>
			<ul>
				<li><em class="italic">Hands-On Concurrency with Rust</em> (2018) by <em class="italic">Brian Troutwine</em>, <em class="italic">Packt Publishing</em></li>
			</ul>
		</div>
	</body></html>
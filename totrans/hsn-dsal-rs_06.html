<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Exploring Maps and Sets</h1>
                </header>
            
            <article>
                
<p class="mce-root">Up until this chapter, data structures have only become faster for searching, and this chapter is no different. What makes it different is why and how data can be found in two higher-level data structures: maps and sets. While the former is also known as dictionary, associative array, object, or hash table, the latter commonly crosses people's minds as a mathematical concept. Both can rely on hashing, a technique that allows for constant (or close to constant) time retrieval of items, checking whether they are contained in a set, or routing requests in distributed hash tables.</p>
<p class="mce-root">These data structures are also one level higher than the previous ones, since all of them build on existing structures, such as dynamic arrays or trees, and to top things off, the chapter starts with an algorithm. Understanding this chapter will be great preparation heading into the second part of the book, where algorithms are the main focus. Topics learned in this chapter include the following<span>:</span></p>
<ul>
<li>Hashing functions and what they are good for</li>
<li>How to implement a set based on different data structures</li>
<li>What makes maps special</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hashing</h1>
                </header>
            
            <article>
                
<p><span>The birthday paradox is a well-known phenomenon; two people share this special day that year, seemingly often, and we still get excited when it happens. Statistically speaking, the probability of meeting someone like this is really high, since in a room of just 23 people, the probability is already at 50%. While this may be an interesting fact, why is this introducing a section about hashing?</span></p>
<p>Birthdays can be considered a hash function—although a bad one. Hash functions are functions that map one value onto another value of a fixed size, like combining the day and month of a birthday into <kbd>u64</kbd>, shown as follows:</p>
<pre>fn bd_hash(p: &amp;Person) -&gt; u64 {<br/>    format!("{}{}", p.day, p.month) as u64<br/>} </pre>
<p>This function will prove very ineffective indeed, shown as follows:</p>
<ul>
<li>It is very hard to find out someone's birthday deterministically without asking them</li>
<li>The space is limited to 366 unique values, which also makes collisions very likely</li>
<li>They are not evenly distributed across the year</li>
</ul>
<p>What makes a good hash function? It depends on the use case. There are many properties that can be associated with a hash function, such as the following:</p>
<ul>
<li>One way or two way (that is, given a hash, can one get the original value back?)</li>
<li>Deterministic</li>
<li>Uniform</li>
<li>Fixed or variable range</li>
</ul>
<p>Designing good hash functions is a <em>very</em> hard task in any field; there are countless algorithms that have been shown to be too weak for their designed purpose after several years of use, with SHA-1 being the latest prominent victim.</p>
<p><span>There is a wide variety of hashing algorithms for all kinds of use cases available, ranging from cryptographically secure to something akin to a parity bit to mitigate tampering. This section will focus on a few areas that we deemed interesting; for a wider picture, Wikipedia (<a href="https://en.wikipedia.org/wiki/List_of_hash_functions">https://en.wikipedia.org/wiki/List_of_hash_functions</a>) provides a list that shows a number of available hashing algorithms and their articles.</span></p>
<p><strong>Signatures</strong> are one of the most important fields for hashing algorithms and they can be as simple as the last digit on a credit card number (to validate the number) to 512-bit strong cryptographic digest functions, where a single collision is the end of that particular algorithm.</p>
<p>Outside of cryptography, hashing is used in completely different areas as well, such as peer-to-peer routing or encoding information in a tree-like structure. <strong>GeoHashes</strong> are a great example; instead of comparing longitude and latitude, these GeoHashes allow to quickly check if an area is located close to (or within) another area by comparing the first few characters of the hash. The algorithm was put into the public domain and can be found under <a href="http://geohash.org/">http://geohash.org/</a>. Collisions in this space can be ruled out since the entire space of possible input variations (coordinates on planet Earth) is known beforehand.</p>
<p><span>What are <strong>collisions</strong>? A collision occurs when two different input parameters lead to the same output, making the hash ambiguous. In cryptography, this fact will lead to a large scale crisis, just like it would if you found another key that matches your door lock. The main difference being that in the physical world, trying every door in your neighborhood is highly impractical, but with fully connected computers, this can be done in a matter of seconds. This means that the potential inputs are just as important as the quality of the hashing function itself—be it time and practicality (like physical items), or the applicable range (Earth coordinates, maximum number of nodes in a cluster)—transferring a function to a domain with a larger range leads to unexpected outcomes.</span></p>
<p>To summarize, collisions appear when the potential space of a key is either not large enough to withstand a full enumeration (brute force), or the outputs of the hash function are unevenly distributed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Create your own</h1>
                </header>
            
            <article>
                
<p>For the purpose of representing an object as a number (for use in a hash map or for comparison), most languages' built-in types come with a solid hash function for exactly that purpose, so building your own is almost never a good idea, unless a lot of time and effort goes into it. The better choice is to use what's built-in, or use a library that provides tested and proven methods.</p>
<p>It is important though to know how those functions are built, so let's create a trivial implementation <span>to analyze</span> the basic principles. The following example is one that uses the XOR operation on the previous and current byte to save their binary differences, then shifts it to the left up to four times (to fill up the <kbd>u32</kbd> type):</p>
<pre>pub fn hashcode(bytes: &amp;[u8]) -&gt; u32 {<br/>    let mut a = 0_u32;<br/>    for (i, b) in bytes.iter().enumerate() {<br/>        a ^= *b as u32;<br/>        a &lt;&lt;= i % 4;</pre>
<pre>    }<br/>    a<br/>}</pre>
<p class="CDPAlignLeft CDPAlign">When this function is applied to a range of repeated letter strings, how are the values distributed? A histogram and a scatter plot tell the story, shown as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9a669b5e-197c-458e-a30f-6af514eaca50.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The output chart of the XOR Hasher</div>
<p>This histogram shows the distribution of the hash output, when the function is applied to all combinations of ten <kbd>AA</kbd>-<kbd>ZZ</kbd>, but each letter repeated ten times, so the first string is <kbd>AAAAAAAAAAAAAAAAAAAA</kbd> <span>(20 letters), the last string is</span> <kbd>ZZZZZZZZZZZZZZZZZZZZ</kbd><span>, yielding 675 combinations of 20 letter "words</span>." This leads to a less optimal distribution, where the highest frequency is five times as high as the lowest. While speed can be a factor in using that function, it will clearly produce suboptimal results for cryptography.</p>
<p>In a scatter plot, this looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2d6b5bb2-22e5-4a17-bfbd-70218b754f4c.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The output graph of the scatter plot</div>
<p>The scatter plot shows a different story. On the <em>x</em> axis, the index of each combination is shown, the <em>y</em> axis shows the hash output. Therefore, horizontal lines mean collisions, and they are all over the place! It can be interesting to explore further properties of a function like this, but the first results look quite dire, and searching for a better algorithm is the best use of anyone's time. Let's move on to checksums and digests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Message digestion</h1>
                </header>
            
            <article>
                
<p style="font-size: 16px">Message digests are created as a way to guarantee authenticity; if a message was sent, a digest or signature of this message provides an ability to check whether the message has been tampered with. Typically, the signature will therefore be transmitted differently than the original message.</p>
<p style="font-size: 16px">Obviously, this requires the hashing function to adhere to some basic rules to be considered good, listed as follows:</p>
<ul>
<li>A signature has to be quick and easy to obtain regardless of message size</li>
<li>The signature can only have a fixed length</li>
<li>The function has to minimize collisions</li>
</ul>
<p>The hash functions contained in this group are the most popular ones and are the objective of many security researchers: MD5, SHA-1/2/3, or Adler 32. Adler 32 is prominently used in the <kbd>zlib</kbd> library to ensure the file's integrity, but should not be used to authenticate messages, thanks to the limited output space of 32-bit. However, it is easy to implement and understand, which makes it great for the purposes of this book:</p>
<pre>const MOD_ADLER: u32 = 65521;<br/><br/>pub fn adler32(bytes: &amp;[u8]) -&gt; u32 {<br/>    let mut a = 1_u32;<br/>    let mut b = 0_u32;<br/><br/>    for byte in bytes {<br/>        a = (a + byte as u32) % MOD_ADLER;<br/>        b = (b + a) % MOD_ADLER;<br/>    }<br/><br/>    (b &lt;&lt; 16) | a<br/>}</pre>
<p>The algorithm sums up the bytes of any byte stream, and avoids an overflow by applying the modulo operation, using a large prime number (<kbd>65521</kbd>), which makes it harder for a byte to change without changing the final result. The algorithm has considerable weaknesses since there are many ways to change the operands of a sum without affecting the outcome!</p>
<p>Additionally, rolling over (after the modulo is applied) gives some weight to the order of bytes, so if the sum of bytes is not large enough, the algorithm is expected to produce even more collisions. Generally, this algorithm primarily protects against random transmission errors that cause bits to change, and is not useful in authenticating messages.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Wrap up</h1>
                </header>
            
            <article>
                
<p>Hashing is a very useful tool that developers use every day—knowingly or unknowingly. Integer comparisons are fast, so checking the equality of two strings can be improved by comparing their hashes. Diverse keys can be made comparable by hashing—a method that is used in distributed databases to assign a partition to a row.</p>
<div class="packt_infobox"><strong>Modulo hashing</strong> is a technique that lets a distributed database assign a row to a partition deterministically. Hash the row's key, then use the modulo operator with the maximum number of partitions to receive a destination to store the row.</div>
<p>Earlier, we explored some hash functions (XOR-based and Adler 32), but we never compared them. Additionally, Rust's standard library offers a hash function (built for <kbd>HashSet&lt;K,V&gt;</kbd>/<kbd>HashMap&lt;K,V&gt;</kbd>, and implemented for all standard types), which is a good baseline.</p>
<p>First, histograms—to show how many occurrences each hash has. <span>As mentioned before, the XOR-based approach yields a very strange distribution, where some hashes clearly appear more often than others, shown as follows</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7cc10d71-afea-442e-abe4-05f40b742a5e.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The output chart of the XOR Hasher</div>
<p><span>The Adler checksum creates a normal distribution in this case, which is probably due to the repetitive content, and the commutative nature of summing up numbers (<em>2 + 1 = 1 + 2</em>). Considering that transmission errors in compressed files are probably creating repetition, it looks like a solid choice for that use case. It would not do well i</span>n most other scenarios though:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ffeb7cb5-8ac8-4035-ba7f-0fab812a6107.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The output chart of Adler 32</span></div>
<p class="CDPAlignLeft CDPAlign">The following is Rust's default choice, the <kbd>SipHash</kbd> based <kbd>DefaultHasher</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6c686356-d22a-4834-ade0-4582e766951f.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The output chart of the Rust DefaultHasher</span></div>
<p>Seeing the three distributions, their use in a hash table, where the frequency directly translates to the length of the lists at each bucket, becomes obvious. While it's best to have a length of one, lists of the same length at least yield the best performance if there is <em>any</em> collision. The Rust standard library clearly made a great choice with the <kbd>SipHash</kbd> based (<a href="https://link.springer.com/chapter/10.1007/978-3-642-34931-7_28">https://link.springer.com/chapter/10.1007/978-3-642-34931-7_28</a>) implementation.</p>
<p>A comparative scatter plot also sheds some light on the behavior of hash functions. Be aware that it is log-scaled to fit the results into a manageable plot, shown as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/26552fa7-52f4-4792-99dd-273042acc591.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The comparison plot for XOR, Adler 32, and DefaultHasher</span></div>
<p>While the scale does not allow for a detailed judgment, what appears to be a line is always a collision-heavy behavior. As expected from the histograms, the Adler 32 and XOR-based approach both do not show a cloud. Since the <em>y</em> axis shows the actual hash (log-scaled), the more vertically spread it is, the better the distribution. Ideally, there would be a unique hash for each <em>x</em> value, but roughly the same number of dots for each <em>y</em> value predict a uniform hash function. Again, Rust's <kbd>DefaultHasher</kbd> looks very good in this plot, while both contenders show less optimal behaviors when used in similar cases.</p>
<p>A word of caution in the end. This is a software developer's perspective on hashing: security researchers and professionals know <em>a lot</em> more about hashing. It should be left to them to come up with new ways to create message signatures, so we can focus on building great software and use the best possible components to do that. In short: <em>do not build your own hash function for any production system.</em></p>
<p>Now, for some practical application of hashing in a data structure: the map.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Maps</h1>
                </header>
            
            <article>
                
<p class="mce-root">Index operations in arrays are fast, simple, and easy to understand, with one drawback: they only work with integers. Since an <strong>array</strong> is a continuous portion in memory that can be accessed by dividing it evenly, which makes the jumps between the elements easy, can this work with arbitrary keys as well? Yes! Enter maps.</p>
<p><strong>Maps</strong> (also called dictionaries or associative arrays), are data structures that store and manage unique key-value pairs in an efficient way. These structures aim to quickly provide access to the values associated with the keys that are typically stored in one of the following two ways:</p>
<ul>
<li>A hashtable</li>
<li>A tree</li>
</ul>
<p>When key-value pairs are stored in a tree, the result is very similar to what was discussed in the previous chapter: self-balancing trees will provide consistent performance, avoiding the worst-case cost of a hash map.</p>
<p>Since trees have been discussed extensively in the previous chapter, the hash <span>map</span> is the main focus in this section. It uses a hashing function to translate the provided key into a number of some sort, which is in turn "mapped" on array buckets. This is where the entire pair is typically stored as a list (or tree) to deal with collisions effectively. Whenever a key is looked up, the map can search the associated bucket for the exact key. A key-value pair is inserted by hashing the key, using the modulo operation to find a spot in the array, and appending the pair to the list at the bucket.</p>
<p>If two or more elements are in that list, one or more collisions have occurred:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aff5dc8b-2f6c-4d11-943a-94de51c78130.png" style="width:24.42em;height:22.33em;"/></p>
<p>While this usually results in great access times, whenever similar hashes have to be stored (due to a bad hash function), the worst case scenario will be a search through an unordered list—with linear performance. This results in a boxed slice that holds all the data in the form of an <kbd>Entry</kbd> type, a vector of tuples. In this case, the implementation is even using generics:</p>
<pre>type Entry&lt;K, V&gt; = Vec&lt;(K, V)&gt;;<br/><br/>pub struct HashMap&lt;K, V&gt;<br/>where<br/> K: PartialEq + Clone,<br/> V: Clone,<br/>{<br/>   hash_fn: Box&lt;dyn (Fn(&amp;K) -&gt; usize)&gt;,<br/>   store: Box&lt;[Entry&lt;K, V&gt;]&gt;,<br/>   pub length: usize,<br/>}</pre>
<p>Additionally, the hash function can be freely chosen and is stored as a boxed function, which makes it handy to store within the object, and call whenever required. This also lets users customize the type of hashing for a particular use case.</p>
<p>By associating an index with a certain hash, a map lacks the ability to traverse its content in any kind of order. Therefore, keys and values cannot be iterated over in any kind of order, requiring sorting before any operation happens.</p>
<p>Once again, the product team is innovating and another feature would really add a lot of value to customers: associating postcodes with their factual data about the location. This way, a web service can cache commonly used data and reduce the load on the database, while serving customers a lot quicker! Since these locations are updated manually, an expiration is not required and the map can be filled on startup.</p>
<p>Customers provided a list of concise requirements as well to assist, shown as follows:</p>
<ul>
<li>Insert location information under their unique name</li>
<li>Quickly retrieve information using their name</li>
<li>Fetch all location names and associated information</li>
<li>Update locations using their name</li>
</ul>
<p>A hash table would do a great job here, would it not?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A location cache</h1>
                </header>
            
            <article>
                
<p>Caching values is a typical use case for maps because even a large number of items won't affect the performance much, since the keys are always distinct. These keys can even carry information themselves!</p>
<p>For the use case defined in the last section, each customer uses postcodes within a country to identify locations; they typically cover an area that only holds a single office. Postal codes are stored as strings to cover the real world's wide variety of systems, and they are unique per country.</p>
<p>Thanks to a previous generic implementation, the entire <kbd>LocationCache</kbd> type can be an alias to a specialized <kbd>HashMap</kbd>, only requiring the hash function to be supplied on creation, shown as follows:</p>
<div>
<pre><span>pub</span><span> </span><span>type</span><span> </span><span>LocationCache</span><span> = HashMap&lt;</span><span>String</span><span>, LocationInformation&gt;;</span></pre></div>
<p><span>The</span> <kbd>HashMap</kbd> <span>itself is a custom implementation that contains a key of type <kbd>K</kbd>, which has to also implement</span> <kbd>PartialEq</kbd> <span>(for comparing key instances directly), and</span> <kbd>Clone</kbd> <span>(for practical reasons).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The hash function</h1>
                </header>
            
            <article>
                
<p>In addition to providing a generic data structure, the implementation lets the user supply a custom hash function that only maps a reference to the key type to a <kbd>usize</kbd> return type. The choice for the return type is arbitrary, and was chosen to avoid overflows.</p>
<p>Since the previously implemented hash function performed better than the Adler 32 checksum algorithm, the location cache will use this. To recall, the algorithm applies XOR between a byte and its predecessor and then bit shifts to the left, based on the byte's index. <span>Alternatively, Rust's</span> <kbd>DefaultHasher</kbd> <span>is available as well:</span></p>
<pre>pub fn hashcode(bytes: &amp;[u8]) -&gt; u32 {<br/>    let mut a = 0_u32;<br/>    for (i, b) in bytes.iter().enumerate() {<br/>        a ^= *b as u32;<br/>        a &lt;&lt;= i % 4;<br/>    }<br/>    a<br/>}</pre>
<p>Choosing a hashing algorithm is an important decision, as we will see in the <em>Wrap up</em> section. But first, locations need to be added!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding locations</h1>
                </header>
            
            <article>
                
<p>In order to add a location, there are two important steps:</p>
<ol>
<li><span>Compute the hash</span></li>
<li><span>Choose a bucket</span></li>
</ol>
<p>Further operations, such as doing a sorted insert, will improve performance too, but they can be omitted by using a tree instead of a list within each bucket.</p>
<p>The location cache implementation uses a simple modulo operation between the hash and the length of the array to choose a bucket, which means that on top of regular hash collisions, choosing the size of the internal storage has a major influence on the performance as well. Choose a size too small and the buckets will overlap, regardless of the hash function!</p>
<p>In Rust code, the first part is done in the first line using the provided boxed <span><kbd>hashcode</kbd></span> function to create a hash. What follows is finding a bucket by applying something akin to the modulo operation (a binary AND operation between the hash and the highest index of the storage array) and a linear search of the attached list. If the key is found, the attached pair is updated and if not, it is added to the vector:</p>
<pre>pub fn insert(&amp;mut self, key: K, value: V) {<br/>    let h = (self.hash_fn)(&amp;key);<br/>    let idx = h &amp; (self.store.len() - 1);<br/>    match self.store[idx].iter().position(|e| e.0 == key) {<br/>        Some(pos) =&gt; self.store[idx][pos] = (key, value),<br/>        None =&gt; {<br/>            self.store[idx].push((key, value));<br/>            self.length += 1<br/>        }<br/>    }<br/>}</pre>
<p>Once a location and the matching hash is stored, it can be retrieved again.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fetching locations</h1>
                </header>
            
            <article>
                
<p>Just like inserting, the retrieval process has the same steps. Whether the <kbd>get()</kbd> function to return a value or the <kbd>remove()</kbd> function, both go through the same steps: hash, match a bucket, do a linear search, and lastly, match with the expected return type. The <kbd>get()</kbd> function can utilize Rust's powerful iterators by using <kbd>find</kbd> to match the predicate within a bucket's vector and, since an <kbd>Option&lt;Item&gt;</kbd> is returned, its <kbd>map</kbd> function to extract the value instead of returning the entire pair:</p>
<pre>pub fn get(&amp;self, key: &amp;K) -&gt; Option&lt;V&gt; {<br/>    let h = (self.hash_fn)(key);<br/>    let idx = h &amp; (self.store.len() - 1);<br/>    self.store[idx]<br/>        .iter()<br/>        .find(|e| e.0 == *key)<br/>        .map(|e| e.1.clone())<br/>}<br/><br/>pub fn remove(&amp;mut self, key: K) -&gt; Option&lt;V&gt; {<br/>    let h = (self.hash_fn)(&amp;key);<br/>    let idx = h &amp; (self.store.len() - 1);<br/>    match self.store[idx].iter().position(|e| e.0 == key) {<br/>        Some(pos) =&gt; {<br/>            self.length -= 1;<br/>            Some(self.store[idx].remove(pos).1)<br/>        }<br/>        _ =&gt; None,<br/>    }<br/>}</pre>
<p>The <kbd>remove</kbd> function is literally the inversion of an <kbd>insert</kbd> function; instead of updating the key-value pair if found, it is removed from the bucket and returned to the caller.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Wrap up</h1>
                </header>
            
            <article>
                
<p>Hash maps are a great data structure, and often their value cannot be overstated, especially in caching or to simplify code that would otherwise have to match labels (or keys) to values using array indices. Their key breaking points are the hash function itself, and the bucket selection and organization, all of which warrant entire PhD theses and papers in computer science.</p>
<p>While a hash map is quick and easy to implement, the real question is: how does it perform? This is a valid question! Software engineers are prone to prefer their own implementation over learning what others already created, and while this is the premise for this entire book, benchmarks keep us honest and help us to appreciate the work that others have done.</p>
<p>How did this <kbd>HashMap</kbd> do, especially compared to <kbd>std::collections::HashMap&lt;K,V&gt;</kbd>? We have seen the hash function is far from ideal in some histograms, but what are the performance implications? Here is a scatter plot to answer all of these questions; it shows the <kbd>HashMap</kbd> implemented here with different hashing functions (Adler 32, <kbd>DefaultHasher</kbd>, XOR-based) compared to the <kbd>HashMap&lt;K,V&gt;</kbd> from the standard library (which uses <kbd>DefaultHasher</kbd> exclusively). The following benchmarks were performed on the same 1,000 to 10,000 randomly permuted strings between <em>A</em> and <em>Z</em> of lengths of 10 to 26 characters. The <em>y</em> axis shows the time required for a <kbd>get()</kbd> operation in nanoseconds, the <em>x</em> axis shows the number of items in the map. The sizes represent the deviation of the result:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5aab1392-5007-4192-a2ca-0a8de69dc326.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The scatter plot of the deviation of the result in Adler 32, DefaultHasher, XOR-based, collections-HashMap</div>
<p>This plot shows the real value and use of the particular hash functions, as they were all applied to this <kbd>HashMap</kbd>, and the work of the amazing Rust community with <kbd>std::collections::HashMap&lt;K,V&gt;</kbd>, which uses the <kbd>DefaultHasher</kbd>. Adler 32, as a checksum algorithm, did rather badly, which was expected, with even an increasing variance as the number of inserted items increased. Surprisingly, the XOR-based algorithm was not as bad as expected, but still had a high variance compared to the <kbd>DefaultHasher</kbd>, which performed consistently well.</p>
<p>All of them are a far cry off the <kbd>HashMap&lt;K,V&gt;</kbd> that comes with the standard library. This is great news, because the performance of this hash map implementation is also worse than the trees and skip lists presented in <a href="84f203ac-a9f6-498b-90ff-e069c41aaca0.xhtml">Chapter 5</a>, <em>Robust Trees</em> and <a href="1a6971bb-ec24-47d5-b44c-cfb4da7d5b24.xhtml">Chapter 4</a>, <em>Lists, Lists, More Lists</em>.</p>
<p>This is proof that while the theory sounds great (constant time retrieval, best case)—implementation details can make or break a particular data structure, which is why we suspect that <kbd>collections::HashMap</kbd> sorts and inserts and use of traits instead of a boxed (hash) function to significantly improve performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upsides</h1>
                </header>
            
            <article>
                
<p>The hash map provides a great way to do key-value associations, which are highlighted as follows:</p>
<ul>
<li>Low overhead storage</li>
<li>Hashed complex keys by default thanks to hashing</li>
<li>Easy to understand</li>
<li>Constant time retrieval</li>
</ul>
<p>Yet, there are a few things that may be troublesome when compared to trees, or other efficient retrieval structures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downsides</h1>
                </header>
            
            <article>
                
<p class="mce-root">Even though constant time retrieval sounds nice, the benchmarks show that it's not that simple. The downsides are as follows:</p>
<ul>
<li>Performance highly depends on the hash function and application</li>
<li>Easy to implement naively, hard to get right</li>
<li>Unordered storage</li>
</ul>
<p>Some of these downsides could be mitigated by using a tree-based map, but that would be a tree as described in the previous chapter, and there is one data structure left to discuss here: the set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sets</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>Structured Query Language</strong> (<strong>SQL</strong>), is a declarative language invented to perform database operations. Its primary qualities are the ability to express <em>what</em> you want, rather than <em>how</em> you want it ("I want a set of items that conform to a predicate X" versus "Filter every item using predicate X"); this also allows non-programmers to work with databases, which is an aspect that today's NoSQL databases often lack.</p>
<p class="mce-root">You may think: how is that relevant? SQL allows us to think of the data as sets linked together with relations, which is what makes it so pleasant to work with. Understanding sets as a distinct collection of objects is sufficient to understand the language and how to manipulate the results. While this definition is also called the naive set theory, it is a useful definition for most purposes.</p>
<p>In general, a set has elements as members that can be described using a sentence or rule, like all positive integers, but it would contain every element only once and allow several basic operations: unions, intersections, differences, and the Cartesian product<span>, which is the combination of two sets so that elements are combined in every possible way</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4557c684-c99f-4c75-bb82-29d9df532471.png" style="width:22.33em;height:5.25em;"/></p>
<p>Since set elements are unique, any implementation of a set, therefore, has to make sure that each element is unique within the data structure, which is what makes the actual data structure special; it optimizes for uniqueness and retrieval.</p>
<p>What about using linear search on a vector to guarantee uniqueness? It works, but inserting in a populated set is going to take a lot longer than a new one. Additionally, the previous chapters talked about how trees are much better at finding things than lists, which is also why no good set implementation should use them.</p>
<p>The Rust collections in the standard library know two types of sets: <kbd>BTreeSet&lt;K,V&gt;</kbd> and <kbd>HashSet&lt;K,V&gt;</kbd>, both names that hint at their implementations. As mentioned in <a href="84f203ac-a9f6-498b-90ff-e069c41aaca0.xhtml">Chapter 5</a>, <em>Robust Trees</em>, the B-Tree is a generic, self-balancing tree implementation that allows an arbitrary number of children per node, and makes search within its keys very efficient.</p>
<p><span><kbd>HashSet&lt;K,V&gt;</kbd> is different. By storing a hash representation of the key, lookup can be done in constant time if the hashes are distributed uniformly. Since hash sets and hash maps have the same inner workings, this section will focus on a tree-based implementation and another section goes further into the depths of a hash map.</span></p>
<p>Other than inserting and checking whether a set contains a certain element, the main operations that a set should provide are union, intersect, and difference, as well as an iterator. Having these operations available will provide an efficient way to combine multiple sets in various ways, which is part of why they are useful.</p>
<p>In Rust code, a trie-based set could look like the following:</p>
<pre>type Link&lt;K&gt; = Box&lt;Node&lt;K&gt;&gt;;<br/><br/>struct Node&lt;K&gt;<br/>where<br/>    K: PartialEq + Clone + Ord,<br/>{<br/>    pub key: K,<br/>    next: BTreeMap&lt;K, Link&lt;K&gt;&gt;,<br/>    ends_here: bool,<br/>}<br/><br/>pub struct TrieSet&lt;K&gt;<br/>where<br/>    K: PartialEq + Clone + Ord,<br/>{<br/>    pub length: u64,<br/>    root: BTreeMap&lt;K, Link&lt;K&gt;&gt;,<br/>}</pre>
<p>This the trie implementation of <a href="84f203ac-a9f6-498b-90ff-e069c41aaca0.xhtml">Chapter 5</a>, <em>Robust Trees</em>, with generics added and using a <kbd>BTreeMap&lt;K,V&gt;</kbd> root node to avoid creating too many trait dependencies. This allows arbitrary chains of simple data types to be stored as a trie, a highly efficient data structure where overlaps are kept together only to branch off once they diverge (read more on tries in <a href="84f203ac-a9f6-498b-90ff-e069c41aaca0.xhtml">Chapter 5</a>, <em>Robust Trees</em>).</p>
<p>Can this store numbers? Yes, although they have to be converted to a byte array, but then anything can be stored in this set.</p>
<p><em>The product team has had an idea: they want to store network addresses for a network analysis software. They want to store these addresses in order to run some basic analysis on top of them: which network devices are in both networks, gathering all the addresses that are in either all or not in some specified networks. Since IP addresses are unique and consist of individual bytes that have to have common prefixes, wouldn't this be a great opportunity to use that trie set?</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storing network addresses</h1>
                </header>
            
            <article>
                
<p class="mce-root">Storing network addresses is not a hard problem and there are many solutions out there. Their binary structure provides an opportunity to create something really specific—if time is not an issue.</p>
<p>In many cases, however, an off-the-shelf implementation of a data structure is enough to cover most basic use cases when that isn't your main concern. Hence, the network address storage can simple be a type alias that specifies the key type for the trie set, shown as follows:</p>
<pre>pub type NetworkDeviceStore = TrieSet&lt;u8&gt;;</pre>
<p>Slight modifications to the <kbd>insert</kbd> (former <kbd>add</kbd>) function of the trie allows users to simply pass a slice of the key type into the function, shown in the following code:</p>
<pre>pub fn insert(&amp;mut self, elements: &amp;[K]) {<br/>    let mut path = elements.into_iter();<br/><br/>    if let Some(start) = path.next() {<br/>        let mut n = self<br/>            .root<br/>            .entry(start.clone())<br/>            .or_insert(Node::new(start.clone(), false));<br/>        for c in path {<br/>            let tmp = n<br/>                .next<br/>                .entry(c.clone())<br/>                .or_insert(Node::new(c.clone(), false));<br/>            n = tmp;<br/>        }<br/>        if !n.ends_here {<br/>            self.length += 1;<br/>        }<br/>        n.ends_here = true;<br/>    }<br/>}</pre>
<p>This implementation differs only in a few details from what was done in the previous chapter. Firstly, it's important to avoid incrementing the length twice, which is avoided by checking if a key ends at the last node of the new key. This flag is also a new addition since the other implementation was specifically implemented to store instances of the <kbd>IoTDevice</kbd> <span>type,</span> and each node would have an optional device attached to it to signal the completion of a key.</p>
<p>A similar reasoning was applied to the <kbd>walk</kbd> and <kbd>contains</kbd> functions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Networked operations</h1>
                </header>
            
            <article>
                
<p>One key requirement of the product team was the ability to run simple analytics on top of this set. As a first step, these analytics can be comprised of set operations and comparing their lengths in order to create simple indicators.</p>
<p>One thing that is important, however, is to also get the addresses back out. For that, the implementation this time provides an iterator implementation that consumes the trie and stores it as a <kbd>Vec&lt;T&gt;</kbd>, shown as follows:</p>
<pre>// [...] trie set implementation<br/>    pub fn into_iter(self) -&gt; SetIterator&lt;K&gt; {<br/>        let v: RefCell&lt;Vec&lt;Vec&lt;K&gt;&gt;&gt; = RefCell::new(vec![]);<br/>        self.walk(|n| v.borrow_mut().push(n.to_vec()));<br/>        SetIterator::new(v.into_inner(), 0)<br/>    }<br/>}<br/><br/>pub struct SetIterator&lt;K&gt;<br/>where<br/>    K: PartialEq + Clone + Ord,<br/>{<br/>    data: Vec&lt;Vec&lt;K&gt;&gt;,<br/>    last_index: usize,<br/>}<br/><br/>impl&lt;K&gt; SetIterator&lt;K&gt;<br/>where<br/>    K: PartialEq + Clone + Ord,<br/>{<br/>    fn new(data: Vec&lt;Vec&lt;K&gt;&gt;, start_at: usize) -&gt; SetIterator&lt;K&gt; {<br/>        SetIterator {<br/>            data: data,<br/>            last_index: start_at,<br/>        }<br/>    }<br/>}<br/><br/>impl&lt;K&gt; Iterator for SetIterator&lt;K&gt;<br/>where<br/>    K: PartialEq + Clone + Ord,<br/>{<br/>    type Item = Vec&lt;K&gt;;<br/><br/>    fn next(&amp;mut self) -&gt; Option&lt;Vec&lt;K&gt;&gt; {<br/>        let result = self.data.get(self.last_index);<br/>        self.last_index += 1;<br/>        result.cloned()<br/>    }<br/>}<br/><br/></pre>
<p>Once the vector is created, an index will do for keeping track of moving the iterator around. The set operations are actually not much more complex than that. However, all of them use the <kbd>walk()</kbd> function, which requires us to provide mutability in a lambda expression (or closure), and consequently a <kbd>RefCell</kbd> to take care of mutability management dynamically.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Union</h1>
                </header>
            
            <article>
                
<p>The definition of a set union is that every element that occurs in either set is required to occur in the result. Therefore, the challenge is to insert elements from both sets into the resulting set, without creating duplicates.</p>
<p>Since this is handled by the <kbd>insert</kbd> process, a naive implementation could look like the following:</p>
<pre>pub fn union(self, other: TrieSet&lt;K&gt;) -&gt; TrieSet&lt;K&gt; {<br/>    let new = RefCell::new(TrieSet::new_empty());<br/>    self.walk(|k| new.borrow_mut().insert(k));<br/>    other.walk(|k| new.borrow_mut().insert(k));<br/>    new.into_inner()<br/>}</pre>
<p>This consumes both sets, returning only the result. The next operation, the intersection, looks very similar.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Intersection</h1>
                </header>
            
            <article>
                
<p>To find the common elements of two sets, the intersection is a way of doing that. The definition also describes exactly that, which is why the naive implementation in Rust also follows that pattern, shown as follows:</p>
<pre>pub fn intersection(self, other: TrieSet&lt;K&gt;) -&gt; TrieSet&lt;K&gt; {<br/>    let new = RefCell::new(TrieSet::new_empty());<br/>    if self.length &lt; other.length {<br/>        self.walk(|k| {<br/>            if other.contains(k) {<br/>                new.borrow_mut().insert(k)<br/>            }<br/>        });<br/>    } else {<br/>        other.walk(|k| {<br/>            if self.contains(k) {<br/>                new.borrow_mut().insert(k)<br/>            }<br/>        });<br/>    }<br/>    new.into_inner()<br/>}</pre>
<p>As a last function, the difference is important, since it excludes common elements from the result set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Difference</h1>
                </header>
            
            <article>
                
<p>Instead of common elements, sometimes the opposite is required—removing elements that occur in both sets. This operation is also referred to as the complement of two sets, which only inserts elements into the result if they don't occur in the other set:</p>
<pre>pub fn difference(self, other: TrieSet&lt;K&gt;) -&gt; TrieSet&lt;K&gt; {<br/>    let new = RefCell::new(TrieSet::new_empty());<br/>    self.walk(|k| {<br/>        if !other.contains(k) {<br/>            new.borrow_mut().insert(k)<br/>        }<br/>    });<br/>    new.into_inner()<br/>}</pre>
<p>With that, the set is finished, and all the desired functionality can be provided.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Wrap up</h1>
                </header>
            
            <article>
                
<p>Sets are not complicated, but are useful. While database indices might be B-Trees, the result sets are the sets of primary keys that get moved around and operated on until the very last step, when the associated row information is fetched from disk. These are the moments when set data structures come in handy and provide a simple solution.</p>
<p>Similarly to everyday tasks, creating a list of unique elements can be very inefficient when a list is used; storing them in a set, however, requires no extra effort. In fact, most elements can then just be thrown into the set, which won't insert duplicates anyway.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upsides</h1>
                </header>
            
            <article>
                
<p>The set is a higher-level data structure that does the following:</p>
<ul>
<li>provides a simple interface for unique lists</li>
<li>Implements a mathematical concept</li>
<li>Has a very efficient way of storing and retrieving its elements</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downsides</h1>
                </header>
            
            <article>
                
<p>The set has some downsides as well, primarily the following:</p>
<ul>
<li>Element order determinism depends on the implementation</li>
<li>Does not always add a lot of value compared to maps</li>
<li>Limited use cases</li>
</ul>
<p>Since maps will be used a lot more often, let's dive into those.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><strong>Hashing</strong> is the art (and science) of creating a single representation (typically a number) from an arbitrary object, be it strings, <kbd>type</kbd> instances, or collections; there is a way to break them down into a number that should reflect a particular use case. The real question is what you want to achieve and what characteristics are expected from the outcome. Cryptographic hashing deals with minimizing collisions and creating signatures that create a very different hash from minor modifications, whereas GeoHashes are a way to hierarchically structure Earth's coordinates into a string. Whenever two (or more) inputs to a hash function lead to the same output, this is called a collision—a bad sign for any cryptographic hashing, but fine if it's mostly about storing something in a hash map, as long as the collisions are evenly distributed. Most importantly, however, software engineers should <em>never</em> come up with their own hash functions, especially if security is a concern.</p>
<p><strong>Maps</strong> store and manage key-value pairs in an underlying data structure, which is typically either a tree or an array that maps hashes to key-value pairs called hash maps. B<span>y using a hash function to describe the key and sort the pair into buckets (array elements), h</span>ash maps are a great use case for hashing. These buckets are basically indices on an array that stores a list (or tree) for whenever different inputs lead to the same bucket. Consequently, the best case performance of a hash map is constant time (<em>O(1)</em>) to retrieve any value, whereas the worst case is linear time (<em>O(n)</em>) if the hash function returns a constant number. In reality, there are other uses that might be beneficial, such as caching, where the use case limits the potential inputs, and best case performance is always achieved.</p>
<p>Contrary to maps, <strong>sets</strong> are great data structures to store a unique collection of elements to perform set operations on. They can be implemented just like a hash map, using a hash function or a tree. In this chapter, we implemented a set based on a modified trie data structure from the previous chapter (<em>Robust Trees</em>), as well as the basic three operations: union, intersection, and difference.</p>
<p>In the next chapter, we will continue to explore Rust's <kbd>std::collections</kbd> library and its contents. This will include some benchmarking and looking into more implementation details, since these are the best implementations of all the concepts discussed in the book so far.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ul>
<li>What makes a good hash function?</li>
<li>How can you estimate the suitability of a hash function for a particular task?</li>
<li>Is a checksum hash useful in other ways?</li>
<li>What are two ways to implement a map?</li>
<li>What are buckets?</li>
<li>Can a set replace a list?</li>
<li>What makes a set useful?</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>Refer to the following links for more information:</p>
<ul>
<li><a href="http://geohash.org/">http://geohash.org/</a></li>
<li><em>Fletcher's checksum</em> (<a href="https://en.wikipedia.org/wiki/Fletcher%27s_checksum">https://en.wikipedia.org/wiki/Fletcher%27s_checksum</a>)</li>
<li>Rust's <kbd>HashMap</kbd> implementation reasoning (<a href="https://www.reddit.com/r/rust/comments/52grcl/rusts_stdcollections_is_absolutely_horrible/d7kcei2">https://www.reddit.com/r/rust/comments/52grcl/rusts_stdcollections_is_absolutely_horrible/d7kcei2</a>)</li>
<li><a href="https://doc.rust-lang.org/std/hash/">https://doc.rust-lang.org/std/hash/</a></li>
<li>Wikipedia's list of hash functions (<a href="https://en.wikipedia.org/wiki/List_of_hash_functions">https://en.wikipedia.org/wiki/List_of_hash_functions</a>)</li>
</ul>


            </article>

            
        </section>
    </body></html>
<html><head></head><body>
		<div><h1 id="_idParaDest-123" class="chapter-number"><a id="_idTextAnchor122"/>7</h1>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor123"/>Coroutines and async/await</h1>
			<p>Now that you’ve gotten a brief introduction to Rust’s async model, it’s time to take a look at how this fits in the context of everything else we’ve covered in this book so far.</p>
			<p>Rust’s futures are an example of an asynchronous model based on stackless coroutines, and in this chapter, we’ll take a look at what that really means and how it differs from stackful coroutines (fibers/green threads).</p>
			<p>We’ll center everything around an example based on a simplified model of futures and <code>async/await</code> and see how we can use that to create suspendable and resumable tasks just like we did when creating our own fibers.</p>
			<p>The good news is that this is a lot easier than implementing our own fibers/green threads since we can stay in Rust, which is safer. The flip side is that it’s a little more abstract and ties into programming language theory as much as it does computer science.</p>
			<p>In this chapter, we’ll cover the following:</p>
			<ul>
				<li>Introduction to stackless coroutines</li>
				<li>An example of hand-written coroutines</li>
				<li><code>async/await</code></li>
			</ul>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor124"/>Technical requirements</h1>
			<p>The examples in this chapter will all be cross-platform, so the only thing you need is Rust installed and the repository that belongs to the book downloaded locally. All the code in this chapter will be found in the <code>ch07</code> folder.</p>
			<p>We’ll use <code>delayserver</code> in this example as well, so you need to open a terminal, enter the <code>delayserver</code> folder at the root of the repository, and write <code>cargo run</code> so it’s ready and available for the examples going forward.</p>
			<p>Remember to change the ports in the code if you for some reason have to change what port <code>delayserver</code> listens on.</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor125"/>Introduction to stackless coroutines</h1>
			<p>So, we’ve <a id="_idIndexMarker415"/>finally arrived at the point where we introduce the last method of modeling asynchronous operations in this book. You probably remember that we gave a high-level overview of stackful and stackless coroutines in <a href="B20892_02.xhtml#_idTextAnchor043"><em class="italic">Chapter 2</em></a>. In <a href="B20892_05.xhtml#_idTextAnchor092"><em class="italic">Chapter 5</em></a>, we implemented an example of stackful coroutines when writing our own fibers/green threads, so now it’s time to take a closer look at how stackless coroutines are implemented and used.</p>
			<p>A stackless coroutine is a way of representing a task that can be interrupted and resumed. If you remember all the way back in <a href="B20892_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, we mentioned that if we want tasks to run concurrently (be <em class="italic">in progress</em> at the same time) but not necessarily in parallel, we need to be able to <strong class="bold">pause and resume</strong> the task.</p>
			<p>In its simplest form, a coroutine is just a task that can stop and resume by yielding control to either its caller, another coroutine, or a scheduler.</p>
			<p>Many languages will have a coroutine implementation that also provides a runtime that handles scheduling and non-blocking I/O for you, but it’s helpful to make a distinction between what a coroutine is and the rest of the machinery involved in creating an asynchronous system.</p>
			<p>This is especially true in Rust, since Rust doesn’t come with a runtime and only provides the infrastructure you need to create coroutines that have native support in the language. Rust makes sure that everyone programming in Rust uses the same abstraction for tasks that can be paused and resumed, but it leaves all the other details of getting an asynchronous system up and running for the programmer.</p>
			<p class="callout-heading">Stackless coroutines or just coroutines?</p>
			<p class="callout">Most often you’ll see <em class="italic">stackless coroutines</em> simply referred to as <em class="italic">coroutines</em>. To try to keep some consistency (you remember I don’t like to introduce terms that mean different things based on the context), I’ve consistently referred to coroutines as either <em class="italic">stackless</em> or <em class="italic">stackful</em>, but going forward, I’ll simply refer to stackless coroutines as <strong class="bold">coroutines</strong>. This <a id="_idIndexMarker416"/>is also what you’ll have to expect when reading about them in other sources.</p>
			<p>Fibers/green threads represent this kind of resumable task in a very similar way to how an operating system does. A task has a stack where it stores/restores its current execution state, making it possible to pause and resume the task.</p>
			<p>A state machine <a id="_idIndexMarker417"/>in its simplest form is a data structure that has a predetermined set of states it can be in. In the case of coroutines, each state represents a possible pause/resume point. We don’t store the state needed to pause/resume the task in a separate stack. We save it in a data structure instead.</p>
			<p>This has some advantages, which I’ve covered before, but the most prominent ones are that they’re very efficient and flexible. The downside is that you’d never want to write these state machines by hand (you’ll see why in this chapter), so you need some kind of support from the compiler or another mechanism for rewriting your code to state machines instead of normal function calls.</p>
			<p>The result is that you get something that looks very simple. It looks like a function/subroutine that you can easily map to something that you can run using a simple <code>call</code> instruction in assembly, but what you actually get is something pretty complex and different from this, and it doesn’t look anything like what you’d expect.</p>
			<p class="callout-heading">Generators vs coroutines</p>
			<p class="callout">Generators are <a id="_idIndexMarker418"/>state machines as well, exactly the kind we’ll cover in this chapter. They’re usually implemented in a language to create state machines that yield values to the calling function.</p>
			<p class="callout">Theoretically, you could make a distinction between coroutines and generators based on what they yield to.  Generators are usually limited to yielding to the calling function. Coroutines can yield to another coroutine, a scheduler, or simply the caller, in which case they’re just like generators.</p>
			<p class="callout">In my eyes, there is really no point in making a distinction between them. They represent the same underlying mechanism for creating tasks that can pause and resume their executions, so in this book, we’ll treat them as basically the same thing.</p>
			<p>Now that we’ve covered what coroutines are in text, we can start looking at what they look like in code.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor126"/>An example of hand-written coroutines</h1>
			<p>The example we’ll <a id="_idIndexMarker419"/>use going forward is a simplified version of Rust’s asynchronous model. We’ll create and implement the following:</p>
			<ul>
				<li>Our own simplified <code>Future</code> trait</li>
				<li>A simple HTTP client that can only make GET requests</li>
				<li>A task we can pause and resume implemented as a state machine</li>
				<li>Our own simplified <code>async/await</code> syntax called <code>coroutine/wait</code></li>
				<li>A homemade preprocessor to transform our <code>coroutine/wait</code> functions into state machines the same way <code>async/await</code> is transformed</li>
			</ul>
			<p>So, to actually demystify coroutines, futures, and <code>async/await</code>, we will have to make some compromises. If we didn’t, we’d end up re-implementing everything that is <code>async/await</code> and futures in Rust today, which is too much for just understanding the underlying techniques and concepts.</p>
			<p>Therefore, our example will do the following:</p>
			<ul>
				<li>Avoid error handling. If anything fails, we panic.</li>
				<li>Be specific and not generic. Creating generic solutions introduces a lot of complexity and makes the underlying concepts harder to reason about since we consequently have to create extra abstraction levels. Our solution will have some generic aspects where needed, though.</li>
				<li>Be limited in what it can do. You are of course free to expand, change, and play with all the examples (I encourage you to do so), but in the example, we only cover what we need and not anything more.</li>
				<li>Avoid macros.</li>
			</ul>
			<p>So, with that out of the way, let’s get started on our example.</p>
			<p>The first thing you need to do is to create a new folder. This first example can be found in <code>ch07/a-coroutine</code> in the repository, so I suggest you name the folder <code>a-coroutine</code> as well.</p>
			<p>Then, initialize <a id="_idIndexMarker420"/>a new crate by entering the folder and write <code>cargo init</code>.</p>
			<p>Now that we have a new project up and running, we can create the modules and folders we need:</p>
			<p>First, in <code>main.rs</code>, declare two modules as follows:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/main.rs</p>
			<pre class="source-code">
mod http;
mod future;</pre>			<p>Next, create two new files in the <code>src</code> folder:</p>
			<ul>
				<li><code>future.rs</code>, which will hold our future-related code</li>
				<li><code>http.rs</code>, which will be the code related to our HTTP client</li>
			</ul>
			<p>One last thing we need to do is to add a dependency on <code>mio</code>. We’ll be using <code>TcpStream</code> from <code>mio</code>, as we’ll build on this example in the following chapters and use <code>mio</code> as our non-blocking I/O library since we’re already familiar with it:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/Cargo.toml</p>
			<pre class="source-code">
[dependencies]
mio = { version = "0.8", features = ["net", "os-poll"] }</pre>			<p>Let’s start in <code>future.rs</code> and implement our future-related code first.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor127"/>Futures module</h2>
			<p>In <code>futures.rs</code>, the first <a id="_idIndexMarker421"/>thing we’ll do is define a <code>Future</code> trait. It looks as follows:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/future.rs</p>
			<pre class="source-code">
pub trait Future {
    type Output;
    fn poll(&amp;mut self) -&gt; PollState&lt;Self::Output&gt;;
}</pre>			<p>If we <a id="_idIndexMarker422"/>contrast this with the <code>Future</code> trait in Rust’s standard library, you’ll see it’s very similar, except that we don’t take <code>cx: &amp;mut Context&lt;'_&gt;</code> as an argument and we return an <code>enum</code> with a slightly different name just to differentiate it so we don’t mix them up:</p>
			<pre class="source-code">
pub trait Future {
    type Output;
    fn poll(self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;Self::Output&gt;;
}</pre>			<p>The next thing we do is to define a <code>PollState&lt;T&gt;</code> <code>enum</code>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/future.rs</p>
			<pre class="source-code">
pub enum PollState&lt;T&gt; {
    Ready(T),
    NotReady,
}</pre>			<p>Again, if we compare this to the <code>Poll</code> <code>enum</code> in Rust’s standard library, we see that they’re practically the same:</p>
			<pre class="source-code">
pub enum Poll&lt;T&gt; {
    Ready(T),
    Pending,
}</pre>			<p>For now, this is all we need to get the first iteration of our example up and running. Let’s move on to the next file: <code>http.rs</code>.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor128"/>HTTP module</h2>
			<p>In this <a id="_idIndexMarker423"/>module, we’ll implement a very simple HTTP client. This client can only make GET requests to our <code>delayserver</code> since we just use this as a representation of a typical I/O operation and don’t care specifically about being able to do more than we need.</p>
			<p>The first thing we’ll do is import some types and traits from the standard library as well as our <code>Futures</code> module:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/http.rs</p>
			<pre class="source-code">
use crate::future::{Future, PollState};
use std::io::{ErrorKind, Read, Write};</pre>			<p>Next, we create a small helper function to write our HTTP requests. We’ve used this exact bit of code before in this book, so I’ll not spend time explaining it again here:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/http.rs</p>
			<pre class="source-code">
fn get_req(path: &amp;str) -&gt; String {
    format!(
        "GET {path} HTTP/1.1\r\n\
             Host: localhost\r\n\
             Connection: close\r\n\
             \r\n"
    )
}</pre>			<p>So, now we can start writing our HTTP client. The implementation is very short and simple:</p>
			<pre class="source-code">
pub struct Http;
impl Http {
    pub fn get(path: &amp;str) -&gt; impl Future&lt;Output = String&gt; {
        HttpGetFuture::new(path)
    }
}</pre>			<p>We don’t <a id="_idIndexMarker424"/>really need a struct here, but we add one since we might want to add some state at a later point. It’s also a good way to group functions belonging to the HTTP client together.</p>
			<p>Our HTTP client only has one function, <code>get</code>, which, eventually, will send a GET request to our <code>delayserver</code> with the path we specify (remember that the path is everything in bold in this example URL: <code>http://127.0.0.1:8080</code><strong class="bold">/1000/HelloWorld)</strong>,</p>
			<p>The first thing you’ll notice in the function body is that there is not much happening here. We only return <code>HttpGetFuture</code> and that’s it.</p>
			<p>In the function signature, you see that it returns an object implementing the <code>Future</code> trait that outputs a <code>String</code> when it’s resolved. The string we return from this function will be the response we get from the server.</p>
			<p>Now, we could have implemented the future trait directly on the <code>Http</code> struct, but I think it’s a better design to allow one <code>Http</code> instance to give out multiple <code>Futures</code> instead of making the <code>Http</code> implement <code>Future</code> itself.</p>
			<p>Let’s take a closer look at <code>HttpGetFuture</code> since there is much more happening there.</p>
			<p>Just to point this out so that there is no doubt going forward, <code>HttpGetFuture</code> is an example of a <strong class="bold">leaf future</strong>, and it will be the only leaf future we’ll use in this example.</p>
			<p>Let’s add the struct declaration to the file:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/http.rs</p>
			<pre class="source-code">
struct HttpGetFuture {
    stream: Option&lt;mio::net::TcpStream&gt;,
    buffer: Vec&lt;u8&gt;,
    path: String,
}</pre>			<p>This data structure <a id="_idIndexMarker425"/>will hold onto some data for us:</p>
			<ul>
				<li><code>stream</code>: This holds an <code>Option&lt;mio::net::TcpStream&gt;</code>. This will be an <code>Option</code> since we won’t connect to the stream at the same point as we create this structure.</li>
				<li><code>buffer</code>: We’ll read the data from the <code>TcpStream</code> and put it all in this buffer until we’ve read all the data returned from the server.</li>
				<li><code>path</code>: This simply stores the path for our GET request so we can use it later.</li>
			</ul>
			<p>The next thing we’ll take a look at is the <code>impl</code> block for our <code>HttpGetFuture</code>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/http.rs</p>
			<pre class="source-code">
impl HttpGetFuture {
    fn new(path: &amp;'static str) -&gt; Self {
        Self {
            stream: None,
            buffer: vec![],
            Path: path.to_string(),
        }
    }
    fn write_request(&amp;mut self) {
        let stream = std::net::TcpStream::connect("127.0.0.1:8080").unwrap();
        stream.set_nonblocking(true).unwrap();
        let mut stream = mio::net::TcpStream::from_std(stream);
        stream.write_all(get_req(&amp;self.path).as_bytes()).unwrap();
        self.stream = Some(stream);
    }
}</pre>			<p>The <code>impl</code> block<a id="_idIndexMarker426"/> defines two functions. The first is <code>new</code>, which simply sets the initial state.</p>
			<p>The next function is <code>write_requst</code>, which sends the GET request to the server. You’ve seen this code before in the example in <a href="B20892_04.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>, so this should look familiar.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">When <em class="italic">creating</em> <code>HttpGetFuture</code>, we don’t actually <em class="italic">do</em> anything related to the GET request, which means that the call to <code>Http::get</code> returns immediately with just a simple data structure.</p>
			<p>In contrast to earlier examples, we pass in the <em class="italic">IP address</em> for <code>localhost</code> instead of the DNS name. We take the same shortcut as before and let <code>connect</code> be blocking and everything else be non-blocking.</p>
			<p>The next step is to write the GET request to the server. This will be non-blocking, and we don’t have to wait for it to finish since we’ll be waiting for the response anyway.</p>
			<p>The last part of this file is the most important one—the implementation of the <code>Future</code> trait we defined:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/http.rs</p>
			<pre class="source-code">
impl Future for HttpGetFuture {
    type Output = String;
    fn poll(&amp;mut self) -&gt; PollState&lt;Self::Output&gt; {
        if self.stream.is_none() {
            println!("FIRST POLL - START OPERATION");
            self.write_request();
            return PollState::NotReady;
        }
        let mut buff = vec![0u8; 4096];
        loop {
            match self.stream.as_mut().unwrap().read(&amp;mut buff) {
                Ok(0) =&gt; {
                    let s = String::from_utf8_lossy(&amp;self.buffer);
                    break PollState::Ready(s.to_string());
                }
                Ok(n) =&gt; {
                    self.buffer.extend(&amp;buff[0..n]);
                    continue;
                }
                Err(e) if e.kind() == ErrorKind::WouldBlock =&gt; {
                    break PollState::NotReady;
                }
                Err(e) if e.kind() == ErrorKind::Interrupted =&gt; {
                    continue;
                }
                Err(e) =&gt; panic!("{e:?}"),
            }
        }
    }
}</pre>			<p>Okay, so <a id="_idIndexMarker427"/>this is where everything happens. The first thing we do is set the associated type called <code>Output</code> to <code>String</code>.</p>
			<p>The next thing we do is to check whether this is the first time <code>poll</code> was called or not. We do this by checking if <code>self.stream</code> is <code>None</code>.</p>
			<p>If it’s the first time we call <code>poll</code>, we print a message (just so we can see the first time this future was polled), and then we write the GET request to the server.</p>
			<p>On the first poll, we return <code>PollState::NotReady</code>, so <code>HttpGetFuture</code> will have to be polled at least once more to actually return any results.</p>
			<p>The next part of the function is trying to read data from our <code>TcpStream</code>.</p>
			<p>We’ve covered this before, so I’ll make this brief, but there are basically five things that can happen:</p>
			<ol>
				<li>The call successfully returns with <code>0</code> bytes read. We’ve read all the data from the stream and have received the entire GET response. We create a <code>String</code> from the data we’ve read and wrap it in <code>PollState::Ready</code> before we return.</li>
				<li>The call successfully returns with <code>n &gt; 0</code> bytes read. If that’s the case, we read the data into our buffer, append the data into <code>self.buffer</code>, and immediately try to read more data from the stream.</li>
				<li>We get an error of kind <code>WouldBlock</code>. If that’s the case, we know that since we set the stream to non-blocking, the data isn’t ready yet or there is more data but we haven’t received it yet. In that case, we return <code>PollState::NotReady</code> to communicate that more calls to the poll are needed to finish the operation.</li>
				<li>We get<a id="_idIndexMarker428"/> an error of kind <code>Interrupted</code>. This is a bit of a special case since reads can be interrupted by a signal. If it does, the usual way to handle the error is to simply try reading once more.</li>
				<li>We get an error that we can’t handle, and since our example does no error handling, we simply <code>panic!</code></li>
			</ol>
			<p>There is one subtle thing I want to point out. We can view this as a very simple state machine with three states:</p>
			<ul>
				<li>Not started, indicated by <code>self.stream</code> being <code>None</code></li>
				<li>Pending, indicated by <code>self.stream</code> being <code>Some</code> and a read to <code>stream.read</code> returning <code>WouldBlock</code></li>
				<li>Resolved, indicated by <code>self.stream</code> being <code>Some</code> and a call to <code>stream.read</code> returning <code>0</code> bytes</li>
			</ul>
			<p>As you see, this model maps nicely to the states reported by the OS when trying to read our <code>TcpStream</code>.</p>
			<p>Most leaf futures such as this will be quite simple, and although we didn’t make the states explicit here, it still fits in the state machine model that we’re basing our coroutines around.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor129"/>Do all futures have to be lazy?</h2>
			<p>A lazy future is <a id="_idIndexMarker429"/>one where no work happens before it’s polled the first time.</p>
			<p>This will come up a lot if you read about futures in Rust, and since our own <code>Future</code> trait is based on that exact same model, the same question will arise here. The simple answer to this question is no!</p>
			<p>There is nothing that forces leaf futures, such as the one we wrote here, to be lazy. We could have sent the HTTP request when we called the <code>Http::get</code> function if we wanted to. If you think about it, if we did just that, it would have caused a potentially big change that would impact how we achieve concurrency in our program.</p>
			<p>The way it works now is that someone has to call <code>poll</code> at least one time to actually send the request. The consequence is that whoever calls <code>poll</code> on this future will have to call <code>poll</code> on many futures to kick off the operation if they want them to run concurrently.</p>
			<p>If we kicked off the operation immediately when the future was created, you could create many futures and they would all run concurrently even though you polled them to completion one by one. If you poll them to completion one by one in the current design, the futures would <em class="italic">not</em> progress concurrently. Let that sink in for a moment.</p>
			<p>Languages such as JavaScript start the operation when the coroutine is created, so there is no “one way” to do this. Every time you encounter a coroutine implementation, you should find out whether they’re lazy or eager since this impacts how you program with them.</p>
			<p>Even though we could make our future eager in this case, we really shouldn’t. Since programmers in Rust expect futures to be lazy, they might depend on nothing happening before you call <code>poll</code> on them, and there may be unexpected side effects if the futures you write behave differently.</p>
			<p>Now, when<a id="_idIndexMarker430"/> you read that Rust’s futures are always lazy, a claim that I see very often, it refers to the compiler-generated state machines resulting from using <code>async/await</code>. As we’ll see later, when your async functions are rewritten by the compiler, they’re constructed in a way so that nothing you write in the body of an <code>async</code> function will execute before the first call to <code>Future::poll</code>.</p>
			<p>Okay, so we’ve covered the <code>Future</code> trait and the leaf future we named <code>HttpGetFuture</code>. The next step is to create a task that we can stop and resume at predefined points.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor130"/>Creating coroutines</h2>
			<p>We’ll continue<a id="_idIndexMarker431"/> to build our knowledge and understanding from the ground up. The first thing we’ll do is create a task that we can stop and resume by modeling it as a state machine by hand.</p>
			<p>Once we’ve done that, we’ll take a look at how this way of modeling pausable tasks enables us to write a syntax much like <code>async/await</code> and rely on code transformations to create these state machines instead of writing them by hand.</p>
			<p>We’ll create a simple program that does the following:</p>
			<ol>
				<li>Prints a message when our pausable task is starting.</li>
				<li>Makes a GET request to our <code>delayserver</code>.</li>
				<li>Waits for the GET request.</li>
				<li>Prints the response from the server.</li>
				<li>Makes a second GET request to our <code>delayserver</code>.</li>
				<li>Waits for the second response from the server.</li>
				<li>Prints the response from the server.</li>
				<li>Exits the program.</li>
			</ol>
			<p>In addition, we’ll <a id="_idIndexMarker432"/>execute our program by calling <code>Future::poll</code> on our hand-crafted coroutine as many times as required to run it to completion. There’s no runtime, reactor, or executor yet since we’ll cover those in the next chapter.</p>
			<p>If we wrote our program as an <code>async</code> function, it would look as follows:</p>
			<pre class="source-code">
async fn async_main() {
    println!("Program starting")
    let txt = Http::get("/1000/HelloWorld").await;
    println!("{txt}");
    let txt2 = Http::("500/HelloWorld2").await;
    println!("{txt2}");
}</pre>			<p>In <code>main.rs</code>, start by making the necessary imports and module declarations:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/main.rs</p>
			<pre class="source-code">
use std::time::Instant;
mod future;
mod http;
use crate::http::Http;
use future::{Future, PollState};</pre>			<p>The next thing we write is our stoppable/resumable task called <code>Coroutine</code>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/main.rs</p>
			<pre class="source-code">
struct Coroutine {
    state: State,
}</pre>			<p>Once that’s done, we <a id="_idIndexMarker433"/>write the different states this task could be in:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/main.rs</p>
			<pre class="source-code">
enum State {
    Start,
    Wait1(Box&lt;dyn Future&lt;Output = String&gt;&gt;),
    Wait2(Box&lt;dyn Future&lt;Output = String&gt;&gt;),
    Resolved,
}</pre>			<p>This specific coroutine can <a id="_idIndexMarker434"/>be in four states:</p>
			<ul>
				<li><code>Coroutine</code> has been created but it hasn’t been polled yet</li>
				<li><code>Http::get</code>, we get a <code>HttpGetFuture</code> returned that we store in the <code>State</code> <code>enum</code>. At this point, we return control back to the calling function so it can do other things if needed. We chose to make this generic over all <code>Future</code> functions that output a <code>String</code>, but since we only have one kind of future right now, we could have made it simply hold a <code>HttpGetFuture</code> and it would work the same way.</li>
				<li><code>Http::get</code> is the second place where we’ll pass control back to the calling function.</li>
				<li><strong class="bold">Resolved</strong>: The future is resolved and there is no more work to do.</li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">We could have simply defined <code>Coroutine</code> as an <code>enum</code> since the only state it holds is an <code>enum</code> indicating its state. But, we’ll set up this example so we can add some state to <code>Coroutine</code> later on in this book.</p>
			<p>Next is<a id="_idIndexMarker435"/> the implementation of <code>Coroutine</code>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/main.rs</p>
			<pre class="source-code">
impl Coroutine {
    fn new() -&gt; Self {
        Self {
            state: State::Start,
        }
    }
}</pre>			<p>So far, this is pretty simple. When creating a new <code>Coroutine</code>, we simply set it to <code>State::Start</code> and that’s it.</p>
			<p>Now we <a id="_idIndexMarker436"/>come to the part where the work is actually done in the <code>Future</code> implementation for <code>Coroutine</code>. I’ll walk you through the code:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/main.rs</p>
			<pre class="source-code">
impl Future for Coroutine {
    type Output = ();
    fn poll(&amp;mut self) -&gt; PollState&lt;Self::Output&gt; {
        loop {
            match self.state {
                State::Start =&gt; {
                    println!("Program starting");
                    let fut = Box::new(Http::get("/600/HelloWorld1"));
                    self.state = State::Wait1(fut);
                }
                State::Wait1(ref mut fut) =&gt; match fut.poll() {
                    PollState::Ready(txt) =&gt; {
                        println!("{txt}");
                        let fut2 = Box::new(Http::get("/400/HelloWorld2"));
                        self.state = State::Wait2(fut2);
                    }
                    PollState::NotReady =&gt; break PollState::NotReady,
                },
                State::Wait2(ref mut fut2) =&gt; match fut2.poll() {
                    PollState::Ready(txt2) =&gt; {
                        println!("{txt2}");
                        self.state = State::Resolved;
                        break PollState::Ready(());
                    }
                    PollState::NotReady =&gt; break PollState::NotReady,
                },
                State::Resolved =&gt; panic!("Polled a resolved future"),
            }
        }
    }
}</pre>			<p>Let’s start from<a id="_idIndexMarker437"/> the top:</p>
			<ol>
				<li>The first thing we do is set the <code>Output</code> type to <code>()</code>. Since we won’t be returning anything, it just makes our example simpler.</li>
				<li>Next up is the implementation of the <code>poll</code> method. The first thing you notice is that we write a <code>loop</code> instance that matches <code>self.state</code>. We do this so we can drive the state machine forward until we reach a point where we can’t progress any further without getting <code>PollState::NotReady</code> from one of our child futures.</li>
				<li>If the state is <code>State::Start</code>, we know that this is the first time it was polled, so we run whatever instructions we need until we reach the point where we get a new future that we need to resolve.</li>
				<li>When we call <code>Http::get</code>, we receive a future in return that we need to poll to completion before we progress any further.</li>
				<li>At this point, we change the state to <code>State::Wait1</code> and we store the future we want to resolve so we can access it in the next state.</li>
				<li>Our state machine has now changed its state from <code>Start</code> to <code>Wait1</code>. Since we’re looping on the <code>match</code> statement, we immediately progress to the next state and will reach the match arm in <code>State::Wait1</code> on the next iteration.</li>
				<li>The first thing we do in <code>Wait1</code> to call <code>poll</code> on the <code>Future</code> instance we’re waiting on.</li>
				<li>If the future returns <code>PollState::NotReady</code>, we simply bubble that up to the caller by breaking out of the loop and returning <code>NotReady</code>.</li>
				<li>If the future returns <code>PollState::Ready</code> together with our data, we know that we can execute the instructions that rely on the data from the first future and advance to the next state. In our case, we only print out the returned data, so that’s only one line of code.</li>
				<li>Next, we get<a id="_idIndexMarker438"/> to the point where we get a new future by calling <code>Http::get</code>. We set the state to <code>Wait2</code>, just like we did when going from <code>State::Start</code> to <code>State::Wait1</code>.</li>
				<li>Like we did the first time we got a future that we needed to resolve before we continue, we save it so we can access it in <code>State::Wait2</code>.</li>
				<li>Since we’re in a loop, the next thing that happens is that we reach the matching arm for <code>Wait2</code>, and here, we repeat the same steps as we did for <code>State::Wait1</code> but on a different future.</li>
				<li>If it returns <code>Ready</code> with our data, we act on it and we set the final state of our <code>Coroutine</code> to <code>State::Resolved</code>. There is one more important change: this time, we want to communicate to the caller that this future is done, so we break out of the loop and return <code>PollState::Ready</code>.</li>
			</ol>
			<p>If anyone tries to call <code>poll</code> on our <code>Coroutine</code> again, we will panic, so the caller must make sure to keep track of when the future returns <code>PollState::Ready</code> and make sure to not call <code>poll</code> on it ever again. The last thing we do before we get to our <code>main</code> function is create a new <code>Coroutine</code> in a function we call <code>async_main</code>. This way, we can keep the changes to a minimum when we start talking about <code>async/await</code> in the last part of this chapter:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/main.rs</p>
			<pre class="source-code">
fn async_main() -&gt; impl Future&lt;Output = ()&gt; {
    Coroutine::new()
}</pre>			<p>So, at this point, we’re finished writing our coroutine and the only thing left is to write some logic to drive our state machine through its different stages of the <code>main</code> function.</p>
			<p>One thing to note here is that our main function is just a regular main function. The loop in our main function is what drives the asynchronous operations to completion:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/a-coroutine/src/main.rs</p>
			<pre class="source-code">
fn main() {
    let mut future = async_main();
    loop {
        match future.poll() {
            PollState::NotReady =&gt; {
                println!("Schedule other tasks");
            },
            PollState::Ready(_) =&gt; break,
        }
        thread::sleep(Duration::from_millis(100));
    }
}</pre>			<p>This <a id="_idIndexMarker439"/>function is very simple. We first get the future returned from <code>async_main</code> and then we call <code>poll</code> on it in a loop until it returns <code>PollState::Ready</code>.</p>
			<p>Every time we receive a <code>PollState::NotReady</code> in return, the control is yielded back to us. we could do other work here, such as scheduling another task, if we want to, but in our case, we just print <code>Schedule </code><code>other tasks</code>.</p>
			<p>We also limit how often the loop is run by sleeping for 100 milliseconds on every call. This way we won’t be overwhelmed with printouts and we can assume that there are roughly 100 milliseconds between every time we see <code>"Schedule other tasks"</code> printed to the console.</p>
			<p>If we run the example, we get this output:</p>
			<pre class="console">
Program starting
FIRST POLL - START OPERATION
Schedule other tasks
Schedule other tasks
Schedule other tasks
Schedule other tasks
Schedule other tasks
Schedule other tasks
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Tue, 24 Oct 2023 20:39:13 GMT
HelloWorld1
FIRST POLL - START OPERATION
Schedule other tasks
Schedule other tasks
Schedule other tasks
Schedule other tasks
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Tue, 24 Oct 2023 20:39:13 GMT
HelloWorld2</pre>			<p>By looking at the printouts, you can get an idea of the program flow.</p>
			<ol>
				<li>First, we see <code>Program starting</code>, which executes at the start of our coroutine.</li>
				<li>We then see that we immediately move on to the <code>FIRST POLL – START OPERATION</code> message that we only print when the future returned from our HTTP client is polled the first time.</li>
				<li>Next, we <a id="_idIndexMarker440"/>can see that we’re back in our <code>main</code> function, and at this point, we could theoretically go ahead and run other tasks if we had any</li>
				<li>Every 100 ms, we check if the task is finished and get the same message telling us that we can schedule other tasks</li>
				<li>Then, after roughly 600 milliseconds, we receive a response that’s printed out</li>
				<li>We repeat the process once more until we receive and print out the second response from the server</li>
			</ol>
			<p>Congratulations, you’ve now created a task that can be paused and resumed at different points, allowing it to be in progress.</p>
			<h3>Who on earth wants to write code like this to accomplish a simple task?</h3>
			<p>The answer is no one!</p>
			<p>Yes, it’s a<a id="_idIndexMarker441"/> bit bombastic, but I dare guess that very few programmers prefer writing a 55-line state machine when you compare it to the 7 lines of normal sequential code you’d have to write to accomplish the same thing.</p>
			<p>If we recall the goals of most userland abstractions over concurrent operations, we’ll see that this way of doing it only checks one of the three boxes that we’re aiming for:</p>
			<ul>
				<li>Efficient</li>
				<li>Expressive</li>
				<li>Easy to use and hard to misuse</li>
			</ul>
			<p>Our state machine will be efficient, but that’s pretty much it.</p>
			<p>However, you might also notice that there is a system to the craziness. This might not come as a surprise, but the code we wrote could be much simpler if we tagged the start of each<a id="_idIndexMarker442"/> function and each point we wanted to yield control back to the caller with a few keywords and had our state machine generated for us. And that’s the basic idea behind <code>async/await</code>.</p>
			<p>Let’s go and see how this would work in our example.</p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor131"/>async/await</h1>
			<p>The previous example <a id="_idIndexMarker443"/>could simply be written as the following using <code>async/await</code> keywords:</p>
			<pre class="source-code">
async fn async_main() {
    println!("Program starting")
    let txt = Http::get("/1000/HelloWorld").await;
    println!("{txt}");
    let txt2 = Http::("500/HelloWorld2").await;
    println!("{txt2}");
}</pre>			<p>That’s seven lines of code, and it looks very familiar to code you’d write in a normal subroutine/function.</p>
			<p>It turns out that we can let the compiler write these state machines for us instead of writing them ourselves. Not only that, we could get very far just using simple macros to help us, which is exactly how the current <code>async/await</code> syntax was prototyped before it became a part of the language. You can see an example of that at <a href="https://github.com/alexcrichton/futures-await">https://github.com/alexcrichton/futures-await</a>.</p>
			<p>The downside is of course that these functions look like normal subroutines but are in fact very different in nature. With a strongly typed language such as Rust, which borrow semantics instead of using a garbage collector, it’s impossible to hide the fact that these functions are different. This can cause a bit of confusion for programmers, who expect everything to behave the same way.</p>
			<p class="callout-heading">Coroutine bonus example</p>
			<p class="callout">To show how close our example is to the behavior we get using the <code>std::future:::Future</code> trait and <code>async/await</code> in Rust, I created the exact same example as we just did in <code>a-coroutines</code> using “proper” futures and the <code>async/await</code> syntax instead. The first thing you’ll notice is that it only required very minor changes to the code. Secondly, you can see for yourself that the output shows the exact same program flow as it did in the example where we hand-wrote the state machine ourselves. You will find this example in the <code>ch07/a-coroutines-bonus</code> folder in the repository.</p>
			<p>So, let’s take <a id="_idIndexMarker444"/>this a step further. To avoid confusion, and since our coroutines only yield to the calling function right now (there is no scheduler, event loop, or anything like that yet), we use a slightly different syntax called <code>coroutine/wait</code> and create a way to have these state machines generated for us.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor132"/>coroutine/wait</h2>
			<p>The <code>coroutine/wait</code> syntax <a id="_idIndexMarker445"/>will have clear similarities to the <code>async/await</code> syntax, although it’s a lot more limited.</p>
			<p>The basic rules are as follows:</p>
			<ul>
				<li>Every function prefixed with <code>coroutine</code> will be rewritten to a state machine like the one we wrote.</li>
				<li>The return type of functions marked with <code>coroutine</code> will be rewritten so they return <code>-&gt; impl Future&lt;Output = String&gt;</code> (yes, our syntax will only deal with futures that output a <code>String</code>).</li>
				<li>Only objects implementing the <code>Future</code> trait can be postfixed with <code>.wait</code>. These points will be represented as separate stages in our state machine.</li>
				<li>Functions prefixed with <code>coroutine</code> can call normal functions, but normal functions can’t call <code>coroutine</code> functions and expect anything to happen unless they call <code>poll</code> on them repeatedly until they return <code>PollState::Ready</code>.</li>
			</ul>
			<p>Our implementation will make sure that if we write the following code, it will compile to<a id="_idIndexMarker446"/> the same state machine we wrote at the start of this chapter(with the exception that all coroutines will return a String):</p>
			<pre class="source-code">
coroutine fn async_main() {
    println!("Program starting")
    let txt = Http::get("/1000/HelloWorld").wait;
    println!("{txt}");
    let txt2 = Http::("500/HelloWorld2").wait;
    println!("{txt2}");
}</pre>			<p>But wait. <code>coroutine/wait</code> aren’t valid keywords in Rust. I would get a compilation error if I wrote that!</p>
			<p>You’re right. So, I created a small program called <code>corofy</code> that rewrites the <code>coroutine/wait</code> functions into these state machines for us. Let’s explain that quickly.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor133"/>corofy—the coroutine preprocessor</h2>
			<p>The best way of <a id="_idIndexMarker447"/>rewriting code in Rust is using the macro system. The downside is that it’s not clear exactly what it compiles down to, and expanding the macros is not<a id="_idIndexMarker448"/> optimal for our use case since one of the main goals is to take a look at the differences between the code we write and what it transforms into. In addition to that, macros can get quite complex to read and understand unless you work a lot with them on a regular basis.</p>
			<p>Instead, corofy is a normal Rust program you can find in the repository under <code>ch07/corofy</code>.</p>
			<p>If you enter that folder, you can install the tool globally by writing the following:</p>
			<pre class="console">
cargo install --path .</pre>			<p>Now you can use the tool from anywhere. It works by providing it with an input file containing the <code>coroutine/wait</code> syntax, such as <code>corofy ./src/main.rs [optional output file]</code>. If you don’t specify an output file, it will create a file in the same folder postfixed with <code>_corofied</code>. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The tool is extremely limited. The honest reason why is that I want to finish this example before we reach the year 2300, and I finished rewriting the entire Rust compiler from scratch just to give a robust experience using the <code>coroutine/wait</code> keywords.</p>
			<p class="callout">It turns out that writing transformations like this without access to Rust’s type system is very difficult. The main use case for this tool will be to transform the examples we write here, but it would probably work for slight variations of the same examples as well (like adding more wait points or doing more interesting tasks in between each wait point). Take a look at the README for <code>corofy</code> for more information about its limitations.</p>
			<p class="callout">One more thing: I assume that you specified no explicit output file going forward so the output file will have the same name as the input file postfixed with <code>_corofied</code>.</p>
			<p>The program <a id="_idIndexMarker449"/>reads the file you give it and searches for usages of the <code>coroutine</code> keyword. It takes these functions, comments them out (so they’re still in the file), puts them last<a id="_idIndexMarker450"/> in the file, and writes out the state machine implementation directly below, indicating what parts of the state machine are the code you actually wrote between the <code>wait</code> points.</p>
			<p>Now that I’ve introduced our new tool, it’s time to put it to use.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor134"/>b-async-await—an example of a coroutine/wait transformation</h2>
			<p>Let’s start by expanding <a id="_idIndexMarker451"/>our example slightly. Now that we have a program that writes out our state machines, it’s easier for us to create some examples and cover some more complex parts of our coroutine implementation.</p>
			<p>We’ll base the following examples on the exact same code as we did in the first one. In the repository, you’ll find this example under <code>ch07/b-async-await</code>.</p>
			<p>If you write every<a id="_idIndexMarker452"/> example from the book and don’t rely on the existing code in the repository, you can do one of two things:</p>
			<ul>
				<li>Keep changing the code in the first example</li>
				<li>Create a new cargo project called <code>b-async-await</code> and copy everything in the <code>src</code> folder and the <code>dependencies</code> section from <code>Cargo.toml</code> from the previous example over to the new one.</li>
			</ul>
			<p>No matter what you choose, you should have the same code in front of you.</p>
			<p>Let’s simply change the code in <code>main.rs</code> to this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/b-async-await/src/main.rs</p>
			<pre class="source-code">
use std::time::Instant;
mod http;
mod future;
use future::*;
use crate::http::Http;
fn get_path(i: usize) -&gt; String {
    format!("/{}/HelloWorld{i}", i * 1000)
}
coroutine fn async_main() {
    println!("Program starting");
    let txt = Http::get(&amp;get_path(0)).wait;
    println!("{txt}");
    let txt = Http::get(&amp;get_path(1)).wait;
    println!("{txt}");
    let txt = Http::get(&amp;get_path(2)).wait;
    println!("{txt}");
    let txt = Http::get(&amp;get_path(3)).wait;
    println!("{txt}");
    let txt = Http::get(&amp;get_path(4)).wait;
    println!("{txt}");
}
fn main() {
    let start = Instant::now();
    let mut future = async_main();
    loop {
        match future.poll() {
            PollState::NotReady =&gt; (),
            PollState::Ready(_) =&gt; break,
        }
    }
    println!("\nELAPSED TIME: {}", start.elapsed().as_secs_f32());
}</pre>			<p>This code <a id="_idIndexMarker453"/>contains a few changes. First, we add a convenience function for creating new paths for our GET request called <code>get_path</code> to create a path we can use in our GET request with a delay and a message based on the integer we pass in.</p>
			<p>Next, in our <code>async_main</code> function, we create five requests with delays varying from <code>0</code> to <code>4</code> seconds.</p>
			<p>The last change we’ve made is in our <code>main</code> function. We no longer print out a message on every call to poll, and therefore, we don’t use <code>thread::sleep</code> to limit the number of calls. Instead, we measure the time from when we enter the <code>main</code> function to when we exit it because we can use that as a way to prove whether our code runs concurrently or not.</p>
			<p>Now that our <code>main.rs</code> looks like the preceding example, we can use <code>corofy</code> to rewrite it into a state machine, so assuming we’re in the root folder of <code>ch07/b-async-await</code>, we can write the following:</p>
			<pre class="console">
corofy ./src/main.rs</pre>			<p>That should output a file called <code>main_corofied.rs</code> in the <code>src</code> folder that you can open and inspect.</p>
			<p>Now, you can copy all the contents of <code>main_corofied.rs</code> in this file and paste it into <code>main.rs</code>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For convenience, there is a file called <code>original_main.rs</code> in the root of the project that contains the code for <code>main.rs</code> that we presented, so you don’t need to save the original content of <code>main.rs</code>. If you write out every example yourself by copying it from the book in your own project, it would be smart to store the original contents of <code>main.rs</code> somewhere before you overwrite it.</p>
			<p>I won’t show the entire state machine here since the 39 lines of code using <code>coroutine/wait</code> end up being 170 lines of code when written as a state machine, but our <code>State</code> <code>enum</code> now <a id="_idIndexMarker454"/>looks like this:</p>
			<pre class="source-code">
enum State0 {
    Start,
    Wait1(Box&lt;dyn Future&lt;Output = String&gt;&gt;),
    Wait2(Box&lt;dyn Future&lt;Output = String&gt;&gt;),
    Wait3(Box&lt;dyn Future&lt;Output = String&gt;&gt;),
    Wait4(Box&lt;dyn Future&lt;Output = String&gt;&gt;),
    Wait5(Box&lt;dyn Future&lt;Output = String&gt;&gt;),
    Resolved,
}</pre>			<p>If you run the program using <code>cargo run</code>, you now get the following output:</p>
			<pre class="console">
Program starting
FIRST POLL - START OPERATION
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Tue, xx xxx xxxx 21:05:55 GMT
HelloWorld0
FIRST POLL - START OPERATION
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Tue, xx xxx xxxx 21:05:56 GMT
HelloWorld1
FIRST POLL - START OPERATION
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Tue, xx xxx xxxx 21:05:58 GMT
HelloWorld2
FIRST POLL - START OPERATION
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Tue, xx xxx xxxx 21:06:01 GMT
HelloWorld3
FIRST POLL - START OPERATION
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Tue, xx xxx xxxx 21:06:05 GMT
HelloWorld4
ELAPSED TIME: 10.043025</pre>			<p>So, you see that our code runs as expected.</p>
			<p>Since we<a id="_idIndexMarker455"/> called <code>wait</code> on every call to <code>Http::get</code>, the code ran sequentially, which is evident when we look at the elapsed time of 10 seconds.</p>
			<p>That makes sense since the delays we asked for were <code>0 + 1 + 2 + 3 + 4</code>, which equals 10 seconds.</p>
			<p>What if we want our futures to run concurrently?</p>
			<p>Do you remember we talked about these futures being <em class="italic">lazy</em>? Good. So, you know that we won’t get concurrency just by creating a future. We need to poll them to start the operation.</p>
			<p>To solve this, we take some inspiration <a id="_idIndexMarker456"/>from <code>join_all</code>. It takes a collection of futures and drives them all to completion concurrently.</p>
			<p>Let’s create the last example for this chapter where we do just this.</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor135"/>c-async-await—concurrent futures</h1>
			<p>Okay, so we’ll build on the<a id="_idIndexMarker457"/> last example and do just the same thing. Create a new project called <code>c-async-await</code> and copy <code>Cargo.toml</code> and everything in the <code>src</code> folder over.</p>
			<p>The first thing we’ll do is go to <code>future.rs</code> and add a <code>join_all</code> function below our existing code:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/c-async-await/src/future.rs</p>
			<pre class="source-code">
pub fn join_all&lt;F: Future&gt;(futures: Vec&lt;F&gt;) -&gt; JoinAll&lt;F&gt; {
    let futures = futures.into_iter().map(|f| (false, f)).collect();
    JoinAll {
        futures,
        finished_count: 0,
    }
}</pre>			<p>This function takes a collection of futures as an argument and returns a <code>JoinAll&lt;F&gt;</code> future.</p>
			<p>The function<a id="_idIndexMarker458"/> simply creates a new collection. In this collection, we will have tuples consisting of the original futures we received and a <code>bool</code> value indicating whether the future is resolved or not.</p>
			<p>Next, we have the definition of our <code>JoinAll</code> struct:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch07/c-async-await/src/future.rs</p>
			<pre class="source-code">
pub struct JoinAll&lt;F: Future&gt; {
    futures: Vec&lt;(bool, F)&gt;,
    finished_count: usize,
}</pre>			<p>This struct will simply store the collection we created and a <code>finished_count</code>. The last field will make it a little bit easier to keep track of how many futures have been resolved.</p>
			<p>As we’re getting used to by now, most of the interesting parts happen in the <code>Future</code> implementation for <code>JoinAll</code>:</p>
			<pre class="source-code">
impl&lt;F: Future&gt; Future for JoinAll&lt;F&gt; {
    type Output = String;
    fn poll(&amp;mut self) -&gt; PollState&lt;Self::Output&gt; {
        for (finished, fut) in self.futures.iter_mut() {
            if *finished {
                continue;
            }
            match fut.poll() {
                PollState::Ready(_) =&gt; {
                    *finished = true;
                    self.finished_count += 1;
                }
                PollState::NotReady =&gt; continue,
            }
        }
        if self.finished_count == self.futures.len() {
            PollState::Ready(String::new())
        } else {
            PollState::NotReady
        }
    }
}</pre>			<p>We set <code>Output</code> to <code>String</code>. This might strike you as strange since we don’t actually return anything from this implementation. The reason is that <code>corofy</code> will only work with futures that return a <code>String</code> (it’s one of its many, many shortcomings), so we just accept that and return an empty string on completion.</p>
			<p>Next up is our <code>poll</code> implementation. The first thing we do is to loop over each (flag, future) tuple:</p>
			<pre class="source-code">
for (finished, fut) in self.futures.iter_mut()</pre>			<p>Inside the <a id="_idIndexMarker459"/>loop, we first check if the flag for this future is set to <code>finished</code>. If it is, we simply go to the next item in the collection.</p>
			<p>If it’s not finished, we <code>poll</code> the future.</p>
			<p>If we get <code>PollState::Ready</code> back, we set the flag for this future to <code>true</code> so that we won’t poll it again and we increase the finished count.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">It’s worth noting that the <code>join_all</code> implementation we create here will not work in any meaningful way with futures that return a value. In our case, we simply throw the value away, but remember, we’re trying to keep this as simple as possible for now and the only thing we want to show is the concurrency aspect of calling <code>join_all</code>.</p>
			<p class="callout">Tokio’s <code>join_all</code> implementation puts all the returned values in a <code>Vec&lt;T&gt;</code> and returns them when the <code>JoinAll</code> future resolves.</p>
			<p>If we get <code>PollState::NotReady</code>, we simply continue to the next future in the collection.</p>
			<p>After iterating through the entire collection, we check if we’ve resolved all the futures we originally received in <code>if self.finished_count == </code><code>self.futures.len()</code>.</p>
			<p>If all our futures have been resolved, we return <code>PollState::Ready</code> with an empty string (to make <code>corofy</code> happy). If there are still unresolved futures, we return <code>PollState::NotReady</code>.</p>
			<p class="callout-heading">Important</p>
			<p class="callout">There is one subtle point to make a note of here. The first time <code>JoinAll::poll</code> is called, it will call <code>poll</code> on each future in the collection. Polling each future will kick off whatever operation they represent and allow them to <em class="italic">progress concurrently</em>. This is one way to achieve concurrency with lazy coroutines, such as the ones we’re dealing with here.</p>
			<p>Next up are the changes we’ll make in <code>main.rs</code>.</p>
			<p>The <code>main</code> function<a id="_idIndexMarker460"/> will be the same, as well as the imports and declarations at the start of the file, so I’ll only present the <code>coroutine/await</code> functions that we’ve changed:</p>
			<pre class="source-code">
coroutine fn request(i: usize) {
    let path = format!("/{}/HelloWorld{i}", i * 1000);
    let txt = Http::get(&amp;path).wait;
    println!("{txt}");
}
coroutine fn async_main() {
    println!("Program starting");
    let mut futures = vec![];
    for i in 0..5 {
        futures.push(request(i));
    }
    future::join_all(futures).wait;
}</pre>			<p class="callout-heading">Note</p>
			<p class="callout">In the repository, you’ll find the correct code to put in <code>main.rs</code> in <code>ch07/c-async-await/original_main.rs</code> if you ever lose track of it with all the copy/pasting we’re doing.</p>
			<p>Now we have two <code>coroutine/wait</code> functions. <code>async_main</code> stores a set of coroutines created by <code>read_request</code> in a <code>Vec&lt;T: Future&gt;</code>.</p>
			<p>Then it creates a <code>JoinAll</code> future and calls <code>wait</code> on it.</p>
			<p>The<a id="_idIndexMarker461"/> next <code>coroutine/wait</code> function is <code>read_requests</code>, which takes an integer as input and uses that to create GET requests. This coroutine will in turn wait for the response and print out the result once it arrives.</p>
			<p>Since we create the requests with delays of <code>0, 1, 2, 3, 4</code> seconds, we should expect the entire program to finish in just over four seconds because all the tasks will be in progress <em class="italic">concurrently</em>. The ones with short delays will be finished by the time the task with a four-second delay finishes.</p>
			<p>We can now transform our <code>coroutine/await</code> functions into state machines by making sure we’re in the folder <code>ch07/c-async-await</code> and writing <code>corofy ./src/main.rs</code>.</p>
			<p>You should now see a file called <code>main_corofied.rs</code> in the <code>src</code> folder. Copy its contents and replace what’s in <code>main.rs</code> with it.</p>
			<p>If you run the program by writing <code>cargo run</code>, you should get the following output:</p>
			<pre class="console">
Program starting
FIRST POLL - START OPERATION
FIRST POLL - START OPERATION
FIRST POLL - START OPERATION
FIRST POLL - START OPERATION
FIRST POLL - START OPERATION
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Tue, xx xxx xxxx 21:11:36 GMT
HelloWorld0
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Tue, xx xxx xxxx 21:11:37 GMT
HelloWorld1
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Tue, xx xxx xxxx 21:11:38 GMT
HelloWorld2
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Tue, xx xxx xxxx 21:11:39 GMT
HelloWorld3
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Tue, xx xxx xxxx 21:11:40 GMT
HelloWorld4
ELAPSED TIME: 4.0084987</pre>			<p>The <a id="_idIndexMarker462"/>thing to make a note of here is the elapsed time. It’s now just over four seconds, just like we expected it would be when our futures run concurrently.</p>
			<p>If we take a look at how <code>coroutine/await</code> changed the experience of writing coroutines from a programmer’s perspective, we’ll see that we’re much closer to our goal now:</p>
			<ul>
				<li><strong class="bold">Efficient</strong>: State machines require no context switches and only save/restore the data associated with that specific task. We have no growing vs segmented stack issues, as they all use the same OS-provided stack.</li>
				<li><strong class="bold">Expressive</strong>: We can write code the same way as we do in “normal” Rust, and with compiler support, we can get the same error messages and use the same tooling</li>
				<li><code>async</code> function from a normal function and expect anything meaningful to happen; you have to actively poll it to completion somehow, which gets more complex as we start adding runtimes into the mix. However, for the most part, we can write programs just the way we’re used to.</li>
			</ul>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor136"/>Final thoughts</h1>
			<p>Before we round off this chapter, I want to point out that it should now be clear to us why coroutines aren’t really <strong class="bold">pre-emptable</strong>. If you remember back in <a href="B20892_02.xhtml#_idTextAnchor043"><em class="italic">Chapter 2</em></a>, we said that a <em class="italic">stackful</em> coroutine (such as our fibers/green threads example) could be <em class="italic">pre-empted</em> and its execution could be paused at any point. That’s because they have a stack, and pausing a task is as simple as storing the current execution state to the stack and jumping to another task.</p>
			<p>That’s not possible here. The only places we can stop and resume execution are at the pre-defined suspension points that we manually tagged with <code>wait</code>.</p>
			<p>In theory, if you have a tightly integrated system where you control the compiler, the coroutine definition, the scheduler, and the I/O primitives, you could add additional states to the state machine and create additional points where the task could be suspended/resumed. These suspension points could be opaque to the user and treated differently than normal wait/suspension points.</p>
			<p>For example, every time you encounter a normal function call, you could add a suspension point (a new state to our state machine) where you check in with the scheduler if the current task has used up its time budget or something like that. If it has, you could schedule another task to run and resume the task at a later point even though this didn’t happen in a cooperative manner.</p>
			<p>However, even though this would be invisible to the user, it’s not the same as being able to stop/resume execution from any point in your code. It would also go against the usually implied cooperative nature of coroutines.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor137"/>Summary</h1>
			<p>Good job! In this chapter, we introduced quite a bit of code and set up an example that we’ll continue using in the following chapters.</p>
			<p>So far, we’ve focused on futures and <code>async/await</code> to model and create tasks that can be paused and resumed at specific points. We know this is a prerequisite to having tasks that are in progress at the same time. We did this by introducing our own simplified <code>Future</code> trait and our own <code>coroutine/wait</code> syntax that’s way more limited than Rust’s futures and <code>async/await</code> syntax, but it’s easier to understand and get a mental idea of how this works in contrast to fibers/green threads (at least I hope so).</p>
			<p>We have also discussed the difference between eager and lazy coroutines and how they impact how you achieve concurrency. We took inspiration from Tokio’s <code>join_all</code> function and implemented our own version of it.</p>
			<p>In this chapter, we simply created tasks that could be paused and resumed. There are no event loops, scheduling, or anything like that yet, but don’t worry. They’re exactly what we’ll go through in the next chapter. The good news is that getting a clear idea of coroutines, like we did in this chapter, is one of the most difficult things to do.</p>
		</div>
	</body></html>
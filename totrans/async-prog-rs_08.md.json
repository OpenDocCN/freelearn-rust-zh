["```rs\ncargo install --force --path .\n```", "```rs\n    [dependencies]\n    mio = { version = \"0.8\", features = [\"net\", \"os-poll\"] }\n    ```", "```rs\nmod future;\nmod http;\nmod runtime;\nuse future::{Future, PollState};\nuse runtime::Runtime;\nfn main() {\n    let future = async_main();\n    let mut runtime = Runtime::new();\n    runtime.block_on(future);\n}\ncoroutine fn async_main() {\n    println!(\"Program starting\");\n    let txt = http::Http::get(\"/600/HelloAsyncAwait\").wait;\n    println!(\"{txt}\");\n    let txt = http::Http::get(\"/400/HelloAsyncAwait\").wait;\n    println!(\"{txt}\");\n}\n```", "```rs\nsrc\n |-- future.rs\n |-- http.rs\n |-- main.rs\n |-- runtime.rs\n```", "```rs\n fn main() {\n    let future = async_main();\n    let mut runtime = Runtime::new();\n    runtime.block_on(future);\n}\n```", "```rs\nuse crate::future::{Future, PollState};\nuse mio::{Events, Poll, Registry};\nuse std::sync::OnceLock;\n```", "```rs\nstatic REGISTRY: OnceLock<Registry> = OnceLock::new();\npub fn registry() -> &'static Registry {\n    REGISTRY.get().expect(\"Called outside a runtime context\")\n}\n```", "```rs\npub struct Runtime {\n    poll: Poll,\n}\n```", "```rs\nimpl Runtime {\n    pub fn new() -> Self {\n        let poll = Poll::new().unwrap();\n        let registry = poll.registry().try_clone().unwrap();\n        REGISTRY.set(registry).unwrap();\n        Self { poll }\n    }\n    pub fn block_on<F>(&mut self, future: F)\n    where\n        F: Future<Output = String>,\n    {\n        let mut future = future;\n        loop {\n            match future.poll() {\n                PollState::NotReady => {\n                    println!(\"Schedule other tasks\\n\");\n                    let mut events = Events::with_capacity(100);\n                    self.poll.poll(&mut events, None).unwrap();\n                }\n                PollState::Ready(_) => break,\n            }\n        }\n    }\n}\n```", "```rs\nuse crate::{future::PollState, runtime, Future};\nuse mio::{Interest, Token};\nuse std::io::{ErrorKind, Read, Write};\n```", "```rs\nimpl Future for HttpGetFuture {\n  type Output = String;\n  fn poll(&mut self) -> PollState<Self::Output> {\n    if self.stream.is_none() {\n      println!(\"FIRST POLL - START OPERATION\");\n      self.write_request();\n      runtime::registry()\n        .register(self.stream.as_mut().unwrap(), Token(0), Interest::READABLE)\n                .unwrap();\n        }\n        let mut buff = vec![0u8; 4096];\n        loop {\n            match self.stream.as_mut().unwrap().read(&mut buff) {\n                Ok(0) => {\n                    let s = String::from_utf8_lossy(&self.buffer);\n                    break PollState::Ready(s.to_string());\n                }\n                Ok(n) => {\n                    self.buffer.extend(&buff[0..n]);\n                    continue;\n                }\n                Err(e) if e.kind() == ErrorKind::WouldBlock => {\n                    break PollState::NotReady;\n                }\n                Err(e) => panic!(\"{e:?}\"),\n            }\n        }\n    }\n}\n```", "```rs\nreturn PollState::NotReady;\n```", "```rs\nProgram starting\nFIRST POLL - START OPERATION\nSchedule other tasks\nSchedule other tasks\nSchedule other tasks\nSchedule other tasks\nSchedule other tasks\nSchedule other tasks\nSchedule other tasks\nHTTP/1.1 200 OK\ncontent-length: 11\nconnection: close\ncontent-type: text/plain; charset=utf-8\ndate: Thu, 16 Nov xxxx xx:xx:xx GMT\nHelloWorld1\nFIRST POLL - START OPERATION\nSchedule other tasks\nSchedule other tasks\nSchedule other tasks\nSchedule other tasks\nSchedule other tasks\nHTTP/1.1 200 OK\ncontent-length: 11\nconnection: close\ncontent-type: text/plain; charset=utf-8\ndate: Thu, 16 Nov xxxx xx:xx:xx GMT\nHelloWorld2\n```", "```rs\nProgram starting\nFIRST POLL - START OPERATION\nSchedule other tasks\nHTTP/1.1 200 OK\ncontent-length: 11\nconnection: close\ncontent-type: text/plain; charset=utf-8\ndate: Thu, 16 Nov xxxx xx:xx:xx GMT\nHelloAsyncAwait\nFIRST POLL - START OPERATION\nSchedule other tasks\nHTTP/1.1 200 OK\ncontent-length: 11\nconnection: close\ncontent-type: text/plain; charset=utf-8\ndate: Thu, 16 Nov xxxx xx:xx:xx GMT\nHelloAsyncAwait\n```", "```rs\n    mod executor;\n    mod reactor;\n    ```", "```rs\nsrc\n |-- runtime\n        |-- executor.rs\n        |-- reactor.rs\n |-- future.rs\n |-- http.rs\n |-- main.rs\n |-- runtime.rs\n```", "```rs\npub use executor::{spawn, Executor, Waker};\npub use reactor::reactor;\nmod executor;\nmod reactor;\npub fn init() -> Executor {\n    reactor::start();\n    Executor::new()\n}\n```", "```rs\nuse crate::future::{Future, PollState};\nuse std::{\n    cell::{Cell, RefCell},\n    collections::HashMap,\n    sync::{Arc, Mutex},\n    thread::{self, Thread},\n};\n```", "```rs\n#[derive(Clone)]\npub struct Waker {\n    thread: Thread,\n    id: usize,\n    ready_queue: Arc<Mutex<Vec<usize>>>,\n}\n```", "```rs\nimpl Waker {\n    pub fn wake(&self) {\n        self.ready_queue\n            .lock()\n            .map(|mut q| q.push(self.id))\n            .unwrap();\n        self.thread.unpark();\n    }\n}\n```", "```rs\nuse crate::runtime::Waker;\n```", "```rs\npub trait Future {\n    type Output;\n    fn poll(&mut self, waker: &Waker) -> PollState<Self::Output>;\n}\n```", "```rs\nuse crate::{future::PollState, runtime::{self, reactor, Waker}, Future};\nuse mio::Interest;\nuse std::io::{ErrorKind, Read, Write};\n```", "```rs\nimpl Future for HttpGetFuture {\n    type Output = String;\n    fn poll(&mut self, waker: &Waker) -> PollState<Self::Output> {\n…\n```", "```rs\nuse runtime::Waker;\n```", "```rs\nfn poll(&mut self, waker: &Waker)\nmatch f1.poll(waker)\nmatch f2.poll(Waker.\n\t\t\tThe next step will be to create a proper `Executor`.\n\t\t\tStep 2 – Implementing a proper Executor\n\t\t\tIn this step, we’ll create an executor that will:\n\n\t\t\t\t*   Hold many top-level futures and switch between them\n\t\t\t\t*   Enable us to spawn new top-level futures from anywhere in our asynchronous program\n\t\t\t\t*   Hand out `Waker` types so that they can sleep when there is nothing to do and wake up when one of the top-level futures can progress\n\t\t\t\t*   Enable us to run several executors by having each run on its dedicated OS thread\n\n\t\t\tNote\n\t\t\tIt’s worth mentioning that our executor won’t be fully multithreaded in the sense that tasks/futures can’t be sent from one thread to another, and the different `Executor` instances will not know of each other. Therefore, executors can’t steal work from each other (no work-stealing), and we can’t rely on executors picking tasks from a global task queue.\n\t\t\tThe reason is that the `Executor` design will be much more complex if we go down that route, not only because of the added logic but also because we have to add constraints, such as requiring everything to be `Send +` `Sync`.\n\t\t\tSome of the complexity in asynchronous Rust today can be attributed to the fact that many runtimes in Rust are multithreaded by default, which makes asynchronous Rust deviate more from “normal” Rust than it actually needs to.\n\t\t\tIt’s worth mentioning that since most production runtimes in Rust are multithreaded by default, most of them also have a work-stealing executor. This will be similar to the last version of our bartender example in [*Chapter 1*](B20892_01.xhtml#_idTextAnchor014), where we achieved a slightly increased efficiency by letting the bartenders “steal” tasks that are *in progress* from each other.\n\t\t\tHowever, this example should still give you an idea of how we can leverage all the cores on a machine to run asynchronous tasks, giving us both concurrency and parallelism, even though it will have limited capabilities.\n\t\t\tLet’s start by opening up `executor.rs` located in the `runtime` subfolder.\n\t\t\tThis file should already contain our `Waker` and the dependencies we need, so let’s start by adding the following lines of code just below our dependencies:\n\t\t\tch08/b-reactor-executor/src/runtime/executor.rs\n\n```", "```rs\n\n\t\t\tThe first line is a *type alias*; it simply lets us create an alias called `Task` that refers to the type: `Box<dyn Future<Output = String>>`. This will help keep our code a little bit cleaner.\n\t\t\tThe next line might be new to some readers. We define a thread-local static variable by using the `thread_local!` macro.\n\t\t\tThe `thread_local!` macro lets us define a static variable that’s unique to the thread it’s first called from. This means that all threads we create will have their own instance, and it’s impossible for one thread to access another thread’s `CURRENT_EXEC` variable.\n\t\t\tWe call the variable `CURRENT_EXEC` since it holds the `Executor` that’s currently running on this thread.\n\t\t\tThe next lines we add to this file is the definition of `ExecutorCore`:\n\t\t\tch08/b-reactor-executor/src/runtime/executor.rs\n\n```", "```rs\n\n\t\t\t`ExecutorCore` holds all the state for our `Executor`:\n\n\t\t\t\t*   `tasks` – This is a `HashMap` with a `usize` as the *key* and a `Task` (remember the alias we created previously) as *data*. This will hold all the top-level futures associated with the executor on this thread and allow us to give each an `id` property to identify them. We can’t simply mutate a static variable, so we need internal mutability here. Since this will only be callable from one thread, a `RefCell` will do so since there is no need for synchronization.\n\t\t\t\t*   `ready_queue` – This is a simple `Vec<usize>` that stores the IDs of tasks that should be polled by the executor. If we refer back to *Figure 8**.7*, you’ll see how this fits into the design we outlined there. As mentioned earlier, we could store something such as an `Arc<dyn Future<…>>` here instead, but that adds quite a bit of complexity to our example. The only downside with the current design is that instead of getting a reference to the task directly, we have to look it up in our `tasks` collection, which takes time. An `Arc<…>` (shared reference) to this collection will be given to each `Waker` that this executor creates. Since the `Waker` can (and will) be sent to a different thread and signal that a specific task is ready by adding the task’s ID to `ready_queue`, we need to wrap it in an `Arc<Mutex<…>>`.\n\t\t\t\t*   `next_id` – This is a counter that gives out the next available I, which means that it should never hand out the same ID twice for this executor instance. We’ll use this to give each top-level future a unique ID. Since the executor instance will only be accessible on the same thread it was created, a simple `Cell` will suffice in giving us the internal mutability we need.\n\n\t\t\t`ExecutorCore` derives the `Default` trait since there is no special initial state we need here, and it keeps the code short and concise.\n\t\t\tThe next function is an important one. The `spawn` function allows us to register new top-level futures with our executor from anywhere in our program:\n\t\t\tch08/b-reactor-executor/src/runtime/executor.rs\n\n```", "```rs\n\n\t\t\tThe `spawn` function does a few things:\n\n\t\t\t\t*   It gets the next available ID.\n\t\t\t\t*   It assigns the ID to the future it receives and stores it in a `HashMap`.\n\t\t\t\t*   It adds the ID that represents this task to `ready_queue` so that it’s polled at least once (remember that `Future` traits in Rust don’t do anything unless they’re polled at least once).\n\t\t\t\t*   It increases the ID counter by one.\n\n\t\t\tThe unfamiliar syntax accessing `CURRENT_EXEC` by calling `with` and passing in a closure is just a consequence of how thread local statics is implemented in Rust. You’ll also notice that we must use a few special methods because we use `RefCell` and `Cell` for internal mutability for `tasks` and `next_id`, but there is really nothing inherently complex about this except being a bit unfamiliar.\n\t\t\tA quick note about static lifetimes\n\t\t\tWhen a `'static` lifetime is used as a trait bound as we do here, it doesn’t actually mean that the lifetime of the `Future` trait we pass in *must be* static (meaning it will have to live until the end of the program). It means that it *must be able to* last until the end of the program, or, put another way, the lifetime can’t be constrained in any way.\n\t\t\tMost often, when you encounter something that requires a `'static` bound, it simply means that you’ll have to give ownership over the thing you pass in. If you pass in any references, they need to have a `'static` lifetime. It’s less difficult to satisfy this constraint than you might expect.\n\t\t\tThe final part of *step 2* will be to define and implement the `Executor` struct itself.\n\t\t\tThe `Executor` struct is very simple, and there is only one line of code to add:\n\t\t\tch08/b-reactor-executor/src/runtime/executor.rs\n\n```", "```rs\n\n\t\t\tSince all the state we need for our example is held in `ExecutorCore`, which is a static thread-local variable, our `Executor` struct doesn’t need any state. This also means that we don’t strictly need a struct at all, but to keep the API somewhat familiar, we do it anyway.\n\t\t\tMost of the executor implementation is a handful of simple helper methods that end up in a `block_on` function, which is where the interesting parts really happen.\n\t\t\tSince these helper methods are short and easy to understand, I’ll present them all here and just briefly go over what they do:\n\t\t\tNote\n\t\t\tWe open the `impl Executor` block here but will not close it until we’ve finished implementing the `block_on` function.\n\t\t\tch08/b-reactor-executor/src/runtime/executor.rs\n\n```", "```rs\n\n\t\t\tSo, we have six methods here:\n\n\t\t\t\t*   `new` – Creates a new `Executor` instance. For simplicity, we have no initialization here, and everything is done lazily by design in the `thread_local!` macro.\n\t\t\t\t*   `pop_ready` – This function takes a lock on `read_queue` and pops off an ID that’s ready from the back of `Vec`. Calling `pop` here means that we also remove the item from the collection. As a side note, since `Waker` pushes its ID to the *back* of `ready_queue` and we pop off from the *back* as well, we essentially get a `VecDeque` from the standard library would easily allow us to choose the order in which we remove items from the queue if we wish to change that behavior.\n\t\t\t\t*   `get_future` – This function takes the ID of a top-level future as an argument, removes the future from the `tasks` collection, and returns it (if the task is found). This means that if the task returns `NotReady` (signaling that we’re not done with it), we need to remember to add it back to the collection again.\n\t\t\t\t*   `get_waker` – This function creates a new `Waker` instance.\n\t\t\t\t*   `insert_task` – This function takes an `id` property and a `Task` property and inserts them into our `tasks` collection.\n\t\t\t\t*   `task_count` – This function simply returns a count of how many tasks we have in the queue.\n\n\t\t\tThe final and last part of the `Executor` implementation is the `block_on` function. This is also where we close the `impl` `Executor` block:\n\t\t\tch08/b-reactor-executor/src/runtime/executor.rs\n\n```", "```rs\n\n\t\t\t`block_on` will be the entry point to our `Executor`. Often, you will pass in one top-level future first, and when the top-level future progresses, it will spawn new top-level futures onto our executor. Each new future can, of course, spawn new futures onto the `Executor` too, and that’s how an asynchronous program basically works.\n\t\t\tIn many ways, you can view this first top-level future in the same way you view the `main` function in a normal Rust program. `spawn` is similar to `thread::spawn`, with the exception that the tasks stay on the same OS thread in this example. This means the tasks won’t be able to run in parallel, which in turn allows us to avoid any need for synchronization between tasks to avoid data races.\n\t\t\tLet’s go through the function step by step:\n\n\t\t\t\t1.  The first thing we do is spawn the future we received onto ourselves. There are many ways this could be implemented, but this is the easiest way to do it.\n\t\t\t\t2.  Then, we have a loop that will run as long as our asynchronous program is running.\n\t\t\t\t3.  Every time we loop, we create an inner `while let Some(…)` loop that runs as long as there are tasks in `ready_queue`.\n\t\t\t\t4.  If there is a task in `ready_queue`, we take ownership of the `Future` object by removing it from the collection. We guard against false wakeups by just continuing if there is no future there anymore (meaning that we’re done with it but still get a wakeup). This will, for example, happen on Windows since we get a `READABLE` event when the connection closes, but even though we could filter those events out, `mio` doesn’t guarantee that false wakeups won’t happen, so we have to handle that possibility anyway.\n\t\t\t\t5.  Next, we create a new `Waker` instance that we can pass into `Future::poll()`. Remember that this `Waker` instance now holds the `id` property that identifies this specific `Future` trait and a handle to the thread we’re currently running on.\n\t\t\t\t6.  The next step is to call `Future::poll`.\n\t\t\t\t7.  If we get `NotReady` in return, we insert the task back into our `tasks` collection. I want to emphasize that when a `Future` trait returns `NotReady`, we know it will arrange it so that `Waker::wake` is called at a later point in time. It’s not the executor’s responsibility to track the readiness of this future.\n\t\t\t\t8.  If the `Future` trait returns `Ready`, we simply continue to the next item in the ready queue. Since we took ownership over the `Future` trait, this will drop the object before we enter the next iteration of the `while` `let` loop.\n\t\t\t\t9.  Now that we’ve polled all the tasks in our ready queue, the first thing we do is get a task count to see how many tasks we have left.\n\t\t\t\t10.  We also get the name of the current thread for future logging purposes (it has nothing to do with how our executor works).\n\t\t\t\t11.  If the task count is larger than `0`, we print a message to the terminal and call `thread::park()`. Parking the thread will yield control to the OS scheduler, and our `Executor` does nothing until it’s woken up again.\n\t\t\t\t12.  If the task count is `0`, we’re done with our asynchronous program and exit the main loop.\n\n\t\t\tThat’s pretty much all there is to it. By this point, we’ve covered all our goals for *step 2*, so we can continue to the last and final step and implement a `Reactor` for our runtime that will wake up our executor when something happens.\n\t\t\tStep 3 – Implementing a proper Reactor\n\t\t\tThe final part of our example is the `Reactor`. Our `Reactor` will:\n\n\t\t\t\t*   Efficiently wait and handle events that our runtime is interested in\n\t\t\t\t*   Store a collection of `Waker` types and make sure to wake the correct `Waker` when it gets a notification on a source it’s tracking\n\t\t\t\t*   Provide the necessary mechanisms for leaf futures such as `HttpGetFuture`, to register and deregister interests in events\n\t\t\t\t*   Provide a way for leaf futures to store the last received `Waker`\n\n\t\t\tWhen we’re done with this step, we should have everything we need for our runtime, so let’s get to it.\n\t\t\tStart by opening the `reactor.rs` file.\n\t\t\tThe first thing we do is add the dependencies we need:\n\t\t\tch08/b-reactor-executor/src/runtime/reactor.rs\n\n```", "```rs\n\n\t\t\tAfter we’ve added our dependencies, we create a *type alias* called `Wakers` that aliases the type for our `wakers` collection:\n\t\t\tch08/b-reactor-executor/src/runtime/reactor.rs\n\n```", "```rs\n\n\t\t\tThe next line will declare a static variable called `REACTOR`:\n\t\t\tch08/b-reactor-executor/src/runtime/reactor.rs\n\n```", "```rs\n\n\t\t\tThis variable will hold a `OnceLock<Reactor>`. In contrast to our `CURRENT_EXEC` static variable, this will be possible to access from different threads. `OnceLock` allows us to define a static variable that we can write to once so that we can initialize it when we start our `Reactor`. By doing so, we also make sure that there can only be a single instance of this specific reactor running in our program.\n\t\t\tThe variable will be private to this module, so we create a public function allowing other parts of our program to access it:\n\t\t\tch08/b-reactor-executor/src/runtime/reactor.rs\n\n```", "```rs\n\n\t\t\tThe next thing we do is define our `Reactor` struct:\n\t\t\tch08/b-reactor-executor/src/runtime/reactor.rs\n\n```", "```rs\n\n\t\t\tThis will be all the state our `Reactor` struct needs to hold:\n\n\t\t\t\t*   `wakers` – A `HashMap` of `Waker` objects, each identified by an integer\n\t\t\t\t*   `registry` – Holds a `Registry` instance so that we can interact with the event queue in `mio`\n\t\t\t\t*   `next_id` – Stores the next available ID so that we can track which event occurred and which `Waker` should be woken\n\n\t\t\tThe implementation of `Reactor` is actually quite simple. It’s only four short methods for interacting with the `Reactor` instance, so I’ll present them all here and give a brief explanation next:\n\t\t\tch08/b-reactor-executor/src/runtime/reactor.rs\n\n```", "```rs\n\n\t\t\tLet’s briefly explain what these four methods do:\n\n\t\t\t\t*   `register` – This method is a thin wrapper around `Registry::register`, which we know from [*Chapter 4*](B20892_04.xhtml#_idTextAnchor081). The one thing to make a note of here is that we pass in an `id` property so that we can identify which event has occurred when we receive a notification later on.\n\t\t\t\t*   `set_waker` – This method adds a `Waker` to our `HashMap` using the provided `id` property as a key to identify it. If there is a `Waker` there already, we replace it and drop the old one. An important point to remember is that `Waker` associated with the `TcpStream`.\n\t\t\t\t*   `deregister` – This function does two things. First, it removes the `Waker` from our `wakers` collection. Then, it deregisters the `TcpStream` from our `Poll` instance.\n\t\t\t\t*   I want to remind you at this point that while we only work with `TcpStream` in our examples, this could, in theory, be done with anything that implements `mio`’s `Source` trait, so the same thought process is valid in a much broader context than what we deal with here.\n\t\t\t\t*   `next_id` – This simply gets the current `next_id` value and increments the counter atomically. We don’t care about any happens before/after relationships happening here; we only care about not handing out the same value twice, so `Ordering::Relaxed` will suffice here. Memory ordering in atomic operations is a complex topic that we won’t be able to dive into in this book, but if you want to know more about the different memory orderings in Rust and what they mean, the official documentation is the right place to start: [https://doc.rust-lang.org/stable/std/sync/atomic/enum.Ordering.html](https://doc.rust-lang.org/stable/std/sync/atomic/enum.Ordering.html).\n\n\t\t\tNow that our `Reactor` is set up, we only have two short functions left. The first one is `event_loop`, which will hold the logic for our event loop that waits and reacts to new events:\n\t\t\tch08/b-reactor-executor/src/runtime/reactor.rs\n\n```", "```rs\n\n\t\t\tThis function takes a `Poll` instance and a `Wakers` collection as arguments. Let’s go through it step by step:\n\n\t\t\t\t*   The first thing we do is create an `events` collection. This should be familiar since we did the exact same thing in [*Chapter 4*](B20892_04.xhtml#_idTextAnchor081).\n\t\t\t\t*   The next thing we do is create a `loop` that in our case will continue to loop for eternity. This makes our example short and simple, but it has the downside that we have no way of shutting our event loop down once it’s started. Fixing that is not especially difficult, but since it won’t be necessary for our example, we don’t cover this here.\n\t\t\t\t*   Inside the loop, we call `Poll::poll` with a timeout of `None`, which means it will never time out and block until it receives an event notification.\n\t\t\t\t*   When the call returns, we loop through every event we receive.\n\t\t\t\t*   If we receive an event, it means that something we registered interest in happened, so we get the `id` we passed in when we first registered an interest in events on this `TcpStream`.\n\t\t\t\t*   Lastly, we try to get the associated `Waker` and call `Waker::wake` on it. We guard ourselves from the fact that the `Waker` may have been removed from our collection already, in which case we do nothing.\n\n\t\t\tIt’s worth noting that we can filter events if we want to here. Tokio provides some methods on the `Event` object to check several things about the event it reported. For our use in this example, we don’t need to filter events.\n\t\t\tFinally, the last function is the second public function in this module and the one that initializes and starts the runtime:\n\t\t\tch08/b-reactor-executor/src/runtime/runtime.rs\n\n```", "```rs\n\n\t\t\tThe `start` method should be fairly easy to understand. The first thing we do is create our `Wakers` collection and our `Poll` instance. From the `Poll` instance, we get an owned version of `Registry`. We initialize `next_id` to `1` (for debugging purposes, I wanted to initialize it to a different start value than our `Executor`) and create our `Reactor` object.\n\t\t\tThen, we set the static variable we named `REACTOR` by giving it our `Reactor` instance.\n\t\t\tThe last thing is probably the *most important one to pay attention to*. We spawn a new OS thread and start our `event_loop` function on that one. This also means that we pass on our `Poll` instance to the event loop thread for good.\n\t\t\tNow, the best practice would be to store the `JoinHandle` returned from `spawn` so that we can join the thread later on, but our thread has no way to shut down the event loop anyway, so joining it later makes little sense, and we simply discard the handle.\n\t\t\tI don’t know if you agree with me, but the logic here is not that complex when we break it down into smaller pieces. Since we know how `epoll` and `mio` work already, the rest is pretty easy to understand.\n\t\t\tNow, we’re not done yet. We still have some small changes to make to our `HttpGetFuture` leaf future since it doesn’t register with the reactor at the moment. Let’s fix that.\n\t\t\tStart by opening the `http.rs` file.\n\t\t\tSince we already added the correct imports when we opened the file to adapt everything to the new `Future` interface, there are only a few places we need to change that so this leaf future integrates nicely with our reactor.\n\t\t\tThe first thing we do is give `HttpGetFuture` an identity. It’s the source of events we want to track with our `Reactor`, so we want it to have the same ID until we’re done with it:\n\t\t\tch08/b-reactor-executor/src/http.rs\n\n```", "```rs\n\n\t\t\tWe also need to retrieve a new ID from the reactor when the future is created:\n\t\t\tch08/b-reactor-executor/src/http.rs\n\n```", "```rs\n\n\t\t\tNext, we have to locate the `poll` implementation for `HttpGetFuture`.\n\t\t\tThe first thing we need to do is make sure that we register interest with our `Poll` instance and register the `Waker` we receive with the `Reactor` the first time the future gets polled. Since we don’t register directly with `Registry` anymore, we remove that line of code and add these new lines instead:\n\t\t\tch08/b-reactor-executor/src/http.rs\n\n```", "```rs\n\n\t\t\tLastly, we need to make some minor changes to how we handle the different conditions when reading from `TcpStream`:\n\t\t\tch08/b-reactor-executor/src/http.rs\n\n```", "```rs\n\n\t\t\tThe first change is to deregister the stream from our `Poll` instance when we’re done.\n\t\t\tThe second change is a little more subtle. If you read the documentation for `Future::poll` in Rust ([https://doc.rust-lang.org/stable/std/future/trait.Future.html#tymethod.poll](https://doc.rust-lang.org/stable/std/future/trait.Future.html#tymethod.poll)) carefully, you’ll see that it’s expected that the `Waker` from the *most recent call* should be scheduled to wake up. That means that every time we get a `WouldBlock` error, we need to make sure we store the most recent `Waker`.\n\t\t\tThe reason is that the future could have moved to a different executor in between calls, and we need to wake up the correct one (it won’t be possible to move futures like those in our example, but let’s play by the same rules).\n\t\t\tAnd that’s it!\n\t\t\tCongratulations! You’ve now created a fully working runtime based on the reactor-executor pattern. Well done!\n\t\t\tNow, it’s time to test it and run a few experiments with it.\n\t\t\tLet’s go back to `main.rs` and change the `main` function so that we get our program running correctly with our new runtime.\n\t\t\tFirst of all, let’s remove the dependency on the `Runtime` struct and make sure our imports look like this:\n\t\t\tch08/b-reactor-executor/src/main.rs\n\n```", "```rs\n\n\t\t\tNext, we need to make sure that we initialize our runtime and pass in our future to `executor.block_on`. Our `main` function should look like this:\n\t\t\tch08/b-reactor-executor/src/main.rs\n\n```", "```rs\n\n\t\t\tAnd finally, let’s try it out by running it:\n\n```", "```rs\n\n\t\t\tYou should get the following output:\n\n```", "```rs\n\n\t\t\tGreat – it’s working just as expected!!!\n\t\t\tHowever, we’re not really using any of the new capabilities of our runtime yet so before we leave this chapter, let’s have some fun and see what it can do.\n\t\t\tExperimenting with our new runtime\n\t\t\tIf you remember from [*Chapter 7*](B20892_07.xhtml#_idTextAnchor122), we implemented a `join_all` method to get our futures running concurrently. In libraries such as Tokio, you’ll find a `join_all` function too, and the slightly more versatile `FuturesUnordered` API that allows you to join a set of predefined futures and run them concurrently.\n\t\t\tThese are convenient methods to have, but it does force you to know which futures you want to run concurrently in advance. If the futures you run using `join_all` want to spawn new futures that run concurrently with their “parent” future, there is no way to do that using only these methods.\n\t\t\tHowever, our newly created spawn functionality does exactly this. Let’s put it to the test!\n\t\t\tAn example using concurrency\n\t\t\tNote\n\t\t\tThe exact same version of this program can be found in the `ch08/c-runtime-executor` folder.\n\t\t\tLet’s try a new program that looks like this:\n\n```", "```rs\n\n\t\t\tThis is pretty much the same example we used to show how `join_all` works in [*Chapter 7*](B20892_07.xhtml#_idTextAnchor122), only this time, we spawn them as top-level futures instead.\n\t\t\tTo run this example, follow these steps:\n\n\t\t\t\t1.  Replace everything *below the imports* in `main.rs` with the preceding code.\n\t\t\t\t2.  Run `corofy ./src/main.rs`.\n\t\t\t\t3.  Copy everything from `main_corofied.rs` to `main.rs` and delete `main_corofied.rs`.\n\t\t\t\t4.  Fix the fact that `corofy` doesn’t know we changed our futures to take `waker: &Waker` as an argument. The easiest way is to simply run `cargo check` and let the compiler guide you to the places we need to change.\n\n\t\t\tNow, you can run the example and see that the tasks run concurrently, just as they did using `join_all` in [*Chapter 7*](B20892_07.xhtml#_idTextAnchor122). If you measured the time it takes to run the tasks, you’d find that it all takes around 4 seconds, which makes sense if you consider that you just spawned 5 futures, and ran them concurrently. The longest wait time for a single future was 4 seconds.\n\t\t\tNow, let’s finish off this chapter with another interesting example.\n\t\t\tRunning multiple futures concurrently and in parallel\n\t\t\tThis time, we spawn multiple threads and give each thread its own `Executor` so that we can run the previous example simultaneously in parallel using the same `Reactor` for all `Executor` instances.\n\t\t\tWe’ll also make a small adjustment to the printout so that we don’t get overwhelmed with data.\n\t\t\tOur new program will look like this:\n\n```", "```rs\n\n\t\t\tThe machine I’m currently running has 12 cores, so when I create 11 new threads to run the same asynchronous tasks, I’ll use all the cores on my machine. As you’ll notice, we also give each thread a unique name that we’ll use when logging so that it’s easier to track what happens behind the scenes.\n\t\t\tNote\n\t\t\tWhile I use 12 cores, you should use the number of cores on your machine. If we increase this number too much, our OS will not be able to give us more cores to run our program in parallel on and instead start pausing/resuming the threads we create, which adds no value to us since we handle the concurrency aspect ourselves in an `a^tsync` runtime.\n\t\t\tYou’ll have to do the same steps as we did in the last example:\n\n\t\t\t\t1.  Replace the code that’s currently in `main.rs` with the preceding code.\n\t\t\t\t2.  Run `corofy ./src/main.rs`.\n\t\t\t\t3.  Copy everything from `main_corofied.rs` to `main.rs` and delete `main_corofied.rs`.\n\t\t\t\t4.  Fix the fact that `corofy` doesn’t know we changed our futures to take `waker: &Waker` as an argument. The easiest way is to simply run `cargo check` and let the compiler guide you to the places we need to change.\n\n\t\t\tNow, if you run the program, you’ll see that it still only takes around 4 seconds to run, but this time we made **60 GET requests instead of 5**. This time, we ran our futures both concurrently and in parallel.\n\t\t\tAt this point, you can continue experimenting with shorter delays or more requests and see how many concurrent tasks you can have before the system breaks down.\n\t\t\tPretty quickly, printouts to `stdout` will be a bottleneck, but you can disable those. Create a blocking version using OS threads and see how many threads you can run concurrently before the system breaks down compared to this version.\n\t\t\tOnly imagination sets the limit, but do take the time to have some fun with what you’ve created before we continue with the next chapter.\n\t\t\tThe only thing to be careful about is testing the concurrency limit of your system by sending these kinds of requests to a random server you don’t control yourself since you can potentially overwhelm it and cause problems for others.\n\t\t\tSummary\n\t\t\tSo, what a ride! As I said in the introduction for this chapter, this is one of the biggest ones in this book, but even though you might not realize it, you’ve already got a better grasp of how asynchronous Rust works than most people do. **Great work!**\n\t\t\tIn this chapter, you learned a lot about runtimes and why Rust designed the `Future` trait and the `Waker` the way it did. You also learned about reactors and executors, `Waker` types, `Futures` traits, and different ways of achieving concurrency through the `join_all` function and spawning new top-level futures on the executor.\n\t\t\tBy now, you also have an idea of how we can achieve both concurrency and parallelism by combining our own runtime with OS threads.\n\t\t\tNow, we’ve created our own async universe consisting of `coro/wait`, our own `Future` trait, our own `Waker` definition, and our own runtime. I’ve made sure that we don’t stray away from the core ideas behind asynchronous programming in Rust so that everything is directly applicable to `async/await`, `Future` traits, `Waker` types, and runtimes in day-to-day programming.\n\t\t\tBy now, we’re in the final stretch of this book. The last chapter will finally convert our example to use the real `Future` trait, `Waker`, `async/await`, and so on instead of our own versions of it. In that chapter, we’ll also reserve some space to talk about the state of asynchronous Rust today, including some of the most popular runtimes, but before we get that far, there is one more topic I want to cover: pinning.\n\t\t\tOne of the topics that seems hardest to understand and most different from all other languages is the concept of pinning. When writing asynchronous Rust, you will at some point have to deal with the fact that `Future` traits in Rust must be pinned before they’re polled.\n\t\t\tSo, the next chapter will explain pinning in Rust in a practical way so that you understand why we need it, what it does, and how to do it.\n\t\t\tHowever, you absolutely deserve a break after this chapter, so take some fresh air, sleep, clear your mind, and grab some coffee before we enter the last parts of this book.\n\n```"]
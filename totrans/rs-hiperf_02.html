<html><head></head><body>
        

                            
                    <h1 class="header-title">Extra Performance Enhancements</h1>
                
            
            
                
<p class="mce-root">Once your application avoids common performance bottlenecks, it's time to move to more complex performance improvements. Rust has many options that allow you to improve the performance of your code by using lesser-known APIs. This will give you parity with C/C++ and, in some scenarios, it can even improve the speed of most of the fastest C/C++ scripts.</p>
<p>In this chapter, we will be looking into the following topics:</p>
<ul>
<li>Compile-time checks</li>
<li>Compile-time state machines</li>
<li>Extra performance enhancements, such as using closures for avoiding runtime evaluation</li>
<li>Unstable sorting</li>
<li>Map hashing</li>
<li>Standard library collections</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Compile-time checks</h1>
                
            
            
                
<p>Rust has an amazing type system. It's so powerful that it is Turing-complete by itself. This means that you can write very complex programs just by using Rust's type system. This can help your code a lot, since the type system gets evaluated at compile time, making your runtime much faster.</p>
<p class="mce-root">Starting from the basics, what do we mean by <em>the type system</em>? Well, it means all those traits, structures, generics, and enums you can use to make your code very specialized at runtime. An interesting thing to know is the following: if you create a generic function that gets used with two different types, Rust will compile two specific functions, one for each type.</p>
<p class="mce-root">This might seem like code duplication but, in reality, it is usually faster to have a specific function for the given type than to try to generalize a function over multiple ones. This also allows for the creation of specialized methods that will take into account the data they are using. Let's see this with an example. Suppose we have two structures and we want them to output a message with some of their information:</p>
<pre>struct StringData {<br/>    data: String,<br/>}<br/><br/>struct NumberData {<br/>    data: i32,<br/>}</pre>
<p>We create a trait that we will implement for them that will return something that can be displayed in the console:</p>
<pre>use std::fmt::Display;<br/><br/>trait ShowInfo {<br/>    type Out: Display;<br/>    fn info(&amp;self) -&gt; Self::Out;<br/>}</pre>
<p>And we implement it for our structures. Note that I have decided to return a reference to the data string in the case of the <kbd>StringData</kbd> structure. This simplifies the logic but adds some lifetimes and some extra referencing to the variable. This is because the reference must be valid while <kbd>StringData</kbd> is valid. If not, it might try to print non-existent data, and Rust prevents us from doing that:</p>
<pre>impl&lt;'sd&gt; ShowInfo for &amp;'sd StringData {<br/>    type Out = &amp;'sd str;<br/>    fn info(&amp;self) -&gt; Self::Out {<br/>        self.data.as_str()<br/>    }<br/>}<br/><br/>impl ShowInfo for NumberData {<br/>    type Out = i32;<br/>    fn info(&amp;self) -&gt; Self::Out {<br/>        self.data<br/>    }<br/>}</pre>
<p>As you can see, one of them returns a string and the other returns an integer, so it would be very difficult to create a function that allows both of them to work, especially in a strongly-typed language. But since Rust will create two completely different functions for them, each using their own code, this can be solved thanks to generics:</p>
<pre>fn print&lt;I: ShowInfo&gt;(data: I) {<br/>    println!("{}", data.info());<br/>}</pre>
<p>In this case, the <kbd>println!</kbd> macro will call to the specific methods of the <kbd>i32</kbd> and <kbd>&amp;str</kbd> structures. We then simply create a small <kbd>main()</kbd> function to test everything, and you should see how it can print both structures perfectly:</p>
<pre>fn main() {<br/>    let str_data = StringData {<br/>        data: "This is my data".to_owned(),<br/>    };<br/>    let num_data = NumberData { data: 34 };<br/><br/>    print(&amp;str_data);<br/>    print(num_data);<br/>}</pre>
<p>You might be tempted to think that this is similar to what languages such as Java do with their interfaces, and, functionally, it is. But talking about performance, our topic in this book, they are very different. Here, the generated machine code will effectively be different between both calls. One clear symptom is that the <kbd>print()</kbd> method gets ownership of the value it receives, so the caller must pass this in the registers of the CPU. Both structures are fundamentally different though. One is bigger than the other (containing the string pointer, the length, and capacity), so the way the call is done must be different.</p>
<p class="mce-root">So, great, Rust does not use the same structure for traits as Java does for interfaces. But why should you care? Well, there are a number of reasons, but there is one that will probably show you what this accomplishes. Let's create a state machine.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Sequential state machines</h1>
                
            
            
                
<p class="mce-root">Let's first think about how to implement this in a C/C++ environment. You will probably have a global state and then a <kbd>while</kbd> loop that would change the state after each iteration. There are, of course, many ways of implementing a state machine but, in C, all of them require either metaprogramming or a runtime evaluation of the global state.</p>
<p class="mce-root">In Rust, we have a Turing-complete type system, so why not try and use it to create that state machine? Let's start by defining some traits that will have the power of creating the state machine. We first define a <kbd>StateMachine</kbd> trait, that will have the functionality of moving from one state to another state:</p>
<pre>pub trait StateMachine {<br/>    type Next: MainLogic;<br/>    fn execute(self) -&gt; Self::Next;<br/>}</pre>
<p>As you can see, I already added a new type, <kbd>MainLogic</kbd>. This will be a trait representing a structure that can perform a logic in a state. The <kbd>StateMachine</kbd> trait itself is pretty simple. It only contains a type that will be the next state and an <kbd>execute()</kbd> function that consumes itself so that nobody can execute the same state twice without going to the next state (the next state could be itself again). It simply returns a new state machine. Here we have the <kbd>MainLogic</kbd> trait:</p>
<pre>pub trait MainLogic {<br/>    fn main_logic(self);<br/>}</pre>
<p>It's just a function to execute the logic of the state. The main functionality of this state machine, that will enable it to go from one state to the next, always doing the proper logic, is defined in the default implementation of the <kbd>MainLogic</kbd> trait:</p>
<pre>impl&lt;S&gt; MainLogic for S<br/>where<br/>    S: StateMachine,<br/>{<br/>    fn main_logic(self) {<br/>        self.execute().main_logic();<br/>    }<br/>}</pre>
<p>This will implement the <kbd>MainLogic</kbd> trait for any state implementing the <kbd>StateMachine</kbd> trait. It will simply execute the state and then call the main logic of the next state. If this new state is also a <kbd>StateMachine</kbd>, it will get executed and then the next state will be executed. This pattern is especially useful if you want to sequentially execute different states. The last state will be the one implementing <kbd>MainLogic</kbd> but not <kbd>StateMachine</kbd>:</p>
<pre>struct FirstState;<br/>struct LastState;<br/><br/>impl StateMachine for FirstState {<br/>    type Next = LastState;<br/><br/>    fn execute(self) -&gt; Self::Next {<br/>        unimplemented!()<br/>    }<br/>}<br/><br/>impl MainLogic for LastState {<br/>    fn main_logic(self) {<br/>        unimplemented!()<br/>    }<br/>}</pre>
<p>The compiler will make sure at compile time that you properly go from the first state to the second one, and it will force you to do so. But, more importantly, this will be compiled into very efficient code, as efficient as doing the sequential calls one by one, but with all the safety Rust gives you. In fact, as you can see, both <kbd>FirstState</kbd> and <kbd>Laststate</kbd> have no attributes. That is because they have no size. They will not occupy space in memory at runtime.</p>
<p class="mce-root">This is the simplest state machine though. It will only allow you to advance from one state to the next. It's helpful if that's what you want, since it will make sure your flow gets checked at compile time, but it will not perform complex patterns. If you loop over a previous state, you will endlessly continue looping. This will also be useful when each state has a defined next state and when no other possibility comes from that state.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Complex state machines</h1>
                
            
            
                
<p>A more complex state machine, that allows you to move from one to another in your code while still using the type system to check for proper usage, can be done. Let's start by defining the state machine. We want a machine that represents the way a robot works in a car-building facility. Let's say its job is to install two doors in a car. It will first wait for the next car to come, take the door, put it in place, put the bolts in place, do the same for the second door, and then wait for the next car.</p>
<p>We will first define some functions that will use sensors and that we will simulate:</p>
<pre>fn is_the_car_in_place() -&gt; bool {<br/>    unimplemented!()<br/>}<br/>fn is_the_bolt_in_place() -&gt; bool {<br/>    unimplemented!()<br/>}<br/>fn move_arm_to_new_door() {<br/>    unimplemented!();<br/>}<br/>fn move_arm_to_car() {<br/>    unimplemented!()<br/>}<br/>fn turn_bolt() {<br/>    unimplemented!()<br/>}<br/>fn grip_door() {<br/>    unimplemented!()<br/>}</pre>
<p>Of course, the real software would need to take many things into account. It should check that the environment is safe, the way it moves the door to the car should be optimal, and so on, but this simplification will do for now. We now define some states:</p>
<pre>struct WaitingCar;<br/>struct TakingDoor;<br/>struct PlacingDoor;</pre>
<p>And we then define the machine itself:</p>
<pre>struct DoorMachine&lt;S&gt; {<br/>    state: S,<br/>}</pre>
<p>This machine will hold an internal state that can have some information attached to it (it can be any kind of structure) or it can have a zero-sized structure, and thus have a size of zero bytes. We will then implement our first transition:</p>
<pre>use std::time::Duration;<br/>use std::thread;<br/><br/>impl From&lt;DoorMachine&lt;WaitingCar&gt;&gt; for DoorMachine&lt;TakingDoor&gt; {<br/>    fn from(st: DoorMachine&lt;WaitingCar&gt;) -&gt; DoorMachine&lt;TakingDoor&gt; {<br/>        while !is_the_car_in_place() {<br/>            thread::sleep(Duration::from_secs(1));<br/>        }<br/>        DoorMachine { state: TakingDoor }<br/>    }<br/>}</pre>
<p>This will simply check every 1 second whether the car is in the proper place. Once it is, it will return the next state, the <kbd>TakingDoor</kbd> state. The function signature makes sure that you cannot return the incorrect state, even if you do a really complex logic inside the <kbd>from()</kbd> function. Moreover, at compile time, this <kbd>DoorMachine</kbd> will have zero byte size, as we saw, so it will not consume RAM regardless of how complex our state transitions are. Of course, the code for the <kbd>from()</kbd> functions will be in RAM, but the necessary checks for proper transitioning will all be done at compile time.</p>
<p>We will then implement the next transition:</p>
<pre>use std::time::Duration;<br/>use std::thread;<br/><br/>impl From&lt;DoorMachine&lt;TakingDoor&gt;&gt; for DoorMachine&lt;PlacingDoor&gt; {<br/>    fn from(st: DoorMachine&lt;TakingDoor&gt;) -&gt; DoorMachine&lt;PlacingDoor&gt; {<br/>        move_arm_to_new_door();<br/>        grip_door();<br/><br/>        DoorMachine { state: PlacingDoor }<br/>    }<br/>}</pre>
<p>And finally, a similar thing can be done for the last state:</p>
<pre>use std::time::Duration;<br/>use std::thread;<br/><br/>impl From&lt;DoorMachine&lt;PlacingDoor&gt;&gt; for DoorMachine&lt;WaitingCar&gt; {<br/>    fn from(st: DoorMachine&lt;PlacingDoor&gt;) -&gt; DoorMachine&lt;WaitingCar&gt; {<br/>        move_arm_to_car();<br/>        while !is_the_bolt_in_place() {<br/>            turn_bolt();<br/>        }<br/><br/>        DoorMachine { state: WaitingCar }<br/>    }<br/>}</pre>
<p>The machine can start in any given state, and moving it from one to another will be as simple as writing the following:</p>
<pre>    let beginning_state = DoorMachine { state: WaitingCar };<br/>    let next_state: DoorMachine&lt;TakingDoor&gt; = beginning_state.into();</pre>
<p>You might be thinking, <em>why don't I simply write two functions and execute them sequentially?</em> The answer is not straightforward, but it's easy to explain. This makes you avoid many issues at compile time. For example, if each state has only one possible next state, you can use a generic <kbd>into()</kbd> function without needing to know the current state, and it will simply work.</p>
<p>In a more complex environment, you might find yourself doing the following pattern:</p>
<pre>    let beginning_state = DoorMachine { state: WaitingCar };<br/>    let next_state: DoorMachine&lt;TakingDoor&gt; = beginning_state.into();<br/><br/>    // Lots of code<br/><br/>    let last_state: DoorMachine&lt;PlacingDoor&gt; = next_state.into();</pre>
<p>Of course, if you look at it properly, we are no longer in the first state! What will happen if the machine tries to change the state again, thinking it's still in the first state? Well, here is where Rust comes handy. The <kbd>into()</kbd> function takes ownership of the binding, so this will simply not compile. Rust will complain that the <kbd>beginning_state</kbd> no longer exists since it has been already converted to <kbd>next_state</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Real-life type system check example</h1>
                
            
            
                
<p>There is an example I love when talking about compile-time checks and high-performance computing: Philipp Oppermann wrote a type-safe paging system for a kernel with only two traits. Let's first understand the problem and then try the solution.</p>
<p>When a program uses the memory on a computer, it must separate virtual memory from physical memory. This is because each program running in the OS will think that the whole address space is theirs. This means that in a 64-bit machine, each program will think it has 16 <strong>exbibytes</strong> (<strong>EiB</strong>) of memory, 2<sup>64</sup> bytes.</p>
<p>That is, of course, not the case for any computer in the world, so what the kernel does is to move memory out of RAM to the HDD/SSD, and put in RAM the required memory. For this to work properly, memory has to be managed in chunks, since it doesn't make sense to move in and out individual memory addresses. These are called pages, and they are usually 4 KiB in size for x86_64 processors (the case for most laptops and desktop computers).</p>
<p>For the paging to be easily manageable, a paging hierarchy gets created. Every 512 pages get added to an index called a P1 table, and every 512 P1 tables get added to a P2 table. That goes on recursively until all pages have been assigned, which will be 4 levels. That is what it's called: 4-level paging.</p>
<p>The idea is that a kernel should be able to ask a table for one of its pages, and if it's a P4 table, it should be able to ask for a P3, then for a P2, then for a P1, and finally load the page referenced by that P1. This address gets passed in a 64-bit registry, so all the data is there. The problem is that it could be easy to end up with tons of code duplication for each table type, or we could end up with a solution that works for all pages but that has to check the current page at runtime if it wants to return the next table (if it's a P4-P2 table) or the actual page (if it's a P1 table).</p>
<p>The first case is really error-prone, and difficult to maintain, while the second one not only continues being error-prone, but it even requires checks at runtime, making it slower. Rust can do better.</p>
<p>The solution is to define a trait that all pages have, let's call it <kbd>PageTable</kbd>, and a trait that only higher-order tables have (tables that cannot directly return a page but that they need to return another page table). Let's call it <kbd>HighTable</kbd>. Since all <kbd>HighTable</kbd> types are also <kbd>PageTable</kbd>, one trait will inherit from the other:</p>
<pre>pub trait PageTable {}<br/><br/>pub enum P4 {}<br/>pub enum P3 {}<br/>pub enum P2 {}<br/>pub enum P1 {}<br/><br/>impl PageTable for P4 {}<br/>impl PageTable for P3 {}<br/>impl PageTable for P2 {}<br/>impl PageTable for P1 {}</pre>
<p>This creates four enumerations representing page table levels. The reason for using enumerations instead of structures is that empty enumerations cannot be instantiated, which will avoid some typos. Then we write the <kbd>HighTable</kbd> trait:</p>
<pre>pub trait HighTable: PageTable {<br/>    type NextTable: PageTable;<br/>}<br/><br/>impl HighTable for P4 {<br/>    type NextTable = P3;<br/>}<br/><br/>impl HighTable for P3 {<br/>    type NextTable = P2;<br/>}<br/><br/>impl HighTable for P2 {<br/>    type NextTable = P1;<br/>}</pre>
<p>As you can see, we add an associated type to each enumeration to represent the next level of paging. But, of course, in the case of the last level, it will not have another page table below.</p>
<p class="mce-root">This allows you to define functions associated to <kbd>HighTable</kbd> that will not be accessible to a P1 table and so on. And it lets you create a <kbd>Page</kbd> type that will contain the contents of a <kbd>Page</kbd> (a byte array, more or less) that is generic over what level it is.</p>
<p>Rust will ensure that you cannot try to get the next table of a P1 table at compile time, and at runtime, these enumerations will disappear, as they are zero-sized. The logic will be safe, though, and checked at compile time with no overhead.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Extra performance tips</h1>
                
            
            
                
<p>Compile-time checks are not the only place where you can benefit from a performance enhancement at no cost. While in <a href="ad672e4d-0f5e-4c59-b823-249da183abc8.xhtml" target="_blank">Chapter 1</a>, <em>Common Performance Pitfalls</em>, we saw the common errors people write in Rust, we left the most advanced tips and tricks for this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using closures to avoid runtime evaluation</h1>
                
            
            
                
<p>Sometimes, it might seem natural to write code that does not perform as fast as expected. Many times, this is due to Rust doing some extra computations at runtime. An example of an unnecessary computation that someone could write is the following:</p>
<pre class="mce-root">    let opt = Some(123);<br/>    let non_opt = opt.unwrap_or(some_complex_function());</pre>
<p class="mce-root">I have intentionally made this example simply because a real example usually takes really long code. The idea behind it is valid though. When you have an <kbd>Option</kbd> or a <kbd>Result</kbd>, you have some very useful functions to allow you get the value inside or a default. There is this specific function, the <kbd>unwrap_or()</kbd> function, that allows you specify the default value. Of course, you can pass anything you want to that function, but if you require a complex calculation to calculate the default value (and it's not a constant), the code will perform poorly.</p>
<p class="mce-root">This happens because when calling the <kbd>unwrap_or()</kbd> function, the value you pass must be calculated beforehand. This does not make much sense if most of the time the value will exist and the computation is not required. A better option is to use <kbd>unwrap_or_else()</kbd>. This function accepts a closure that will only be executed if the <kbd>Option</kbd>/<kbd>Result</kbd> is <kbd>None</kbd>/<kbd>Err</kbd> respectively. In this concrete case, since <kbd>some_complex_function()</kbd> does not have any arguments, you can directly use that as the closure:</p>
<pre class="mce-root">    let opt = Some(123);<br/>    let non_opt = opt.unwrap_or_else(some_complex_function);</pre>
<p class="mce-root">But if the function requires arguments, you will need to build the closure yourself:</p>
<pre class="mce-root">    let opt = Some(123);<br/>    let non_opt = opt.unwrap_or_else(|| {<br/>        even_more_complex_function(get_argument())<br/>    });</pre>
<p class="mce-root">This way, you can use a very complicated function, as complicated as you'd like, and you will avoid calling it if there is something inside the <kbd>Option</kbd> type. You will also reduce the cyclomatic complexity of the function.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Unstable sorting</h1>
                
            
            
                
<p>There is also an interesting place where some gains can be made. Usually, when you want to sort a vector, for example, stable sorting is used. This means that if two elements have the same ordering, the original ordering will be preserved. Let's see it with an example. Suppose we have a list of fruits we want to order alphabetically, taking into account only their first letter:</p>
<pre>    let mut fruits = vec![<br/>        "Orange", "Pome", "Peach", "Banana", "Kiwi", "Pear"<br/>    ];<br/>    fruits.sort_by(|a, b| a.chars().next().cmp(&amp;b.chars().next()));<br/><br/>    println!("{:?}", fruits);</pre>
<p>This will print exactly the following:</p>
<div><img src="img/2f922eee-3536-4d96-845c-da8a7b362b97.png" style="width:29.42em;height:1.25em;"/></div>
<p>And in that order. Even though <kbd>Peach</kbd> and <kbd>Pear</kbd> should be before <kbd>Pome</kbd> if we did the sorting by the whole word, since we only take the first character into account, the ordering is correct. The final order depends on the one at the beginning. If I changed the first list and put <kbd>Pome</kbd> after <kbd>Peach</kbd>, the final order would have <kbd>Pome</kbd> after <kbd>Peach</kbd>. This is called <strong>stable ordering</strong>.</p>
<p>On the other hand, unstable ordering doesn't try to preserve previous ordering. So, <kbd>Pome</kbd>, <kbd>Peach</kbd>, and <kbd>Pear</kbd> could end up in any order between them. This is consistent with the condition of being ordered by the first letter, but without preserving the original order.</p>
<p>This unstable sorting is actually faster than the stable sorting, and if you don't care about respecting the initial ordering, you can save valuable time doing the sorting operation, one of the most time-consuming operations. A simple example is ordering a list of results alphabetically. In the case of a mismatch, you usually don't care how they were ordered in the database, so it doesn't matter if one comes after the other or the other way around.</p>
<p>To use unstable sorting, you will need to call <kbd>sort_unstable()</kbd> or <kbd>sort_unstable_by()</kbd>, depending on whether you want to use the default comparison of each <kbd>PartialOrd</kbd> element or use your own classifier, if you want a custom one or if the elements in the vector are not <kbd>PartialOrd</kbd>. Consider the following example using unstable sorting:</p>
<pre>    let mut fruits = vec![<br/>        "Orange", "Pome", "Peach", "Banana", "Kiwi", "Pear"<br/>    ];<br/>    fruits.sort_unstable_by(|a, b| a.chars().next().cmp(&amp;b.chars().next()));<br/><br/>    println!("{:?}", fruits);</pre>
<p>A possible output for this would be the following, impossible with stable sorting:</p>
<div><img src="img/2f922eee-3536-4d96-845c-da8a7b362b97.png" style="width:30.83em;height:1.25em;"/></div>
<p>So, summarizing, if you really need to maintain the ordering of the input, use stable sorting; if not, use unstable sorting, since you will make your program much faster.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Map hashing</h1>
                
            
            
                
<p>Rust also has another development option that allows you to make the hashing of the maps faster. This comes from the idea that when storing information in a <kbd>HashMap</kbd>, for example, the key gets hashed or a faster lookup. This is great, since it allows using arbitrary long and complex keys, but adds overhead when retrieving a value or inserting a new value, since the hash must be calculated.</p>
<p>Rust allows you to change the hashing method for a <kbd>HashMap</kbd>, and even create your own. Of course, usually, the best thing is to use the default hashing algorithm, since it has been thoroughly tested and avoids collisions (different keys having the same hash and overwriting one another). The default hasher for Rust is a very efficient hasher, but if you need performance and you are working with a really small <kbd>HashMap</kbd> or even a somehow predictable <kbd>HashMap</kbd>, it could make sense to use your own function or even a faster function included in Rust.</p>
<p>But beware—it's very risky to use one of these functions in an environment where a user can provide (or manipulate) keys. They could generate a collision and modify the value of a key they should not have access to. They could even create a denial of service attack using it.</p>
<p>Using a different hashing is as simple as using the <kbd>with_hasher()</kbd> function when creating the <kbd>HashMap</kbd>:</p>
<pre>use std::collections::HashMap;<br/>use std::collections::hash_map::RandomState;<br/><br/>// &lt;u8, u8&gt; as an example, just to make the type inference happy.<br/>let map: HashMap&lt;u8, u8&gt; = HashMap::with_hasher(RandomState::new());<br/><br/></pre>
<p>Currently, only <kbd>RandomState</kbd> is available in the standard library; the rest have been deprecated. But you can create your own by implementing the <kbd>Hasher</kbd> trait:</p>
<pre>use std::hash::{BuildHasher, Hasher};<br/><br/>#[derive(Clone)]<br/>struct MyHasher {<br/>    count: u64,<br/>}<br/><br/>impl Hasher for MyHasher {<br/>    fn finish(&amp;self) -&gt; u64 {<br/>        self.count<br/>    }<br/><br/>    fn write(&amp;mut self, bytes: &amp;[u8]) {<br/>        for byte in bytes {<br/>            self.count = self.count.wrapping_add(*byte as u64);<br/>        }<br/>    }<br/>}<br/><br/>impl BuildHasher for MyHasher {<br/>    type Hasher = Self;<br/>    fn build_hasher(&amp;self) -&gt; Self::Hasher {<br/>        self.clone()<br/>    }<br/>}</pre>
<p>This creates the <kbd>MyHasher</kbd> structure, which contains a count that can be initialized as you wish. The <kbd>hash</kbd> function is really simple; it just adds all the bytes of the key and returns a <kbd>u64</kbd> with the sum result. Generating a collision here is pretty easy: you just need to make your bytes sum the same. So <kbd>[45, 23]</kbd> will have the same hash as <kbd>[23, 45]</kbd>. But it works as an example of a hasher. The <kbd>BuildHasher</kbd> trait is also required, and it only needs to return an instance of a <kbd>Hasher</kbd>. I derived the <kbd>Clone</kbd> trait and just cloned it.</p>
<p>This can be easily used, as we saw before:</p>
<pre>    use std::collections::HashMap;<br/><br/>    let mut map = HashMap::with_hasher(MyHasher { count: 12345 });<br/>    map.insert("Hello", "World");</pre>
<p>This will probably be faster than the default hasher, but it will also be much, much less secure. So be careful about what hash function you use.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Perfect hash functions</h1>
                
            
            
                
<p>If the map is known at compile time, and it does not change during the runtime, there is a very, very fast system that can improve by orders of magnitude the use of maps. It's called <strong>perfect hash functions</strong>, and that's the key to them: they perform the minimum required computation for a hash to know whether it's stored in the hash map. This is because it maps one, and only one, integer for each element. And it has no collisions. Of course, this requires a constant, known hash map at compilation time.</p>
<p>To use them, you will need the <kbd>phf</kbd> crate. With this crate, you will be able to define a hash map at compile time in the <kbd>build.rs</kbd> file at the same level as the <kbd>Cargo.toml</kbd> file and use it with no more overhead than a comparison in your code. Let's see how to configure it.</p>
<p>First, you will need to add the <kbd>phf_codegen</kbd> crate as a development dependency. For that, you will need to add a <kbd>build-dependencies</kbd> section to your <kbd>Cargo.toml</kbd>, with the same syntax as the <kbd>dependencies</kbd> section. Then, you will need to create a <kbd>build.rs</kbd> file and, inside, you will need something like the following:</p>
<pre>extern crate phf_codegen;<br/><br/>use std::path::Path;<br/>use std::env;<br/>use std::fs::File;<br/>use std::io::{BufWriter, Write};<br/><br/>fn main() {<br/>    let out_dir = env::var("OUT_DIR").unwrap();<br/>    let path = Path::new(&amp;out_dir).join("phf.rs");<br/>    let mut file = BufWriter::new(File::create(&amp;path).unwrap());<br/><br/>    let map = [("key1", "\"value1\""), ("key2", "\"value2\"")];<br/><br/>    write!(<br/>        &amp;mut file,<br/>        "static MAP: phf::Map&lt;&amp;'static str, &amp;'static str&gt; =\n"<br/>    ).unwrap();<br/><br/>    let mut phf_map = phf_codegen::Map::new();<br/>    for &amp;(key, value) in &amp;map {<br/>        phf_map.entry(key, value);<br/>    }<br/><br/>    phf_map.build(&amp;mut file).unwrap();<br/>    write!(&amp;mut file, ";\n").unwrap();<br/>}</pre>
<p>Let's check what is happening here. The <kbd>build.rs</kbd> script is run before the compilation starts (if it's present). We have a map that is an array of key/value tuples. It then creates a code generation map and adds entries one by one to the map. This has to be done in a loop, since the compiler stack could overflow due to deep recursion.</p>
<p>It will write into a file, called <kbd>phf.rs</kbd>, starting with a line adding a static variable, and then writing the whole map into the file, ending it with a new line. This means that once the compilation starts, a new file named <kbd>phf.rs</kbd> will exist that we can use from our code. How? You will need to directly include the file in your code:</p>
<pre class="rust rust-example-rendered">extern crate phf;<br/><br/>include!(concat!(env!("OUT_DIR"), "/phf.rs"));<br/><br/>fn main() {<br/>    println!("{}", MAP.get("key1").unwrap());<br/>}</pre>
<p>This will print the value associated to <kbd>key1</kbd>, in this case, <kbd>value1</kbd>.</p>
<p>Note that when creating the map in the <kbd>build.rs</kbd> file, the values are written directly, so if you want to put a string, you need to add the quotation marks and escape them. This enables you to add enumeration variants, for example, or to write code directly for values.</p>
<p class="mce-root">Once you have learned how to use compile-time hash maps, you should understand the different kinds of collections the standard library allows you to use, since it will be crucial to the speed and memory footprint of your application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Standard library collections</h1>
                
            
            
                
<p>Rust's standard library has eight different collection types in the <kbd>std::collections</kbd> module. They are divided into sequences, maps, sets, and a binary heap that does not fit in any group. The most well known ones are arguably <kbd>HashMap</kbd> and <kbd>Vec</kbd>, but each of them has a use case, and you should know about them to use the proper one in each moment.</p>
<p>The official standard library documentation is really good, so you should check it thoroughly. In any case, though, I will introduce the types so that you can familiarize yourself with them. Let's start with sequences.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Sequences</h1>
                
            
            
                
<p>The most-used dynamic sequence in Rust and in most languages is the vector, represented in Rust as <kbd>Vec</kbd>. You can add elements to the back of a vector with the <kbd>push()</kbd> method, and get the last element back with the <kbd>pop()</kbd> method. You can also iterate through the vector and, by default, it will go from front to back, but you can also reverse the iterator to go from back to front. In general, a vector in Rust can be compared to a stack, since it's primarily a LIFO structure.</p>
<p>Vectors are really useful when you want to add new elements to a list, and when you are fine with working with indexes in slices to get the elements. Remember that vectors can be referenced as slices, and can be indexed with ranges. An interesting thing is that you can convert a vector into a boxed slice, that is similar to an array, but allocated in heap instead of a stack. You only have to call the <kbd>into_boxed_slice()</kbd> method. This is useful when you have finished growing the vector and want it to occupy less RAM. A vector has a capacity, a length, and a pointer to the elements, while a boxed slice will only have the pointer and the length, avoiding some extra memory usage.</p>
<p>Another useful sequence is the <kbd>VecDeque</kbd> sequence. This structure is a FIFO queue, where you can append elements to the back of the queue using the <kbd>push_back()</kbd> method, and pop elements from the front by using <kbd>pop_front()</kbd>. This can be used as a buffer since it can be consumed from the front while you continue adding elements to the back. Of course, to use it as a buffer crossing thread boundaries, you will need to lock it with a <kbd>Mutex</kbd>, for example. Iterations in these queues go from front to back, the same way as in vectors. It's implemented with a growable ring buffer.</p>
<p>Finally, the <kbd>LinkedList</kbd> is another sequential list where its peculiarity is that instead of having a chunk of elements in the memory, each element gets linked to the one before and the one after so that there is no need for an index. It's easy to iterate, and easy to remove any element in the list without leaving gaps or having to reorder the memory, but in general, it's not very memory-friendly and requires more CPU consumption.</p>
<p>You will, most of the time, prefer a <kbd>Vec</kbd> or a <kbd>VecDeque</kbd>. <kbd>LinkedLists</kbd> are usually only a good option when many inserts and removes have to be done in the middle of the sequence, since in that case, <kbd>Vecs</kbd> and <kbd>VecDeques</kbd> will have to reorder themselves, which takes a lot of time. But if you will usually only change the structure of the list from the back, a <kbd>Vec</kbd> is the best option; if you will also change it from the front, a <kbd>VecDeque</kbd>. Remember that in both you can read any element easily by indexing, it's just that it's more time-consuming to remove or add them in the middle of the list.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Maps</h1>
                
            
            
                
<p>There are two kinds of maps: <kbd>HashMap</kbd> and <kbd>BTreeMap</kbd>. The main difference between them is how they order themselves in memory. They have similar methods to insert and retrieve elements, but their performance changes a lot depending on the operation.</p>
<p><kbd>HashMap</kbd> creates an index, where for every key a hash points to the element. That way, you do not need to check the whole key for every new <kbd>insert</kbd>/<kbd>delete</kbd>/<kbd>get</kbd> operation. You simply hash it and search it in the index. If it exists, it can be retrieved, modified, or deleted; if it doesn't, it can be inserted. It is pretty fast to insert new elements. It's as simple as adding it to the index. Retrieving it is also pretty much the same: just get the value if the hashed index exists; all operations are done in <kbd>O(1)</kbd>. You cannot append one <kbd>HashMap</kbd> to another, though, because their hashing algorithms will be different, or at least be in different states.</p>
<p><kbd>BTreeMap</kbd>, on the other hand, does not create indexes. It maintains an ordered list of elements and, that way, when you want to insert or get a new element, it does a binary search. Check whether the key is bigger than the key in the middle of the list. If it is, divide the second half of the list into two and try it again with the element of the middle of the second half; if it's not, do the same with the first half.</p>
<p>That way, you don't have to compare each element with all elements in the map, and you can quickly retrieve them. Adding new elements is a similarly costly algorithm, and all operations can be done in <kbd>O(log n)</kbd>. You can also append another <kbd>BTreeSet</kbd> to this one, and the elements will be reordered for the search to be as fast as possible.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Sets</h1>
                
            
            
                
<p>Both <kbd>HashMap</kbd> and <kbd>BTreeMap</kbd> have their set counterparts, called <kbd>HashSet</kbd> and <kbd>BTreeSet</kbd>. Both are implemented with the same idea in mind: sometimes you don't need a key/value store, but just an element store, where you can retrieve the list of the elements by iterating through them, or where you can check whether an element is inside just by comparing it to the ones inside.</p>
<p>Their approach is the same as with the case of their map counterparts, and you can think of them as their counterpart maps but with a null value, where only keys are the ones doing the job.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, you learned how to use compile-time checks to your advantage. You learned how Rust's type system can help you create complex and safe behaviour without runtime overhead. You learned how to create state machines and how to make your code less error-prone.</p>
<p>You also learned about some extra performance enhancements that complement those of <a href="ad672e4d-0f5e-4c59-b823-249da183abc8.xhtml" target="_blank">Chapter 1</a>, <em>Common Performance Pitfalls</em>. You learned about unstable sorting and map hashing, including perfect hash functions created at compile time, and how to create compile-time hash maps that will have no runtime overhead.</p>
<p>Finally, you learned about the collections in the standard library, how they are classified, and which type of collection you should use depending on the situation. You learned about sequences, maps, and sets, and how they can be adapted for your code.</p>
<p>In <a href="71d38dd3-1f0b-408e-b454-3d342b413f7c.xhtml" target="_blank">Chapter 3</a>, <em>Memory Management in Rust</em>, we will talk about memory management in Rust. Even if, in Rust, you do not need to manually allocate and de-allocate memory, there are still plenty of things you can do to improve your memory footprint.</p>


            

            
        
    </body></html>
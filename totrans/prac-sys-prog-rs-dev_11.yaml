- en: '*Chapter 9*: Managing Concurrency'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrent systems are all around us. When you download a file, listen to streaming
    music, initiate a text chat with a friend, and print something in the background
    on your computer, *all at the same time*, you are experiencing the magic of concurrency
    in action. The operating system manages all these for you in the background, scheduling
    tasks across available processors (CPUs).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'But do you know how to write a program that can do multiple things at the same
    time? More importantly, do you know how to do it in a way that is both memory-
    and thread-safe, while ensuring optimal use of system resources? Concurrent programming
    is one way to achieve this. But concurrent programming is considered to be a difficult
    topic in most programming languages due to challenges in *synchronizing tasks*
    and *sharing data safely across multiple threads of execution*. In this chapter,
    you''ll learn about the basics of concurrency in Rust and how Rust makes it easier
    to prevent common pitfalls and enables us to write concurrent programs in a safe
    manner. This chapter is structured as shown here:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing concurrency basics
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spawning and configuring threads
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error handling in threads
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Message passing between threads
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving concurrency with shared state
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pausing thread execution with timers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you'll have learned how to write concurrent programs
    in Rust by spawning new threads, handling thread errors, transferring and sharing
    data safely across threads to synchronize tasks, understanding the basics of thread-safe
    data types, and pausing the execution of current threads for synchronization.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Verify that `rustup`, `rustc`, and `cargo` have been installed correctly with
    the following commands:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The Git repo for the code in this chapter can be found at: [https://github.com/PacktPublishing/Practical-System-Programming-for-Rust-Developers/tree/master/Chapter09](https://github.com/PacktPublishing/Practical-System-Programming-for-Rust-Developers/tree/master/Chapter09).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started with some basic concepts of concurrency.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing concurrency basics
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll cover the basics of **multi-threading** and clarify the
    terminology around **concurrency** and **parallelism**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: To appreciate the value of concurrent programming, we have to understand the
    need of today's programs to make decisions quickly or process a large amount of
    data in a short period of time. Several use cases become impossible to achieve
    if we strictly rely on sequential execution. Let's consider a few examples of
    systems that must perform multiple things simultaneously.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: An autonomous car needs to perform many tasks at the same time, such as processing
    inputs from a wide array of sensors (to construct an internal map of its surroundings),
    plotting the path of the vehicle, and sending instructions to the vehicle's actuators
    (to control the brakes, acceleration, and steering). It needs to process continually
    arriving input events, and respond in tenths of a second.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一辆自动驾驶汽车需要同时执行许多任务，例如处理来自各种传感器的输入（以构建其周围环境的内部地图），规划车辆的路径，并向车辆的执行器发送指令（以控制刹车、加速和转向）。它需要持续处理不断到达的输入事件，并在十分之一秒内做出响应。
- en: There are also other, more mundane examples. A web browser handles user inputs
    while simultaneously rendering a web page incrementally, as new data is received.
    A website handles requests from multiple simultaneous users. A web crawler has
    to access many thousands of sites simultaneously to gather information about the
    websites and their contents. It is impractical to do all these things sequentially.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 同时还有其他一些更为常见的例子。一个网页浏览器在接收新数据的同时，会处理用户输入并逐步渲染网页。一个网站会处理来自多个同时在线用户的请求。一个网络爬虫需要同时访问成千上万的网站以收集有关网站及其内容的信息。按顺序执行所有这些事情是不切实际的。
- en: We've so far seen a few use cases that require multiple tasks to be performed
    simultaneously. But there is also a technical reason that is driving concurrency
    in programming, which is that CPU clock speeds on a single core are hitting upper
    practical limits. So, it is becoming necessary to add more CPU cores, and more
    processors on a single machine. This is in turn driving the need for software
    that can efficiently utilize the additional CPU cores. To achieve this, portions
    of a program should be executable concurrently on different CPU cores, rather
    than being constrained by the sequential execution of instructions on a single
    CPU core.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了一些需要同时执行多个任务的用例。但还有一个技术原因在推动编程中的并发，那就是单核CPU的时钟速度已经接近实际上限。因此，有必要增加更多的CPU核心和单台机器上的更多处理器。这反过来又推动了需要能够高效利用额外CPU核心的软件的需求。为了实现这一点，程序的一部分应该在不同的CPU核心上并发执行，而不是被限制在单核CPU上的指令顺序执行。
- en: These factors have resulted in the increased use of multi-threading concepts
    in programming. Here, there are two related terms that need to be understood –
    *concurrency* and *parallelism*. Let's take a closer look at this.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些因素导致了在编程中多线程概念的广泛应用。这里有两个相关的术语需要理解——*并发* 和 *并行*。让我们更深入地了解一下。
- en: Concurrency versus parallelism
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并发与并行
- en: In this section, we'll review the fundamentals of multi-threading and understand
    the differences between *concurrent* and *parallel* execution models of a program.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾多线程的基本原理，并了解程序中 *并发* 和 *并行* 执行模型之间的区别。
- en: '![Figure 9.1 – Concurrency basics](img/Figure_9.1_B16405.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – 并发基础](img/Figure_9.1_B16405.jpg)'
- en: Figure 9.1 – Concurrency basics
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 并发基础
- en: '*Figure 9.1* shows three different computation scenarios within a Unix/Linux
    process:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.1* 展示了Unix/Linux进程内的三个不同的计算场景：'
- en: '**Sequential execution**: Let''s assume that a process has two tasks **A**
    and **B**. **Task A** has three subtasks **A1**, **A2**, and **A3**, which are
    executed sequentially. Likewise, **Task B** has two tasks, **B1** and **B2**,
    that are executed one after the other. Overall, the process executes all tasks
    of process *A* before taking on process *B* tasks. There is a challenge in this
    model. Assume the case where task **A2** involves waiting for an external network
    or user input, or for a system resource to become available. Here, all tasks lined
    up after task **A2** will be blocked until **A2** completes. This is not an efficient
    use of the CPU and causes a delay in the completion of all the scheduled tasks
    that belong to the process.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序执行**：假设一个进程有两个任务 **A** 和 **B**。**任务 A** 包含三个子任务 **A1**、**A2** 和 **A3**，它们是顺序执行的。同样，**任务
    B** 包含两个任务，**B1** 和 **B2**，它们是依次执行的。总体来说，进程在执行进程 *A* 的所有任务之后，才会开始执行进程 *B* 的任务。这种模型存在一个挑战。假设任务
    **A2** 需要等待外部网络或用户输入，或者等待系统资源可用。在这种情况下，所有排在任务 **A2** 后面的任务都将被阻塞，直到 **A2** 完成。这不是CPU的高效使用，并且会导致属于该进程的所有预定任务完成延迟。'
- en: '**Concurrent execution**: Sequential programs are limited as they do not have
    the ability to deal with multiple simultaneous inputs. This is the reason many
    modern applications are *concurrent* where there are multiple threads of execution
    running concurrently.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the concurrent model, the process interleaves the tasks, that is, alternates
    between the execution of **Task A** and **Task B**, until both of them are complete.
    Here, even if **A2** is blocked, it allows progress with the other sub-tasks.
    Each sub-task, **A1**, **A2**, **A3**, **B1**, and **B2**, can be scheduled on
    separate execution threads. These threads could run either on a single processor
    or scheduled across multiple processor cores. One thing to bear in mind is that
    concurrency is about *order-independent* computations as opposed to sequential
    execution, which relies on steps executed in a specific order to arrive at the
    correct program outcome. Writing programs to accommodate *order-independent* computations
    is more challenging than writing sequential programs.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Parallel execution**: This is a variant of the *concurrent execution* model.
    In this model, the process executes **Task A** and **Task B** truly in parallel,
    on separate CPU processors or cores. This assumes, of course, that the software
    is written in a way that such parallel execution is possible, and there are no
    dependencies between **Task A** and **Task B** that could stall the execution
    or corrupt the data.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel computing is a broad term. *Parallelism* can be achieved either within
    a single machine by having **multi-cores** or **multi-processors** or there can
    be clusters of different computers that can cooperatively perform a set of tasks.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When to use concurrent versus parallel execution?
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A program or a function is *compute-intensive* when it involves a lot of computations
    such as in graphics, meteorological, or genome processing. Such programs spend
    the bulk of their time using CPU cycles and will benefit from having better and
    faster CPUs.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A program is *I/O-intensive* when a bulk of the processing involves communicating
    with input/output devices such as network sockets, filesystems, and other devices.
    Such programs benefit from having faster I/O subsystems, such as for disk or network
    access.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Broadly, *parallel execution* (true parallelism) is more relevant for increasing
    the throughput of programs in *compute-intensive* use cases, while *concurrent
    processing* (or pseudo-parallelism) can be suitable for increasing throughput
    and reducing latency in *I/O-intensive* use cases.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this section, we've seen two ways to write concurrent programs – *concurrency*
    and *parallelism*, and how these differ from sequential models of execution. Both
    these models use *multi-threading* as the foundational concept. Let's talk more
    about this in the next section.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Concepts of multi-threading
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we'll deep-dive into how multi-threading is implemented in
    Unix.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Unix supports threads as a mechanism for a process to perform multiple tasks
    concurrently. A Unix process starts up with a single thread, which is the main
    thread of execution. But additional threads can be spawned, that can *execute
    concurrently in a single-processor system*, or *execute in parallel in a multi-processor
    system*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Each thread has access to its own *stack* for storing its own *local variables*
    and *function parameters*. Threads also maintain their own register state including
    the *stack pointer* and *program counter*. All the threads in a process share
    the same memory address space, which means that they share access to the *data*
    segments (*initialized data*, *uninitialized data*, and the *heap*). Threads also
    share the same *program code* (process instructions).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: In a multi-threaded process, multiple threads concurrently execute the same
    program. They may be executing different parts of a program (such as different
    functions) or they may be invoking the same function in different threads (working
    with a different set of data for processing). But note that for a function to
    be invoked by multiple threads at the same time, it needs to be *thread-safe*.
    Some ways to make a function thread-safe are to avoid the usage of *global* or
    *static* variables in the function, using a *mutex* to restrict usage of a function
    to just one thread at a time, or using *mutex* to synchronize usage of a piece
    of shared data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: But it is a design choice to model a concurrent program either as a group of
    processes or as a group of threads within the same process. Let's compare the
    two approaches, for a Unix-like system.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: It is much easier to share data across threads as they are in the same process
    space. Threads also share common resources of a process such as *file descriptors*
    and *user/group IDs*. Thread creation is faster than process creation. Context
    switching between threads is also faster for the CPU due to their sharing the
    same memory space. But threads bring their own share of complexities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: As discussed earlier, shared functions must be thread-safe and access to shared
    global data should be carefully synchronized. Also, a critical defect in one of
    the threads can affect other threads or even bring the entire process down. Additionally,
    there is no guarantee about the order in which different parts of code in different
    threads will run, which can lead to data races, deadlocks, or hard-to-reproduce
    bugs. Bugs related to concurrency are difficult to debug since factors such as
    CPU speed, the number of threads, and the set of running applications at a point
    in time, can alter the outcome of a concurrent program. In spite of these drawbacks,
    if one decides to proceed with the thread-based concurrency model, aspects such
    as code structure, the use of global variables, and thread synchronization should
    be carefully designed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.2* shows the memory layout of threads within a process:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Memory layout of threads in a process](img/Figure_9.2_B16405.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Memory layout of threads in a process
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The figure shows how a set of tasks in process P1 are represented in memory
    when they are executed in a multi-threaded model. We've seen in detail the memory
    layout of a process, in [*Chapter 5*](B16405_05_Final_NM_ePUB.xhtml#_idTextAnchor083),
    *Memory Management in Rust*. *Figure 9.2* extends the process memory layout with
    details of how memory is allocated for individual threads within a process.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: As discussed earlier, all threads are allocated memory within the process memory
    space. By default, the main thread is created with its own stack. Additional threads
    are also assigned their own stack as and when they are created. The shared model
    of concurrency, which we discussed earlier in the chapter, is possible because
    global and static variables of a process are accessible by all threads, and each
    thread also can pass around pointers to memory created on the heap to other threads.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: The program code, however, is common for the threads. Each thread can execute
    a different section of the code from the program text segment, and store the local
    variables and function parameters within their respective thread stack. When it
    is the turn of a thread to execute, its program counter (containing the address
    of the instruction to execute) is loaded for the CPU to execute the set of instructions
    for a given thread.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: In the example shown in the diagram, if task *A2* is blocked waiting for I/O,
    then the CPU will switch execution to another task such as *B1* or *A1*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: With this, we conclude the section on concurrency and multi-threading basics.
    We are now ready to get started with writing concurrent programs using the Rust
    Standard Library.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Spawning and configuring threads
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we reviewed the fundamentals of multi-threading that
    apply broadly to all user processes in the Unix environment. There is, however,
    another aspect of threading that is dependent on the programming language for
    implementation – this is the *threading model*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Rust implements a *1:1 model* of threading where each operating system thread
    maps to one user-level thread created by the Rust Standard Library. The alternative
    model is *M:N* (also known as **green threads**) where there are *M green threads*
    (user-level threads managed by a runtime) that map to *N kernel-level threads*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll cover the fundamentals of creating *1:1* operating system
    threads using the Rust Standard Library. The Rust Standard Library module for
    thread-related functions is `std::thread`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to create a new thread using the Rust Standard Library.
    The first method uses the `thread::spawn` function, and the second method uses
    the builder pattern using the `thread::Builder` struct. Let''s look at an example
    of the `thread::spawn` function first:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `std::thread` module is used in this program. `thread::spawn()` is the function
    used to spawn a new thread. In the program shown, we're spawning four new child
    threads in the main function (which runs in the main thread in the process). Run
    this program with `cargo run`. Run it a few more times. What did you expect to
    see, and what did you actually see?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个程序中使用了`std::thread`模块。`thread::spawn()`是用于创建新线程的函数。在显示的程序中，我们在主函数中（在进程的主线程中运行）创建了四个新的子线程。使用`cargo
    run`运行此程序。运行几次。你期望看到什么，实际上看到了什么？
- en: You would have expected to see four lines printed to the terminal listing the
    *thread IDs*. But you would have noticed that the results vary each time. Sometimes
    you see one line printed, sometimes you see more, and sometimes none. Why is this?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你本应看到四行打印到终端，列出*线程ID*。但你可能会注意到每次的结果都不同。有时你会看到一行打印，有时你会看到更多，有时则没有。这是为什么？
- en: The reason for this inconsistency is that there is no guarantee of the order
    in which the threads are executed. Further, if the `main()` function completes
    before the child threads are executed, you won't see the expected output in your
    terminal.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不一致的原因在于无法保证线程执行的顺序。此外，如果`main()`函数在子线程执行之前完成，你将不会在终端看到预期的输出。
- en: 'To fix this, what we need to do is to join the *child threads* that are created
    to the *main thread*. Then the `main()` thread waits until all the child threads
    have been executed. To see this in action, let''s alter the program as shown:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要做的是将创建的*子线程*连接到*主线程*。然后`main()`线程等待直到所有子线程都执行完毕。为了看到这个效果，让我们按照以下方式修改程序：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The changes from the previous program are highlighted. `thread::spawn()` returns
    a thread handle that we're storing in a `Vec` collection data type. Before the
    end of the `main()` function, we join each child thread to the main thread. This
    ensures that the `main()` function waits until the completion of all the child
    threads before it exits.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一个程序相比，这些更改被突出显示。`thread::spawn()`返回一个线程句柄，我们将其存储在`Vec`集合数据类型中。在`main()`函数结束之前，我们将每个子线程连接到主线程。这确保了`main()`函数在退出之前等待所有子线程完成。
- en: Let's run the program again. You'll notice four lines printed, one for each
    thread. Run the program a few more times. You'll see four lines printed every
    time. This is progress. It shows that joining the child threads to the main threads
    is helping. However, the order of thread execution (as seen by the order of print
    outputs on the terminal) varies with each run. This is because, when we span multiple
    child threads, there is no guarantee of the order in which the threads are executed.
    This is a feature of multi-threading (as discussed earlier), not a bug. But this
    is also one of the challenges of working with threads, as this brings difficulties
    in synchronizing activities across threads. We'll learn how to address this a
    little later in the chapter.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次运行程序。你会注意到打印了四行，每行对应一个线程。再次运行程序几次。每次都会打印四行。这是进步。这表明将子线程连接到主线程是有帮助的。然而，线程执行的顺序（如终端上打印输出的顺序所示）每次运行都会变化。这是因为，当我们创建多个子线程时，无法保证线程执行的顺序。这是多线程（如前所述）的一个特性，而不是一个错误。但这也是与线程一起工作的挑战之一，因为它给跨线程同步活动带来了困难。我们将在本章稍后学习如何解决这个问题。
- en: We've so far seen how to use the `thread::spawn()` function to create a new
    thread. Let's now see the second way to create a new thread.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何使用`thread::spawn()`函数创建新线程。现在让我们看看创建新线程的第二种方法。
- en: 'The `thread::spawn()` function uses default parameters for thread name and
    stack size. If you''d like to set them explicitly, you can use `thread:Builder`.
    This is a *thread factory* that uses the `Builder` pattern to configure the properties
    of a new thread. The previous example has been rewritten here using the `Builder`
    pattern:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`thread::spawn()`函数使用默认参数来设置线程名称和堆栈大小。如果你想明确设置它们，可以使用`thread:Builder`。这是一个*线程工厂*，它使用`Builder`模式来配置新线程的属性。以下示例已经使用`Builder`模式重写：'
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The changes are highlighted in the code. We are creating a new `builder` object
    by using the `new()` function, and then configuring the name of the thread using
    the `name()` method. We're then using the `spawn()` method on an instance of the
    `Builder` pattern. Note that the `spawn()` method returns a `JoinHandle` type
    wrapped in `io::Result<JoinHandle<T>>`, so we have to unwrap the return value
    of the method to retrieve the child process handle.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Run the code and you'll see the four thread names printed to your terminal.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: We've so far seen how to spawn new threads. Let's now take a look at error handling
    while working with threads.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Error handling in threads
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Rust Standard Library contains the `std::thread::Result` type, which is
    a specialized `Result` type for threads. An example of how to use this is shown
    in the following code:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We have a function, `copy_file()`, that copies a source file to a destination
    file. This function returns a `thread::Result<()>` type, which we are unwrapping
    using a `match` statement in the `main()` function. If the `copy_file()` function
    returns a `Result::Err` variant, we handle it by printing an error message.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the program with `cargo run` with an invalid source filename. You will
    see the error message: `Ok()` branch of the `match` clause, and the success message
    will be printed.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'This example shows us how to handle errors propagated by a thread in the calling
    function. What if we want a way to recognize that the current thread is panicking,
    even before it is propagated to the calling function. The Rust Standard Library
    has a function, `thread::panicking()`, available in the `std::thread` module for
    this. Let''s learn how to use it by modifying the previous example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We've created a struct, `Filenames`, which contains the source and destination
    filenames to copy. We're initializing the source filename with an invalid value.
    We're also implementing the `Drop` trait for the `Filenames` struct, which gets
    called when an instance of the struct goes out of scope. In this `Drop` trait
    implementation, we are using the `thread::panicking()` function to check if the
    current thread is panicking, and are handling it by printing out an error message.
    The error is then propagated to the main function, which also handles the thread
    error and prints out another error message.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the program with `cargo run` and an invalid source filename, and you will
    see the following messages printed to your terminal:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Also, note the use of the `move` keyword in the `closure` supplied to the `spawn()`
    function. This is needed for the thread to transfer ownership of the `file_struct`
    data structure from the `main` thread to the newly spawned thread.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: We've seen how to handle thread panic in the calling function and also how to
    detect if the current thread is panicking. Handling errors in child threads is
    very important to ensure that the error is isolated and does not bring the whole
    process down. Hence special attention is needed to design error handling for multi-threaded
    programs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll move on to the topic of how to synchronize computations across threads,
    which is an important aspect of writing concurrent programs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Message passing between threads
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency is a powerful feature that enables the writing of new kinds of applications.
    However, the execution and debugging of concurrent programs are difficult because
    their execution is non-deterministic. We saw this through examples in the previous
    section where the order of print statements varied for each run of the program.
    The order in which the threads will be executed is not known ahead of time. A
    concurrent program developer must make sure that the program will execute correctly
    overall, regardless of the order in which the individual threads are executed.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: One way to ensure program correctness in the face of the unpredictable ordering
    of thread execution is to introduce mechanisms for synchronizing activities across
    threads. One such model for concurrent programming is *message-passing concurrency*.
    It is a way to structure the components of a concurrent program. In our case,
    concurrent components are *threads* (but they can also be processes). The Rust
    Standard Library has implemented a *message-passing concurrency* solution called
    **channels**. *A channel* is basically like a pipe, with two parts – a *producer*
    and a *consumer*. The *producer* puts a message into a *channel*, and a *consumer*
    reads from the *channel*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'Many programming languages implement the concept of channels for inter-thread
    communications. But Rust''s implementation of *channels* has a special property
    – *multiple producer single consumer* (`mpsc`). This means, there can be multiple
    sending ends but only one consuming end. Translate this to the world of threads:
    we can have multiple threads that send values into a channel, but there can be
    only one thread that can receive and consume these values. Let''s see how this
    works with an example that we''ll build out step by step. The complete code listing
    is also provided in the Git repo for the chapter under `src/message-passing.rs`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first declare the module imports – the `mpsc` and `thread` modules from
    the standard library:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Within the `main()` function, create a new `mpsc` channel:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Clone the channel so we can have two transmitting threads:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that we now have two transmission handles – `transmitter1` and `transmitter2`,
    and one receiving handle – `receiver`.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Spawn a new thread moving the transmission handle `transmitter1` into the thread
    closure. Inside this thread, send a bunch of values into the channel using the
    transmission handle:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Spawn a second thread moving the transmission handle `transmitter2` into the
    thread closure. Inside this thread, send another bunch of values into the channel
    using the transmission handle:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the main thread of the program, use the receiving handle of the channel
    to consume the values being written into the channel by the two child threads:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The complete code listing is shown:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Run the program with `cargo run`. (*Note:* If you are running code from the
    Packt Git repo, use `cargo run --bin message-passing`). You'll see the values
    printed out in the main program thread, which are sent from the two child threads.
    Each time you run the program, you may get a different order in which the values
    are received, as the order of thread execution is *non-deterministic*.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `mpsc` channel offers a lightweight inter-thread synchronization mechanism
    that can be used for message-based communications across threads. This type of
    concurrent programming model is useful when you want to spawn out multiple threads
    for different types of computations and want to have the main thread aggregate
    the results.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: One aspect to note in `mpsc` is that once a value is sent down a channel, the
    sending thread no longer has ownership of it. If you want to retain ownership
    or continue to use a value, but still need a way to share the value with other
    threads, there is another concurrency model that Rust supports called **shared-state
    concurrency**. We'll look at that next.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Achieving concurrency with shared state
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll discuss the second model of concurrent programming supported
    in the Rust Standard Library – the *shared-state* or *shared-memory* model of
    concurrency. Recall that all threads in a process share the same process memory
    space, so why not use that as a way to communicate between threads, rather than
    message-passing? We'll look at how to achieve this using Rust.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: A combination of `Mutex` and `Arc` constitutes the primary way to implement
    *shared-state concurrency*. `Mutex` (mutual exclusion lock) is a mechanism that
    allows only one thread to access a piece of data at one time. First, a data value
    is wrapped in a `Mutex` type, which acts as a lock. You can visualize `Mutex`
    like a box with an external lock, protecting something valuable inside. To access
    what's in the box, first of all, we have to ask someone to open the lock and hand
    over the box. Once we're done, we hand over the box back and someone else asks
    to take charge of it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, to access or mutate a value protected by a `Mutex`, we must acquire
    the lock first. Asking for a lock on a `Mutex` object returns a `MutexGuard` type,
    which lets us access the inner value. During this time, no other thread can access
    this value protected by the `MutexGuard`. Once we're done using it, we have to
    release the `MutexGuard` (which Rust does for us automatically as the `MutexGuard`
    goes out of scope, without us having to call a separate `unlock()` method).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: But there is another issue to resolve. Protecting a value with a lock is just
    one part of the solution. We also have to give ownership of a value to multiple
    threads. To support multiple ownership of a value, Rust uses *reference-counted*
    *smart pointers* – `Rc` and `Arc`. `Rc` allows multiple owners for a value through
    its `clone()` method. But `Rc` is not safe to use across threads, and `Arc` (which
    stands for Atomically Reference Counted) is the thread-safe equivalent of `Rc`.
    So, we need to wrap the `Mutex` with an `Arc` reference-counted smart-pointer,
    and transfer ownership of the value across threads. Once the ownership of the
    Arc-protected Mutex is transferred to another thread, the receiving thread can
    call `lock()` on the Mutex to get exclusive access to the inner value. The Rust
    ownership model helps in enforcing the rules around this model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: The way the `Arc<T>` type works is that it provides the shared ownership of
    a value of type `T`, allocated in the heap. By calling the associated function
    `clone()` on an `Arc` instance, a new instance of the `Arc` reference-counted
    pointer is created, which points to the same allocation on the heap as the source
    `Arc`, while increasing a reference count. With each `clone()`, the reference
    count is increased by the `Arc` smart pointer. When each `cloned()` pointer goes
    out of scope, the reference counter is decremented. When the last of the clones
    go out of scope, both the `Arc` pointer and the value it points to (in the heap)
    are destroyed.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, `Mutex` ensures that at most one thread is able to access some
    data at one time, while `Arc` enables shared ownership of some data and prolongs
    its lifetime until all the threads have finished using it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the usage of `Mutex` with `Arc` to demonstrate shared-state concurrency
    with a step-by-step example. This time, we'll write a more complex example than
    just incrementing a shared counter value across threads. We'll take the example
    we wrote in [*Chapter 6*](B16405_06_Final_NM_ePUB.xhtml#_idTextAnchor101), *Working
    with Files and Directories in Rust*, to compute source file stats for all Rust
    files in a directory tree, and modify it to make it a concurrent program. We'll
    define the structure of the program in the next section. The complete code for
    this section can be found in the Git repo under `src/shared-state.rs`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Defining the program structure
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What we'd like to do is to take a list of directories as input to our program,
    compute source file statistics for each file within each of these directories,
    and print out a consolidated set of source code stats.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Let's first create a `dirnames.txt` file in the root folder of the cargo project,
    containing a list of directories with a full path, one per line. We'll read each
    entry from this file and spawn a separate thread to compute the source file stats
    for the Rust files within that directory tree. So, if there are five directory-name
    entries in the file, there will be five threads created from the main program,
    each of which will recursively walk through the directory structure of the entry,
    and compute the consolidated Rust source file stats. Each thread will increment
    the computed value in a shared data structure. We'll use `Mutex` and `Arc` to
    protect access and update the shared data safely across threads.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start writing the code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start with the module imports for this program:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define a struct to store the source file stats:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Within the `main()` function, create a new instance of `SrcStats`, protect
    it with a `Mutex` lock, and then wrap it inside an `Arc` type:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Read the `dirnames.txt` file, and store the individual entries in a vector:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Iterate through the `dir_lines` vector, and for each entry, spawn a new thread
    to perform the following two steps:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Accumulate the list of files from each subdirectory in the tree.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Then open each file and compute the stats. Update the stats in the shared-memory
    struct protected by `Mutex` and `Arc`.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The overall skeletal structure of the code for this step looks like this:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this section, we read the list of directory entries for computing source
    file statistics from a file. We then iterated through the list to spawn a thread
    to process each entry. In the next section, we'll define the processing to be
    done in each thread.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating source file statistics in shared state
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we''ll write the code for computing source file statistics
    in each thread and aggregate the results in shared state. We''ll look at the code
    in two parts – *sub-steps A* and *B*:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'In *sub-step A*, let''s read through each subdirectory under the directory
    entry, and accumulate the consolidated list of all Rust source files in the `file_entries`
    vector. The code for *sub-step A* is shown. Here, we are first creating two vectors
    to hold the directory and filenames respectively. Then we are iterating through
    the directory entries of each item from the `dirnames.txt` file, and accumulating
    the entry names into the `dir_entries` or `file_entries` vector depending upon
    whether it is a directory or an individual file:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: At the end of *sub-step A*, all individual filenames are stored in the `file_entries`
    vector, which we will use in *sub-step B* for further processing.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In *sub-step B*, we''ll read each file from the `file_entries` vector, compute
    the source stats for each file, and save the values in the shared memory struct.
    Here is the code snippet for *sub-step B*:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s again review the skeletal structure of the program shown next. We''ve
    so far seen the code to be executed within the thread, which includes processing
    for steps A and B:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s look at the last part of the code now. As discussed earlier, in order
    to ensure that the main thread does not complete before the child threads are
    completed, we have to join the child thread handles with the main threads. Also,
    let''s print out the final value of the thread-safe `stats_counter` struct, which
    contains aggregated source stats from all the Rust source files under the directory
    (updated by the individual threads):'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The complete code listing can be found in the Git repo for the chapter in `src/shared-state.rs`.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Before running this program, ensure to create a file, `dirnames.txt`, in the
    root folder of the cargo project, containing a list of directory entries with
    a full path, each on a separate line.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the project with `cargo run`. (*Note*: If you are running code from the
    Packt Git repo, use `cargo run --bin shared-state`.) You will see the consolidated
    source stats printed out. Note that we have now implemented a multi-threaded version
    of the project we wrote in [*Chapter 6*](B16405_06_Final_NM_ePUB.xhtml#_idTextAnchor101),
    *Working with Files and Directories in Rust*. As an exercise, alter this example
    to implement the same project with the *message-passing concurrency* model.'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we've seen how multiple threads can safely write to a shared
    value (wrapped in `Mutex` and `Arc`) that is stored in process heap memory, in
    a thread-safe manner. In the next section, we will review one more mechanism available
    to control thread execution, which is to selectively pause the processing of the
    current thread.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Send and Sync traits
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: We saw earlier how a data type can be shared across threads, and how messages
    can be passed between threads. There is another aspect of concurrency in Rust
    though. Rust defines data types as thread-safe or not.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'From a concurrency perspective, there are two categories of data types in Rust:
    those that are `Send` (that is, implement the `Send` trait), which means they
    are safe to be transferred from one thread to another. And the rest are *thread-unsafe*
    types. A related concept is `Sync`, which is associated with references of types.
    A type is considered to be `Sync` if its reference can be passed to another thread
    safely. So, `Send` means it is safe to transfer ownership of a type from one thread
    to another, while `Sync` means the data type can be shared (using references)
    safely by multiple threads at the same time. Note though that in `Send`, after
    a value has been transferred from the sending to the receiving thread, the sending
    thread can no longer use that value.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '`Send` and `Sync` are also automatically derived traits. This means that if
    a type consists of members that implement `Send` or `Sync` types, the type itself
    automatically becomes `Send` or `Sync`. The Rust primitives (almost all of them)
    implement `Send` and `Sync`, which means if you create a custom type from Rust
    primitives, your custom type also becomes `Send` or `Sync`. We''ve seen an example
    of this in the previous section, where the `SrcStats` (source stats) struct was
    transferred across the boundaries of threads without us having to explicitly implement
    `Send` or `Sync` on the struct.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: However, if there is a need to implement `Send` or `Sync` traits for a data
    type manually, it would have to be done in unsafe Rust.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, in Rust, every data type is classified as either *thread-safe*
    or *thread-unsafe*, and the Rust compiler enforces the safe transfer or sharing
    of thread-safe types across threads.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Pausing thread execution with timers
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, during the processing of a thread, there may be a need to pause
    execution either to wait for another event or to synchronize execution with other
    threads. Rust provides support for this using the `std::thread::sleep` function.
    This function takes a time duration of type `time::Duration` and pauses execution
    of the thread for the specified time. During this time, the processor time can
    be made available to other threads or applications running on the computer system.
    Let''s see an example of the usage of `thread::sleep`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Using the `sleep()` function is fairly straightforward, but this blocks the
    current thread and it is important to make judicious use of this in a multi-threaded
    program. An alternative to using `sleep()` would be to use an async programming
    model to implement threads with non-blocking I/O.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Async I/O in Rust
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: In the multi-threaded model, if there is a blocking I/O call in any thread,
    it blocks the program workflow. The *async* model relies on non-blocking system
    calls for I/O, for example, to access the filesystem or network. In the example
    of a web server with multiple simultaneous incoming connections, instead of spawning
    a separate thread to handle each connection in a blocking manner, *async* I/O
    relies on a runtime that does not block the current thread but instead schedules
    other tasks while waiting on I/O.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: While Rust has built-in `Async/Await` syntax, which makes it easier to write
    *async* code, it does not provide any asynchronous system call support. For this,
    we need to rely on external libraries such as `Tokio`, which provide both the
    *async runtime* (executor) and the *async* versions of the I/O functions that
    are present in the Rust Standard Library.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: So, when would one use *async* versus the *multi-threaded* approach to concurrency?
    The broad thumb-rule is that the *async* model is suited to programs that perform
    a lot of I/O, whereas, for computation-intensive (CPU-bound) tasks, *multi-threaded
    concurrency* is a better approach. Keep in mind though that it is not a binary
    choice, as in practice it is not uncommon to see *async* programs that also utilize
    *multi-threading* in a hybrid model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on async in Rust, refer to the following link: [https://rust-lang.github.io/async-book/](https://rust-lang.github.io/async-book/).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the basics of concurrency and multi-threaded programming
    in Rust. We started by reviewing the need for concurrent programming models. We
    understood the differences between the concurrent and parallel execution of programs.
    We learned how to spawn new threads using two different methods. We handled errors
    using a special `Result` type in the thread module and also learned how to check
    whether the current thread is panicking. We looked at how threads are laid out
    in process memory. We discussed two techniques for synchronizing processing across
    threads – *message-passing concurrency* and *shared-state concurrency*, with practical
    examples. As a part of this, we learned about channels, `Mutex` and `Arc` in Rust,
    and the role they play in writing concurrent programs. We then discussed how Rust
    classifies data types as *thread-safe* or not, and saw how to pause the execution
    of the current thread.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the chapter on managing concurrency in Rust. This also concludes
    *Section 2* of this book, which is on managing and controlling system resources
    in Rust.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: We will now move on to the last part of the book – *Section 3* covering *advanced
    topics*. In the next chapter, we will cover how to perform *device I/O* in Rust,
    and internalize learning through an example project.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL

- en: Ordering Things
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Tidy house, tidy mind* is a saying that, as in its German variation, implies
    that order plays an important part in our lives. Anyone who wants to maximize
    efficiency has to rely on order, or risk the occasional time-consuming search
    through the chaos that has slowly unfolded. Having things in a particular order
    is great; it''s the process of getting there that is expensive.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This often does not feel like a good use of our time, or simply may not be
    worth it. While a computer does not exactly feel, the time required to sort things
    is of a similar cost. Minimizing this time is the goal of inventing new algorithms
    and improving their efficiency, which is necessary for a task as common as sorting.
    A call to `mycollection.sort()` is not expected to take seconds (or minutes or
    even hours), so this is also a matter of usability. In this chapter, we will explore
    several solutions for that, so you can look forward to learning about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing and analyzing sorting algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowing more about (in)famous sorting strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From chaos to order
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many sorting algorithms (and their individual variations), each with
    their individual characteristics. Since it is impossible to cover every algorithm
    in a single chapter, and considering their limited usefulness, this chapter covers
    a selected few.
  prefs: []
  type: TYPE_NORMAL
- en: The selection should show the different strategies that are common in sorting
    a collection of items, many of which have been implemented in various libraries
    across different languages. Since many of you will never implement any sorting
    algorithms for productive use, this section is supposed to familiarize you with
    what's behind the scenes when a call to `mycollection.sort()` is issued, and why
    this could take a surprising amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sorting algorithms fall into a group on each of these properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stable**: Maintains a relative order when comparing equal values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid**: Combines two or more sorting approaches (for example, by collection
    length)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**In-place**: Uses indices instead of full copies for passing collections around'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While stable and hybrid algorithms are more complex and, in many cases, at a
    higher level (because they combine various approaches), in-place sorting is common
    and reduces the space and amount of copying an algorithm has to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have touched on a very basic sorting algorithm already: **insertion sort**.
    It is the exact algorithm most real life things are done with: when adding a new
    book to a bookshelf, most people will pick up the book, look at the property to
    order by (such as the author''s last name), and find the spot in their current
    collection, starting from the letter *A*. This is a very efficient approach and
    is used to build a new collection with minimal overhead, but it does not warrant
    its own section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start off with an absolute classic that is always a part of any university''s
    curriculum because of its simplicity: bubble sort.'
  prefs: []
  type: TYPE_NORMAL
- en: Bubble sort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bubble sort is the infamous algorithm that university students often learn as
    their first sorting algorithm. In terms of performance and runtime complexity,
    it is certainly among the worst ways to sort a collection, but it's great for
    teaching.
  prefs: []
  type: TYPE_NORMAL
- en: 'The principle is simple: walk through an array, scanning two elements and bringing
    them into the correct order by swapping. Repeat these steps until no swaps occur.
    The following diagram shows this process on the example array `[8, 9, 7, 6]`,
    where a total of four swaps establishes the order of `[6, 7, 8, 9]` by repeatedly
    comparing two succeeding elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49af9e15-7037-4151-87d3-8a79f6380e00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This diagram also shows an interesting (and name-giving) property of the algorithm:
    the "bubbling up" of elements to their intended position. The number `6` in the
    diagram travels, swap by swap, from the last position to the first position in
    the collection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When this is transformed into Rust code, the simplicity remains: two nested
    loops iterate over the collection, whereas the outer loop could just as well run
    till infinity, since the inner portion does all the comparing and swapping.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bubble sort is, infamously, a short snippet of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For easier handling, the algorithm creates a copy of the input array (using
    the `Into<T>` trait's `into()` method) and swaps around elements using the `swap()`
    method provided by `Vec<T>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The nested loops already hint toward the (worst case) runtime complexity: *O(n²)*.
    However, thanks to the early stopping when there are no swaps in a run, a partially
    ordered collection will be sorted surprisingly quickly. In fact, the best case
    scenario is really fast with bubble sort, since it''s basically a single run-through
    (in other words, *O(n)* in this case).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following chart shows three cases: sorting an already sorted collection
    (ascending numbers and descending numbers), as well as sorting a randomly shuffled
    array of distinct numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d516145-94c2-4c8b-8794-612388aa77f5.png)'
  prefs: []
  type: TYPE_IMG
- en: The output graph comparison between Bubble sort ascending, descending, and randomly
    sorted arrays
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm will produce an ascending sequence, yet the shuffled collection
    shows a worse absolute runtime than the traditional worst case: a collection sorted
    in descending order. In any case, the exponential nature of these runtimes shows
    why bubble sort is not fit for real-world use.'
  prefs: []
  type: TYPE_NORMAL
- en: Shell sort is sometimes dubbed as an optimized version of bubble sort!
  prefs: []
  type: TYPE_NORMAL
- en: Shell sort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bubble sort always compares an element to the neighboring element, but is this
    important? Many would say that it depends on the pre-existing order of the unsorted
    collection: are these future neighbors far apart or close together?'
  prefs: []
  type: TYPE_NORMAL
- en: Donald Shell, the inventor of shell sort, must have had a similar idea and used
    a "gap" between elements to make further jumps with the swapping approach adopted
    by bubble sort. By utilizing a specified strategy to choose those gaps, the runtime
    can change dramatically. Shell's original strategy is to start with half the collection's
    length and, by halving the gap size until zero, a runtime of *O(n²)* is achieved.
    Other strategies include choosing numbers based on some form of calculation of
    the current iteration *k* (for example, *2^k - 1*), or empirically collected gaps
    ([http://sun.aei.polsl.pl/~mciura/publikacje/shellsort.pdf](http://sun.aei.polsl.pl/~mciura/publikacje/shellsort.pdf)),
    which do not have a fixed runtime complexity yet!
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram explains some of the workings of shell sort. First, the
    initial gap is chosen, which is `n / 2` in the original paper. Starting at that
    gap (`2`, in this particular example), the element is saved and compared to the
    element *at the other end of the gap*, in other words, the current index minus
    the gap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0ef36fb-3d18-4a9a-9dc4-22eacb87ec54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the element at the other end of the gap is greater, it replaces the origin.
    Then, the process walks toward index zero with gap-sized steps, so the question
    becomes: what is going to fill that hole (`7` is overwritten by `8`, so the hole
    is where `8` was)—the original element, or element "gap" steps before it?'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, it's `7`, since there is no preceding element. In longer collections,
    a lot more moving around can occur before the original element is inserted. After
    this insertion process has finished for index 2, it's repeated for index 3, moving
    from the gap toward the end of the collection. Following that, the gap size is
    reduced (in our case, by half) and the insertion steps are repeated until the
    collection is in order (and the gap size is zero).
  prefs: []
  type: TYPE_NORMAL
- en: 'Words, and even an image, make it surprisingly hard to understand what is going
    on. Code, however, shows the workings nicely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This snippet shows the value of shell sort: with the correct gap strategy,
    it can achieve results that are similar to more sophisticated sorting algorithms,
    but it is a lot shorter to implement and understand. Because of this, it can be
    a good choice for embedded use cases, where no library and only limited space
    is available.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual performance on the test sets is good:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13270bdd-7cb8-4119-ab99-f66fe233620e.png)'
  prefs: []
  type: TYPE_IMG
- en: The output graph comparison between shell sort ascending, descending, and randomly
    sorted arrays
  prefs: []
  type: TYPE_NORMAL
- en: Even with the original gap strategy that is said to produce *O(n²)* runtimes,
    the random set produces something more akin to linear behavior. Definitely a solid
    performance, but can it compare to heap sort?
  prefs: []
  type: TYPE_NORMAL
- en: Heap sort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ordering numbers was already a topic that we covered earlier in this book ([Chapter
    5](84f203ac-a9f6-498b-90ff-e069c41aaca0.xhtml), *Robust Trees*) while discussing
    trees: with heaps. A heap is a tree-like data structure with the highest (max-heap)
    or lowest number (min-heap) at the root that maintains order when inserting or
    removing elements. Hence, a sorting mechanism could be as simple as inserting
    everything into a heap and retrieving it again!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since a (binary) heap has a known runtime complexity of *O(log n)*, and the
    entire array has to be inserted, the estimated runtime complexity will be *O(n
    log n)*, among the best sorting performances in sorting. The following diagram
    shows the binary heap in tree notation on the right, and the array implementation
    on the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87a58184-896d-489c-8fe7-5d4ce6ea28e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the Rust standard library, there is a `BinaryHeap` structure available,
    which makes the implementation quick and easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The fact that a heap is used to do the sorting will generate fairly uniform
    outcomes, making it a great choice for unordered collections, but an inferior
    choice for presorted ones. This is due to the fact that a heap is filled and emptied,
    regardless of the pre-existing ordering. Plotting the different cases shows almost
    no difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ba32ddb-8ca6-4a97-b5ce-4e9e58789cf4.png)'
  prefs: []
  type: TYPE_IMG
- en: The output graph comparison between heap sort ascending, descending, and randomly
    sorted arrays
  prefs: []
  type: TYPE_NORMAL
- en: A very different strategy, called *divide and conquer*, is employed by an entire
    group of algorithms. This group is what we are going to explore now, starting
    with merge sort.
  prefs: []
  type: TYPE_NORMAL
- en: Merge sort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One fundamental strategy in battle, as well as in sorting collections, is to
    divide and conquer. Merge sort does exactly that, by splitting the collection
    in half recursively until only a single element remains. The merging operation
    can then put these single elements together in the correct order with the benefit
    of working with presorted collections.
  prefs: []
  type: TYPE_NORMAL
- en: 'What this does is reduce the problem size (in other words, the number of elements
    in the collection) to more manageable chunks that come presorted for easier comparison,
    resulting in a worst case runtime complexity of *O(n log n)*. The following diagram
    shows the split and merge process (note that comparing and ordering only starts
    at the merge step):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8471c780-90ab-4e95-98a8-0162ddaff0f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are various implementations of this principle: bottom up, top down, using
    blocks, and other variations. In fact, as of 2018, Rust''s default sorting algorithm
    is Timsort, a stable, hybrid algorithm that combines insertion sort (up until
    a certain size) with merge sort.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a vanilla merge sort in Rust is, again, a great place to use recursion.
    First, the left half is evaluated, then the right half of a sequence, and only
    then does merging begin, first by comparing the two sorted results (left and right)
    and picking elements from either side. Once one of these runs out of elements,
    the rest is simply appended since the elements are obviously larger. This result
    is returned to the caller, repeating the merging on a higher level until the original
    caller is reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the Rust code for a typical merge sort implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This behavior also pays off, creating a quasi-linear runtime complexity, as
    shown in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24ed5a69-8014-4793-a25b-93bf72a593a0.png)'
  prefs: []
  type: TYPE_IMG
- en: The output graph comparison between Quicksort asc, desc, and random
  prefs: []
  type: TYPE_NORMAL
- en: Another divide-and-conquer-type algorithm is Quicksort. It's a very interesting
    way to sort a list for a variety of reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Quicksort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This algorithm significantly outperformed merge sort in best case scenarios
    and was quickly adopted as Unix''s default sorting algorithm, as well as in Java''s
    reference implementation. By using a similar strategy to merge sort, Quicksort
    achieves faster average and best case speeds. Unfortunately, the worst case complexity
    is just as bad as bubble sort: *O(n²)*. How so? you might ask.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Quicksort operates, sometimes recursively, on parts of the full collection,
    and swaps elements around to establish an order. Hence, the critical question
    becomes: how do we choose these parts? This choosing bit is called the partitioning
    scheme and typically includes the swapping as well, not just choosing a split
    index. The choice is made by picking a pivot element, the value of which is what
    everything is compared with.'
  prefs: []
  type: TYPE_NORMAL
- en: Everything less than the pivot value goes to one side, and everything greater
    goes to the other—by swapping. Once the algorithm detects a nice ascending (on
    the one side) and descending (from the other side) order, the split can be made
    where the two sequences intersect. Then, the entire process starts anew with each
    of the partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following illustration shows the picking and ordering of the elements based
    on the previous example collection. While the partitions in this example are only
    length one versus the rest, the same process would apply if these were longer
    sequences as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71ad8529-8a9f-4779-bdf2-7b9ac0cf55f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The partitioning scheme used here is called the Hoare scheme, named after the
    inventor of Quicksort, Sir Anthony Hoare, in 1959\. There are other schemes (Lomuto
    seems to be the most popular alternative) that may provide better performance
    by trading off various other aspects, such as memory efficiency or the number
    of swaps. Whatever the partition scheme, picking a pivot value plays a major role
    in performance as well and the more equal parts it produces (like the median),
    the better the value is. Potential strategies include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the median
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the arithmetic mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Picking an element (random, first, or last, as chosen here)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Rust code, Quicksort is implemented in three functions:'
  prefs: []
  type: TYPE_NORMAL
- en: The public API to provide a usable interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A wrapped recursive function that takes a low and high index to sort in-between
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The partition function implementing the Hoare partition scheme
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This implementation can be considered in-place since it operates on the same
    vector that was provided in the beginning, swapping elements based on their indices.
    Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Another new aspect in this implementation is the use of loop labels, which allow
    for better structure and readability. This is due to Hoare's use of a do-until
    type loop, a syntax that is not available in Rust, but that required the algorithm
    to avoid an infinite loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `break`/`continue` instructions are relatives of the infamous go-to instruction,
    so they should only be used sparingly and with great care for the purpose of readability.
    Loop labels provide a tool to achieve that. They allow a reader to track exactly
    which loop is being exited or continued. The syntax leans slightly on that of
    the lifetimes: `''mylabel: loop { break ''mylabel; }`.'
  prefs: []
  type: TYPE_NORMAL
- en: Quicksort's performance characteristics are definitely interesting. The rare
    worst case behavior or *O(n²)* has triggered many optimizations over the decades
    since its invention, the latest of which is called Dual-Pivot Quicksort from 2009,
    which has been adopted in Oracle's library for Java 7\. Refer to the *Further
    reading* section for a more detailed explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the original Quicksort on the previous dataset, the worst case and
    best case behaviors are clearly visible. The performance on the descending and
    (curiously) the ascending datasets is clearly *O(n²)*, while the randomized array
    is quickly processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ef3c141-c993-4c43-957c-2fbef567911d.png)'
  prefs: []
  type: TYPE_IMG
- en: The output graph comparison between Quicksort ascensding, descending and randomly
    sorted arrays
  prefs: []
  type: TYPE_NORMAL
- en: This behavior speaks for the Quicksort's strong sides, which are more "real-world"
    type scenarios, where the worst case rarely appears. In current libraries around
    various programming languages though, sorting is done in a hybrid fashion, which
    means that these generic algorithms are used according to their strengths. This
    approach is called **Introsort** (from introspective sort) and, in C++'s `std::sort`,
    relies on Quicksort up to a certain point. Rust's standard library, however, uses
    Timsort.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Putting things in order is a very fundamental problem that has been solved in
    many different ways, varying in aspects such as worst-case runtime complexity,
    memory required, the relative order of equal elements (stability), as well as
    overall strategies. A few fundamental approaches were presented in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bubble sort** is one of the simplest algorithms to implement, but it comes
    at a high runtime cost, with a worst-case behavior of *O(n²)*. This is due to
    the fact that it simply swaps elements based on a nested loop, which makes elements
    "bubble up" to either end of the collection.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shell sort** can be seen as an improved version of bubble sort, with a major
    upside: it does not start off by swapping neighbors. Instead, there is a gap that
    elements are compared and swapped across, covering a greater distance. This gap
    size changes with every round that shows worst-case runtime complexities of *O(n²)*
    for the original scheme to *O(n log n)* in the fastest variant. In fact, the runtime
    complexity of some empirically derived gaps cannot even be measured reliably!'
  prefs: []
  type: TYPE_NORMAL
- en: '**Heap sort** makes use of a data structure''s property to create a sorted
    collection. The heap, as presented earlier, retains the largest (or smallest)
    element at its root, returning it at every `pop()`. Heap sort therefore simply
    inserts the entire collection into a heap, only to retrieve it one by one in a
    sorted fashion. This leads to a runtime complexity of *O(n log n)*.'
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based strategies are also found in **merge sort**, a divide-and-conquer
    approach. This algorithm recursively splits the collection in half to sort the
    subset before working on the entire collection. This work is done when returning
    from the recursive calls when the resulting sub-collections have to be merged,
    hence the name. Typically, this will exercise a runtime complexity of *O(n log
    n)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Quicksort** also uses a divide-and-conquer approach, but instead of simply
    breaking the collection in half every time, it works with a pivot value, where
    the other values are swapped before looking at each sub-collection. This results
    in a worst-case behavior of *O(n²)*, but Quicksort is often used for its frequent
    average complexity of *O(n log n)*.'
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, standard libraries use hybrid approaches such as Timsort, Introsort,
    or pattern-defeating Quicksort to get the best absolute and relative runtime performance.
    Rust's standard library provides either a stable sorting function for slices (`slice::sort()`
    versus `slice::sort_unstable()`) based on merge sort, and an unstable sorting
    function based on the pattern-defeating Quicksort.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter aimed to be the basis for the next chapter, which will cover how
    to find a specific element, something that typically requires a sorted collection!
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why is sorting an important aspect of programming?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What makes values bubble up in bubble sort?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is shell sort useful?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can heap sort outperform bubble sort in its best case scenario?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do merge sort and Quicksort have in common?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are hybrid sorting algorithms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is some additional reference material that you may refer to regarding
    what has been covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dual-Pivot Quicksort* ([https://web.archive.org/web/20151002230717/http://iaroslavski.narod.ru/quicksort/DualPivotQuicksort.pdf](https://web.archive.org/web/20151002230717/http://iaroslavski.narod.ru/quicksort/DualPivotQuicksort.pdf))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C++ sorting explained ([https://medium.com/@lucianoalmeida1/exploring-some-standard-libraries-sorting-functions-dd633f838182](https://medium.com/@lucianoalmeida1/exploring-some-standard-libraries-sorting-functions-dd633f838182))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wikipedia on Introsort ([https://en.wikipedia.org/wiki/Introsort](https://en.wikipedia.org/wiki/Introsort))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wikipedia on Timsort ([https://en.wikipedia.org/wiki/Timsort](https://en.wikipedia.org/wiki/Timsort))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pattern defeating Quicksort ([https://github.com/orlp/pdqsort](https://github.com/orlp/pdqsort))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

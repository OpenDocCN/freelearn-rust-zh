- en: Collections in Rust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapters, we implemented a range of data structures, something
    that rarely happens in reality. Especially in Rust, the excellent `Vec<T>` covers
    a lot of cases, and if a map type structure is required, the `HashMap<T>` covers
    most of these too. So what else is there? How are they implemented? Why were they
    implemented if they won''t be used? These are all great questions, and they''ll
    get answered in this chapter. You can look forward to learning about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Sequence data types such as `LinkedList<T>`, `Vec<T>`, or `VecDeque<T>`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rust's `BinaryHeap<T>` implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HashSet<T>` and `BTreeSet<T>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to map things with the `BTreeMap<T>` and `HashMap<T>`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lists of any kind are the most essential data structure in a typical program;
    they provide flexibility and can be used as a queue, as a stack, as well as a
    searchable structure. Yet the limitations and the operations make a huge of difference
    between different data structures, which is why the documentation for `std::collections`
    offers a decision tree to find out the collection type that is actually required
    to solve a particular problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following were discussed in [Chapter 4](1a6971bb-ec24-47d5-b44c-cfb4da7d5b24.xhtml),
    *Lists, Lists, More Lists*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dynamic arrays** (`Vec<T>`) are the most universal and straightforward to
    use sequential data structure. They capture the speed and accessibility of an
    array, the dynamic sizing of a list, and they are the fundamental building block
    for higher order structures (such as stacks, heaps, or even trees). So, when in
    doubt a `Vec<T>` is always a good choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VecDeque<T>` is a close relative of the `Vec<T>`, implemented as a **ring
    buffer**â€”a dynamic array that wraps around the ends end, making it look like a
    circular structure. Since the underlying structure is still the same as `Vec<T>`,
    many of its aspects also apply here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `LinkedList<T>` is very limited in its functionality in Rust. Direct index
    access will be inefficient (it's a counted iteration), which is probably why it
    can only iterate, merge and split, and insert or retrieve from the back and front.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This was a nice primer, so let's look deeper into each of Rust's data structures
    in `std::collections`!
  prefs: []
  type: TYPE_NORMAL
- en: Vec<T> and VecDeque<T>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like the dynamic array in [Chapter 4](1a6971bb-ec24-47d5-b44c-cfb4da7d5b24.xhtml),
    *Lists, Lists, More Lists*, `Vec<T>` and `VecDeque<T>` are growable, list-like
    data structures with support for indexing and based on a heap-allocated array.
    Other than the previously implemented dynamic array, it is generic by default
    without any constraints for the generic type, allowing literally any type to be
    used.
  prefs: []
  type: TYPE_NORMAL
- en: '`Vec<T>` aims to have as little overhead as possible, while providing a few
    guarantees. At its core, it is a triple of (`pointer`, `length`, `capacity`) that
    provides an API to modify these elements. The `capacity` is the amount of memory
    that is allocated to hold items, which means that it fundamentally differs from
    `length`, the number of elements currently held. In case a zero-sized type or
    no initial length is provided, `Vec<T>` won''t actually allocate any memory. The
    `pointer` only points to the reserved area in memory that is encapsulated as a
    `RawVec<T>` structure.'
  prefs: []
  type: TYPE_NORMAL
- en: The main drawback of `Vec<T>` is its lack of efficient insertion at the front,
    which is what `VecDeque<T>` aims to provide. It is implemented as a ring, which
    wraps around the edges of the array, creating a more complex situation when the
    memory has to be expanded, or an element is to be inserted at a specified position.
    Since the implementations of `Vec<T>` and `VecDeque<T>` are quite similar, they
    can be used in similar contexts. This can be shown in their architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Both structures, `Vec<T>` and `RawVec<T>`, allocate memory in the same way:
    by using the `RawVec<T>` type. This structure is a wrapper around lower level
    functions to allocate, reallocate, or deallocate an array in the heap part of
    the memory, built for use in higher level data structures. Its primary goal is
    to avoid capacity overflows, out-of-memory errors, and general overflows, which
    saves the developer a lot of boilerplate code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of this buffer by `Vec<T>` is straightforward. Whenever the length
    threatens to exceed capacity, allocate more memory and transfer all elements,
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'So this goes on to call the `reserve()` function, followed by the `try_reserve()`,
    followed by the `amortized_new_size()` of `RawVec<T>`, which also makes the decision
    about the size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let's take a look at `VecDeque<T>`. On top of memory allocation, `VecDeque<T>`
    has to deal with wrapping the data around the ring, which adds considerable complexity
    to inserting an element at a specified position, or when the capacity has to increase.
    Then, the old elements need to be copied to the new memory area, starting with
    the shortest part of a wrapped list.
  prefs: []
  type: TYPE_NORMAL
- en: Like the `Vec<T>`, the `VecDeque<T>` doubles its buffer in size if it is full,
    but uses the `double()` function to do so. *Be aware that doubling is not a guaranteed
    strategy and might change.*
  prefs: []
  type: TYPE_NORMAL
- en: '*However, whatever replaces it will have to retain the runtime complexities
    of the operations.* The following are the functions used to determine whether
    the data structure is full and if it needs to grow in size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `handle_cap_increase()` function will then decide where the new ring should
    live and how the copying into the new buffer is handled, prioritizing copying
    as little data as possible. Other than `Vec<T>`, calling the `new()` function
    on `VecDeque<T>` allocates at `RawVec<T>` with enough space for seven elements,
    which then can be inserted without growing the underlying memory, therefore it
    is not a zero-size structure when empty.
  prefs: []
  type: TYPE_NORMAL
- en: Insert
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two ways to add elements to `Vec<T>`: `insert()` and `push()`. The
    former takes two parameters: an index of where to insert the element and the data.
    Before inserting, the position on the index will be freed by moving all succeeding
    elements towards the end (to the right). Therefore, if an element is inserted
    at the front, every element has to be shifted by one. `Vec<T>` code shows the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'While shifting is done efficiently, by calling `push()`, the new item can be
    added without moving data around, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The main drawback of regular `Vec<T>` is the inability of efficiently adding
    data to the front, which is where `VecDeque<T>` excels. The code for doing this
    is nice and short, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: With the use of `unsafe {}` in these functions, the code is much shorter and
    faster than it would be using safe Rust exclusively.
  prefs: []
  type: TYPE_NORMAL
- en: Look up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One major upside of using array-type data allocation is the simple and fast
    element access, which `Vec<T>` and `VecDeque<T>` share. The formal way to implement
    the direct access using brackets (`let my_first_element= v[0];`) is provided by
    the `Index<I>` trait.
  prefs: []
  type: TYPE_NORMAL
- en: Other than direct access, iterators are provided to search, fold, map, and so
    on the data. Some are equivalent to the `LinkedList<T>` part of this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, the `Vec<T>`''s owning iterator (`IntoIter<T>`) owns the pointer
    to the buffer and moves a pointer to the current element forward. There is also
    a catch though: if the size of an element is zero bytes, how should the pointer
    be moved? What data is returned? The `IntoIter<T>` structure comes up with a clever
    solution (**ZSTs** are **zero-sized types**, so types that don''t actually take
    up space):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The comments already state what's happening, the iterator avoids returning the
    same pointer over and over again, and instead, increments it by one and returns
    a zeroed out memory. This is clearly something that the Rust compiler would not
    tolerate, so `unsafe` is a great choice here. Furthermore, the regular iterator
    (`vec![].iter()`) is generalized in the `core::slice::Iter` implementation, which
    works on generic, array-like parts of the memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Contrary to that, the iterator of `VecDeque<T>` resorts to moving an index
    around the ring until a full circle is reached. Here is its implementation, shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Among other traits, both implement the `DoubleEndedIterator<T>` work on both
    ends, a special function called `DrainFilter<T>`, in order to retrieve items in
    an iterator only if a predicate applies.
  prefs: []
  type: TYPE_NORMAL
- en: Remove
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Vec<T>` and `VecDeque<T>` both remain efficient when removing items. Although,
    they don''t change the amount of memory allocated to the data structure, both
    types provide a function called `shrink_to_fit()` to readjust the capacity to
    the length it has.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On `remove`, `Vec<T>` shifts the remaining elements toward the start of the
    sequence. Like the `insert()` function, it simply copies the entire remaining
    data with an offset, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For `VecDeque<T>`, the situation is much more complex: since the data can wrap
    around the ends of the underlying buffer (for example, the tail is on index three,
    head on index five, so the space from three to five is considered empty), it can''t
    blindly copy in one direction. Therefore, there is some logic that deals with
    these different situations, but it is much too long to add here.'
  prefs: []
  type: TYPE_NORMAL
- en: LinkedList<T>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rust''s `std::collection::LinkedList<T>` is a doubly linked list that uses
    an `unsafe` pointer operation to get around the `Rc<RefCell<Node<T>>>` unpacking
    we had to do in [Chapter 4](1a6971bb-ec24-47d5-b44c-cfb4da7d5b24.xhtml), *Lists,
    Lists, and More Lists*. While unsafe, this is a great solution to that problem,
    since the pointer operations are easy to comprehend and provide significant benefits.
    Let''s look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`NonNull` is a structure that originates from `std::ptr::NonNull`, which provides
    a non-zero pointer to a portion of heap memory in unsafe territory. Hence, the
    interior mutability pattern can be skipped at this fundamental level, eliminating
    the need for runtime checks.'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fundamentally, `LinkedList` is built just the way we built the doubly linked
    list in [Chapter 4](1a6971bb-ec24-47d5-b44c-cfb4da7d5b24.xhtml), *Lists, Lists,
    and More Lists*, with the addition of a `PhantomData<T>` type pointer. Why? This
    is necessary to inform the compiler about the properties of the type that contains
    the marker when generics are involved. With it, the compiler can determine a range
    of things, including drop behavior, lifetimes, and so on. The `PhantomData<T>`
    pointer is a zero-size addition, and pretends to own type `T` content, so the
    compiler can reason about that.
  prefs: []
  type: TYPE_NORMAL
- en: Insert
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `std::collections::LinkedList` employs several unsafe methods in order to
    avoid the `Rc<RefCell<Node<T>>>` and `next.as_ref().unwrap().borrow()` calls that
    we saw when implementing a doubly linked list in a safe way. This also means that
    adding a node at either end entails the use of `unsafe` to set these pointers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the code is easy to read and comprehend, which is important to
    avoid sudden crashes due to unsound code being executed. This is the core function
    to add a node in the front, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This code is wrapped by the publicly facing `push_front()` function, shown
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `push_back()` function, which performs the same action but on the end of
    the list, works just like this. Additionally, the linked list can append another
    list just as easily, since it is almost the same as adding a single node, but
    with additional semantics (such as: is the list empty?) to take care of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Adding things is one of the strong suits of a linked list. But how about looking
    up elements?
  prefs: []
  type: TYPE_NORMAL
- en: Look up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `collections::LinkedList` relies a lot on the `Iterator` trait to look
    up various items, which is great since it saves a lot of effort. This is achieved
    by extensively implementing various iterator traits using several structures,
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Iter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IterMut`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IntoIter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Technically, `DrainFilter` also implements `Iterator`, but it''s really a convenience
    wrapper. The following is the `Iter` structure declaration that the `LinkedList`
    uses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If you remember the list's declaration earlier, it will become obvious that
    they are very similar! In fact, they are the same, which means that when iterating
    over a linked list, you are essentially creating a new list that gets shorter
    with every call to `next()`. As expected, this is a very efficient process that
    is employed here, since no data is copied and the `Iter` structures' head can
    move back and forth with the `prev`/`next` pointers of the current head.
  prefs: []
  type: TYPE_NORMAL
- en: '`IterMut` and `IntoIter` have a slightly different structure, due to their
    intended purposes. `IntoIter` takes ownership of the entire list, and just calls
    `pop_front()` or `pop_back()` as requested.'
  prefs: []
  type: TYPE_NORMAL
- en: '`IterMut` has to retain a mutable reference to the original list in order to
    provide mutable references to the caller, but other than that, it''s basically
    an `Iter` type structure.'
  prefs: []
  type: TYPE_NORMAL
- en: The other structure that also does iteration is `DrainFilter`, which as the
    name suggests, removes items.
  prefs: []
  type: TYPE_NORMAL
- en: Remove
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The linked list contains two functions: `pop_front()` and `pop_back()`, and
    they simply wrap around an "inner" function called `pop_front_node()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This way, removing a specific element from `LinkedList<T>` has to be done either
    by splitting and appending the list (skipping the desired element), or by using
    `drain_filter()` function, which does almost exactly that.
  prefs: []
  type: TYPE_NORMAL
- en: Wrap up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Vec<T>` and `VecDeque<T>` both build on a heap-allocated array, and perform
    very well on `insert` and `find` operations, thanks to the elimination of several
    steps. However, the dynamic array implementation from earlier in the book can
    actually hold its own against these.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The doubly-linked list implemented previously does not look good against the
    `LinkedList<T>` provided by `std::collections`, which is built far simpler and
    does not use `RefCells` that do runtime borrow checking:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/870e3231-79b6-4d11-b995-18007bf90028.png)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, if you need a linked list, do not implement it yourself, `std::collections::LinkedList<T>`
    is excellent as far as linked lists go. Commonly, `Vec<T>` will perform better
    while providing more features, so unless the linked list is absolutely necessary,
    `Vec<T>` should be the default choice.
  prefs: []
  type: TYPE_NORMAL
- en: Maps and sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rust''s maps and sets are based largely on two strategies: B-Tree search and
    hashing. They are very distinct implementations, but achieve the same results:
    associating a key with a value (map) and providing a fast unique collection based
    on keys (set).'
  prefs: []
  type: TYPE_NORMAL
- en: Hashing in Rust works with a `Hasher` trait, which is a universal, stateful
    hasher, to create a hash value from an arbitrary byte stream. By repeatedly calling
    the appropriate `write()` function, data can be added to the hasher's internal
    state and finished up with the `finish()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly the B-Tree in Rust is highly optimized. The `BTreeMap` documentation
    provides rich details on why the regular implementation (as previously shown)
    is cache inefficient and not optimized for modern CPU architectures. Hence, they
    provide a more efficient implementation, which is definitely fascinating, and
    you should check it out in the source code.
  prefs: []
  type: TYPE_NORMAL
- en: HashMap and HashSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both `HashMap` and `HashSet` use a hashing algorithm to produce the unique key
    required for storing and retrieving values. Hashes are created with an instance
    of the `Hasher` trait (`DefaultHasher` if nothing is specified) for each key that
    implements the `Hash` and `Eq` traits. They allow a `Hasher` instance to be passed
    into the `Hash` implementor to generate the required output and the data structure
    to compare keys for equality.
  prefs: []
  type: TYPE_NORMAL
- en: If a custom structure is to be used as a hashed key (for the map, or simply
    to store in the set), this implementation can be derived as well, which adds every
    field of the structure to the `Hasher`'s state. In case the trait is implemented
    by hand, it has to create equal hashes whenever two keys are equal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since both data structures build on keys having implemented this trait, and
    both should be highly optimized, one question comes up: why bother with two variants?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look into the source, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The rest of this section will only talk about `HashMap`.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`HashMap` is a highly optimized data structure that employs a performance heuristic
    called **Robin Hood hashing** to improve caching behavior, and thereby lookup
    times.'
  prefs: []
  type: TYPE_NORMAL
- en: Robin Hood hashing is best explained together with the insertion algorithm linear
    probing, which is somewhat similar to the algorithm used in the hash map of the
    previous chapter. However, instead of an array of arrays (or `Vec<Vec<(K, V)>>`),
    the basic data structure is a flat array wrapped (together with all unsafe code)
    in a structure called `RawTable<K, V>`.
  prefs: []
  type: TYPE_NORMAL
- en: The table organizes its data into buckets (empty or full) that represent the
    data at a particular hash. Linear probing means that whenever a collision occurs
    (two hashes are equal without their keys being equal), the algorithm keeps looking
    into ("probing") the following buckets until an empty bucket is found.
  prefs: []
  type: TYPE_NORMAL
- en: The Robin Hood part is to count the steps from the original (ideal) position,
    and whenever an element in a bucket is closer to its ideal position (that is,
    richer), the bucket content is swapped, and the search continues with the element
    that was swapped out of its bucket. Thus, the search takes from the rich (with
    only a few steps removed from their ideal spot) and gives to the poor (those that
    are further away from their ideal spot).
  prefs: []
  type: TYPE_NORMAL
- en: This strategy organizes the array into clusters around the hash values and greatly
    reduces the key variance, while improving CPU cache-friendliness. Another main
    factor that influences this behavior is the size of the table and how many buckets
    are occupied (called **load factor**). `DefaultResizePolicy` of `HashMap` changes
    the table's size to a higher power of two at a load factor of 90.9%â€”a number that
    provides ideal results for the Robin Hood bucket stealing. There are also some
    great ideas on how to manage that growth without having to reinsert every element,
    but they would certainly exceed the scope of this chapter. It's recommended to
    read the source's comments if you are interested (see *Further reading* section).
  prefs: []
  type: TYPE_NORMAL
- en: Insert
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Robin Hood hashing strategy already describes a large portion of the `insert`
    mechanism: hash the key value, look for an empty bucket, and reorder elements
    along the way according to their probing distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This function only does the first step and expands the basic data structureâ€”if
    needed. The `insert_hashed_nocheck()` function provides the next step by searching
    for the hash in the existing table, and returning the appropriate bucket for it.
    The element is responsible for inserting itself into the right spot. The steps
    necessary to do that depend on whether the bucket is full or empty, which is modeled
    as two different structures: `VacantEntry` and `OccupiedEntry`. While the latter
    simply replaces the value (this is an update), `VacantEntry` has to find a spot
    not too far from the assigned bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The call to `robin_hood()` executes the search and swap described earlier.
    One interesting variable here is the `DISPLACEMENT_THRESHOLD`. Does this mean
    that there is an upper limit of how many displacements a value can have? Yes!
    This value is `128` (so `128` misses are required), but it wasn''t chosen randomly.
    In fact, the code comments go into the details of why and how it was chosen, shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As the comment states, the chance is *very* low that an element actually exceeds
    that threshold. Once a spot was found for every element, a look up can take place.
  prefs: []
  type: TYPE_NORMAL
- en: Lookup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Looking up entries is part of the insert process of `HashMap` and it relies
    on the same functions to provide a suitable entry instance to add data. Just like
    the insertion process, the lookup process does almost the same, save some steps
    in the end, listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a hash of the key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the hash's bucket in the table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Move away from the bucket comparing keys (linear search) until found
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since all of this has already been implemented for use in other functions,
    `get()` is pretty short, shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, the `remove` function requires `search`, and removal is implemented
    on the entry type.
  prefs: []
  type: TYPE_NORMAL
- en: Remove
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `remove` function looks a lot like the `search` function, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'There is one major difference: `search` returns a mutable bucket from which
    the key can be removed (or rather, the entire bucket since it''s now empty). `HashMap`
    turns out to be an impressive piece of code; can `BTreeMap` compete?'
  prefs: []
  type: TYPE_NORMAL
- en: BTreeMap and BTreeSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Talking about B-Trees in [Chapter 5](84f203ac-a9f6-498b-90ff-e069c41aaca0.xhtml),
    *Robust Trees*, their purpose is storing key-value pairsâ€”ideal for a map-type
    data structure. Their ability to find and retrieve these pairs is achieved by
    effectively minimizing the number of comparisons required to get to (or rule out)
    a key. Additionally, a tree keeps the keys in order, which means iteration is
    going to be implicitly ordered. Compared to `HashMap`, this can be an advantage
    since it skips a potentially expensive step.
  prefs: []
  type: TYPE_NORMAL
- en: Sinceâ€”just like `HashSet`â€”`BTreeSet` simply uses `BTreeMap` with an empty value
    (only the key) underneath, only the latter is discussed in this section since
    the working is assumed to be the same. Again, let's start with the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rust's `BTreeMap` chose an interesting approach to maximize performance for
    search by creating large individual nodes. Recalling the typical sizes of nodes
    (that is, the number of children they have), they were more than two (root only),
    or half the tree's level to the tree's level number of children. In a typical
    B-Tree, the level rarely exceeds 10, meaning that the nodes stay rather small,
    and the number of comparisons within a node do too.
  prefs: []
  type: TYPE_NORMAL
- en: The implementors of the Rust `BTreeMap` chose a different strategy in order
    to improve caching behavior. In order to improve cache-friendliness and reduce
    the number of heap allocations required, Rusts' `BTreeMap` stores from *level
    - 1* to *2 * level - 1* number of elements per node, which results in a rather
    large array of keys.
  prefs: []
  type: TYPE_NORMAL
- en: While the oppositeâ€”small arrays of keysâ€”fit the CPU's cache well enough, the
    tree itself has a larger number of them, so more nodes might need to be looked
    at. If the number of key-value pairs in a single node is higher, the overall node
    count shrinks, and if the key array still fits into the CPU's cache, these comparisons
    are as fast as they can be. The downside of larger arrays to search the key in
    is mitigated by using more intelligent searches (like binary search), so the overall
    performance gain of having fewer nodes outweighs the downside.
  prefs: []
  type: TYPE_NORMAL
- en: In general, when comparing the B-Tree from earlier in this book to `BTreeMap`,
    only a few similarities stand out, one of them being inserting a new element.
  prefs: []
  type: TYPE_NORMAL
- en: Insert
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like every B-Tree, inserts are done by first searching a spot to insert, and
    then applying the split procedure in case the node has more than the expected
    number of values (or children). Insertion is split into three parts and it starts
    with the first method to be called, which glues everything together and returns
    an expected result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The second step is finding the handle for the node that the pair can be inserted
    into, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the handle is known, the entry (which is either a structure modeling a
    vacant or occupied spot) inserts the new key-value pair. If the entry was occupied
    before, the value is simply replacedâ€”no further steps required. If the spot was
    vacant, the new value could trigger a tree rebalancing where the changes are bubbled
    up the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Looking up keys is already part of the insert process, but it deserves a closer
    look too.
  prefs: []
  type: TYPE_NORMAL
- en: Look up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a tree structure, inserts and deletes are based on looking up the keys that
    are being modified. In the case of `BTreeMap`, this is done by a function called
    `search_tree()` which is imported from the parent module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The code itself is very easy to read, which is a good sign. It also avoids the
    use of recursion and uses a `loop{}` construct instead, which is a benefit for
    large lookups since Rust does not expand tail-recursive calls into loops (yet?).
    In any case, this function returns the node that the key resides in, letting the
    caller do the work of extracting the value and key from it.
  prefs: []
  type: TYPE_NORMAL
- en: Remove
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `remove` function wraps the occupied node's `remove_kv()` function, which
    removes a key-value pair from the handle that `search_tree()` unearthed. This
    removal also triggers a merging of nodes if a node now has less than the minimum
    amount of children.
  prefs: []
  type: TYPE_NORMAL
- en: Wrap up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As shown in this section, maps and sets have a lot in common and there are
    two ways that the Rust collections library provides them. `HashMap` and `HashSet`
    use a smart approach to finding and inserting values into buckets called Robin
    Hood hashing. Recalling the comparison benchmarks from [Chapter 6](95653045-6e1c-4ef7-bd0c-8e45b1ccfa1d.xhtml),
    *Exploring Maps and Sets*, it provided a more stable and significantly better
    performance over a naive implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6182aa28-e48e-4ff7-91cb-9b6846b469db.png)'
  prefs: []
  type: TYPE_IMG
- en: '`BTreeMap` and `BTreeSet` are based on a different, more efficient implementation
    of a B-Tree. How much more efficient (and effective)? Let''s find out!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/624801cf-0f59-4aea-9923-88f54cce1df4.png)'
  prefs: []
  type: TYPE_IMG
- en: For a naive implementation of a B-Tree (from [Chapter 5](84f203ac-a9f6-498b-90ff-e069c41aaca0.xhtml),
    *Robust Trees*), the performance is not that bad. However, while there might be
    some tweaks to be added here and there, evidence shows that there is a better
    and faster tree out there, so why not use that?
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Rust standard library features a great collections part, providing a few
    highly optimized implementations of basic data structures.
  prefs: []
  type: TYPE_NORMAL
- en: We started with `Vec<T>` and `VecDeque<T>`, both based on a heap-allocated array
    and wrapped in the `RawVec<T>` structure. They show excellent performance while
    memory efficiency remains high, thanks to the array base and `unsafe` operations
    based on pointers.
  prefs: []
  type: TYPE_NORMAL
- en: '`LinkedList<T>` is a doubly-linked list that performs really well, thanks to
    direct data manipulation and the lack of runtime checking. While it excels at
    splitting and merging, most other operations are slower than `Vec<T>` and it lacks
    some useful features.'
  prefs: []
  type: TYPE_NORMAL
- en: '`HashSet` and `HashMap` are based on the same implementation (`HashMap`) andâ€”unless
    specified differentlyâ€”use `DefaultHasher` to generate a hashed key of an object.
    This key is stored (and later retrieved) using the Robin Hood hashing method,
    which provides major performance benefits over a naive implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, `BTreeSet` and `BTreeMap` use a B-Tree structure to organize
    keys and values. This implementation is also specialized and geared towards CPU-cache
    friendliness, and reducing the number of nodes (thereby minimizing the number
    of allocations) in order to create the high performance data structure that it
    is.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will decrypt the O notation, something that has been
    used sparingly up until this point, but is necessary for what follows: algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which `std::collections` data structure is not discussed here?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does `Vec<T>` or `VecDeque<T>` grow as of 2018?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is `LinkedList<T>` a good default data structure?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What hashing implementation does the 2018 `HashMap` use by default?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are three benefits of `BTreeMap` over a `HashMap`?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is a `BTreeMap` internal tree wider or higher?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following links for more information on topics covered
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://doc.rust-lang.org/std/collections/index.html](https://doc.rust-lang.org/std/collections/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://cglab.ca/~abeinges/blah/rust-btree-case/](http://cglab.ca/~abeinges/blah/rust-btree-case/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://doc.rust-lang.org/src/std/collections/hash/map.rs.html#148](https://doc.rust-lang.org/src/std/collections/hash/map.rs.html#148)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

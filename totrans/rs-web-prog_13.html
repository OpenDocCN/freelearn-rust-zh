<html><head></head><body>
		<div><h1 id="_idParaDest-263" class="chapter-number"><a id="_idTextAnchor264"/>13</h1>
			<h1 id="_idParaDest-264"><a id="_idTextAnchor265"/>Best Practices for a Clean Web App Repository</h1>
			<p>Throughout this book, we have been building out our application piece by piece and adding automation scripts and tools to help us with testing and deploying our application. However, although this path is useful for learning tools and concepts, the structure of our projects in previous chapters has not been optimal for running a project for production.</p>
			<p>In this chapter, we will create a new repository, lift our Rust code into that repository, and then structure the code for clean database migrations, tests, and optimized Docker builds for our application so that it can be deployed smoothly.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>The general layout of a clean repository</li>
				<li>Getting our configuration from environment variables</li>
				<li>Setting up a local development database</li>
				<li>Managing variables in Postman tests</li>
				<li>Building distroless tiny server Docker images</li>
				<li>Building a clean test pipeline</li>
				<li>Building continuous integration with GitHub Actions</li>
			</ul>
			<p>By the end of this chapter, you will be able to structure a repository with scripts, Docker builds, and tests that will make development smooth and easy to add new features. You will also be able to build <strong class="bold">distroless</strong> Docker images for the application, making them secure and dropping the size of our server images from 1.5 GB to 45 MB!</p>
			<h1 id="_idParaDest-265"><a id="_idTextAnchor266"/>Technical requirements</h1>
			<p>In this chapter, we will be referencing parts of the code defined in <a href="B18722_09.xhtml#_idTextAnchor182"><em class="italic">Chapter 9</em></a>, <em class="italic">Testing Our Application Endpoints and Components</em>. This can be found at the following URL: <a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter09">https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter09</a>.</p>
			<p>The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter13">https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter13</a>.</p>
			<h1 id="_idParaDest-266"><a id="_idTextAnchor267"/>The general layout of a clean repository</h1>
			<p>When it comes to a <a id="_idIndexMarker1210"/>clean layout, we must have directories in the repository that have a single focus, just like our isolated code, which is modularized. In the clean approach taken in this chapter, our repository will have the following layout:</p>
			<pre class="source-code">
├── Cargo.toml
├── README.md
├── .dockerignore
├── .gitignore
├── .github
│   . . .
├── builds
│   . . .
├── database
│   . . .
├── docker-compose.yml
├── scripts
│   . . .
├── src
│   . . .
└── tests
    . . .</pre>
			<p>These files and directories have the following responsibilities:</p>
			<ul>
				<li><code>Cargo.toml</code>: Defines the requirements for the Rust build.</li>
				<li><code>README.md</code>: Renders on a GitHub page when visited, telling the reader what the project is about and how to interact with the project.</li>
				<li><code>.dockerignore</code>: Tells the Docker build what to ignore when copying directories and files into the Docker image.</li>
				<li><code>.gitignore</code>: Tells git what to ignore when committing code to the git repository.</li>
				<li><code>.github</code>: A directory that houses GitHub Actions workflows.</li>
				<li><code>builds</code>: A directory that houses different builds for Docker, depending on the chip architecture.</li>
				<li><code>database</code>: A directory <a id="_idIndexMarker1211"/>that houses all scripts and Docker builds required to handle database migrations.</li>
				<li><code>docker-compose.yml</code>: Defines the containers needed to run a development build.</li>
				<li><code>scripts</code>: A directory that houses all the Bash scripts needed to run dev servers or tests.</li>
				<li><code>src</code>: A directory that houses all the Rust code to build the server.</li>
				<li><code>tests</code>: A directory that houses a <code>docker-compose</code> configuration and a Postman collection to enable a fully integrated test. We must remember that unit tests are coded within the <code>src</code> directory and are conditionally compiled when the <code>test</code> command in Cargo is executed. In standard and release builds, the unit tests are excluded.</li>
			</ul>
			<p>Now that we know what our repository structure is like, we can add some rules and files to ensure that our builds and git commits behave in exactly the right way. It is good to do this at the very start of a project to avoid accidentally adding unwanted code to the git history or Docker builds.</p>
			<p>First, we will start with the <code>.gitignore</code> file, which has the following rules defined:</p>
			<pre class="source-code">
/target/
Cargo.lock
# These are backup files generated by rustfmt
**/*.rs.bk
# jetbrains
.idea
# mac
.DS_Store</pre>
			<p>Here, we can see that <a id="_idIndexMarker1212"/>we avoid anything in the <code>target</code> directory, which gets filled up with a lot of files when performing Rust builds and tests. These files add nothing to the project’s development and will balloon the size of your project very quickly. If you like to use JetBrains or are using a Mac, I have added <code>.idea</code> and <code>.DS_Store</code> as these files can sneak into repositories; they are not required for running any of the web application’s code.</p>
			<p>Now, let’s look at our <code>.dockerignore</code> file, which has the following rules:</p>
			<pre class="source-code">
./tests
./target
./scripts
./database
.github</pre>
			<p>These rules should make sense. We do not want to add our build files, scripts, database migrations, or GitHub workflows to our Docker builds.</p>
			<p>We have now defined all our rules for the repository. Before we move on to the next section, we might as well define the general layout of our application. Here, we can lift the source Rust code from the existing to-do application into our new repository with the following command:</p>
			<pre class="console">
cp -r web_app/src ./clean_web_app/src</pre>
			<p>If you created an <code>src</code> directory in the clean app repository before running the preceding command, you must delete the <code>src</code> directory in the clean app repository; otherwise, you will end up with two <code>src</code> directories, where the copied <code>src</code> is inside the existing <code>src</code>. Our <code>Cargo.tml</code> file has the same dependencies as our existing web application; however, we <a id="_idIndexMarker1213"/>can change its name with the following code:</p>
			<pre class="source-code">
[package]
name = "clean_app"
version = "0.1.0"
edition = "2021"</pre>
			<p>Let’s check if lifting our code works with the following <code>test</code> command:</p>
			<pre class="console">
cargo test</pre>
			<p>This should give us the following output:</p>
			<pre class="console">
running 9 tests
test to_do::structs::base::base_tests::new ... ok
test to_do::structs::done::done_tests::new ... ok
test to_do::structs::pending::pending_tests::new ... ok
test jwt::jwt_tests::get_key ... ok
test jwt::jwt_tests::decode_incorrect_token ... ok
test jwt::jwt_tests::encode_decode ... ok
test jwt::jwt_tests::test_no_token_request ... ok
test jwt::jwt_tests::test_false_token_request ... ok
test jwt::jwt_tests::test_passing_token_request ... ok</pre>
			<p>This output shows that our code compiles and that our <code>Cargo.tml</code> file is properly defined. Now that we have confirmed that our unit tests have passed, we have some assurance that our code is working. However, how we define our configuration will give us some hurdles when we are deploying applications to the cloud. In the next section, we will smooth out our <a id="_idIndexMarker1214"/>deployments by using environment variables to configure our web applications.</p>
			<h1 id="_idParaDest-267"><a id="_idTextAnchor268"/>Getting our configuration from environment variables</h1>
			<p>So far, we have <a id="_idIndexMarker1215"/>been loading configuration variables from YML files. This has a few issues. First, we must move these files around where our deployment is. Also, files do not work efficiently with orchestration tools such as Kubernetes. Kubernetes uses ConfigMaps, which essentially define environment variables for each container they are running. Environment variables also work well with tools such as Secret Manager and AWS credentials. We can also directly overwrite the environment variables in <code>docker-compose</code>. With all these advantages in mind, we will switch our configuration values from files to environment variables. To map where we have implemented configuration variables from a file, all we must do is delete our <code>src/config.rs</code> file and the module declaration of that <code>config</code> module in the <code>main.rs</code> file. Then, we can run the <code>cargo test</code> command again to get the following output:</p>
			<pre class="console">
--&gt; src/database.rs:12:12
   |
12 | use crate::config::Config;
   |            ^^^^^^ could not find `config` in the crate root
error[E0432]: unresolved import `crate::config`
 --&gt; src/jwt.rs:9:12
  |
9 | use crate::config::Config;
  |            ^^^^^^ could not find `config` in the crate root
error[E0432]: unresolved import `crate::config`
 --&gt; src/counter.rs:4:12
  |
4 | use crate::config::Config;
  |            ^^^^^^ could not find `config` in the crate root</pre>
			<p>Here, we used <code>config</code> in the <code>jwt</code>, <code>database</code>, and <code>counter</code> modules. This makes sense because we <a id="_idIndexMarker1216"/>must connect to external structures when using these modules. To fix the breaking imports, all we must do is replace the config references with environment variable references. To demonstrate this, we can use the <code>src/counter.rs</code> file. First, we must delete the following lines of code:</p>
			<pre class="source-code">
...
use crate::config::Config;
...
let config = Config::new();
let redis_url = config.map.get("REDIS_URL")
                          .unwrap().as_str()
                          .unwrap().to_owned();
...</pre>
			<p>Then, we must replace the preceding lines of code with the following code:</p>
			<pre class="source-code">
...
use std::env;
...
let redis_url = env::var("REDIS_URL").unwrap();
...</pre>
			<p>We can follow this format for the <code>JWT</code> and <code>database</code> modules as well. In the <code>JWT</code> module, there is one variable that is not a string and has to be converted into an integer, which is <code>expire minutes</code>. This can be done with the following line of code:</p>
			<pre class="source-code">
let minutes = env::var("EXPIRE_MINUTES").unwrap()
                                        .parse::&lt;i64&gt;()
                                        .unwrap();</pre>
			<p>If we run <a id="_idIndexMarker1217"/>the <code>cargo test</code> command now, we will get the following output:</p>
			<pre class="console">
running 9 tests
test jwt::jwt_tests::encode_decode ... FAILED
test jwt::jwt_tests::get_key ... FAILED
test jwt::jwt_tests::decode_incorrect_token ... FAILED
test to_do::structs::pending::pending_tests::new ... ok
test to_do::structs::base::base_tests::new ... ok
test to_do::structs::done::done_tests::new ... ok
test jwt::jwt_tests::test_passing_token_request ... FAILED
test jwt::jwt_tests::test_no_token_request ... ok
test jwt::jwt_tests::test_false_token_request ... FAILED</pre>
			<p>Our tests ran, so we know that compiling the code worked. However, some of the JWT tests are failing. If we scroll further down the log, we will see the following error:</p>
			<pre class="console">
---- jwt::jwt_tests::encode_decode stdout ----
thread 'jwt::jwt_tests::encode_decode' panicked at
'called `Result::unwrap()` on an `Err` value: NotPresent',
src/jwt.rs:52:50</pre>
			<p>This is telling us that our environment variables are not present. Considering that this is failing in the <code>JWT</code> module, we can be sure that it will also fail in the <code>database</code> and <code>counter</code> modules. Therefore, before we run or test our application, we need to define these environment variables. We can build a test pipeline for our application with environment variables by building a <code>scripts/run_unit_tests.sh</code> script with the following code:</p>
			<pre class="source-code">
#!/usr/bin/env bash
# navigate to directory
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH
cd ..
export SECRET_KEY="secret"
export EXPIRE_MINUTES=60
cargo test</pre>
			<p>Here, we navigate to<a id="_idIndexMarker1218"/> the root directory, export the environment variables, and then run the <code>test</code> command. Running the preceding script results in all the unit tests passing.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">While I like putting as much as possible in the Bash script as it acts like documentation for other developers to see all the moving parts, there are other approaches. For instance, you may find that the approach outlined previously is clunky as this approach deviates from running the standard <code>cargo test</code> command. Other approaches include the following:</p>
			<p class="callout"> - Manually injecting variables into the test</p>
			<p class="callout">- Using the <code>dotenv</code> crate to load environment variables from a file (https://github.com/dotenv-rs/dotenv)</p>
			<p class="callout">- Having sensible defaults for environment variables</p>
			<p>How would you create the script that runs the dev server? This would be a good time for you to try and write the script yourself. If you have attempted writing the script yourself, your <code>scripts/run_dev_server.sh</code> script should look something like the following code:</p>
			<pre class="source-code">
#!/usr/bin/env bash
# navigate to directory
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH
cd ..
export SECRET_KEY="secret"
export EXPIRE_MINUTES=60
export DB_URL="postgres://username:password@localhost:5433/to_do"
export REDIS_URL="redis://127.0.0.1/"
cargo run</pre>
			<p>However, if we try<a id="_idIndexMarker1219"/> and run the preceding script, it will crash because we cannot connect to the Redis database. We need to define our dev services in the <code>docker-compose.yml</code> file with the following code:</p>
			<pre class="source-code">
version: "3.7"
services:
  postgres:
    container_name: 'to-do-postgres'
    image: 'postgres:11.2'
    restart: always
    ports:
      - '5433:5432'
    environment:
      - 'POSTGRES_USER=username'
      - 'POSTGRES_DB=to_do'
      - 'POSTGRES_PASSWORD=password'
  redis:
      container_name: 'to-do-redis'
      image: 'redis:5.0.5'
      ports:
        - '6379:6379'</pre>
			<p>Now that our dev services have been defined, we can spin up our <code>docker-compose</code> and run our <code>run_dev_server.sh</code> script, resulting in our dev server running. However, if we<a id="_idIndexMarker1220"/> try and perform any requests, the server will crash. This is because we have not performed migrations on the database. In the next section, we will perform migrations on our dev database.</p>
			<h1 id="_idParaDest-268"><a id="_idTextAnchor269"/>Setting up a local development database</h1>
			<p>When it comes to <a id="_idIndexMarker1221"/>migrations, there is the advantage of decoupling the programming language that we are using from the migrations. In the past, I have had to switch servers from one language to another and simply wished that the migration’s implementation was not coupled with the language. This is also a deployment issue. For instance, in Kubernetes, deploying a new server or an update might require a migration to be run. Ideally, you want to run the migration automatically through what we call <em class="italic">init Pods</em>. This is a container that is spun up and executed before the main server is deployed. This init Pod can perform a database migration command. However, if the init Pod requires something such as Rust to be present to execute the migration, this can greatly increase the size of the init pod. Therefore, I built an open source Bash tool that is only dependent on the <code>psql</code> and <code>wget</code> libraries. It can create new migrations and roll the database up and down versions. However, it must be stressed that this tool is not for every use. To quote the documentation of the migrations tool I wrote (<a href="https://github.com/yellow-bird-consult/build_tools/tree/develop#use-yb-database-migrations-if-you-have-the-following)">https://github.com/yellow-bird-consult/build_tools/tree/develop#use-yb-database-migrations-if-you-have-the-following)</a>, you should choose to use the migrations tool for projects if you have the following:</p>
			<ul>
				<li><strong class="bold">A light throughput of migrations</strong>: Migrations are not timestamped; they are simply <a id="_idIndexMarker1222"/>numbered. The design of the tool is simple to keep track of what’s going on. Light applications in microservices are an ideal environment.</li>
				<li><strong class="bold">Well-tested code</strong>: There are no guardrails. If there is an error in part of your SQL script, your database will be scarred with a partly run migration. You should have testing regimes with Docker databases before implementing migrations on a live production database.</li>
				<li><strong class="bold">You plan on writing your own SQL</strong>: Because this tool is completely decoupled from any programming language, you have to write your own SQL scripts for each migration. This is not as daunting as you might think and gives you more control.</li>
				<li><strong class="bold">You want complete control</strong>: SQL migrations and the simple implementation are essentially defined in a single Bash script. This simple implementation gives you 100% control. Nothing is stopping you from opening up your database in a GUI and directly altering the version number or manually running particular sections of the migration.</li>
			</ul>
			<p>Now that we know what we are getting ourselves in for, we can navigate to the <code>database</code> directory and install the migration tool with the following command:</p>
			<pre class="console">
wget -O - https://raw.githubusercontent.com/yellow-bird-consult
/build_tools/develop/scripts/install.sh | bash</pre>
			<p>This installs a couple of Bash scripts in your home directory. You may have to refresh your terminal to get the command alias. Not all operating systems will support the command alias. If your command alias does work, we can create a new set of migrations by using the following command:</p>
			<pre class="console">
yb db init</pre>
			<p>However, if the alias does not work, you can run all your commands through the Bash script as each Bash script is 100% self-contained. All we must do is pass the same arguments in with the following command:</p>
			<pre class="console">
bash ~/yb_tools/database.sh db init</pre>
			<p>With the <code>init</code> command, we get the following structure:</p>
			<pre class="source-code">
├── database_management
│   └── 1
│       ├── down.sql
│       └── up.sql</pre>
			<p>This is the same<a id="_idIndexMarker1223"/> as the Diesel migrations tool but with just plain numbers. We have two migrations from our to-do application, so we can create them with the following commands:</p>
			<pre class="console">
cp -r database_management/1 database_management/2</pre>
			<p>Once we have done this, we can create our migration files. The <code>database_management/1/up.sql</code> file creates the <code>to_do</code> table with the following code:</p>
			<pre class="source-code">
CREATE TABLE to_do (
  id SERIAL PRIMARY KEY,
  title VARCHAR NOT NULL,
  status VARCHAR NOT NULL,
  date timestamp NOT NULL DEFAULT NOW()
)</pre>
			<p>The <code>database_management/1/down.sql</code> file drops the <code>to_do</code> table with the following code:</p>
			<pre class="source-code">
DROP TABLE to_do</pre>
			<p>The <code>database_management/2/up.sql</code> file creates the <code>user</code> table and links all existing items to a placeholder user with the following code:</p>
			<pre class="source-code">
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR NOT NULL,
    email VARCHAR NOT NULL,
    password VARCHAR NOT NULL,
    unique_id VARCHAR NOT NULL,
    UNIQUE (email),
    UNIQUE (username)
);
INSERT INTO users (username, email, password, unique_id)
VALUES ('placeholder', 'placeholder email',
'placeholder password', 'placeholder unique id');
ALTER TABLE to_do ADD user_id integer default 1
CONSTRAINT user_id REFERENCES users NOT NULL;</pre>
			<p>The <code>database_management/2/down.sql</code> file drops the <code>users</code> table with the following<a id="_idIndexMarker1224"/> code:</p>
			<pre class="source-code">
ALTER TABLE to_do DROP COLUMN user_id;
DROP TABLE users</pre>
			<p>Our migrations are now ready. However, we need to connect to our database to get information and perform migrations. We can spin up our <code>docker-compose</code> to get the dev database up and running. Once this is done, we must define our database URL in the environment variables. The migration tool looks for the URL in environment variables. However, if there is a <code>.env</code> file in the current working directory, the migration tool will also load all the variables in this file. In our <code>database_management/.env</code> file, we can define the database URL with the following code:</p>
			<pre class="source-code">
DB_URL="postgres://username:password@localhost:5433/to_do"</pre>
			<p>Now that our database is running and we have our URL defined, we can get what migration level the database is currently at with the following command:</p>
			<pre class="console">
# with alias
yb db get
# without alias
bash ~/yb_tools/database.sh db get</pre>
			<p>Right now, we should get a <code>-1</code>. This means that there is no migrations versioning table at all on the database. If there is, but no migrations have been performed on the database, the version will be <code>0</code>. If there are any migrations, then the response will be the migration number that it is currently at. We can use the following <code>db</code> commands when using the build<a id="_idIndexMarker1225"/> tool to perform commands on the database:</p>
			<ul>
				<li><code>set</code>: Creates a migrations version table if there is not one</li>
				<li><code>up</code>: Goes up one migration version by applying the <code>up.sql</code> script</li>
				<li><code>down</code>: Goes down one migration version by applying the <code>down.sql</code> script</li>
				<li><code>new</code>: Creates a new migration folder if you are on the latest version</li>
				<li><code>rollup</code>: Creates a new migrations version table if there is not one and then loops up all the versions in the <code>database_management</code> directory, starting from the current version of the database</li>
			</ul>
			<p>We will run the <code>rollup</code> command with the following command:</p>
			<pre class="console">
# with alias
yb db rollup
# without alias
bash ~/yb_tools/database.sh db rollup</pre>
			<p>This will perform migrations on the database. If you run the <code>get</code> command, you will see that the version of the database is now <code>2</code>. Our database is now ready to be queried by our application.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Migrations can also be achieved using the <code>sqlx-cli</code> crate, which can be found at the following link: https://crates.io/crates/sqlx-cli.</p>
			<p class="callout">However, Cargo is needed to install <code>sqlx-cli</code>, which will complicate the creation of init Pods for executing these migrations.</p>
			<p>Instead of randomly making requests, in the next section, we will refine our Postman tests so that we <a id="_idIndexMarker1226"/>can run a series of requests and check that our application runs in the way that we want it to.</p>
			<h1 id="_idParaDest-269"><a id="_idTextAnchor270"/>Managing variables in Postman tests</h1>
			<p>In <a href="B18722_09.xhtml#_idTextAnchor182"><em class="italic">Chapter 9</em></a>, <em class="italic">Testing Our Application Endpoints and Components</em>, we built a Postman<a id="_idIndexMarker1227"/> collection. However, it was a bit ropey as we had to rely on Python to load the new token into the Newman collection. While this was important to use as using Python as glue code between processes is a useful skill, our old version of readying a Newman collection with Python is not the cleanest approach. At the start of our collection, we will add two new requests. The first one will create a user with the following parameters:</p>
			<div><div><img src="img/Figure_13.1_B18722.jpg" alt="Figure 13.1 – Create user Postman request"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – Create user Postman request</p>
			<p>With the create user request, we get the following JavaScript in the <strong class="bold">Tests</strong> tab in Postman:</p>
			<pre class="source-code">
pm.test("response is created", function () {
    pm.response.to.have.status(201);
});</pre>
			<p>With this, the first request of our collection will create the user and throw an error if the request was not successful. Then, we can create the second request for our collection, which consists <a id="_idIndexMarker1228"/>of logging in, with the following parameters:</p>
			<div><div><img src="img/Figure_13.2_B18722.jpg" alt="Figure 13.2 – Login Postman request"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – Login Postman request</p>
			<p>With this request, we must check the response and set a collection variable as the token that we just got from the login by running the following JavaScript in the <strong class="bold">Tests</strong> tab in Postman:</p>
			<pre class="source-code">
var result = pm.response.json()
pm.test("response is ok", function () {
    pm.response.to.have.status(200);
});
pm.test("response returns token", function () {
    pm.collectionVariables.set("login_token", result["token"]);
})</pre>
			<p>Once we have set our collection variable, we will be able to reference our token throughout the rest of the collection. To do this, we must update the authorization for the entire collection so that our new token value will propagate through all our requests. To access<a id="_idIndexMarker1229"/> the authorization settings, click on the header of a <code>create</code> request to get the following:</p>
			<div><div><img src="img/Figure_13.3_B18722.jpg" alt="Figure 13.3 – Header of the request"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – Header of the request</p>
			<p>On the right-hand side of the previous screenshot, we can see that there is a <strong class="bold">Go to authorization</strong> button. If we click on this, we get the following:</p>
			<div><div><img src="img/Figure_13.4_B18722.jpg" alt="Figure 13.4 – Configuring authorization"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – Configuring authorization</p>
			<p>We can see that the<a id="_idIndexMarker1230"/> value has been changed to <code>{{login_token}}</code>. If we save this and then export the collection JSON file to the <code>tests</code> directory in our repository, the value that belongs to <code>{{login_token}}</code> will be inserted into the collection JSON file.</p>
			<p>We now have a Postman collection that updates itself with a fresh token after the login request without having to rely on Python to glue processes together. This is much cleaner; however, we want to ensure that the rest of our testing pipeline mimics as much of a production setting as possible. In the next section, we will build Docker images that contain our application that are a fraction of the size of what our server images were in previous chapters.</p>
			<h1 id="_idParaDest-270"><a id="_idTextAnchor271"/>Building distroless tiny server Docker images</h1>
			<p>In previous chapters, our <a id="_idIndexMarker1231"/>server Docker images were roughly around 1.5 GB. This is pretty big and not ideal when we want to distribute our Rust images on servers or to other developers. Note that there is a shell that we can access in the Docker container when the image is running. This is useful in development but not great in production because if anyone manages to gain access to the Docker container, they will be able to look around and run commands in the Docker container. If the permissions on the server are not locked down, the hacker could even start running commands on the cluster that you have. I have seen cryptojacking happen through this method, where a hacker spun up a load of mining Pods at the expense of the owner of the AWS account.</p>
			<p>We are going to solve these problems by using distroless images. These distroless images are tiny in size and do not have shells. So, if someone manages to gain access to our server, they will not be able to do anything because there are no shells. We will be able to drop the size of our image from 1.5 GB to 45 MB! This is something we want. However, before we start building our distroless images, we must know that distroless images have close to nothing on them. This means that if we compile our application and stuff it into a distroless image, it will not work. For instance, if we make a connection to a database, we need the <code>libpq</code> library in our distroless image. As the distroless image does not contain the library, the image will not be able to run because our static binary will not be able to locate the <code>libpq</code> library.</p>
			<p>We know that our 1.5 GB image runs because it contains everything and the kitchen sink. We can use our 1.5 GB to inspect what dependencies the static binary has in the image. We can do this by moving to our <code>deployment</code> directory where we wrote code to deploy our application on AWS and spinning up <code>docker-compose</code> there. Once this is running, we can inspect our containers with the following command:</p>
			<pre class="console">
docker container ls</pre>
			<p>This will give us the following output:</p>
			<pre class="console">
CONTAINER ID   IMAGE                 . . .
0ae94ab0bbc5   nginx:latest          . . .
6b49526250e3   deployment_rust_app.  . . .
9f4dcdc8a455   redis:5.0.5           . . .</pre>
			<p>Your IDs will be different, but we will use these IDs to SSH into our Rust app by using the following command:</p>
			<pre class="console">
docker exec -it 6b49526250e3 /bin/bash</pre>
			<p>This opens an interactive shell so that we can navigate the Docker container. Here, we must remember that the static binary – that is, the Rust server – is called <code>web_app</code> and that this is in the root directory, so we do not need to go anywhere within the container. We can list the dependencies by using the following command:</p>
			<pre class="console">
ldd web_app</pre>
			<p>This will give us the following output:</p>
			<pre class="console">
linux-vdso.so.1 (0x0000ffffb8a9d000)
libpq.so.5 =&gt; /usr/lib/aarch64-linux-gnu/libpq.so.5
libgcc_s.so.1 =&gt; /lib/aarch64-linux-gnu/libgcc_s.so.1
libpthread.so.0 =&gt; /lib/aarch64-linux-gnu/libpthread.so.0
libm.so.6 =&gt; /lib/aarch64-linux-gnu/libm.so.6
. . .</pre>
			<p>There are 29 dependencies <a id="_idIndexMarker1232"/>in total. On the left of the list, there is the name of the library. On the right of the list, there is the path to where the library is. We can see that the database <code>libpq</code> library is needed alongside other libraries. Your paths may look different. This is because I am running this image on a MacBook M1, which has an ARM chip architecture. If you do not have this, then you will have <code>x86_64-linux-gnu</code> in your path as opposed to <code>aarch64-linux-gnu</code>. This is fine – we will supply both Docker files in the GitHub repository online.</p>
			<p>In our Docker build, we must copy these libraries into our distroless image. In our <code>clean_web_app/builds</code> directory, we must create two files: <code>aarch64_build</code> and <code>x86_64_build</code>. Both these files are essentially the same Dockerfiles but with different references to libraries. At the time of writing, I wish that there was a smarter way to achieve builds with different chips in one Docker file; however, Docker builds are terrible at passing variables throughout the build as each step is isolated, and conditional logic is limited at best. It is easier to just have two different files. Also, if the builds change in the future, then the two different chip builds are decoupled. In our <code>clean_web_app/builds/arch_build</code> file, we must get the Rust image, install the database library, copy over the code of the application to be compiled, and define what type of build we are doing:</p>
			<pre class="source-code">
FROM rust:1.62.1 as build
RUN apt-get update
RUN apt-get install libpq5 -y
WORKDIR /app
COPY . .
ARG ENV="PRODUCTION"
RUN echo "$ENV"</pre>
			<p>We can see that the environment is set to <code>"PRODUCTION"</code> by default. If there is an accident and the environment is not defined, it should be <code>"PRODUCTION"</code> by default. Accidentally taking<a id="_idIndexMarker1233"/> longer to compile on a test build is much better than accidentally deploying a non-production server into production. Then, we compile using the release flag if it is production and switch the static binary into the release directory if it is not compiled using the release flag:</p>
			<pre class="source-code">
RUN if [ "$ENV" = "PRODUCTION" ] ; then cargo build --release ; \
else cargo build ; fi
RUN if [ "$ENV" = "PRODUCTION" ] ; then echo "no need to copy" ; \
else mkdir /app/target/release/ &amp;&amp; cp /app/target/debug/clean_app \
/app/target/release/clean_app ; fi</pre>
			<p>At this point, our application has been compiled. Everything we have covered is independent of what type of chip we are using, so the <code>x86_64_build</code> file will contain the same code that we have just laid out in the <code>aarch64_build</code> file. For both build files, we can also get our distroless image with the following code:</p>
			<pre class="source-code">
FROM gcr.io/distroless/cc-debian10</pre>
			<p>Now, this is where the build scripts differ. In the ARM chip build, we must copy the libraries needed from the previous Rust image into our distroless image, like so:</p>
			<pre class="source-code">
COPY --chown=1001:1001 --from=build \
/usr/lib/aarch64-linux-gnu/libpq.so.5 \
/lib/aarch64-linux-gnu/libpq.so.5
. . .
COPY --chown=1001:1001 --from=build \
/lib/aarch64-linux-gnu/libcom_err.so.2 \
/lib/aarch64-linux-gnu/libcom_err.so.2</pre>
			<p>Including them all <a id="_idIndexMarker1234"/>would simply provide needless bloat for the book and again, these files are available in this book’s GitHub repository. What we must note, however, is that the directory for the first part of each copy is the directory listed when we explored the Docker image of our large application. The second part is the same path; however, if there is a <code>/usr/lib/</code> at the start of the path, it is shortened to <code>/lib/</code>. There is no shell or users in the distroless image.</p>
			<p>Once all the libraries have been copied over, we must copy the static binary of our web application into the root of our image, expose the port, and define the entry point, which is the static binary, with the following code:</p>
			<pre class="source-code">
COPY --from=build /app/target/release/clean_app \
/usr/local/bin/clean_app
EXPOSE 8000
ENTRYPOINT ["clean_app"]</pre>
			<p>With this, our distroless image is done. Right now, both builds are stored away, and we will get them out depending on the chip type in a bash script to be built.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We don’t have to build our distroless application manually. Instead, we can use Apko via the following link: <a href="https://github.com/chainguard-dev/apko">https://github.com/chainguard-dev/apko</a>.</p>
			<p>You can copy your chosen build to the root directory of the repository under the <code>Dockerfile</code> filename. Then, run the following command:</p>
			<pre class="console">
docker build . -t clean_app</pre>
			<p>When you list your <a id="_idIndexMarker1235"/>Docker images, you will see that this image is 46.5 MB! This is a massive reduction from 1.5 GB. In the next section, we will include these build files in a test pipeline.</p>
			<h1 id="_idParaDest-271"><a id="_idTextAnchor272"/>Building a clean test pipeline</h1>
			<p>When it comes to <a id="_idIndexMarker1236"/>testing our application, we want to package it in the Docker image that we wish to deploy onto the servers, run migrations on the database as we would on the servers, and run a series of Postman requests and tests to mimic a user making a series of requests. This can be orchestrated with one Bash script in the <code>scripts/run_full_release_test.sh</code> file. First, we must find out what chip we are running on with the following code:</p>
			<pre class="source-code">
#!/usr/bin/env bash
# navigate to directory
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH
if [ "$(uname -m)" = "arm64" ]
then
    cp ../builds/aarch64_build ../Dockerfile
else
    cp ../builds/x86_64_build ../Dockerfile
fi</pre>
			<p>Here, we pull the correct build depending on the type of chip. Depending on the computer you are using, this might be different. I am using a Mac M1, so when I call the <code>uname -m</code> command in the terminal, I get an <code>arm64</code> output. If you are not using an arch or ARM chip, you do not need the conditional logic. Instead, you just need to pull the <code>x86_64_build</code> file. Then, we must move to the <code>tests</code> directory and build our <code>docker-compose</code> with the following code:</p>
			<pre class="source-code">
cd ../tests
# build the images and network
docker-compose build --no-cache
docker-compose up -d
# wait until rust server is running
sleep 5</pre>
			<p>We are now ready to<a id="_idIndexMarker1237"/> run our tests and clean up the images with the following code:</p>
			<pre class="source-code">
# run the api tests
newman run to_do_items.postman_collection.json
# destroy the container and image
docker-compose down
docker image rm test_server
docker image rm init_test_db
docker image rm test_postgres
rm ../Dockerfile</pre>
			<p>Before we run this, however, we need to build our <code>docker-compose</code> in our <code>tests</code> directory. Our <code>tests/docker-compose.yml</code> file has the following outline:</p>
			<pre class="source-code">
version: "3.7"
services:
    test_server:
      . . .
    test_postgres:
      . . .
    test_redis:
      . . .
    init_test_db:
        . . .</pre>
			<p>First, we will focus on the test server. Seeing as we are running a test, we need to point to the build, pass a <code>NOT PRODUCTION</code> argument into the build, define the environment<a id="_idIndexMarker1238"/> variables for the server to utilize, and then wait for the Redis database to be operational before spinning it up. We can do this with the following code:</p>
			<pre class="source-code">
test_server:
  container_name: test_server
  image: test_auth_server
  build:
    context: ../
    args:
      ENV: "NOT_PRODUCTION"
  restart: always
  environment:
    - 'DB_URL=postgres://username:password@test_postgres:54
          32/to_do'
    - 'SECRET_KEY=secret'
    - 'EXPIRE_MINUTES=60'
    - 'REDIS_URL=redis://test_redis/'
  depends_on:
      test_redis:
        condition: service_started
  ports:
    - "8000:8000"
  expose:
    - 8000</pre>
			<p>As we can see, <code>docker-compose</code> is a powerful tool. A few tags can result in some complex <a id="_idIndexMarker1239"/>orchestration. Then, we can move to our database and Redis containers with the following code:</p>
			<pre class="source-code">
test_postgres:
  container_name: 'test_postgres'
  image: 'postgres'
  restart: always
  ports:
    - '5433:5432'
  environment:
    - 'POSTGRES_USER=username'
    - 'POSTGRES_DB=to_do'
    - 'POSTGRES_PASSWORD=password'
test_redis:
  container_name: 'test_redis'
  image: 'redis:5.0.5'
  ports:
    - '6379:6379'</pre>
			<p>These databases are nothing new. However, in the last service, we create an init container that spins up briefly just to run the migrations on the server:</p>
			<pre class="source-code">
init_test_db:
    container_name: init_test_db
    image: init_test_db
    build:
      context: ../database
    environment:
      - 'DB_URL=postgres://username:password@test_postgres:
            5432/to_do'
    depends_on:
        test_postgres:
          condition: service_started
    restart: on-failure</pre>
			<p>As we can see, there <a id="_idIndexMarker1240"/>must be a Docker build in the <code>database</code> directory for our init container to make a database migration before closing. This means that our init container must have <code>psql</code> installed, our migrations tool, and the <code>rollup</code> command as the entry point. Initially, we install what we need in our <code>database/Dockerfile</code> file with the following code:</p>
			<pre class="source-code">
FROM postgres
RUN apt-get update \
  &amp;&amp; apt-get install -y wget \
  &amp;&amp; wget -O - https://raw.githubusercontent.com/\
  yellow-bird-consult/build_tools/develop/scripts/\
  install.sh | bash \
  &amp;&amp; cp ~/yb_tools/database.sh ./database.sh</pre>
			<p>Here, we can see that we get the <code>psql</code> library from the <code>postgres</code> Docker image. Then, we install <code>wget</code> and use this to install our migrations build tool. Finally, we copy the <code>database.sh</code> Bash script from the home directory into the root directory of the image so that we do not have to worry about aliases. Once we have configured our installments, we must copy the migrations SQL files from the current directory into the root directory of the image and define the migration command as the entry point:</p>
			<pre class="source-code">
WORKDIR .
ADD . .
CMD ["bash", "./database.sh", "db", "rollup"]</pre>
			<p>This will work<a id="_idIndexMarker1241"/> fine; however, we do have to define a <code>database/.dockerignore</code> file with the following content to avoid the environment variable being passed into the image:</p>
			<pre class="source-code">
.env</pre>
			<p>If we do not stop this environment variable from being copied into the image, then whatever variables we pass into the init container through <code>docker-compose</code> could get overwritten.</p>
			<p>We now have everything we need in place, so all we must do is run our <code>scripts/run_full_release.sh</code> script. This will produce a lengthy printout of building the images, spinning up <code>docker-compose</code>, and running the API tests via Newman. The last output should look like this:</p>
			<div><div><img src="img/Figure_13.5_B18722.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5 – Result of a full test run</p>
			<p>We can see that all the tests ran and passed. Our distroless build works and our init container for the database also makes the migrations. Nothing is stopping us from putting this infrastructure on AWS, with the difference of pointing to images on Docker Hub as opposed to local builds. Considering how small our distroless server is, pulling the image from Docker Hub and spinning it up will be very quick.</p>
			<p>We’ve now got <a id="_idIndexMarker1242"/>all the ingredients to build continuous integration for our GitHub repository to ensure that tests are run when we create pull requests. In the next and final section, we will configure continuous integration through <strong class="bold">GitHub Actions</strong>.</p>
			<h1 id="_idParaDest-272"><a id="_idTextAnchor273"/>Building continuous integration with GitHub Actions</h1>
			<p>When it <a id="_idIndexMarker1243"/>comes to ensuring <a id="_idIndexMarker1244"/>that code quality is maintained, it can be handy to have a continuous integration pipeline that will run every time a pull request is done. We can do this with GitHub Actions. It must be noted that with GitHub Actions, you get several free minutes every month; then, you must pay for the minutes you go over. So, be careful and keep an eye on how much time you’re spending using GitHub Actions.</p>
			<p>GitHub Actions gives us flexibility when it comes to implementing tasks. We can run workflows when a pull request is merged or made and when an issue is created and much more. We can also be selective about the type of branches we use. In this example, we will merely focus on a pull request on any branch to run unit tests and then full integration tests.</p>
			<p>To build a workflow called <code>tests</code>, we need to create a file called <code>.github/workflows/run-tests.yml</code>. In this file, we will define the general outline of the <a id="_idIndexMarker1245"/>unit <a id="_idIndexMarker1246"/>and integration tests with the following code:</p>
			<pre class="source-code">
name: run tests
on: [pull_request]
jobs:
  run-unit-tests:
    . . .
  run-integration-test:
    . . .</pre>
			<p>Here, we have defined the name of the workflow and the conditions that the workflow is triggered on pull requests for all branches. Then, we define two jobs – one to run unit tests and the other to run integration tests.</p>
			<p>Each job has steps. We can also define dependencies for our steps. We can define our unit test job with the following code:</p>
			<pre class="source-code">
run-unit-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v2
      - uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
      - name: run the unit test
        run: |
          export SECRET_KEY="secret"
          export EXPIRE_MINUTES=60
          cargo test</pre>
			<p>Here, we used the <code>checkout</code> action. If we do not use the <code>checkout</code> action, we will not be able to access any of the files in the GitHub repository. Then, we export the environment<a id="_idIndexMarker1247"/> variables<a id="_idIndexMarker1248"/> that are needed for the unit tests to run, and then we run the unit tests using Cargo. Also, note that we define a timeout. Defining a timeout is important just in case something ends up in a loop and you do not burn all your minutes in one job.</p>
			<p>Now, let’s move on to our integration test job:</p>
			<pre class="source-code">
run-integration-test:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v2
      - name: create environment build and run newman
        run: |
          cd tests
          cp ../builds/server_build ../Dockerfile
          docker-compose build --no-cache
          docker-compose up -d
          sleep 5
      - uses: actions/checkout@master
      - uses: matt-ball/newman-action@master
        with:
          collection:
              ./tests/cerberus.postman_collection.json</pre>
			<p>Here, we move into the <code>tests</code> directory, get the server build Docker file, spin up <code>docker-compose</code>, and then use the <code>newman</code> action to run the Newman tests. If we make a pull request, the actions will be shown on the pull request. If we click on the GitHub Actions <a id="_idIndexMarker1249"/>button, we<a id="_idIndexMarker1250"/> can access the status and results, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_13.6_B18722.jpg" alt="Figure 13.6 – GitHub Actions options"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.6 – GitHub Actions options</p>
			<p>Then, we can click on the test to see the steps of the job, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_13.7_B18722.jpg" alt="Figure 13.7 – GitHub Actions job view"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.7 – GitHub Actions job view</p>
			<p>Now, if we click on <a id="_idIndexMarker1251"/>a <a id="_idIndexMarker1252"/>step in the job, it will expand. We will see that our Newman tests work:</p>
			<div><div><img src="img/Figure_13.8_B18722.jpg" alt="Figure 13.8 – Newman step result"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.8 – Newman step result</p>
			<p>As we can see, our <a id="_idIndexMarker1253"/>continuous<a id="_idIndexMarker1254"/> integration works! We have now come to the end of this chapter as our repository is clean and functional.</p>
			<h1 id="_idParaDest-273"><a id="_idTextAnchor274"/>Summary</h1>
			<p>We have finally made it to the end of structuring a web application in Rust and building the infrastructure around the application to make ongoing development of new features safe and easy to integrate. We have structured our repository into one that’s clean and easy to use where directories have individual purposes. Like in well-structured code, our well-structured repository can enable us to slot tests and scripts in and out of the repository easily. Then, we used pure bash to manage migrations for our database without any code dependencies so that we can use our migrations on any application, regardless of the language being used. Then, we built init containers to automate database migrations, which will work even when deployed on a server or cluster. We also refined the Docker builds for our server, making them more secure and reducing the size from 1.5 GB to 45 MB. After, we integrated our builds and tests into an automated pipeline that is fired when new code is merged into the GitHub repository.</p>
			<p>This brings a natural end to building a web application and deploying it on a server. In the following chapters, we will dive deeper into web programming with Rust, looking at lower-level frameworks so that we can build custom protocols over TCP sockets. This will enable you to build lower-level applications for web servers or even local processes. In the next chapter, we will explore the Tokio framework, a building block of async programs such as TCP servers.</p>
			<h1 id="_idParaDest-274"><a id="_idTextAnchor275"/>Further reading</h1>
			<ul>
				<li>Database migrations documentation and repository: <a href="https://github.com/yellow-bird-consult/build_tools">https://github.com/yellow-bird-consult/build_tools</a></li>
				<li>GitHub Actions documentation: <a href="https://docs.github.com/en/actions/guides">https://docs.github.com/en/actions/guides</a></li>
			</ul>
			<h1 id="_idParaDest-275"><a id="_idTextAnchor276"/>Questions</h1>
			<ol>
				<li>The bash migrations tool uses incremental single-digit integers to denote migrations. What is the big downside to this?</li>
				<li>Why are distroless servers more secure?</li>
				<li>How did we remove the need for Python when running our Newman tests that required a fresh token?</li>
				<li>What are the advantages of using environment variables for configuration values?</li>
			</ol>
			<h1 id="_idParaDest-276"><a id="_idTextAnchor277"/>Answers</h1>
			<ol>
				<li value="1">Using incremental single-digit integers exposes the migrations to clashes. So, if one developer writes migrations on one branch while another developer writes migrations on a different branch, there will be a conflict of migrations when they both merge. GitHub should pick this up, but it’s important to keep the traffic of migrations low, plan out database alterations properly, and keep the services using the migrations small. If this is a concern for you, however, please use a different migrations tool that is heavier but has more guardrails.</li>
				<li>Distroless servers do not have shells. This means that if a hacker manages to access our server container, they cannot run any commands or inspect the contents of the container.</li>
				<li>In the login request, we get the token that is returned from the server in the test script and assign it to a collection variable that can be accessed by other requests, removing the reliance on Python.</li>
				<li>Environment variables are simply easier to implement when deploying our application to the cloud. For instance, Kubernetes’s ConfigMaps use environment variables to pass variables into Docker containers. It is also easier to implement services such as Secrets Manager on AWS by using environment variables.</li>
			</ol>
		</div>
	

		<div><h1 id="_idParaDest-277"><a id="_idTextAnchor278"/>Part 6:Exploring Protocol Programming and Async Concepts with Low-Level Network Applications</h1>
			<p>Web programming has evolved to more than just simple applications that interact with databases. In this part, we cover more advanced concepts with async Rust by covering the basics of async Rust, Tokio, and Hyper. With Tokio and Hyper, we leverage async Rust and the actor model to implement async designs such as passing messages between actors in different threads, queuing tasks in Redis to be consumed by multiple workers, and processing byte streams with Tokio framing and TCP ports. By the end of this part, you will be able to implement more complex event-processing solutions on your server to handle more complex problems. You will also have practical knowledge of how to implement async Rust, which is an up-and-coming field.</p>
			<p>This part includes the following chapters:</p>
			<ul>
				<li><a href="B18722_14.xhtml#_idTextAnchor279"><em class="italic">Chapter 14</em></a>, <em class="italic">Exploring the Tokio Framework</em></li>
				<li><a href="B18722_15.xhtml#_idTextAnchor291"><em class="italic">Chapter 15</em></a>, <em class="italic">Accepting TCP Traffic with Tokio</em></li>
				<li><a href="B18722_16.xhtml#_idTextAnchor306"><em class="italic">Chapter 16</em></a>, <em class="italic">Building Protocols on Top of TCP</em></li>
				<li><a href="B18722_17.xhtml#_idTextAnchor323"><em class="italic">Chapter 17</em></a>, <em class="italic">Implementing Actors and Async with the Hyper Framework</em></li>
				<li><a href="B18722_18.xhtml#_idTextAnchor335"><em class="italic">Chapter 18</em></a>, <em class="italic">Queuing Tasks with Redis</em></li>
			</ul>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
	</body></html>
<html><head></head><body>
		<div id="_idContainer177">
			<h1 id="_idParaDest-263" class="chapter-number"><a id="_idTextAnchor264"/>13</h1>
			<h1 id="_idParaDest-264"><a id="_idTextAnchor265"/>Best Practices for a Clean Web App Repository</h1>
			<p>Throughout this book, we have been building out our application piece by piece and adding automation scripts and tools to help us with testing and deploying our application. However, although this path is useful for learning tools and concepts, the structure of our projects in previous chapters has not been optimal for running a project <span class="No-Break">for production.</span></p>
			<p>In this chapter, we will create a new repository, lift our Rust code into that repository, and then structure the code for clean database migrations, tests, and optimized Docker builds for our application so that it can be <span class="No-Break">deployed smoothly.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>The general layout of a <span class="No-Break">clean repository</span></li>
				<li>Getting our configuration from <span class="No-Break">environment variables</span></li>
				<li>Setting up a local <span class="No-Break">development database</span></li>
				<li>Managing variables in <span class="No-Break">Postman tests</span></li>
				<li>Building distroless tiny server <span class="No-Break">Docker images</span></li>
				<li>Building a clean <span class="No-Break">test pipeline</span></li>
				<li>Building continuous integration with <span class="No-Break">GitHub Actions</span></li>
			</ul>
			<p>By the end of this chapter, you will be able to structure a repository with scripts, Docker builds, and tests that will make development smooth and easy to add new features. You will also be able to build <strong class="bold">distroless</strong> Docker images for the application, making them secure and dropping the size of our server images from 1.5 GB to <span class="No-Break">45 MB!</span></p>
			<h1 id="_idParaDest-265"><a id="_idTextAnchor266"/>Technical requirements</h1>
			<p>In this chapter, we will be referencing parts of the code defined in <a href="B18722_09.xhtml#_idTextAnchor182"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Testing Our Application Endpoints and Components</em>. This can be found at the following <span class="No-Break">URL: </span><a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter09"><span class="No-Break">https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter09</span></a><span class="No-Break">.</span></p>
			<p>The code for this chapter can be found <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter13"><span class="No-Break">https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter13</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-266"><a id="_idTextAnchor267"/>The general layout of a clean repository</h1>
			<p>When it comes to a <a id="_idIndexMarker1210"/>clean layout, we must have directories in the repository that have a single focus, just like our isolated code, which is modularized. In the clean approach taken in this chapter, our repository will have the <span class="No-Break">following layout:</span></p>
			<pre class="source-code">
├── Cargo.toml
├── README.md
├── .dockerignore
├── .gitignore
├── .github
│   . . .
├── builds
│   . . .
├── database
│   . . .
├── docker-compose.yml
├── scripts
│   . . .
├── src
│   . . .
└── tests
    . . .</pre>
			<p>These files and directories have the <span class="No-Break">following responsibilities:</span></p>
			<ul>
				<li><strong class="source-inline">Cargo.toml</strong>: Defines the requirements for the <span class="No-Break">Rust build.</span></li>
				<li><strong class="source-inline">README.md</strong>: Renders on a GitHub page when visited, telling the reader what the project is about and how to interact with <span class="No-Break">the project.</span></li>
				<li><strong class="source-inline">.dockerignore</strong>: Tells the Docker build what to ignore when copying directories and files into the <span class="No-Break">Docker image.</span></li>
				<li><strong class="source-inline">.gitignore</strong>: Tells git what to ignore when committing code to the <span class="No-Break">git repository.</span></li>
				<li><strong class="source-inline">.github</strong>: A directory that houses GitHub <span class="No-Break">Actions workflows.</span></li>
				<li><strong class="source-inline">builds</strong>: A directory that houses different builds for Docker, depending on the <span class="No-Break">chip architecture.</span></li>
				<li><strong class="source-inline">database</strong>: A directory <a id="_idIndexMarker1211"/>that houses all scripts and Docker builds required to handle <span class="No-Break">database migrations.</span></li>
				<li><strong class="source-inline">docker-compose.yml</strong>: Defines the containers needed to run a <span class="No-Break">development build.</span></li>
				<li><strong class="source-inline">scripts</strong>: A directory that houses all the Bash scripts needed to run dev servers <span class="No-Break">or tests.</span></li>
				<li><strong class="source-inline">src</strong>: A directory that houses all the Rust code to build <span class="No-Break">the server.</span></li>
				<li><strong class="source-inline">tests</strong>: A directory that houses a <strong class="source-inline">docker-compose</strong> configuration and a Postman collection to enable a fully integrated test. We must remember that unit tests are coded within the <strong class="source-inline">src</strong> directory and are conditionally compiled when the <strong class="source-inline">test</strong> command in Cargo is executed. In standard and release builds, the unit tests <span class="No-Break">are excluded.</span></li>
			</ul>
			<p>Now that we know what our repository structure is like, we can add some rules and files to ensure that our builds and git commits behave in exactly the right way. It is good to do this at the very start of a project to avoid accidentally adding unwanted code to the git history or <span class="No-Break">Docker builds.</span></p>
			<p>First, we will start with the <strong class="source-inline">.gitignore</strong> file, which has the following <span class="No-Break">rules defined:</span></p>
			<pre class="source-code">
/target/
Cargo.lock
# These are backup files generated by rustfmt
**/*.rs.bk
# jetbrains
.idea
# mac
.DS_Store</pre>
			<p>Here, we can see that <a id="_idIndexMarker1212"/>we avoid anything in the <strong class="source-inline">target</strong> directory, which gets filled up with a lot of files when performing Rust builds and tests. These files add nothing to the project’s development and will balloon the size of your project very quickly. If you like to use JetBrains or are using a Mac, I have added <strong class="source-inline">.idea</strong> and <strong class="source-inline">.DS_Store</strong> as these files can sneak into repositories; they are not required for running any of the web <span class="No-Break">application’s code.</span></p>
			<p>Now, let’s look at our <strong class="source-inline">.dockerignore</strong> file, which has the <span class="No-Break">following rules:</span></p>
			<pre class="source-code">
./tests
./target
./scripts
./database
.github</pre>
			<p>These rules should make sense. We do not want to add our build files, scripts, database migrations, or GitHub workflows to our <span class="No-Break">Docker builds.</span></p>
			<p>We have now defined all our rules for the repository. Before we move on to the next section, we might as well define the general layout of our application. Here, we can lift the source Rust code from the existing to-do application into our new repository with the <span class="No-Break">following command:</span></p>
			<pre class="console">
cp -r web_app/src ./clean_web_app/src</pre>
			<p>If you created an <strong class="source-inline">src</strong> directory in the clean app repository before running the preceding command, you must delete the <strong class="source-inline">src</strong> directory in the clean app repository; otherwise, you will end up with two <strong class="source-inline">src</strong> directories, where the copied <strong class="source-inline">src</strong> is inside the existing <strong class="source-inline">src</strong>. Our <strong class="source-inline">Cargo.tml</strong> file has the same dependencies as our existing web application; however, we <a id="_idIndexMarker1213"/>can change its name with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
[package]
name = "clean_app"
version = "0.1.0"
edition = "2021"</pre>
			<p>Let’s check if lifting our code works with the following <span class="No-Break"><strong class="source-inline">test</strong></span><span class="No-Break"> command:</span></p>
			<pre class="console">
cargo test</pre>
			<p>This should give us the <span class="No-Break">following output:</span></p>
			<pre class="console">
running 9 tests
test to_do::structs::base::base_tests::new ... ok
test to_do::structs::done::done_tests::new ... ok
test to_do::structs::pending::pending_tests::new ... ok
test jwt::jwt_tests::get_key ... ok
test jwt::jwt_tests::decode_incorrect_token ... ok
test jwt::jwt_tests::encode_decode ... ok
test jwt::jwt_tests::test_no_token_request ... ok
test jwt::jwt_tests::test_false_token_request ... ok
test jwt::jwt_tests::test_passing_token_request ... ok</pre>
			<p>This output shows that our code compiles and that our <strong class="source-inline">Cargo.tml</strong> file is properly defined. Now that we have confirmed that our unit tests have passed, we have some assurance that our code is working. However, how we define our configuration will give us some hurdles when we are deploying applications to the cloud. In the next section, we will smooth out our <a id="_idIndexMarker1214"/>deployments by using environment variables to configure our <span class="No-Break">web applications.</span></p>
			<h1 id="_idParaDest-267"><a id="_idTextAnchor268"/>Getting our configuration from environment variables</h1>
			<p>So far, we have <a id="_idIndexMarker1215"/>been loading configuration variables from YML files. This has a few issues. First, we must move these files around where our deployment is. Also, files do not work efficiently with orchestration tools such as Kubernetes. Kubernetes uses ConfigMaps, which essentially define environment variables for each container they are running. Environment variables also work well with tools such as Secret Manager and AWS credentials. We can also directly overwrite the environment variables in <strong class="source-inline">docker-compose</strong>. With all these advantages in mind, we will switch our configuration values from files to environment variables. To map where we have implemented configuration variables from a file, all we must do is delete our <strong class="source-inline">src/config.rs</strong> file and the module declaration of that <strong class="source-inline">config</strong> module in the <strong class="source-inline">main.rs</strong> file. Then, we can run the <strong class="source-inline">cargo test</strong> command again to get the <span class="No-Break">following output:</span></p>
			<pre class="console">
--&gt; src/database.rs:12:12
   |
12 | use crate::config::Config;
   |            ^^^^^^ could not find `config` in the crate root
error[E0432]: unresolved import `crate::config`
 --&gt; src/jwt.rs:9:12
  |
9 | use crate::config::Config;
  |            ^^^^^^ could not find `config` in the crate root
error[E0432]: unresolved import `crate::config`
 --&gt; src/counter.rs:4:12
  |
4 | use crate::config::Config;
  |            ^^^^^^ could not find `config` in the crate root</pre>
			<p>Here, we used <strong class="source-inline">config</strong> in the <strong class="source-inline">jwt</strong>, <strong class="source-inline">database</strong>, and <strong class="source-inline">counter</strong> modules. This makes sense because we <a id="_idIndexMarker1216"/>must connect to external structures when using these modules. To fix the breaking imports, all we must do is replace the config references with environment variable references. To demonstrate this, we can use the <strong class="source-inline">src/counter.rs</strong> file. First, we must delete the following lines <span class="No-Break">of code:</span></p>
			<pre class="source-code">
...
use crate::config::Config;
...
let config = Config::new();
let redis_url = config.map.get("REDIS_URL")
                          .unwrap().as_str()
                          .unwrap().to_owned();
...</pre>
			<p>Then, we must replace the preceding lines of code with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
...
use std::env;
...
let redis_url = env::var("REDIS_URL").unwrap();
...</pre>
			<p>We can follow this format for the <strong class="source-inline">JWT</strong> and <strong class="source-inline">database</strong> modules as well. In the <strong class="source-inline">JWT</strong> module, there is one variable that is not a string and has to be converted into an integer, which is <strong class="source-inline">expire minutes</strong>. This can be done with the following line <span class="No-Break">of code:</span></p>
			<pre class="source-code">
let minutes = env::var("EXPIRE_MINUTES").unwrap()
                                        .parse::&lt;i64&gt;()
                                        .unwrap();</pre>
			<p>If we run <a id="_idIndexMarker1217"/>the <strong class="source-inline">cargo test</strong> command now, we will get the <span class="No-Break">following output:</span></p>
			<pre class="console">
running 9 tests
test jwt::jwt_tests::encode_decode ... FAILED
test jwt::jwt_tests::get_key ... FAILED
test jwt::jwt_tests::decode_incorrect_token ... FAILED
test to_do::structs::pending::pending_tests::new ... ok
test to_do::structs::base::base_tests::new ... ok
test to_do::structs::done::done_tests::new ... ok
test jwt::jwt_tests::test_passing_token_request ... FAILED
test jwt::jwt_tests::test_no_token_request ... ok
test jwt::jwt_tests::test_false_token_request ... FAILED</pre>
			<p>Our tests ran, so we know that compiling the code worked. However, some of the JWT tests are failing. If we scroll further down the log, we will see the <span class="No-Break">following error:</span></p>
			<pre class="console">
---- jwt::jwt_tests::encode_decode stdout ----
thread 'jwt::jwt_tests::encode_decode' panicked at
'called `Result::unwrap()` on an `Err` value: NotPresent',
src/jwt.rs:52:50</pre>
			<p>This is telling us that our environment variables are not present. Considering that this is failing in the <strong class="source-inline">JWT</strong> module, we can be sure that it will also fail in the <strong class="source-inline">database</strong> and <strong class="source-inline">counter</strong> modules. Therefore, before we run or test our application, we need to define these environment variables. We can build a test pipeline for our application with environment variables by building a <strong class="source-inline">scripts/run_unit_tests.sh</strong> script with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
#!/usr/bin/env bash
# navigate to directory
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH
cd ..
export SECRET_KEY="secret"
export EXPIRE_MINUTES=60
cargo test</pre>
			<p>Here, we navigate to<a id="_idIndexMarker1218"/> the root directory, export the environment variables, and then run the <strong class="source-inline">test</strong> command. Running the preceding script results in all the unit <span class="No-Break">tests passing.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">While I like putting as much as possible in the Bash script as it acts like documentation for other developers to see all the moving parts, there are other approaches. For instance, you may find that the approach outlined previously is clunky as this approach deviates from running the standard <strong class="source-inline">cargo test</strong> command. Other approaches include <span class="No-Break">the following:</span></p>
			<p class="callout"> - Manually injecting variables into <span class="No-Break">the test</span></p>
			<p class="callout">- Using the <strong class="source-inline">dotenv</strong> crate to load environment variables from a <span class="No-Break">file (</span><span class="No-Break">https://github.com/dotenv-rs/dotenv</span><span class="No-Break">)</span></p>
			<p class="callout">- Having sensible defaults for <span class="No-Break">environment variables</span></p>
			<p>How would you create the script that runs the dev server? This would be a good time for you to try and write the script yourself. If you have attempted writing the script yourself, your <strong class="source-inline">scripts/run_dev_server.sh</strong> script should look something like the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
#!/usr/bin/env bash
# navigate to directory
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH
cd ..
export SECRET_KEY="secret"
export EXPIRE_MINUTES=60
export DB_URL="postgres://username:password@localhost:5433/to_do"
export REDIS_URL="redis://127.0.0.1/"
cargo run</pre>
			<p>However, if we try<a id="_idIndexMarker1219"/> and run the preceding script, it will crash because we cannot connect to the Redis database. We need to define our dev services in the <strong class="source-inline">docker-compose.yml</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
version: "3.7"
services:
  postgres:
    container_name: 'to-do-postgres'
    image: 'postgres:11.2'
    restart: always
    ports:
      - '5433:5432'
    environment:
      - 'POSTGRES_USER=username'
      - 'POSTGRES_DB=to_do'
      - 'POSTGRES_PASSWORD=password'
  redis:
      container_name: 'to-do-redis'
      image: 'redis:5.0.5'
      ports:
        - '6379:6379'</pre>
			<p>Now that our dev services have been defined, we can spin up our <strong class="source-inline">docker-compose</strong> and run our <strong class="source-inline">run_dev_server.sh</strong> script, resulting in our dev server running. However, if we<a id="_idIndexMarker1220"/> try and perform any requests, the server will crash. This is because we have not performed migrations on the database. In the next section, we will perform migrations on our <span class="No-Break">dev database.</span></p>
			<h1 id="_idParaDest-268"><a id="_idTextAnchor269"/>Setting up a local development database</h1>
			<p>When it comes to <a id="_idIndexMarker1221"/>migrations, there is the advantage of decoupling the programming language that we are using from the migrations. In the past, I have had to switch servers from one language to another and simply wished that the migration’s implementation was not coupled with the language. This is also a deployment issue. For instance, in Kubernetes, deploying a new server or an update might require a migration to be run. Ideally, you want to run the migration automatically through what we call <em class="italic">init Pods</em>. This is a container that is spun up and executed before the main server is deployed. This init Pod can perform a database migration command. However, if the init Pod requires something such as Rust to be present to execute the migration, this can greatly increase the size of the init pod. Therefore, I built an open source Bash tool that is only dependent on the <strong class="source-inline">psql</strong> and <strong class="source-inline">wget</strong> libraries. It can create new migrations and roll the database up and down versions. However, it must be stressed that this tool is not for every use. To quote the documentation of the migrations tool I wrote (<a href="https://github.com/yellow-bird-consult/build_tools/tree/develop#use-yb-database-migrations-if-you-have-the-following)">https://github.com/yellow-bird-consult/build_tools/tree/develop#use-yb-database-migrations-if-you-have-the-following)</a>, you should choose to use the migrations tool for projects if you have <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">A light throughput of migrations</strong>: Migrations are not timestamped; they are simply <a id="_idIndexMarker1222"/>numbered. The design of the tool is simple to keep track of what’s going on. Light applications in microservices are an <span class="No-Break">ideal environment.</span></li>
				<li><strong class="bold">Well-tested code</strong>: There are no guardrails. If there is an error in part of your SQL script, your database will be scarred with a partly run migration. You should have testing regimes with Docker databases before implementing migrations on a live <span class="No-Break">production database.</span></li>
				<li><strong class="bold">You plan on writing your own SQL</strong>: Because this tool is completely decoupled from any programming language, you have to write your own SQL scripts for each migration. This is not as daunting as you might think and gives you <span class="No-Break">more control.</span></li>
				<li><strong class="bold">You want complete control</strong>: SQL migrations and the simple implementation are essentially defined in a single Bash script. This simple implementation gives you 100% control. Nothing is stopping you from opening up your database in a GUI and directly altering the version number or manually running particular sections of <span class="No-Break">the migration.</span></li>
			</ul>
			<p>Now that we know what we are getting ourselves in for, we can navigate to the <strong class="source-inline">database</strong> directory and install the migration tool with the <span class="No-Break">following command:</span></p>
			<pre class="console">
wget -O - https://raw.githubusercontent.com/yellow-bird-consult
/build_tools/develop/scripts/install.sh | bash</pre>
			<p>This installs a couple of Bash scripts in your home directory. You may have to refresh your terminal to get the command alias. Not all operating systems will support the command alias. If your command alias does work, we can create a new set of migrations by using the <span class="No-Break">following command:</span></p>
			<pre class="console">
yb db init</pre>
			<p>However, if the alias does not work, you can run all your commands through the Bash script as each Bash script is 100% self-contained. All we must do is pass the same arguments in with the <span class="No-Break">following command:</span></p>
			<pre class="console">
bash ~/yb_tools/database.sh db init</pre>
			<p>With the <strong class="source-inline">init</strong> command, we get the <span class="No-Break">following structure:</span></p>
			<pre class="source-code">
├── database_management
│   └── 1
│       ├── down.sql
│       └── up.sql</pre>
			<p>This is the same<a id="_idIndexMarker1223"/> as the Diesel migrations tool but with just plain numbers. We have two migrations from our to-do application, so we can create them with the <span class="No-Break">following commands:</span></p>
			<pre class="console">
cp -r database_management/1 database_management/2</pre>
			<p>Once we have done this, we can create our migration files. The <strong class="source-inline">database_management/1/up.sql</strong> file creates the <strong class="source-inline">to_do</strong> table with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
CREATE TABLE to_do (
  id SERIAL PRIMARY KEY,
  title VARCHAR NOT NULL,
  status VARCHAR NOT NULL,
  date timestamp NOT NULL DEFAULT NOW()
)</pre>
			<p>The <strong class="source-inline">database_management/1/down.sql</strong> file drops the <strong class="source-inline">to_do</strong> table with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
DROP TABLE to_do</pre>
			<p>The <strong class="source-inline">database_management/2/up.sql</strong> file creates the <strong class="source-inline">user</strong> table and links all existing items to a placeholder user with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR NOT NULL,
    email VARCHAR NOT NULL,
    password VARCHAR NOT NULL,
    unique_id VARCHAR NOT NULL,
    UNIQUE (email),
    UNIQUE (username)
);
INSERT INTO users (username, email, password, unique_id)
VALUES ('placeholder', 'placeholder email',
'placeholder password', 'placeholder unique id');
ALTER TABLE to_do ADD user_id integer default 1
CONSTRAINT user_id REFERENCES users NOT NULL;</pre>
			<p>The <strong class="source-inline">database_management/2/down.sql</strong> file drops the <strong class="source-inline">users</strong> table with the <span class="No-Break">following</span><span class="No-Break"><a id="_idIndexMarker1224"/></span><span class="No-Break"> code:</span></p>
			<pre class="source-code">
ALTER TABLE to_do DROP COLUMN user_id;
DROP TABLE users</pre>
			<p>Our migrations are now ready. However, we need to connect to our database to get information and perform migrations. We can spin up our <strong class="source-inline">docker-compose</strong> to get the dev database up and running. Once this is done, we must define our database URL in the environment variables. The migration tool looks for the URL in environment variables. However, if there is a <strong class="source-inline">.env</strong> file in the current working directory, the migration tool will also load all the variables in this file. In our <strong class="source-inline">database_management/.env</strong> file, we can define the database URL with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
DB_URL="postgres://username:password@localhost:5433/to_do"</pre>
			<p>Now that our database is running and we have our URL defined, we can get what migration level the database is currently at with the <span class="No-Break">following command:</span></p>
			<pre class="console">
# with alias
yb db get
# without alias
bash ~/yb_tools/database.sh db get</pre>
			<p>Right now, we should get a <strong class="source-inline">-1</strong>. This means that there is no migrations versioning table at all on the database. If there is, but no migrations have been performed on the database, the version will be <strong class="source-inline">0</strong>. If there are any migrations, then the response will be the migration number that it is currently at. We can use the following <strong class="source-inline">db</strong> commands when using the build<a id="_idIndexMarker1225"/> tool to perform commands on <span class="No-Break">the database:</span></p>
			<ul>
				<li><strong class="source-inline">set</strong>: Creates a migrations version table if there is <span class="No-Break">not one</span></li>
				<li><strong class="source-inline">up</strong>: Goes up one migration version by applying the <span class="No-Break"><strong class="source-inline">up.sql</strong></span><span class="No-Break"> script</span></li>
				<li><strong class="source-inline">down</strong>: Goes down one migration version by applying the <span class="No-Break"><strong class="source-inline">down.sql</strong></span><span class="No-Break"> script</span></li>
				<li><strong class="source-inline">new</strong>: Creates a new migration folder if you are on the <span class="No-Break">latest version</span></li>
				<li><strong class="source-inline">rollup</strong>: Creates a new migrations version table if there is not one and then loops up all the versions in the <strong class="source-inline">database_management</strong> directory, starting from the current version of <span class="No-Break">the database</span></li>
			</ul>
			<p>We will run the <strong class="source-inline">rollup</strong> command with the <span class="No-Break">following command:</span></p>
			<pre class="console">
# with alias
yb db rollup
# without alias
bash ~/yb_tools/database.sh db rollup</pre>
			<p>This will perform migrations on the database. If you run the <strong class="source-inline">get</strong> command, you will see that the version of the database is now <strong class="source-inline">2</strong>. Our database is now ready to be queried by <span class="No-Break">our application.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Migrations can also be achieved using the <strong class="source-inline">sqlx-cli</strong> crate, which can be found at the following <span class="No-Break">link: </span><span class="No-Break">https://crates.io/crates/sqlx-cli</span><span class="No-Break">.</span></p>
			<p class="callout">However, Cargo is needed to install <strong class="source-inline">sqlx-cli</strong>, which will complicate the creation of init Pods for executing <span class="No-Break">these migrations.</span></p>
			<p>Instead of randomly making requests, in the next section, we will refine our Postman tests so that we <a id="_idIndexMarker1226"/>can run a series of requests and check that our application runs in the way that we want <span class="No-Break">it to.</span></p>
			<h1 id="_idParaDest-269"><a id="_idTextAnchor270"/>Managing variables in Postman tests</h1>
			<p>In <a href="B18722_09.xhtml#_idTextAnchor182"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Testing Our Application Endpoints and Components</em>, we built a Postman<a id="_idIndexMarker1227"/> collection. However, it was a bit ropey as we had to rely on Python to load the new token into the Newman collection. While this was important to use as using Python as glue code between processes is a useful skill, our old version of readying a Newman collection with Python is not the cleanest approach. At the start of our collection, we will add two new requests. The first one will create a user with the <span class="No-Break">following parameters:</span></p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/Figure_13.1_B18722.jpg" alt="Figure 13.1 – Create user Postman request"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – Create user Postman request</p>
			<p>With the create user request, we get the following JavaScript in the <strong class="bold">Tests</strong> tab <span class="No-Break">in Postman:</span></p>
			<pre class="source-code">
pm.test("response is created", function () {
    pm.response.to.have.status(201);
});</pre>
			<p>With this, the first request of our collection will create the user and throw an error if the request was not successful. Then, we can create the second request for our collection, which consists <a id="_idIndexMarker1228"/>of logging in, with the <span class="No-Break">following parameters:</span></p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="image/Figure_13.2_B18722.jpg" alt="Figure 13.2 – Login Postman request"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – Login Postman request</p>
			<p>With this request, we must check the response and set a collection variable as the token that we just got from the login by running the following JavaScript in the <strong class="bold">Tests</strong> tab <span class="No-Break">in Postman:</span></p>
			<pre class="source-code">
var result = pm.response.json()
pm.test("response is ok", function () {
    pm.response.to.have.status(200);
});
pm.test("response returns token", function () {
    pm.collectionVariables.set("login_token", result["token"]);
})</pre>
			<p>Once we have set our collection variable, we will be able to reference our token throughout the rest of the collection. To do this, we must update the authorization for the entire collection so that our new token value will propagate through all our requests. To access<a id="_idIndexMarker1229"/> the authorization settings, click on the header of a <strong class="source-inline">create</strong> request to get <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<img src="image/Figure_13.3_B18722.jpg" alt="Figure 13.3 – Header of the request"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – Header of the request</p>
			<p>On the right-hand side of the previous screenshot, we can see that there is a <strong class="bold">Go to authorization</strong> button. If we click on this, we get <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<img src="image/Figure_13.4_B18722.jpg" alt="Figure 13.4 – Configuring authorization"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – Configuring authorization</p>
			<p>We can see that the<a id="_idIndexMarker1230"/> value has been changed to <strong class="source-inline">{{login_token}}</strong>. If we save this and then export the collection JSON file to the <strong class="source-inline">tests</strong> directory in our repository, the value that belongs to <strong class="source-inline">{{login_token}}</strong> will be inserted into the collection <span class="No-Break">JSON file.</span></p>
			<p>We now have a Postman collection that updates itself with a fresh token after the login request without having to rely on Python to glue processes together. This is much cleaner; however, we want to ensure that the rest of our testing pipeline mimics as much of a production setting as possible. In the next section, we will build Docker images that contain our application that are a fraction of the size of what our server images were in <span class="No-Break">previous chapters.</span></p>
			<h1 id="_idParaDest-270"><a id="_idTextAnchor271"/>Building distroless tiny server Docker images</h1>
			<p>In previous chapters, our <a id="_idIndexMarker1231"/>server Docker images were roughly around 1.5 GB. This is pretty big and not ideal when we want to distribute our Rust images on servers or to other developers. Note that there is a shell that we can access in the Docker container when the image is running. This is useful in development but not great in production because if anyone manages to gain access to the Docker container, they will be able to look around and run commands in the Docker container. If the permissions on the server are not locked down, the hacker could even start running commands on the cluster that you have. I have seen cryptojacking happen through this method, where a hacker spun up a load of mining Pods at the expense of the owner of the <span class="No-Break">AWS account.</span></p>
			<p>We are going to solve these problems by using distroless images. These distroless images are tiny in size and do not have shells. So, if someone manages to gain access to our server, they will not be able to do anything because there are no shells. We will be able to drop the size of our image from 1.5 GB to 45 MB! This is something we want. However, before we start building our distroless images, we must know that distroless images have close to nothing on them. This means that if we compile our application and stuff it into a distroless image, it will not work. For instance, if we make a connection to a database, we need the <strong class="source-inline">libpq</strong> library in our distroless image. As the distroless image does not contain the library, the image will not be able to run because our static binary will not be able to locate the <span class="No-Break"><strong class="source-inline">libpq</strong></span><span class="No-Break"> library.</span></p>
			<p>We know that our 1.5 GB image runs because it contains everything and the kitchen sink. We can use our 1.5 GB to inspect what dependencies the static binary has in the image. We can do this by moving to our <strong class="source-inline">deployment</strong> directory where we wrote code to deploy our application on AWS and spinning up <strong class="source-inline">docker-compose</strong> there. Once this is running, we can inspect our containers with the <span class="No-Break">following command:</span></p>
			<pre class="console">
docker container ls</pre>
			<p>This will give us the <span class="No-Break">following output:</span></p>
			<pre class="console">
CONTAINER ID   IMAGE                 . . .
0ae94ab0bbc5   nginx:latest          . . .
6b49526250e3   deployment_rust_app.  . . .
9f4dcdc8a455   redis:5.0.5           . . .</pre>
			<p>Your IDs will be different, but we will use these IDs to SSH into our Rust app by using the <span class="No-Break">following command:</span></p>
			<pre class="console">
docker exec -it 6b49526250e3 /bin/bash</pre>
			<p>This opens an interactive shell so that we can navigate the Docker container. Here, we must remember that the static binary – that is, the Rust server – is called <strong class="source-inline">web_app</strong> and that this is in the root directory, so we do not need to go anywhere within the container. We can list the dependencies by using the <span class="No-Break">following command:</span></p>
			<pre class="console">
ldd web_app</pre>
			<p>This will give us the <span class="No-Break">following output:</span></p>
			<pre class="console">
linux-vdso.so.1 (0x0000ffffb8a9d000)
libpq.so.5 =&gt; /usr/lib/aarch64-linux-gnu/libpq.so.5
libgcc_s.so.1 =&gt; /lib/aarch64-linux-gnu/libgcc_s.so.1
libpthread.so.0 =&gt; /lib/aarch64-linux-gnu/libpthread.so.0
libm.so.6 =&gt; /lib/aarch64-linux-gnu/libm.so.6
. . .</pre>
			<p>There are 29 dependencies <a id="_idIndexMarker1232"/>in total. On the left of the list, there is the name of the library. On the right of the list, there is the path to where the library is. We can see that the database <strong class="source-inline">libpq</strong> library is needed alongside other libraries. Your paths may look different. This is because I am running this image on a MacBook M1, which has an ARM chip architecture. If you do not have this, then you will have <strong class="source-inline">x86_64-linux-gnu</strong> in your path as opposed to <strong class="source-inline">aarch64-linux-gnu</strong>. This is fine – we will supply both Docker files in the GitHub <span class="No-Break">repository online.</span></p>
			<p>In our Docker build, we must copy these libraries into our distroless image. In our <strong class="source-inline">clean_web_app/builds</strong> directory, we must create two files: <strong class="source-inline">aarch64_build</strong> and <strong class="source-inline">x86_64_build</strong>. Both these files are essentially the same Dockerfiles but with different references to libraries. At the time of writing, I wish that there was a smarter way to achieve builds with different chips in one Docker file; however, Docker builds are terrible at passing variables throughout the build as each step is isolated, and conditional logic is limited at best. It is easier to just have two different files. Also, if the builds change in the future, then the two different chip builds are decoupled. In our <strong class="source-inline">clean_web_app/builds/arch_build</strong> file, we must get the Rust image, install the database library, copy over the code of the application to be compiled, and define what type of build we <span class="No-Break">are doing:</span></p>
			<pre class="source-code">
FROM rust:1.62.1 as build
RUN apt-get update
RUN apt-get install libpq5 -y
WORKDIR /app
COPY . .
ARG ENV="PRODUCTION"
RUN echo "$ENV"</pre>
			<p>We can see that the environment is set to <strong class="source-inline">"PRODUCTION"</strong> by default. If there is an accident and the environment is not defined, it should be <strong class="source-inline">"PRODUCTION"</strong> by default. Accidentally taking<a id="_idIndexMarker1233"/> longer to compile on a test build is much better than accidentally deploying a non-production server into production. Then, we compile using the release flag if it is production and switch the static binary into the release directory if it is not compiled using the <span class="No-Break">release flag:</span></p>
			<pre class="source-code">
RUN if [ "$ENV" = "PRODUCTION" ] ; then cargo build --release ; \
else cargo build ; fi
RUN if [ "$ENV" = "PRODUCTION" ] ; then echo "no need to copy" ; \
else mkdir /app/target/release/ &amp;&amp; cp /app/target/debug/clean_app \
/app/target/release/clean_app ; fi</pre>
			<p>At this point, our application has been compiled. Everything we have covered is independent of what type of chip we are using, so the <strong class="source-inline">x86_64_build</strong> file will contain the same code that we have just laid out in the <strong class="source-inline">aarch64_build</strong> file. For both build files, we can also get our distroless image with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
FROM gcr.io/distroless/cc-debian10</pre>
			<p>Now, this is where the build scripts differ. In the ARM chip build, we must copy the libraries needed from the previous Rust image into our distroless image, <span class="No-Break">like so:</span></p>
			<pre class="source-code">
COPY --chown=1001:1001 --from=build \
/usr/lib/aarch64-linux-gnu/libpq.so.5 \
/lib/aarch64-linux-gnu/libpq.so.5
. . .
COPY --chown=1001:1001 --from=build \
/lib/aarch64-linux-gnu/libcom_err.so.2 \
/lib/aarch64-linux-gnu/libcom_err.so.2</pre>
			<p>Including them all <a id="_idIndexMarker1234"/>would simply provide needless bloat for the book and again, these files are available in this book’s GitHub repository. What we must note, however, is that the directory for the first part of each copy is the directory listed when we explored the Docker image of our large application. The second part is the same path; however, if there is a <strong class="source-inline">/usr/lib/</strong> at the start of the path, it is shortened to <strong class="source-inline">/lib/</strong>. There is no shell or users in the <span class="No-Break">distroless image.</span></p>
			<p>Once all the libraries have been copied over, we must copy the static binary of our web application into the root of our image, expose the port, and define the entry point, which is the static binary, with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
COPY --from=build /app/target/release/clean_app \
/usr/local/bin/clean_app
EXPOSE 8000
ENTRYPOINT ["clean_app"]</pre>
			<p>With this, our distroless image is done. Right now, both builds are stored away, and we will get them out depending on the chip type in a bash script to <span class="No-Break">be built.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">We don’t have to build our distroless application manually. Instead, we can use Apko via the following <span class="No-Break">link: </span><a href="https://github.com/chainguard-dev/apko"><span class="No-Break">https://github.com/chainguard-dev/apko</span></a><span class="No-Break">.</span></p>
			<p>You can copy your chosen build to the root directory of the repository under the <strong class="source-inline">Dockerfile</strong> filename. Then, run the <span class="No-Break">following command:</span></p>
			<pre class="console">
docker build . -t clean_app</pre>
			<p>When you list your <a id="_idIndexMarker1235"/>Docker images, you will see that this image is 46.5 MB! This is a massive reduction from 1.5 GB. In the next section, we will include these build files in a <span class="No-Break">test pipeline.</span></p>
			<h1 id="_idParaDest-271"><a id="_idTextAnchor272"/>Building a clean test pipeline</h1>
			<p>When it comes to <a id="_idIndexMarker1236"/>testing our application, we want to package it in the Docker image that we wish to deploy onto the servers, run migrations on the database as we would on the servers, and run a series of Postman requests and tests to mimic a user making a series of requests. This can be orchestrated with one Bash script in the <strong class="source-inline">scripts/run_full_release_test.sh</strong> file. First, we must find out what chip we are running on with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
#!/usr/bin/env bash
# navigate to directory
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH
if [ "$(uname -m)" = "arm64" ]
then
    cp ../builds/aarch64_build ../Dockerfile
else
    cp ../builds/x86_64_build ../Dockerfile
fi</pre>
			<p>Here, we pull the correct build depending on the type of chip. Depending on the computer you are using, this might be different. I am using a Mac M1, so when I call the <strong class="source-inline">uname -m</strong> command in the terminal, I get an <strong class="source-inline">arm64</strong> output. If you are not using an arch or ARM chip, you do not need the conditional logic. Instead, you just need to pull the <strong class="source-inline">x86_64_build</strong> file. Then, we must move to the <strong class="source-inline">tests</strong> directory and build our <strong class="source-inline">docker-compose</strong> with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
cd ../tests
# build the images and network
docker-compose build --no-cache
docker-compose up -d
# wait until rust server is running
sleep 5</pre>
			<p>We are now ready to<a id="_idIndexMarker1237"/> run our tests and clean up the images with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# run the api tests
newman run to_do_items.postman_collection.json
# destroy the container and image
docker-compose down
docker image rm test_server
docker image rm init_test_db
docker image rm test_postgres
rm ../Dockerfile</pre>
			<p>Before we run this, however, we need to build our <strong class="source-inline">docker-compose</strong> in our <strong class="source-inline">tests</strong> directory. Our <strong class="source-inline">tests/docker-compose.yml</strong> file has the <span class="No-Break">following outline:</span></p>
			<pre class="source-code">
version: "3.7"
services:
    test_server:
      . . .
    test_postgres:
      . . .
    test_redis:
      . . .
    init_test_db:
        . . .</pre>
			<p>First, we will focus on the test server. Seeing as we are running a test, we need to point to the build, pass a <strong class="source-inline">NOT PRODUCTION</strong> argument into the build, define the environment<a id="_idIndexMarker1238"/> variables for the server to utilize, and then wait for the Redis database to be operational before spinning it up. We can do this with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
test_server:
  container_name: test_server
  image: test_auth_server
  build:
    context: ../
    args:
      ENV: "NOT_PRODUCTION"
  restart: always
  environment:
    - 'DB_URL=postgres://username:password@test_postgres:54
          32/to_do'
    - 'SECRET_KEY=secret'
    - 'EXPIRE_MINUTES=60'
    - 'REDIS_URL=redis://test_redis/'
  depends_on:
      test_redis:
        condition: service_started
  ports:
    - "8000:8000"
  expose:
    - 8000</pre>
			<p>As we can see, <strong class="source-inline">docker-compose</strong> is a powerful tool. A few tags can result in some complex <a id="_idIndexMarker1239"/>orchestration. Then, we can move to our database and Redis containers with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
test_postgres:
  container_name: 'test_postgres'
  image: 'postgres'
  restart: always
  ports:
    - '5433:5432'
  environment:
    - 'POSTGRES_USER=username'
    - 'POSTGRES_DB=to_do'
    - 'POSTGRES_PASSWORD=password'
test_redis:
  container_name: 'test_redis'
  image: 'redis:5.0.5'
  ports:
    - '6379:6379'</pre>
			<p>These databases are nothing new. However, in the last service, we create an init container that spins up briefly just to run the migrations on <span class="No-Break">the server:</span></p>
			<pre class="source-code">
init_test_db:
    container_name: init_test_db
    image: init_test_db
    build:
      context: ../database
    environment:
      - 'DB_URL=postgres://username:password@test_postgres:
            5432/to_do'
    depends_on:
        test_postgres:
          condition: service_started
    restart: on-failure</pre>
			<p>As we can see, there <a id="_idIndexMarker1240"/>must be a Docker build in the <strong class="source-inline">database</strong> directory for our init container to make a database migration before closing. This means that our init container must have <strong class="source-inline">psql</strong> installed, our migrations tool, and the <strong class="source-inline">rollup</strong> command as the entry point. Initially, we install what we need in our <strong class="source-inline">database/Dockerfile</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
FROM postgres
RUN apt-get update \
  &amp;&amp; apt-get install -y wget \
  &amp;&amp; wget -O - https://raw.githubusercontent.com/\
  yellow-bird-consult/build_tools/develop/scripts/\
  install.sh | bash \
  &amp;&amp; cp ~/yb_tools/database.sh ./database.sh</pre>
			<p>Here, we can see that we get the <strong class="source-inline">psql</strong> library from the <strong class="source-inline">postgres</strong> Docker image. Then, we install <strong class="source-inline">wget</strong> and use this to install our migrations build tool. Finally, we copy the <strong class="source-inline">database.sh</strong> Bash script from the home directory into the root directory of the image so that we do not have to worry about aliases. Once we have configured our installments, we must copy the migrations SQL files from the current directory into the root directory of the image and define the migration command as the <span class="No-Break">entry point:</span></p>
			<pre class="source-code">
WORKDIR .
ADD . .
CMD ["bash", "./database.sh", "db", "rollup"]</pre>
			<p>This will work<a id="_idIndexMarker1241"/> fine; however, we do have to define a <strong class="source-inline">database/.dockerignore</strong> file with the following content to avoid the environment variable being passed into <span class="No-Break">the image:</span></p>
			<pre class="source-code">
.env</pre>
			<p>If we do not stop this environment variable from being copied into the image, then whatever variables we pass into the init container through <strong class="source-inline">docker-compose</strong> could <span class="No-Break">get overwritten.</span></p>
			<p>We now have everything we need in place, so all we must do is run our <strong class="source-inline">scripts/run_full_release.sh</strong> script. This will produce a lengthy printout of building the images, spinning up <strong class="source-inline">docker-compose</strong>, and running the API tests via Newman. The last output should look <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer173" class="IMG---Figure">
					<img src="image/Figure_13.5_B18722.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5 – Result of a full test run</p>
			<p>We can see that all the tests ran and passed. Our distroless build works and our init container for the database also makes the migrations. Nothing is stopping us from putting this infrastructure on AWS, with the difference of pointing to images on Docker Hub as opposed to local builds. Considering how small our distroless server is, pulling the image from Docker Hub and spinning it up will be <span class="No-Break">very quick.</span></p>
			<p>We’ve now got <a id="_idIndexMarker1242"/>all the ingredients to build continuous integration for our GitHub repository to ensure that tests are run when we create pull requests. In the next and final section, we will configure continuous integration through <span class="No-Break"><strong class="bold">GitHub Actions</strong></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-272"><a id="_idTextAnchor273"/>Building continuous integration with GitHub Actions</h1>
			<p>When it <a id="_idIndexMarker1243"/>comes to ensuring <a id="_idIndexMarker1244"/>that code quality is maintained, it can be handy to have a continuous integration pipeline that will run every time a pull request is done. We can do this with GitHub Actions. It must be noted that with GitHub Actions, you get several free minutes every month; then, you must pay for the minutes you go over. So, be careful and keep an eye on how much time you’re spending using <span class="No-Break">GitHub Actions.</span></p>
			<p>GitHub Actions gives us flexibility when it comes to implementing tasks. We can run workflows when a pull request is merged or made and when an issue is created and much more. We can also be selective about the type of branches we use. In this example, we will merely focus on a pull request on any branch to run unit tests and then full <span class="No-Break">integration tests.</span></p>
			<p>To build a workflow called <strong class="source-inline">tests</strong>, we need to create a file called <strong class="source-inline">.github/workflows/run-tests.yml</strong>. In this file, we will define the general outline of the <a id="_idIndexMarker1245"/>unit <a id="_idIndexMarker1246"/>and integration tests with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
name: run tests
on: [pull_request]
jobs:
  run-unit-tests:
    . . .
  run-integration-test:
    . . .</pre>
			<p>Here, we have defined the name of the workflow and the conditions that the workflow is triggered on pull requests for all branches. Then, we define two jobs – one to run unit tests and the other to run <span class="No-Break">integration tests.</span></p>
			<p>Each job has steps. We can also define dependencies for our steps. We can define our unit test job with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
run-unit-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v2
      - uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
      - name: run the unit test
        run: |
          export SECRET_KEY="secret"
          export EXPIRE_MINUTES=60
          cargo test</pre>
			<p>Here, we used the <strong class="source-inline">checkout</strong> action. If we do not use the <strong class="source-inline">checkout</strong> action, we will not be able to access any of the files in the GitHub repository. Then, we export the environment<a id="_idIndexMarker1247"/> variables<a id="_idIndexMarker1248"/> that are needed for the unit tests to run, and then we run the unit tests using Cargo. Also, note that we define a timeout. Defining a timeout is important just in case something ends up in a loop and you do not burn all your minutes in <span class="No-Break">one job.</span></p>
			<p>Now, let’s move on to our integration <span class="No-Break">test job:</span></p>
			<pre class="source-code">
run-integration-test:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v2
      - name: create environment build and run newman
        run: |
          cd tests
          cp ../builds/server_build ../Dockerfile
          docker-compose build --no-cache
          docker-compose up -d
          sleep 5
      - uses: actions/checkout@master
      - uses: matt-ball/newman-action@master
        with:
          collection:
              ./tests/cerberus.postman_collection.json</pre>
			<p>Here, we move into the <strong class="source-inline">tests</strong> directory, get the server build Docker file, spin up <strong class="source-inline">docker-compose</strong>, and then use the <strong class="source-inline">newman</strong> action to run the Newman tests. If we make a pull request, the actions will be shown on the pull request. If we click on the GitHub Actions <a id="_idIndexMarker1249"/>button, we<a id="_idIndexMarker1250"/> can access the status and results, as shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer174" class="IMG---Figure">
					<img src="image/Figure_13.6_B18722.jpg" alt="Figure 13.6 – GitHub Actions options"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.6 – GitHub Actions options</p>
			<p>Then, we can click on the test to see the steps of the job, as shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="image/Figure_13.7_B18722.jpg" alt="Figure 13.7 – GitHub Actions job view"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.7 – GitHub Actions job view</p>
			<p>Now, if we click on <a id="_idIndexMarker1251"/>a <a id="_idIndexMarker1252"/>step in the job, it will expand. We will see that our Newman <span class="No-Break">tests work:</span></p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="image/Figure_13.8_B18722.jpg" alt="Figure 13.8 – Newman step result"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.8 – Newman step result</p>
			<p>As we can see, our <a id="_idIndexMarker1253"/>continuous<a id="_idIndexMarker1254"/> integration works! We have now come to the end of this chapter as our repository is clean <span class="No-Break">and functional.</span></p>
			<h1 id="_idParaDest-273"><a id="_idTextAnchor274"/>Summary</h1>
			<p>We have finally made it to the end of structuring a web application in Rust and building the infrastructure around the application to make ongoing development of new features safe and easy to integrate. We have structured our repository into one that’s clean and easy to use where directories have individual purposes. Like in well-structured code, our well-structured repository can enable us to slot tests and scripts in and out of the repository easily. Then, we used pure bash to manage migrations for our database without any code dependencies so that we can use our migrations on any application, regardless of the language being used. Then, we built init containers to automate database migrations, which will work even when deployed on a server or cluster. We also refined the Docker builds for our server, making them more secure and reducing the size from 1.5 GB to 45 MB. After, we integrated our builds and tests into an automated pipeline that is fired when new code is merged into the <span class="No-Break">GitHub repository.</span></p>
			<p>This brings a natural end to building a web application and deploying it on a server. In the following chapters, we will dive deeper into web programming with Rust, looking at lower-level frameworks so that we can build custom protocols over TCP sockets. This will enable you to build lower-level applications for web servers or even local processes. In the next chapter, we will explore the Tokio framework, a building block of async programs such as <span class="No-Break">TCP servers.</span></p>
			<h1 id="_idParaDest-274"><a id="_idTextAnchor275"/>Further reading</h1>
			<ul>
				<li>Database migrations documentation and <span class="No-Break">repository: </span><a href="https://github.com/yellow-bird-consult/build_tools"><span class="No-Break">https://github.com/yellow-bird-consult/build_tools</span></a></li>
				<li>GitHub Actions <span class="No-Break">documentation: </span><a href="https://docs.github.com/en/actions/guides"><span class="No-Break">https://docs.github.com/en/actions/guides</span></a></li>
			</ul>
			<h1 id="_idParaDest-275"><a id="_idTextAnchor276"/>Questions</h1>
			<ol>
				<li>The bash migrations tool uses incremental single-digit integers to denote migrations. What is the big downside <span class="No-Break">to this?</span></li>
				<li>Why are distroless servers <span class="No-Break">more secure?</span></li>
				<li>How did we remove the need for Python when running our Newman tests that required a <span class="No-Break">fresh token?</span></li>
				<li>What are the advantages of using environment variables for <span class="No-Break">configuration values?</span></li>
			</ol>
			<h1 id="_idParaDest-276"><a id="_idTextAnchor277"/>Answers</h1>
			<ol>
				<li value="1">Using incremental single-digit integers exposes the migrations to clashes. So, if one developer writes migrations on one branch while another developer writes migrations on a different branch, there will be a conflict of migrations when they both merge. GitHub should pick this up, but it’s important to keep the traffic of migrations low, plan out database alterations properly, and keep the services using the migrations small. If this is a concern for you, however, please use a different migrations tool that is heavier but has <span class="No-Break">more guardrails.</span></li>
				<li>Distroless servers do not have shells. This means that if a hacker manages to access our server container, they cannot run any commands or inspect the contents of <span class="No-Break">the container.</span></li>
				<li>In the login request, we get the token that is returned from the server in the test script and assign it to a collection variable that can be accessed by other requests, removing the reliance <span class="No-Break">on Python.</span></li>
				<li>Environment variables are simply easier to implement when deploying our application to the cloud. For instance, Kubernetes’s ConfigMaps use environment variables to pass variables into Docker containers. It is also easier to implement services such as Secrets Manager on AWS by using <span class="No-Break">environment variables.</span></li>
			</ol>
		</div>
	

		<div id="_idContainer178" class="Content">
			<h1 id="_idParaDest-277"><a id="_idTextAnchor278"/>Part 6:Exploring Protocol Programming and Async Concepts with Low-Level Network Applications</h1>
			<p>Web programming has evolved to more than just simple applications that interact with databases. In this part, we cover more advanced concepts with async Rust by covering the basics of async Rust, Tokio, and Hyper. With Tokio and Hyper, we leverage async Rust and the actor model to implement async designs such as passing messages between actors in different threads, queuing tasks in Redis to be consumed by multiple workers, and processing byte streams with Tokio framing and TCP ports. By the end of this part, you will be able to implement more complex event-processing solutions on your server to handle more complex problems. You will also have practical knowledge of how to implement async Rust, which is an <span class="No-Break">up-and-coming field.</span></p>
			<p>This part includes the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B18722_14.xhtml#_idTextAnchor279"><em class="italic">Chapter 14</em></a>, <em class="italic">Exploring the Tokio Framework</em></li>
				<li><a href="B18722_15.xhtml#_idTextAnchor291"><em class="italic">Chapter 15</em></a>, <em class="italic">Accepting TCP Traffic with Tokio</em></li>
				<li><a href="B18722_16.xhtml#_idTextAnchor306"><em class="italic">Chapter 16</em></a>, <em class="italic">Building Protocols on Top of TCP</em></li>
				<li><a href="B18722_17.xhtml#_idTextAnchor323"><em class="italic">Chapter 17</em></a>, <em class="italic">Implementing Actors and Async with the Hyper Framework</em></li>
				<li><a href="B18722_18.xhtml#_idTextAnchor335"><em class="italic">Chapter 18</em></a>, <em class="italic">Queuing Tasks with Redis</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer179">
			</div>
		</div>
		<div>
			<div id="_idContainer180">
			</div>
		</div>
	</body></html>
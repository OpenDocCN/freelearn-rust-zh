- en: Sequential Rust Performance and Testing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序Rust性能和测试
- en: '"Make it work, then make it beautiful, then if you really, really have to,
    make it fast."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '"先让它工作，然后让它美观，然后如果你真的、真的需要，再让它快速。"'
- en: '- *Joe Armstrong*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- *乔·阿姆斯特朗*'
- en: In the previous chapter, we discussed the basics of modern computer architectures—the
    CPU and its function, memory hierarchies, and their interplay. We left off with
    a brief introduction to debugging and performance analysis of Rust programs. In
    this chapter, we'll continue that discussion, digging into the performance characteristics
    of sequential Rust programs, deferring, for now, considerations of concurrent
    performance. We'll also be discussing testing techniques for demonstrating the
    fitness for purpose of a Rust program. Why, in a book about parallel programming,
    would we wish to devote an entire chapter to just sequential programs? The techniques
    we'll discuss in this sequential setting are applicable and vital to a parallel
    setting. What we gain here is the meat of the concern—being fast *and* correct—without
    the complication that parallel programming brings, however, we'll come to that
    in good time. It is also important to understand that the production of fast parallel
    code comes part and parcel with the production of fast sequential code. That's
    on account of there being a cold, hard mathematical reality that we'll deal with
    throughout the book.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了现代计算机架构的基础——CPU及其功能、内存层次结构及其相互作用。我们简要介绍了Rust程序的调试和性能分析。在本章中，我们将继续这一讨论，深入探讨顺序Rust程序的性能特性，暂时推迟对并发性能的考虑。我们还将讨论用于展示Rust程序适用性的测试技术。为什么在一本关于并行编程的书中，我们会专门用一整章来讨论顺序程序？我们在这种顺序环境中所讨论的技术适用于并行环境，并且至关重要。在这里我们所获得的是关注的重点——快速且正确——尽管如此，我们将在适当的时候讨论并行编程。同样重要的是要理解，快速并行代码的生产与快速顺序代码的生产是密不可分的。这是因为我们将在整本书中处理的一个冷酷、严峻的数学现实。
- en: 'By the close of the chapter, we will:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将：
- en: Have learned about Amdahl's and Gustafson's laws
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经了解了Amdahl定律和Gustafson定律
- en: Have investigated the internals of the Rust standard library `HashMap`
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经调查了Rust标准库`HashMap`的内部结构
- en: Be able to use QuickCheck to perform randomized validating of an alternative
    HashMap implementation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够使用QuickCheck对替代HashMap实现进行随机验证
- en: Be able to use American Fuzzy Lop to demonstrate the lack of crashes in the
    same
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够使用American Fuzzy Lop来演示同一
- en: Have used Valgrind and Linux Perf to examine the performance of Rust software
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已使用Valgrind和Linux Perf来检查Rust软件的性能
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires a working Rust installation. The details of verifying
    your installation are covered in [Chapter 1](5f3aec9d-fd53-48ff-9ba8-43ce13e91cff.xhtml),
    *Preliminaries – Machine Architecture and Getting Started with Rust*. The Valgrind
    suite of tools will be used here. Many operating systems bundle valgrind packages
    but you can find further installation instructions for your system at [valgrind.org](http://valgrind.org/).
    Linux Perf is used and is bundled by many Linux distributions. Any other software
    required for this chapter is installed as a part of the text.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要安装一个可工作的Rust环境。验证安装的详细步骤在[第1章](5f3aec9d-fd53-48ff-9ba8-43ce13e91cff.xhtml)，*预备知识——机器架构和Rust入门*中有所介绍。这里将使用Valgrind工具集。许多操作系统捆绑了valgrind包，但您可以在[http://valgrind.org/](http://valgrind.org/)找到您系统的进一步安装说明。Linux
    Perf也被许多Linux发行版捆绑。本章所需的其他任何软件都将作为文本的一部分安装。
- en: 'You can find the source code for this book''s projects on GitHub: [https://github.com/PacktPublishing/Hands-On-Concurrency-with-Rust](https://github.com/PacktPublishing/Hands-On-Concurrency-with-Rust).
    The source code for this chapter is under `Chapter02`.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到本书项目的源代码：[https://github.com/PacktPublishing/Hands-On-Concurrency-with-Rust](https://github.com/PacktPublishing/Hands-On-Concurrency-with-Rust)。本章的源代码位于`Chapter02`目录下。
- en: Diminishing returns
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递减回报
- en: 'The hard truth is that there''s a diminishing return when applying more and
    more concurrent computational resources to a problem. Performing parallel computations
    implies some coordination overhead—spawning new threads, chunking data, and memory
    bus issues in the presence of barriers or fences, depending on your CPU. Parallel
    computing is not free. Consider this `Hello, world!` program:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 严峻的事实是，当将越来越多的并发计算资源应用于问题时，回报是递减的。执行并行计算意味着一些协调开销——在存在屏障或围栏的情况下，创建新线程、分块数据和内存总线问题。并行计算不是免费的。考虑这个`Hello,
    world!`程序：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Straightforward enough, yeah? Compile and run it 100 times:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单，对吧？编译并运行它100次：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, consider basically the same program but involving the overhead of spawning
    a thread:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑基本上相同的程序，但涉及到创建线程的开销：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Compile and run it 100 times:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并运行它100次：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This implies a thread-spawn time on my test system of 0.24 milliseconds. That
    is, of course, without any synchronization. The point is, it's not free. It's
    also not magic. Say you have a program that runs on a single processor in 24 hours
    and there are parts of the program that will have to be done sequentially and
    which, added together, consume 1 hour of the total runtime. The remaining 23 hours
    represent computations that can be run in parallel. If this computation is important
    and needs to be done in a hurry, the temptation is going to be to chuck in as
    much hardware as possible. How much of an improvement should you expect?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在我的测试系统中线程创建的时间为0.24毫秒。当然，这是在没有任何同步的情况下。关键是，这并不是免费的。它也不是魔法。假设你有一个程序在一个处理器上运行需要24小时，并且程序中有一部分必须顺序执行，总共消耗了总运行时间的1小时。剩下的23小时代表可以并行运行的计算。如果这个计算很重要并且需要尽快完成，那么可能会倾向于投入尽可能多的硬件。你应该期望有多大的改进？
- en: One well-known answer to this question is Amdahl's law. It states that the speedup
    of a computation is proportional to the inverse of the percentage of time taken
    by the sequential bit plus the percentage of the parallel time divided by the
    total new computation units, *1/(s + p / N)*. As *N* tends toward infinity, *1/s
    == 1/(1-p),* in our example, *1/(1 - (23/24)) = 24*. That is, the maximum factor
    speedup you can ever hope to see is 24 times, with infinite additional capacity.
    Ouch.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题的一个众所周知的答案是阿姆达尔定律。它表明计算的加速与顺序部分所占时间的倒数加上并行时间所占百分比除以总新计算单元数成比例，即*1/(s +
    p / N)*。当*N*趋向于无穷大时，*1/s == 1/(1-p)*，在我们的例子中，*1/(1 - (23/24)) = 24*。也就是说，你所能期望的最大加速因子是24倍，具有无限额外的容量。哎呀。
- en: 'Amdahl''s law is a touch pessimistic, as noted by John Gustafson in his 1988
    Reevaluating Amdahl''s Law:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 阿姆达尔定律有点悲观，正如John Gustafson在1988年重新评估阿姆达尔定律时所指出的：
- en: '"At Sandia National Laboratories, we are currently engaged in research involving
    massively-parallel processing. There is considerable skepticism regarding the
    viability of massive parallelism; the skepticism centers around Amdahl''s law,
    an argument put forth by Gene Amdahl in 1967 that even when the fraction of serial
    work in a given problem is small, say s, the maximum speedup obtainable from even
    an infinite number of parallel processors is only 1/s. We now have timing results
    for a 1024-processor system that demonstrate that the assumptions underlying Amdahl''s
    1967 argument are inappropriate for the current approach to massive ensemble parallelism."'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: “在桑迪亚国家实验室，我们目前正在研究涉及大规模并行处理的项目。对于大规模并行化的可行性存在相当多的怀疑；这种怀疑主要集中在阿姆达尔定律上，这是Gene
    Amdahl在1967年提出的一个论点，即即使在一个给定问题中串行工作的比例很小，比如说s，那么从无限数量的并行处理器中获得的最大加速比也只有1/s。我们现在有一个1024处理器系统的计时结果，证明了阿姆达尔1967年论点背后的假设对于当前的大规模并行方法是不适当的。”
- en: Gustafson argues that real workloads will take the time factor of a computation
    as fixed and vary the input work accordingly. In some hypothetical, very important
    computations we'd see 24 hours as acceptable and, on increasing the total number
    of processors available, then rush to figure out how to add in more computation
    so as to get the time back up to one day. As we increase the total workload, the
    serial portion of the computation tends towards zero. This is, itself, maybe somewhat
    optimistic and is certainly not applicable to problems where there's no more datasets
    available. Communication overhead is not included in either analysis.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, what has to be internalized is this—performing computations in parallel
    is subject to diminishing returns. Exactly how that take shape depends strongly
    on your problem, the machine, and so on, but it's there. What the programmer must
    do to bend that curve is shrink the percentage of time spent in serial computation,
    either by increasing the relative portion of parallel computation per Gustafson
    with a larger dataset or by optimizing the serial computation's runtime. The remainder
    of this chapter will be focused on the latter approach—improving the runtime of
    serial computations.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll focus on the serial performance of a common data structure—associative
    arrays. We'll apply the tools we learned about in the previous chapter to probe
    different implementations. We'll focus on the associative array because it is
    fairly well-trod territory, studied in introductory computer science courses,
    and is available in most higher-level languages by default, Rust being no exception
    save the higher-level bit. We'll look at Rust's associative array first, which
    is called `std::collections::HashMap`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Standard library HashMap
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's poke around in `HashMap`'s internals. A good starting place, I find, for
    inspecting unfamiliar Rust data structures is jumping into the source to the struct
    definition itself. Especially in the Rust codebase, there will be public `rustdoc`
    comments and private comments explaining implementation ambitions. The reader
    is warmly encouraged to inspect the `HashMap` comments for themselves. In this
    book, we're inspecting Rust at SHA `da569fa9ddf8369a9809184d43c600dc06bd4b4d`.
    The comments of `src/libstd/collections/hash/map.rs` explain that the `HashMap`
    is implemented with linear probing Robin Hood bucket stealing. Linear probing
    implies that we'll find `HashMap` implemented in terms of a cell storage—probably
    a contiguous memory structure for reasons we'll discuss shortly—and should be
    able to understand the implementation pretty directly, linear probing being a
    common method of implementing associative arrays. Robin Hood bucket stealing is
    maybe less common, but the private comments describe it as *the main performance
    trick in this hashmap* and then goes on to quote Pedro Celis' 1986 *Robin Hood
    Hashing:*
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '"If an insertion collides with an existing element, and that element''s "probe
    distance" (how far away the element is from its ideal location) is higher than
    how far we''ve already probed, swap the elements."'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'If you pull the thesis yourself, you''ll further find:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '"(Robin Hood Hashing) leads to a new search algorithm which requires less than
    2.6 probes on average to perform a successful search even when the table is nearly
    full. Unsuccessful searches require only O(ln n) probes."'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Not bad. Here''s `HashMap`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Okay, so, it interacts with `RawTable<K, V>` and we can jump to that definition:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'But it''s maybe not the most illuminating struct definition either. Common
    operations on a collection are often a good place to dig in, so let''s look at
    `HashMap::insert`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And then on into `HashMap::reserve`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Whether `reserve` expands the underlying storage capacity because we''ve got
    near the current total capacity of that storage or to reduce probe lengths, `HashMap::resize`
    is called. That''s:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We''re back at `RawTable` and have a lead on a structure called `Bucket`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Okay, we''re going in some circles here. The bucket parameterizes the table
    on `M` rather than the table holding some collection of buckets. The `RawBucket`
    is described as an unsafe view of a `RawTable` bucket of which there are two variants—`EmptyBucket`
    and `FullBucket`. These variants are used to populate a `BucketState` enumeration:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here, we can see it being used back in `HashMap::insert_hashed_ordered`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If we explore down into `empty.push(hash, k, v)`, we find ourselves at the
    following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Tidy. `ptr::write(self.raw.pair(), (key, value))` demonstrates that we''re
    working with a structure built out of raw memory pointer manipulation, befitting
    a structure that will see a lot of use in critical `paths`. `self.raw.pair()`,
    which returns the appropriate offset to move `(key, value)` into, matching the
    `HashMap::insert` move semantics we''re already familiar with. Take a look at
    the definition of `RawBucket`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here, we''ve got two raw pointers:  `hash_start` and `pair_start`. The former
    is the first location in memory of our stored pairs'' hashes, the latter is the
    first location in the memory of the pairs. What this ends up being is contiguous
    storage in memory of two sets of data, which is about what you''d expect. The
    table module documentation refers to this approach as *unzipped* arrays. `RawTable`
    holds the capacity and the data, kind of. In reality, the data held by `RawTable`
    is carefully placed in memory, as we''ve seen, but there''s no *owner* as the
    Rust type system understands it. That''s where `marker: marker::PhantomData<(K,
    V)>` comes in. `PhantomData` instructs the compiler to behave as if `RawTable<K,
    V>` owns pairs of `(K, V)`, even though with all the unsafe pointer manipulation
    we''re doing that can''t actually be proven by Rust. We human observers can determine
    by inspection that `RawTable` owns its data via `RawTable::raw_bucket_at` as it
    computes where in memory the data exists:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Well, by inspection and testing, as you can see at the bottom of the module.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，通过检查和测试，正如您可以在模块底部看到的那样。
- en: Naive HashMap
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的 HashMap
- en: How else could we implement an associative array in Rust and how would it compare
    to standard library's `HashMap`? The standard library `HashMap` is clever, clearly,
    storing just enough information to reduce the total probes from ideal offsets
    by sharing information between modifications to the underlying table. In fact,
    the comments in the table module assert that the design we've just worked through
    is *a lot faster* than a table structured as `Vec<Option<(u64, K, V)>>`—where
    `u64` is the key hash, but presumably still using Robin Hood hashing and linear
    probing. What if we went even simpler? We'll support only two operations—`insert`
    and `lookup`—to keep things straightforward for ourselves. We'll also keep more
    or less the same type constraints as `HashMap` so we compare similar things.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还能如何实现 Rust 中的关联数组，它与标准库中的 `HashMap` 又将如何比较？显然，标准库的 `HashMap` 是巧妙的，它只存储足够的信息来减少从理想偏移量到总探测次数，通过在底层表的修改之间共享信息来实现。实际上，表模块中的注释断言，我们刚刚完成的设计与作为
    `Vec<Option<(u64, K, V)>>` 结构的表相比要快得多——其中 `u64` 是键哈希，但可能仍然使用 Robin Hood 哈希和线性探测。如果我们做得更简单会怎样呢？我们将只支持两种操作——`insert`
    和 `lookup`——以保持事情简单明了。我们还将保持与 `HashMap` 类似的大致类型约束，以便比较类似的事物。
- en: 'Start a new Rust project called `naive_hashmap`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 开始一个新的 Rust 项目，命名为 `naive_hashmap`：
- en: '[PRE15]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Edit `naive_hashmap/Cargo.toml` to look like so:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑 `naive_hashmap/Cargo.toml` 以看起来像这样：
- en: '[PRE16]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Don''t worry too much right now about some of the development dependencies,
    they''ll be explained in due course. Please note that in the following discussion
    we will run compilation commands against the project before all the code has been
    discussed. If you are following along with the book''s pre-written source code
    open already, you should have no issues. If you are writing the source out as
    the book goes along, you will need to comment targets out. Now, open `naive_hashmap/src/lib.rs`
    and add the following preamble:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 目前不必过于担心一些开发依赖项，它们将在适当的时候解释。请注意，在接下来的讨论中，我们将在所有代码讨论之前对项目运行编译命令。如果您正在跟随书中预先编写的源代码，应该没有问题。如果您在书中进行编写，则需要注释掉目标。现在，打开
    `naive_hashmap/src/lib.rs` 并添加以下前言：
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Most crate roots begin with a fairly long preamble, and `naive_hashmap` is
    no exception. Next up, our `HashMap` struct:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 crate 根开始都有一个相当长的前言，`naive_hashmap` 也不例外。接下来是我们的 `HashMap` 结构：
- en: '[PRE18]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'How does our naive `HashMap` differ from the standard library''s `HashMap`
    we''ve seen so far? The naive implementation maintains the parameterized hasher
    and type constraints, plus a constraint on `V` to implement the `Debug` trait
    for ease of fiddling. The primary difference, in terms of underlying structure,
    is the use of a `Vec`—as called out in the comments of the standard library `HashMap`—but
    without an `Option` wrapper. We''re not implementing a delete operation so there''s
    no reason to have an `Option` but, even if we were, the plan for this implementation
    would be to rely solely on `Vec::remove`. The fact that it is less than ideal
    that `Vec::remove` shifts all elements from the right of the removal index to
    the left should be well understood. Folks, this won''t be a fast implementation.
    Now:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的简单 `HashMap` 与我们迄今为止看到的标准库 `HashMap` 有何不同？简单实现维护了参数化哈希器和类型约束，以及 `V` 实现的 `Debug`
    特性约束，以便于调整。在底层结构方面，主要区别在于使用了 `Vec`——正如标准库 `HashMap` 的注释所指出的——但没有使用 `Option` 包装器。我们不实现删除操作，因此没有必要使用
    `Option`，即使我们这样做，这个实现的计划也将完全依赖于 `Vec::remove`。`Vec::remove` 将移除索引右侧的所有元素向左移动的事实应该被充分理解。朋友们，这不会是一个快速的实现。现在：
- en: '[PRE19]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Our naive implementation is following along with the standard library here,
    implementing a `HashMap` that is parameterized on `RandomState`—so users don't
    have to think about the underlying hasher—and in which the hasher is swappable,
    via `HashMap::with_hasher`. The Rust team chose to implement `RandomState` in
    terms of a verified cryptographically secure hash algorithm, befitting a language
    that is intended for use on the public internet. Some users won't desire this
    property—opting instead for a much faster, potentially vulnerable hash—and will
    swap `RandomState` out for something else. Our naive `HashMap` retains this ability.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的朴素实现在这里遵循标准库，实现了一个参数化为`RandomState`的`HashMap`——这样用户就不必考虑底层的哈希器——并且哈希器可以通过`HashMap::with_hasher`进行交换。Rust团队选择以经过验证的密码学安全哈希算法来实现`RandomState`，这对于一个打算在公共互联网上使用的语言来说很合适。一些用户可能不会希望这个属性——而是选择一个更快、可能存在漏洞的哈希算法——并将`RandomState`替换为其他东西。我们的朴素`HashMap`保留了这种能力。
- en: 'Let''s examine insertion into our naive `HashMap`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查我们的朴素`HashMap`的插入操作：
- en: '[PRE20]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Our naive implementation maintains the API of the standard library `HashMap`
    but that''s about it. The key is hashed and then a linear search is done through
    the entire data store to find an index in that store where one of two conditions
    hold:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的朴素实现保持了标准库`HashMap`的API，但仅此而已。键被哈希处理，然后在整个数据存储中进行线性搜索以找到满足以下两个条件之一的索引：
- en: Our new hash is greater than some hash in the store, in which case we can insert
    our key/value pair
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的新哈希值大于存储中的某个哈希值，在这种情况下，我们可以插入我们的键值对
- en: Our new hash is equal to some hash in the store, in which case we can replace
    the existing value
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的新哈希值等于存储中的某个哈希值，在这种情况下，我们可以替换现有的值
- en: 'Key/value pairs are stored in terms of their ordered hashes. The expense of
    an insert includes:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 键值对按照它们的有序哈希值存储。插入的开销包括：
- en: Hashing the key
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对键进行哈希处理
- en: Searching for the insert index, a linear operation to the number of stored key/value
    pairs
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索插入索引，这是一个与存储的键值对数量成线性关系的操作
- en: Potentially shifting key/value pairs in memory to accommodate a new key to the
    store
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能将内存中的键值对进行移位以适应存储中新的键
- en: 'Lookup follows a similar scheme:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 查找遵循类似的方案：
- en: '[PRE21]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The key is hashed and a linear search is done for that exact hash in storage.
    Here, we only pay the cost for:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 键被哈希处理，并在存储中进行针对该特定哈希值的线性搜索。在这里，我们只支付以下成本：
- en: Hashing the key
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对键进行哈希处理
- en: Searching for the retrieval offset, if one exists
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果存在，搜索检索偏移量
- en: 'Let''s take a moment and consider this before asking ourselves if our program
    is *fast* if it is *correct*. The usual way of demonstrating fitness for purpose
    of software is through unit testing, a minimal setup for which is built right
    into the Rust language. Unit testing is two processes wrapped into a single method.
    When writing unit tests, a programmer will:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们询问程序是否*快速*且*正确*之前，让我们花点时间考虑这一点。证明软件适用性的通常方式是通过单元测试，Rust语言中直接内置了这种最小设置。单元测试是两个过程封装在一个方法中。当编写单元测试时，程序员将：
- en: Produce *example data* that exercises some code path in the software
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成*示例数据*以测试软件中的某些代码路径
- en: Write further code to demonstrate that when the example data is applied to the
    system under test, a desirable property/properties holds
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写更多代码以证明当示例数据应用于测试的系统时，存在一个或多个期望的属性
- en: This is a good testing methodology for demonstrating that the happy path of
    a system under test works as expected. The weak point of unit testing comes in
    the first process, where the programmer must think very hard and produce an example
    dataset that exercises the correct code paths, demonstrates the lack of edge cases,
    and so on. Human beings are poor at this task, owing to it being tedious, biased
    toward demonstrating the functioning of a thing made by one's own hands, chronic
    blind spots, or otherwise. What we *are* pretty good at doing is cooking up high-level
    properties for our systems that must always hold or hold in particular situations.
    Fortunately for us programmers, *computers* are exceptionally good at doing tedious,
    repetitive tasks and the generation of example data for tests is such a thing.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Testing with QuickCheck
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With that in mind, in this book, we''ll make extensive use of *property-based*
    testing, also called *generative* testing by some literature. Property-based testing
    has a slightly different, if similar, workflow to unit testing. When writing property
    tests, the programmer will:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Produce a method for generating valid inputs to a system under test
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write further code to demonstrate that for all valid inputs that a desirable
    property or property of the system holds
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Property-based testing is exceptionally good at finding corner cases in software,
    wacky inputs that the programmer might not have dreamed up, or, as happens from
    time to time, even understand as potentially problematic. We''ll use Andrew Gallant''s
    QuickCheck, a tool that is patterned from Haskell QuickCheck, introduced by Koen
    Claessen and John Hughes in their 2000 *QuickCheck: A Lightweight Tool for Random
    Testing of Haskell Programs*. First, a little preamble to get us into the testing
    module:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If we were to unit test our naive `HashMap`, we''d probably write a test where
    a particular arbitrary key/value pair would be inserted and then we''d assert
    the ability to retrieve the same value with that same key. How does this map to
    property-based testing? The property is that if we perform an insertion on an
    empty `HashMap` with a key `k` of value `v` and immediately perform a retrieval
    of `k`, we''ll receive `v` back out. Here it is:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The test is named `get_what_you_give` and is annotated `#[test]` as per usual
    Rust tests. What''s different is this test has a property `inner` function that
    encodes the property we elaborated on earlier and a call out to QuickCheck for
    actually running the property test, by default 100 times, with different inputs.
    Exactly how QuickCheck manages this is pretty swell. Per the Claessen and Hughes
    paper, QuickCheck implements the `Arbitrary` trait for many of the types available
    in the standard library. QuickCheck defines `Arbitrary` like so, in `quickcheck/src/arbitrary.rs` :'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `Gen` parameter is a wrapper around `rand:Rng` with some extra machinery
    for controlling distributions. The two functions are `arbitrary` and `shrink`.
    The purpose of `arbitrary` is easy to explain: it generates new, random instances
    of a type that is `Arbitrary`. The `shrink` function is a little more complicated
    to get across. Let''s say we''ve produced a function that takes a vector of `u16` but
    just so happens to have a bug and will crash if a member of the vector is `0`.
    A QuickCheck test will likely find this but the first arbitrary vector it generates
    may have a thousand members and a bunch of `0` entries. This is not a very useful
    failure case to begin diagnosis with. After a failing case is found by QuickCheck
    the case is shrunk, per the definition of `Arbitrary::shrink`. Our hypothetical
    vector will be cut in half and if that new case is also a failure then it''ll
    be shrunk by half again and so on until it no longer fails, at which point—hopefully—our
    failing case is a much more viable diagnosis tool.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of `Arbitrary` for `u16` is a touch complicated due to macro
    use, but if you squint some, it''ll become clear:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Arbitrary instances are generated by `unsigned_arbitrary!`, each of which in
    turn generates a shrinker via `unsigned_shrinker!`. This macro is too long to
    reprint here but the basic idea is to remove half of the unsigned integer on every
    shrink again, until zero is hit, at which point give up shrinking.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Fifteen years after the original QuickCheck paper, John Hughes summarized his
    experience in the intervening 15 years with his 2016 *Experiences with QuickCheck:
    Testing the Hard Stuff and Staying Sane*. Hughes noted that many property-based
    tests in the wild don''t generate primitive types. The domain of applications
    in which primitive types are sufficient tends to be those rare, blessed pure functions
    in a code base. Instead, as many functions in a program are inherently stateful,
    property-based tests tend to generate arbitrary *actions* against a stateful system,
    *modeling* the expected behavior and validating that the system under test behaves
    according to its model.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'How does that apply to our naive `HashMap`? The system under test is the naive
    HashMap, that''s clear enough. Our actions are:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '`INSERT` key value'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LOOKUP` key'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hopefully, that''s straightforward. What about a model? In this particular
    instance, we''re in luck. Our model is written for us: the standard library''s
    `HashMap`. We need only confirm that if the same actions are applied to both our
    naive `HashMap` and the standard `HashMap` in the same order then we''ll get the
    same returns from both the model and system under test. How does that look? First,
    we need actions:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Our `Action<T>` is parameterized on a `T: Arbitrary` and all values via the
    Insert are pegged as `u16`s. This is done primarily for convenience''s sake. Both
    key and value could be arbitrary or concrete types, depending on the preference
    of the tester. Our `Arbitrary` definition is as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Either action is equally likely to happen and any instance of `T` or `u16`
    is valid for use. The validation of our naive `HashMap` against the standard library''s
    `HashMap`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Hopefully, this looks familiar to our previous QuickCheck test. Again, we have
    an inner property function that does the actual work and we request that QuickCheck
    run this property test over arbitrary vectors of `Action<u8>`. For every action
    present in the vector we apply them to both the model and the system under test,
    validating that the results are the same. The `u8` type was intentionally chosen
    here compared to a large domain type such as `u64.` One of the key challenges
    of writing QuickCheck tests is probing for extremely unlikely events. QuickCheck
    is blind in the sense that it's possible for the same path to be chosen through
    the program for each run if that path is the most likely path to be chosen. While
    millions of QuickCheck tests can give high confidence in fitness for purpose the
    blind nature of the runs means that QuickCheck should also be paired with tools
    known as fuzzers. These do not check the correct function of a program against
    a model. Instead, their sole purpose is to validate the absence of program crashes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Testing with American Fuzzy Lop
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this book, we''ll make use of American Fuzzy Lop, a best-of-breed fuzzer
    commonly used in other systems languages. AFL is an external tool that takes a
    corpus of inputs, an executable that reads inputs from `STDIN`, and mutates the
    corpus with a variety of heuristics to find crashing inputs. Our aim is to seek
    out crash bugs in naive `HashMap`, this implies that we''re going to need some
    kind of program to run our `HashMap` in. In fact, if you''ll recall back to the
    project''s `Cargo.toml`, we already had the infrastructure for such in place:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The source for `naive_interpreter` is a little goofy looking but otherwise
    uneventful:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Lines are read from `stdin`, and these lines are split along spaces and interpreted
    as commands, either `LOOKUP` or `INSERT` and these are interpreted into actions
    on the naive `HashMap`. The corpus for an AFL run can live pretty much anywhere.
    By convention, in this book we''ll store the corpus in-project in a top-level
    resources/directory. Here''s `resources/in/mixed_gets_puts`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The larger your input corpus, the more material AFL has to mutate and the faster—maybe—you''ll
    find crashing bugs. Even in a case such as naive `HashMap` where we can be reasonably
    certain that there will be no crashing bugs—owing to the lack of pointer manipulation
    and potentially fatal integer operations—it''s worthwhile building up a good corpus
    to support future efforts. After building a release of the project, getting an
    AFL run going is a cargo command away:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This says to execute `target/release/naive_interpreter` under AFL with input
    corpus at `resources/in` and to output any failing cases under `resources/out`.
    Such crashes often make excellent unit tests. Now, the trick with fuzzing tools
    in general is they''re not part of any kind of quick-cycle test-driven development
    loop. These tool runs are long and often get run on dedicated hardware overnight
    or over many days. Here, for example, is the AFL run I had going while writing
    this chapter:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b4a6366-3c4e-4fa5-80fc-182300d5a773.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: There's a fair bit of information here but consider that the AFL runtime indicator
    is measured in days. One key advantage to the use of AFL compared to other fuzzers
    is the prominence of AFL in the security community. There are a good many papers
    describing its implementation and the interpretation of its, uh, *comprehensive*
    interface. You are warmly encouraged to scan the *Further reading* section of
    this chapter for more information.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Performance testing with Criterion
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now be fairly confident that our naive `HashMap` has the same behavior
    as the standard library for the properties we''ve checked. But how does the runtime
    performance of naive `HashMap` stack up? To answer this, we''ll write a benchmark,
    but not a benchmark using the unstable Bencher subsystem that''s available in
    the nightly channel. Instead, we''ll use Jorge Aparicio''s criterion—inspired
    by the Haskell tool of the same name by Bryan O''Sullivan—which is available on
    stable and does statistically valid sampling of runs. All Rust benchmark code
    lives under the top-level `benches/` directory and criterion benchmarks are no
    different. Open `benches/naive.rs` and give it this preamble:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This benchmark incorporates pseudorandomness to produce an interesting run.
    Much like unit tests, handcrafted datasets in benchmarks will trend towards some
    implicit bias of the author, harming the benchmark. Unless, of course, a handcrafted
    dataset is exactly what''s called for. Benchmarking programs well is a non-trivial
    amount of labor. The `Rng` we use is `XorShift`, a pseudo-random generator known
    for its speed and less cryptographic security. That suits our purposes here:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The first benchmark, named `insert_and_lookup_native`, performs pseudo-random
    insertions and lookups against the naive `HashMap.` The careful reader will note
    that `XorShiftRng` is given the same seed every benchmark. This is important.
    While we want to avoid handcrafting a benchmark dataset, we do want it to be the
    same every run, else the benchmark comparisons have no basis. That noted, the
    rest of the benchmark shouldn''t be much of a surprise. Generate random actions,
    apply them, and so forth. As we''re interested in the times for standard library
    `HashMap` we have a benchmark for that, too:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Criterion offers, as of writing this book, a method for comparing two functions
    *or* comparing a function run over parameterized inputs but not both. That''s
    an issue for us here, as we''d like to compare two functions over many inputs.
    To that end, this benchmark relies on a small macro called `insert_lookup!`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The meat here is we create two `Fun` for comparison called `naive` and `standard,`
    then use `Criterion::bench_functions` to run a comparison between the two. In
    the invocation of the macro, we evaluate `insert_and_lookup_*` from `1` to `100_000`,
    with it being the total number of insertions to be performed against the standard
    and naive `HashMap`s. Finally, we need the criterion group and main function in
    place:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Running criterion benchmarks is no different to executing Rust built-in benchmarks:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: And so forth. Criterion also helpfully produces gnuplot graphs of your runtimes,
    if gnuplot is installed on your benchmark system. This is highly recommended.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting with the Valgrind Suite
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Your specific benchmarking output will likely vary but it''s pretty clear that
    the naive implementation is, well, not very fast. What tools do we have available
    to us to diagnose the issues with our code, guide us toward hot spots for optimization,
    or help convince us of the need for better algorithms? We''ll now leave the realm
    of Rust-specific tools and dip into tools familiar to systems programmers at large.
    If you''re not familiar with them personally that''s a-okay—we''ll describe their
    use and there are plenty of external materials available for the motivated reader.
    Okay, first, we''re going to need some programs that exercise our naive `HashMap`
    and the standard `HashMap`. `naive_interpreter` would work, but it''s doing a
    lot of extra things that''ll muddy the water some. To that end, for examination
    purposes, we''ll need two programs, one to establish a baseline and one for our
    implementation. Our baseline, called `bin/standard.rs`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Our test article program is exactly the same, save the preamble is a bit different:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Easy enough and similar in spirit to the benchmark code explored earlier. Now,
    let''s set our baselines. First up, memory allocations:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Seven total allocations, seven total frees, and a grand total of 2,032 bytes
    allocated. Running memcheck against naive has the same result:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The naive run is noticeably slower, so it''s not allocating memory that kills
    us. As so little memory gets allocated by these programs we''ll skip Valgrind
    massif—it''s unlikely to turn up anything useful. Valgrind cachegrind should be
    interesting though. Here''s baseline:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let''s break this up some:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The first section details our instruction cache behavior. `I refs: 25,733,614`
    tells us that the standard program executed 25,733,614 instructions in all. This
    is often a useful comparison between closely related implementations, as we''ll
    see here in a bit. Recall that cachegrind simulates a machine with two levels
    of instruction and data caching, the first level of caching being referred to
    as `I1` or `D1` for instruction or data caches and the last level cache being
    prefixed with `LL`. Here, we see the first and last level instruction caches each
    missed around 2,500 times during our 25 million instruction run. That squares
    with how tiny our program is. The second section is as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This is the data cache behavior, split into the two cache layers previously
    discussed and further divided into read and writes. The first line tells us that
    5,400,581 total reads and writes were made to the caches, 2,774,345 reads and
    2,626,236 writes. These totals are then further divided by first level and last
    level caches. Here it turns out that the standard library `HashMap` does real
    well, a `D1` miss rate of 5.1% and a `LLd` miss rate of 0.7%. That last percentage
    is key: the higher it is, the more our program is accessing main memory. Doing
    so, as you''ll recall from [Chapter 1](5f3aec9d-fd53-48ff-9ba8-43ce13e91cff.xhtml),
    *Preliminaries – Machine Architecture and Getting Started with Rust*, is painfully
    slow. The third section is as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This focuses on the combined behavior of data and instruction cache accesses
    to the LL cache. Personally, I don''t often find this section illuminating compared
    to the previously discussed sections. Your mileage may vary. The final section
    is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This details the branch misprediction behavior of our program. We're drowning
    out the standard library's `HashMap` by branching so extensively in the runner
    program but it will still serve to establish some kind of baseline. The first
    line informs us that 3,008,198 total branches were taken during execution. The
    majority—3,006,105—were conditional branches, branches that jump to a location
    based on some condition. This squares with the number of conditional statements
    we have in the runner. The small majority of branches were indirect, meaning they
    jumped to offsets in memory based on the results of previous instructions. Overall,
    our branch misprediction rate is 10.5%.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Alright, how does the naive `HashMap` implementation stack up?
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Already there are some standouts. Firstly, naive executed 15,385,395,657 instructions
    compared to the 25,733,614 of baseline. The naive implementation is simply doing
    much, much more work than that standard library is. At this point, without looking
    at any further data, it''s reasonable to conclude that the program under inspection
    is fundamentally flawed: a rethink of the algorithm is in order. No amount of
    micro-optimization will fix this. But, that was understood; it''s why the program
    is called *naive* to start with. The second major area of concern is that the
    `D1` cache miss rate is just shy of 20% higher than baseline, not to ignore that
    there are simply just more reads and writes to the first level cache than at baseline.
    Curiously, the naive implementation suffers fewer `LLd` cache misses compared
    to baseline—10,494 to 34,105\. No hypothesis there. Skipping on ahead to the branch
    misprediction section, we find that naive stays on-theme and performs drastically
    more branches than standard but with a lower total number of mispredictions. This
    squares with an algorithm dominated by linear seek and compares, as naive is.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting with Linux perf
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's worth keeping in mind that Valgrind's cachegrind is a simulation. If you
    have access to a Linux system you can make use of the perf tool to get real, honest
    numbers about your program's cache performance and more. This is highly recommended
    and something we'll do throughout this book. Like git, perf is many tools arranged
    under a banner—perf—with its own subcommands that have their own options.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: It is a tool well worth reading the documentation for. Anyhow, what does the
    standard look like under perf?
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: How does the naive implementation stand up?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This squares pretty well with the Valgrind simulation and leads to the same
    conclusion: too much work is being done to insert. Too many branches, too many
    instructions, the well-studied reader will have seen this coming a mile off, it''s
    just worthwhile to be able to put a thing you know to numbers.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: A better naive HashMap
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How can we do better than our naive implementation? Obviously, there''s scope
    for algorithmic improvement—we could implement any kind of probing—but if we''re
    trying to compete with standard library''s HashMap it''s likely that we have a
    specialized reason for doing so. A specialized reason implies we know something
    unique about our data, or are willing to make a trade-off that a general data
    structure cannot. Speaking broadly, the main goals one should have when building
    software at the limit of machine performance are as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Improve the underlying algorithm, reducing total work done.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Improve cache locality of data accesses. This may mean:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keeping your working-set in L1 cache
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compressing your data to fit better into cache.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Avoid branch mispredictions. This may mean:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shaving off branches entirely when possible
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Constraining the probability distribution of your branches.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: How does that apply here? Well, say we knew that for our application `K == u8`
    but we still have an unconstrained `V`. `u8` is a key with low cardinality and
    we ought to be able to trade a little memory to build a faster structure for `u8`
    keys. Unfortunately, Rust does not yet have type specialization in a stable channel.
    Type specialization is *very important* for producing high-performance software
    without breaking abstractions. It allows the programmer to define an abstract
    interface and implementation for such and then, at a later date, specialize some
    of the parameterized types into concrete form with a special purpose implementation.
    Rust RFC 1210 ([https://github.com/rust-lang/rfcs/pull/1210/files#diff-b652f1feca90247198ee29514ac22cf3](https://github.com/rust-lang/rfcs/pull/1210/files#diff-b652f1feca90247198ee29514ac22cf3))
    details how specialization in Rust will work and Rust PR 31844 ([https://github.com/rust-lang/rust/issues/31844](https://github.com/rust-lang/rust/issues/31844))
    tracks the ongoing implementation, which is to say, all of this is only exposed
    in nightly. This chapter sticks to stable and so, unfortunately, we'll need to
    create a new HashMap rather than specializing. The reader is encouraged to try
    out specialization for themselves. It's quite nice.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll park our `HashMapU8` implementation in `naive_hashmap/src/lib.rs`. The
    implementation is quite small:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The idea here is simple—`u8` is a type with such cardinality that we can rework
    every possible key into an array offset. The value for each key is an `Option<V>`,
    `None` if no value has ever been set for the key, and `Some` otherwise. No hashing
    needs to be done and, absent specialization, we drop the type requirements for
    that. Every `HashMapU8` will reserve `256 * ::core::mem::size_of::<Option<V>>()`
    bytes. Being that there''s unsafe code in this implementation, it''s worthwhile
    doing an AFL run to search for crashes. The interpreter for the specialized map
    is similar to the naive interpreter, except that we now take care to parse for
    `u8` keys:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'I''ll spare you the AFL output but, as a reminder, here''s how you run the
    specialized interpreter through:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Producing a criterion benchmark will be very similar to the approach taken
    for the naive implementation, save that we''ll swap out a few names here and there.
    We''ll skip listing the code with the hopes that you''ll be able to reproduce
    it as desired. The results, however, are promising:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'In a like manner to `naive.rs` and `standard.rs` from previously, we''ve also
    got a `specialized.rs` runner which, to avoid duplication, we''ll avoid listing
    here. Let''s run specialized through Valgrind cachegrind:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Compared to the standard `valgrind` run, we''re doing around 1/5 the total
    number of instructions and with substantially fewer `D1` and `LLd` misses. No
    surprise here. Our *hash* for `HashMapU8` is an exceedingly cheap pointer offset
    and the size of the storage is going to fit comfortably into the cache. Linux
    perf tells a similar story:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Phew! Let''s summarize our efforts:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '| name | Task Clock (ms) | Instructions | Branches | Branch Misses | Cache
    References | Cache Misses |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| specialized | 1.433884 | 6,141,529 | 749,155 | 59,914 | 74,760 | n/a |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| standard | 6.923765 | 26,234,708 | 2,802,334 | 290,475 | 635,526 | 67,304
    |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| naive | 1323.724713 | 15,390,499,356 | 4,428,637,974 | 204,132 | 455,719,875
    | 21,311 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What should we understand from all of this? To produce software that operates
    at the edge of the machine's ability, you must understand some important things.
    Firstly, if you aren't measuring your program, you're only guessing. Measuring
    runtime, as criterion does, is important but a coarse insight. *Where is my program
    spending its time?* is a question the Valgrind suite and perf can answer, but
    you've got to have benchmarks in place to contextualize your questions. Measuring
    and then validating behavior is also an important chunk of this work, which is
    why we spent so much time on QuickCheck and AFL. Secondly, have a goal in mind.
    In this chapter, we've made the speed of standard library `HashMap` our goal but,
    in an actual code base, there's always going to be places to polish and improve.
    What matters is knowing what needs to happen to solve the problem at hand, which
    will tell you where your time needs to be spent. Thirdly, understand your machine.
    Modern superscalar, parallel machines are odd beasts to program and, without giving
    a thought to their behaviors, it's going to be tough understanding why your program
    behaves the way it does. Finally, algorithms matter above all else. Our naive
    `HashMap` failed to perform well because it was a screwy idea to perform an average
    O(n/2) operations for every insertion, which we proved out in practice. Standard
    library's `HashMap` is a good, general-purpose structure based on linear probing
    and clearly, a lot of thought went into making it function well for a variety
    of cases. When your program is too slow, rather than micro-optimizing, take a
    step back and consider the problem space. Are there better algorithms available,
    is there some insight into the data that can be exploited to shift the algorithm
    to some other direction entirely?
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: That's performance work in a nutshell. Pretty satisfying, in my opinion.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered measuring and improving the performance of a serial
    Rust program while demonstrating the program's fitness for purpose. This is a
    huge area of work and there's a deep well of literature to pull from.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '*Rust''s std::collections is absolutely horrible*, available at [https://www.reddit.com/r/rust/comments/52grcl/rusts_stdcollections_is_absolutely_horrible/](https://www.reddit.com/r/rust/comments/52grcl/rusts_stdcollections_is_absolutely_horrible/). The
    original poster admitted the title is a bit on the click-baity side but the discussion
    on Reddit is well worth reading. The original author of standard library''s `HashMap`
    weighs in on the design decisions in the implementation.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Robin Hood Hashing*, 1985, Pedro Celis. This thesis introduced the Robin Hood
    hashing strategy for constructing associative arrays and is the foundation for
    the implementation you''ll find in Rust. The paper also goes into further search
    strategies that didn''t find their way into Rust''s implementation but should
    be of interest to readers with ambitions toward building hashing search structures.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Robin Hood hashing*, Emmanuel Goossaert, available at [http://codecapsule.com/2013/11/11/robin-hood-hashing/](http://codecapsule.com/2013/11/11/robin-hood-hashing/).
    The Rust standard library HashMap makes continued reference to this blog post
    and its follow-on, linked in the text. The description here is of a higher-level
    than that of Celis'' thesis and potentially easier to understand as a result.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Denial of Service via Algorithmic Complexity Attacks*, 2003, Scott Crosby
    and Dan Wallach. This paper outlines a denial of service attack on network services
    by exploiting algorithmic blow-ups in their implementations. The consequence of
    this paper influenced Rust''s decision to ship a safe-by-default HashMap.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*QuickCheck: Lightweight Tool for Random Testing of Haskell Programs*, 2000,
    Koen Claessen and John Hughes. This paper introduces the QuickCheck tool for Haskell
    and introduces property-based testing to the world. The research here builds on
    previous work into randomized testing but is novel for realizing that computers
    had got fast enough to support type-directed generation as well as shipping with
    the implementation in a single page appendix. Many, many subsequent papers have
    built on this one to improve the probing ability of property testers.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*An Evaluation of Random Testing*, 1984, Joe Duran and Simeon Ntafos. The 1970s
    and 1980s were an interesting time for software testing. Formal methods were seen
    as being just around the corner and the preferred testing methods relied on intimate
    knowledge of the program''s structure. Duran and Ntafos evaluated the ideal techniques
    of the day against random generation of data and found that randomness compared
    favorably with significantly less programmer effort. This paper put random testing
    on the map.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Experiences with QuickCheck: Testing the Hard Stuff and Staying Sane*, 2016,
    John Hughes. This paper is a follow-on to the original QuickCheck paper by Claessen
    and Hughes in which Hughes describes his subsequent fifteen years of experience
    doing property testing. The techniques laid out in this paper are a significant
    evolution of those presented in the 2000 paper and well-worth studying by anyone
    doing property tests as a part of their work. That ought to be most people, is
    my take.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*American Fuzzy Lop website*, available at [http://lcamtuf.coredump.cx/afl](http://lcamtuf.coredump.cx/afl/). AFL
    is the product of a long tail of research into efficiently mutating inputs for
    the purpose of triggering bugs. As of writing this book, it is best of breed and
    has a long trophy list to show for it. The website has links to AFL''s documentation
    and relevant research to understand its function in deeper detail.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Compact Data Structures: A Practical Approach*, 2016, Gonzalo Navarro. One
    of the major techniques of exploiting cache locality is to shrink the individual
    elements of a working set, implying more elements are available in the working
    set. Compact data structures, those that can be operated on, at, or near their
    information theory minimal representation, is an ongoing and exciting area. Navarro''s
    book is excellent and well-worth studying for anyone who is interested in exploring
    this avenue of optimization.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*vec_map*, various authors. `vec_map` is a Rust crate that exploits the same
    ideas as this chapter''s `HashMapU8` but in a generic implementation, with full
    compatibility to the standard library HashMap. The source code is quite interesting
    and warmly recommended.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reevaluating Amdahl''s Law*, 1988, John Gustafson. This is an exceptionally
    short paper and clearly explains Amdahl''s formulation as well as Gustafson''s
    objection to its underlying assumptions. That the paper is describing an interpretation
    in which the serial portion is shrunk is clear only after a few readings, or once
    some kind soul explains this to you.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tracking issue for specialization (RFC 1210)*, available at [https://github.com/rust-lang/rust/issues/31844](https://github.com/rust-lang/rust/issues/31844).This
    issue is a pretty good insight into the way the Rust community goes about stabilizing
    a major feature. The original RFC is from 2016\. Pretty much ever since the point
    it was accepted that there''s been a feature flag in nightly for experimentation
    and a debate on the consequences of making the work stable.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

<html><head></head><body>
        

                            
                    <h1 class="header-title">Algorithm Evaluation</h1>
                
            
            
                
<p class="mce-root">When looking at algorithms as defined entities, what makes one algorithm better than the other? Is it the number of steps required to finish? The amount of memory that is committed? CPU cycles? How do they compare across machines and operating systems with different memory allocators?</p>
<p>There are a lot of questions here that need answers, since comparing work with others is important in order to find the best approach possible to solve a given problem. In this chapter, you can look forward to learning about the following:</p>
<ul>
<li>Evaluating algorithms in practice</li>
<li>Classifying algorithm and data structure behaviors</li>
<li>Estimating the plausibility of a better algorithm</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">The Big O notation</h1>
                
            
            
                
<p>Physics is not a topic in this book, but its influence is far-reaching and powerful enough to be obeyed everywhere, even by virtual constructs such as algorithms! However great their design, they still are constrained by two important factors: time and space.</p>
<p>Time? Whenever anything needs to be done, a sequence of steps is required. By multiplying the number of steps by the time for each step, the total—absolute—time is easy to calculate. Or so we think. For computers, this is <em>mostly</em> true, but many questions make it very hard to really know, since modern CPUs go way beyond what previous generations were able to achieve. Is that only thanks to higher clock rates? What about the additional cores? SIMD? Simply taking the absolute time won't achieve real comparability between algorithms. Maybe the number of steps is what we should use.</p>
<p>Space (as in memory) has become a commodity in many domains over the last few years, even in the embedded space. While the situation has improved, it still pays to be mindful of how many bytes are stored in memory and how much that contributes to the goal of the algorithm. Or in other words, is this worth it? Many algorithmic tasks face a trade-off between what's stored in memory and what's computed on demand. The latter might be just enough to solve the problem, or it might not be; this is a decision the developer has to make.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Other people's code</h1>
                
            
            
                
<p>Consequently, every algorithm must have a "number of steps required" and "bytes of memory required" property, right? Close: since they are ever-changing variables, a universal way of describing what other people have achieved is necessary.</p>
<p>Typically, programmers instinctively know how to do that: "is this thing really doing everything twice?!" should be a familiar outcry. What has been said here? Assuming it's a function that has an input parameter <kbd>x</kbd>, it sounds like the function is doing something with <kbd>x</kbd> twice. Mathematically speaking, this would be expressed as <em>f(x) = 2x</em>.</p>
<p>What this is really saying is that for every input, the required number of steps to fully execute the function is twice the input—isn't this exactly what we have been looking for? What would be a better way to write it down?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Big O</h1>
                
            
            
                
<p>Looking at that issue from a (mathematical) function perspective, this is a shared need across mathematics, computer science, physics, and so on: they all want to know how expensive a function is. This is why a common notation was invented by Edmund Landau: the Big O notation (or Landau notation) consisting of the uppercase letter <em>O,</em> which declares the <em>order</em> of a function. The main growth factor is then put into parentheses following the letter <em>O</em>.</p>
<p>There are other, related notations that use small <em>o</em>, Omegas, Theta, and others, but those are less relevant in practical terms. Check the <em>Further reading</em> section for an article by Donald Knuth on this.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Asymptotic runtime complexity</h1>
                
            
            
                
<p>For computer science, the exact, absolute runtime is typically not important when implementing algorithms (you can always get a faster computer). Instead, the runtime complexity is more important since it directly influences performance as an overall measure of work, independent of details.</p>
<p>Since this is not an exact measurement and the actual performance is influenced by other factors, sticking with an asymptotic (read: rough) measure is the best strategy. In addition to that, algorithms have best and worst cases. Unless you are trying to improve on a particular case, the worst case is what's typically compared:</p>
<pre>let my_vec = vec![5,6,10,33,53,77];<br/>for i in my_vec.iter() {<br/>    if i == 5 {<br/>        break;<br/>    }    <br/>    println!("{}", i);<br/>}</pre>
<p>Iterating over this, <kbd>Vec&lt;T&gt;</kbd> has a runtime complexity of <em>O(n)</em> where <em>n</em> is the length of <kbd>Vec&lt;T&gt;</kbd>, regardless of the fact that the loop will break right away. Why? Because of pessimism. In reality, it is often hard to say what the input vector looks like and when it will actually exit, so the worst case is that it goes over the entire sequence without breaking, that is, <em>n</em> times. Now that we have seen how to write this down, let's see how to find out the runtime complexity of our own algorithms.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Making your own</h1>
                
            
            
                
<p>There are only a few aspects that change the complexity of an algorithm, those that have been shown to proportionally increase the total time required of an algorithm.</p>
<p>These are as follows:</p>
<ul>
<li>An arithmetic operation (<kbd>10 + 30</kbd>)</li>
<li>An assignment (<kbd>let x = 10</kbd>)</li>
<li>A test (<kbd>x == 10</kbd>)</li>
<li>A read or write of a basic type (<kbd>u32</kbd>, <kbd>bool</kbd>, and so on)</li>
</ul>
<p>If a piece of code only does one of these operations, it is one step, that is, <em>O(1),</em> and whenever there is a choice (<kbd>if</kbd> or <kbd>match</kbd>), the more complex branch has to be picked. Regardless of any input parameters, it will be the same number of steps—or constant time. If they are run in a loop, things get more interesting.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Loops</h1>
                
            
            
                
<p>When in a loop, and the number of iterations is not known at compile time, it will be a major influence on runtime complexity. If an operation mentioned earlier is executed in the loop (for example, a <kbd>sum</kbd> operation), one could declare the complexity as <em>O(1 * n)</em> for the arithmetic operation. After adding another operation, we could express it as <em>O(2 * n)</em> and, while this would be correct, these are not the driving forces of the loop. Regardless of the number of operations that are executed <em>n</em> times, the main growth factor remains <em>n</em>. Hence, we simply say <em>O(n),</em> unless you are trying to compare the same algorithm, where the number of iterations actually makes a difference. If there are subsequent loops, the most expensive one is picked.</p>
<p>However, upon nesting loops, the complexity changes considerably. Consider this (really bad) algorithm for comparing two lists:</p>
<pre><br/>let my_vec = vec![1,1,1,4,6,7,23,4];<br/>let my_other_vec = vec![66,2,4,6,892];<br/><br/><br/>for i in my_vec.iter() {<br/>    for j in my_other_vec.iter() {<br/>        if i == j {<br/>            panic!();<br/>        }<br/>    }<br/>}</pre>
<p>For each element in the first collection, the second collection is fully iterated. In other words, each element is looked at <em>n * m</em> times, resulting in a runtime complexity of <em>O(n*m)</em>, or, if both collections are the same size, <em>O(n²)</em>.</p>
<p>Can it get even worse? Yes!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Recursion</h1>
                
            
            
                
<p>Since all recursive algorithms can be unrolled into a loop, they can achieve the same results. However, recursion, or more specifically backtracking (which will be discussed in more detail in <a href="0131b10b-0ea4-4663-966a-46d6ecda142b.xhtml">Chapter 11</a>, <em>Random and Combinatorial</em>), makes it easier to create higher runtime complexities.</p>
<p>Typical combinatorial problems result in exponential runtimes, since there are a number of variations (such as different colors) that have to be enumerated <em>n</em> times so that a constraint is satisfied, which is only evaluated at the end. If there are two colors, the runtime complexity will therefore be <em>O(2<sup>n</sup>)</em> for a sequence of <em>n</em> colors, if no two colors can be adjacent to each other in a graph (graph coloring problem).</p>
<p>Recursive algorithms also make it hard to estimate runtime complexity quickly, since the branch development is hard to visualize.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Complexity classes</h1>
                
            
            
                
<p>In general, all algorithms fall into one of a few classes. Let's look at these classes ordered by their growth speed. Depending on the literature, there might be more or fewer classes, but this is a good set to start with since they represent the major directions of growth behavior.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">O(1)</h1>
                
            
            
                
<p>Constant time, which means everything will take the same amount of time. Since this chart would be a horizontal line at the <em>y</em> value of <em>1</em>, we will skip it in favor of sparing a tree.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">O(log(n))</h1>
                
            
            
                
<p>Growth is defined by the logarithmic function (in general, base 2), which is better than linear growth.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Here is the plot of the mathematical function:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/ad843385-9de2-43d1-a95e-9a7289852ef4.png" style="width:28.75em;height:21.58em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">O(n)</h1>
                
            
            
                
<p>Linear time, which means that the solution performance depends on the input in a linear way:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/3db23b99-73e0-4ae4-9a72-52f08be1f8b7.png" style="width:30.75em;height:23.08em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">O(n log(n))</h1>
                
            
            
                
<p>This is sometimes called quasilinear time and is the best achievable complexity for sorting:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0084a6fa-de2c-496a-af29-8ad592f8c8af.png" style="width:26.75em;height:19.75em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">O(n²)</h1>
                
            
            
                
<p>The squared runtime is typical for the naive implementation of search or sorting algorithms:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b6f8dd4b-d78c-4562-844f-758d836ef0f8.png" style="width:27.33em;height:19.92em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">O(2n)</h1>
                
            
            
                
<p>This is among the most expensive classes and can often be found in really hard-to-solve problems. This plot has a significantly smaller <em>x</em> value (<em>0 - 10</em>) and generates a higher <em>y</em> value (or runtime) than the <kbd>O(n log(n))</kbd> chart:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/a186b9ab-3fdf-48e7-995a-9ccd28266e92.png" style="width:27.67em;height:20.42em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Comparison</h1>
                
            
            
                
<p>Having individual charts is great for imagining the projected runtime and estimating what a task's performance could look like when its input is increased. If we plot all of these lines into a single chart, however, their performance will become obvious.</p>
<p>The typical comparison is against the linear time complexity (<em>O(n)</em>), since most naive solutions would be expected to achieve this performance:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/becff7dd-48c4-4026-a18b-67110922a5f3.png" style="width:32.17em;height:23.50em;"/></p>
<p>With this chart in mind, we can look at problems and their expected performance in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">In the wild</h1>
                
            
            
                
<p>In reality, there are a lot of factors that may influence the choice of space and runtime complexity. Typically, these factors are forms of resource constraints, such as power consumption on embedded devices, clock cycles in a cloud-hosted environment, and so on.</p>
<p class="mce-root">Since it is difficult to find out the complexities of a particular algorithm, it is helpful to know a few, so the choice comes intuitively. Often, the runtime complexity is not the only important aspect, but the absolute execution time counts. Under these conditions, a higher runtime complexity can be preferable if <em>n</em> is sufficiently small.</p>
<p class="mce-root">This is best demonstrated when <kbd>Vec&lt;T&gt;</kbd> contains only a few elements, where a linear search is a lot faster than sorting and then running a binary search. The overhead of sorting might just be too much compared to searching right away.</p>
<p class="mce-root">Getting this trade-off and the overall implementation right is hugely beneficial for the entire program and will outweigh any other optimizations. Let's take a look at a few runtime complexities that can be found in everyday life.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Data structures</h1>
                
            
            
                
<p>Algorithms on lists of all kinds almost always exhibit <em>O(n)</em> behavior, since most actions involve shifting or going through other elements. Hence, operations such as insert at or remove from a position, as well as finding elements (when unsorted), are <em>O(n)</em>. This is very visible, particularly in linked lists, with only a few exceptions: a dynamic array's element access (<em>O(1)</em>), prepending/appending elements or lists, and splitting lists appending elements in a linked list (<em>O(1)</em>).</p>
<p>Special cases of lists, such as <strong>stacks</strong> and <strong>queues</strong>, make use of these exceptions and let a user insert to or remove from only the ends of that list. <strong>Skip lists</strong> on the other hand employ a tree-like strategy for achieving great search performance, which speeds up inserts and removals too. But this comes at the expense of memory, since the additional elements are proportional (<em>log(n)</em>) to the list length.</p>
<p class="mce-root">For search, <strong>trees</strong> are great. Regular trees (that is, anything that can be a B-Tree) exhibit <em>O(log(n))</em> complexities on many operations, including insert, remove, and find. This is particularly great since difference to <em>O(n)</em> actually increases the more elements there are in the collection.</p>
<p class="mce-root">The only thing potentially better are <strong>maps</strong> and <strong>sets</strong>, if the underlying implementation uses an appropriate hashing algorithm. Any operation <em>s</em><em>hould</em> be completed in constant time (<em>O(1)</em>), if there are no collisions. Typically, there will be some collisions, but the runtime complexity will not exceed <em>O(n)</em> because, if all else fails, a linear search works. Consequently, real performance will be somewhere in between, with the hashing algorithm being the most important influence. For most libraries, hash maps (and sets) are faster than their tree-based counterparts.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Everyday things</h1>
                
            
            
                
<p>Whenever something needs sorting, there are a lot of ways to achieve that, but the baseline is <em>O(n²)</em>. It's the same way most people order their socks: pick one and find the match, then repeat (called <strong>selection sort</strong>). How else would one compare all elements to find their order? Better approaches, such as heap sort, merge sort, and so on, all exhibit <em>O(n log(n))</em> behavior in the worst case, which is the best possible (consistent) performance for sorting algorithms. Additionally, since the best case for any sorting algorithm is <em>O(n)</em>—making sure everything was already in order—the average case matters the most. We will get into strategies about that later in this book.</p>
<p class="mce-root"/>
<p>Search (or lookup) is another topic that we will get into in <a href="32002bad-c2bb-46e9-918d-12d7dabfe579.xhtml">Chapter 10</a>, <em>Finding Stuff</em>, but the associated runtime complexities are great examples. Searching on any unsorted data structure will be <em>O(n)</em> most of the time, while sorted collections can utilize binary search (a tree's search strategy) and achieve <em>O(log(n))</em>. In order to save the cost of sorting, ideal hash tables provide the absolute best case for search: <em>O(1)</em>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exotic things</h1>
                
            
            
                
<p class="mce-root">One class that was omitted from the earlier list is <strong>polynomial time</strong> (<strong>P</strong> in short). This class is quicker to solve than the exponential time class, but worse than <em>O(n²)</em>. These problems include checking whether a number is a prime number, or solving a Sudoku. However, there are other problems in this class as well that actually have <em>no</em> "quick" (that is, solvable in P) solution, but a solution can be verified in P time. These are called <strong>NP</strong> (an abbreviation of <strong>non-deterministic polynomial time</strong>) problems and the hardest of them are NP-hard (see the information box).</p>
<p>The distinction between P, NP, NP-complete, and NP-hard is not intuitive. NP problems are problems that can be solved using a non-deterministic Turing machine in P time. <strong>NP-hard</strong> problems are problems without a solution that, if solved, would have a polynomial time solution and if it is also an NP problem, it is also considered NP-complete. Additionally, finding a solution for one of either class (NP-hard or NP-complete) would imply a solution for <em>all</em> NP-hard/NP-complete problems.</p>
<p>While there are no known algorithms to solve these problems quickly, there typically are naive approaches that result in <em>very</em> long runtimes. Popular problems in this space include the traveling salesman problem (<em>O(n!)</em>), the knapsack problem (<em>O(2<sup>n</sup>)</em>, and the subset sum problem (<em>O(2<sup>n/2</sup>)</em>), all of which are currently solved (or approximated) using heuristics or programming techniques. For those interested, check the further reading section for links.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>The Big O notation is a way to describe the time and space requirements of an algorithm (or data structure). This is not an exact science, however; it's about finding the primary growth factor of each of the things mentioned to answer this question: what happens when the problem space grows bigger?</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Any algorithm will fall within a few relevant classes that describe that behavior. By applying the algorithm to one more element, how many more steps have to be taken? One easy way is to visualize the individual charts and think of whether it will be linear (<em>O(n)</em>), quasilinear (<em>O(n log(n))</em>), quadratic (<em>O(n²)</em>), or even exponential (<em>O(2<sup>n</sup>)</em>). Whatever the case may be, it is always best to do less work than there are elements to be looked at, such as constant (<em>O(1)</em>) or logarithmic (<em>O(log(n)</em>) behaviors!</p>
<p>Selecting the operations is typically done based on the worst-case behavior, that is, the upper limit of what is going to happen. In the next chapter, we will take a closer look at these behaviors in the cases of popular search algorithms.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Questions</h1>
                
            
            
                
<ul>
<li>Why estimate runtime complexity over, for example, number of statements?</li>
<li>How does runtime complexity relate to math functions?</li>
<li>Is the complexity class that is typically provided the best or worst case?</li>
<li>Why are loops important in estimating complexity?</li>
<li>Is <em>O(n log(n))</em> a better or worse runtime complexity than <em>O(log(n))</em>?</li>
<li>What are some commonly known complexity classes?</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Further reading</h1>
                
            
            
                
<p>You can refer to the following links to get more information on the topics covered in this chapter:</p>
<ul>
<li>Wikipedia's list of best-, worst-, and average-case complexities (<a href="https://en.wikipedia.org/wiki/Best,_worst_and_average_case">https://en.wikipedia.org/wiki/Best,_worst_and_average_case</a>)</li>
<li>Big O Cheatsheet (<a href="http://bigocheatsheet.com/">http://bigocheatsheet.com/</a>)</li>
<li>Heuristic algorithms at Northwestern University (<a href="https://optimization.mccormick.northwestern.edu/index.php/Heuristic_algorithms">https://optimization.mccormick.northwestern.edu/index.php/Heuristic_algorithms</a>)</li>
<li>Heuristic design and optimization at MIT (<a href="http://www.mit.edu/~moshref/Heuristics.html">http://www.mit.edu/~moshref/Heuristics.html</a>)</li>
<li><em>Big Omicron And Big Omega And Big Theta</em> by Donald Knuth (<a href="http://www.phil.uu.nl/datastructuren/10-11/knuth_big_omicron.pdf">http://www.phil.uu.nl/datastructuren/10-11/knuth_big_omicron.pdf</a>)</li>
</ul>


            

            
        
    </body></html>
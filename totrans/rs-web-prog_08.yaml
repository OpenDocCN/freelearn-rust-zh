- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building RESTful Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our to-do application, written in Rust, technically works. However, there are
    some improvements that we need to make. In this chapter, we will apply these improvements
    as we explore the concepts of the **RESTful** **API** design.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will finally reject unauthorized users before the request
    hits the view by assessing the layers of our system and refactoring how we handle
    requests throughout the request lifetime. We’ll then use this authentication to
    enable individual users to have their own list of to-do items. Finally, we will
    log our requests so that we can troubleshoot our application and get a deeper
    look into how our application runs, caching data in the frontend to reduce API
    calls. We will also explore nice-to-have concepts such as executing code on command
    and creating a uniform interface to split the frontend URLs from the backend URLs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What are RESTful services?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mapping our layered system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a uniform interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing statelessness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging our server traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code on demand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, we will have refactored our Rust application to
    support the principles of RESTful APIs. This means that we are going to map out
    the layers of our Rust application, create uniform API endpoints, log requests
    in our application, and cache results in the frontend.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter08](https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter08)
  prefs: []
  type: TYPE_NORMAL
- en: What are RESTful services?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`GET`), update (`PUT`), create (`POST`), and delete (`DELETE`) our users and
    to-do items. The goal of a RESTful approach is to increase speed/performance,
    reliability, and the ability to grow by reusing components that can be managed
    and updated without affecting the system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have noticed that before Rust, slow, high-level languages seemed to
    be a wise choice for web development. This is because they are quicker and safer
    to write. This is due to the main bottleneck for the speed of processing data
    in web development being the network connection speed. The RESTful design aims
    to improve the speed by economizing the system, such as reducing API calls, as
    opposed to just focusing on algorithm speed. With that in mind, in this section,
    we will be covering the following RESTful concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Layered system**: This enables us to add extra functionality, such as authorization,
    without having to change the interface. For instance, if we must check the **JSON
    Web Token** (**JWT**) in every view, then this is a lot of repetitive code that
    is hard to maintain and is prone to error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uniform system**: This simplifies and decouples the architecture, enabling
    whole parts of the application to evolve independently without clashing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statelessness**: This ensures that our application does not directly save
    anything on the server. This has implications for microservices and cloud computing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logging**: This enables us to peek into our application and see how it runs,
    exposing undesirable behavior even if there are no errors displayed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Caching**: This enables us to store data in the frontend to reduce the number
    of API calls to our backend API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code on demand**: This is where our backend server directly runs code on
    the frontend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll look at the layered system concept in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping our layered system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A layered system consists of layers with different units of functionality.
    It could be argued that these layers are different servers. This can be true in
    microservices and big systems. This can be the case when it comes to different
    layers of data. In big systems, it makes sense to have *hot data* that gets accessed
    and updated regularly and *cold data* where it is rarely accessed. However, while
    it is easy to think of layers as on different servers, they can be on the same
    server. We can map our layers with the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – The layers in our app](img/Figure_8.1_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – The layers in our app
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, our app follows this process:'
  prefs: []
  type: TYPE_NORMAL
- en: First, our *HTTP Handler* accepts the call by listening to the port that we
    defined when creating the server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it goes through the *middleware*, which is defined by using the `wrap_fn`
    function on our app.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once this is done, the URL of the request is mapped to the right view and the
    schemas we defined in our `src/json_serialization/` directory. These get passed
    into the resource (our views) defined in the `src/views` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we then want to update or get data from the database, we use the Diesel ORM
    to map these requests. At this stage, all our layers have been defined to manage
    the flow of data effectively, apart from our middleware. As pointed out in the
    previous chapter, [*Chapter 7*](B18722_07.xhtml#_idTextAnchor149), *Managing User
    Sessions*, we have implemented our middleware for authentication with the `JwToken`
    struct by implementing the `FromRequest` trait. With this, we can see that we
    can implement our middleware using either the `wrap_fn` or implementing the `FromRequest`
    trait. When do you think we should use the `wrap_fn` or `FromRequest` trait? Both
    have advantages and disadvantages. If we want to implement our middleware for
    specific individual views, then implementing the `FromRequest` trait is the best
    option. This is because we can slot a struct implementing the `FromRequest` trait
    into the view that we want. Authentication is a good use case for implementing
    the `FromRequest` trait because we want to pick and choose what endpoints require
    authentication. However, if we want to implement a blanket rule, we would be better
    off implementing the selection of views for authentication in the `wrap_fn` function.
    Implementing our middleware in the `wrap_fn` function means that it is implemented
    for every request.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of this could be that we are no longer supporting version one for
    all our endpoints. If we were going to do this, we would have to warn third-party
    users of our decision to no longer support version one of our API. Once our date
    has passed, we will have to give a helpful message that we are no longer supporting
    version one. Before we start working on our middleware layer, we must define the
    following imports at the top of our `main.rs` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To ensure that we know that the incoming request is destined for a `v1` endpoint,
    we must define a flag that we can check later when deciding whether to process
    the request or reject it. We can do this by using the following code in our `main.rs`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we can see that we declare that there is a Boolean
    under the name of `passed`. If `v1` is not in the URL, then it is set to `true`.
    If `v1` is present in the URL, then `passed` is set to `false`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have defined a flag, we can use it to dictate what happens to the
    request. Before we do this, we must take note of the last lines of `wrap_fn`,
    as denoted in this code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We are waiting for the call to finish, then returning the result as the variable
    called `result`. With our blocking of the `v1` API called, we must check to see
    whether the request passes. If it does, we then run the preceding code. However,
    if the request fails, we must bypass this and define another future, which is
    just the response.
  prefs: []
  type: TYPE_NORMAL
- en: At face value, this can seem straightforward. Both will return the same thing,
    that is, a response. However, Rust will not compile. It will throw an error based
    on incompatible types. This is because `async` blocks behave like closures. This
    means that every `async` block is its own type. This can be frustrating, and due
    to this subtle detail, it can lead to developers burning hours trying to get the
    two futures to play with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, there is an enum in the futures crate that solves this problem for
    us. The `Either` enum combines two different futures, streams, or sinks that have
    the same associated types into a single type. This enables us to match the `passed`
    flag, and fire and return the appropriate process with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we can see that we assign `end_result` to be called
    as a view, or directly return it to an unauthorized response depending on the
    `passed` flag. We then return this at the end of `wrap_fn`. Knowing how to use
    the `Either` enum is a handy trick to have up your sleeve and will save you hours
    when you need your code to choose between two different futures.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check to see whether we are blocking `v1`, we can call a simple `get` request
    as seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – The response to blocked v1](img/Figure_8.2_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – The response to blocked v1
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that our API call is blocked with a helpful message. If we do the
    API call through Postman, we will see that we get a `501 Not Implemented` error
    as seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Postman’s response to blocked v1](img/Figure_8.3_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Postman’s response to blocked v1
  prefs: []
  type: TYPE_NORMAL
- en: We might want to add more resources for getting items in the future. This poses
    a potential problem because some views might start clashing with the app views.
    For instance, our to-do item API views only have the prefix `item`. Getting all
    the items requires the `v1/item/get` endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: It could be reasonable to develop a view for the app that looks at a to-do item
    in detail for editing with the `v1/item/get/{id}` endpoint later. However, this
    increases the risk of clashes between the frontend app views and the backend API
    calls. To prevent this, we are going to have to ensure that our API has a uniform
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: Building a uniform interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having a uniform interface means that our resources can be uniquely identifiable
    through a URL. This decouples the backend endpoints and frontend views, enabling
    our app to scale without clashes between the frontend views and backend endpoints.
    We have decoupled our backend from the frontend using the version tag. When a
    URL endpoint includes a version tag, such as `v1` or `v2`, we know that call is
    hitting the backend Rust server. When we are developing our Rust server, we might
    want to work on a newer version of our API calls. However, we do not want to allow
    users to access the version in development. To enable live users to access one
    version while we deploy another version on a test server, we will need to dynamically
    define the API version for the server. With the knowledge that you have acquired
    so far in this book, you could simply define the version number in the `config.yml`
    file and load it. However, we would have to read the `config.yml` config file
    for every request. Remember that when we set up the database connection pool we
    read the connection string from the `config.yml` file once, meaning that it is
    present for the entire lifetime of the program. We would like to define the version
    once and then refer to it for the lifecycle of the program. Intuitively, you might
    want to define the version in the `main` function in the `main.rs` file before
    we define the server and then access the definition of the version inside the
    `wrap_fn`, as seen in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if we try and compile the preceding code, it will fail as the lifetime
    of the `outcome` variable is not long enough. We can convert our `outcome` variable
    as a constant with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will run without any lifetime issues. However, if we were
    to load our version, we will have to read it from a file. In Rust, if we read
    from a file, we do not know what the size of the variable being read from the
    file is. Therefore, the variable we read from the file is going to be a string.
    The problem here is that allocating a string is not something that can be computed
    at compile time. Therefore, we are going to have to write the version directly
    into our `main.rs` file. We can do this by using a `build` file.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We utilize `build` files in this problem to teach the concept of `build` files
    so you can use them if needed. There is nothing stopping you from hardcoding the
    constant in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where a single Rust file runs before the Rust application is run. This
    `build` file will automatically run when we are compiling the main Rust application.
    We can define the dependencies needed to run our `build` file in the `build dependencies`
    section in the `Cargo.toml` file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that our `build` Rust file is defined in the root of the application
    in the `build.rs` file. We will then define the dependencies needed for the build
    phase in the `[build-dependencies]` section. Now that our dependencies are defined,
    our `build.rs` file can take the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we can see that we need to import what we need to read from a YAML file
    and write it to a standard text file. Then, we will open a `build_config.yml`
    file, which is in the root of the web application next to the `config.yml` file.
    We will then extract the `ALLOWED_VERSION` from the `build_config.yml` file and
    write it into a text file. Now that we have defined the build process and what
    is needed from the `build_config.yml` file, our `build_config.yml` file will have
    to take the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have everything defined for our build, we can introduce a `const`
    instance for our version through the file that we wrote to in our `build.rs` file.
    To do this, our `main.rs` file will need a few changes. First, we define the `const`
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We then deem the request to pass if the version is allowed with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the error response and service call with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to build and run our application with a specific version supported.
    If we run our application and make a `v2` request, we get the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Postman response to blocked v2](img/Figure_8.4_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Postman response to blocked v2
  prefs: []
  type: TYPE_NORMAL
- en: We can see that our version guard is now working. This also means that we must
    use the React application to access the frontend views or you can add a `v1` to
    the frontend API endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we run our app, we can see that our frontend works with the new endpoints.
    With this, we are one step closer to developing a RESTful API for our app. However,
    we still have some glaring shortcomings. Right now, we can create another user
    and log in under that user. In the next section, we’ll explore how to manage our
    user state in a stateless fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing statelessness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statelessness is where the server does not store any information about the client
    session. The advantages here are straightforward. It enables our application to
    scale more easily as we free up resources on the server side by storing session
    information on the client’s side instead.
  prefs: []
  type: TYPE_NORMAL
- en: It also empowers us to be more flexible with our computing approach. For instance,
    let’s say that our application has exploded in popularity. As a result, we may
    want to spin our app up on two computing instances or servers and have a load
    balancer direct traffic to both instances in a balanced manner. If information
    is stored on the server, the user will have an inconsistent experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'They may update the state of their session on one computing instance, but then,
    when they make another request, they may hit another computing instance that has
    outdated data. Considering this, statelessness cannot just be achieved by storing
    everything in the client. If our database is not dependent on a computing instance
    of our app, we can also store our data on this database, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Our stateless approach](img/Figure_8.5_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Our stateless approach
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, our app is already stateless. It stores the user ID in a JWT
    in the frontend and we store our user data models and to-do items in our PostgreSQL
    database. However, we might want to store Rust structs in our application. For
    instance, we could build a struct that counts the number of requests hitting the
    server. With reference to *Figure 8**.5*, we cannot just save our structs locally
    on the server. Instead, we will store our structs in Redis, carrying out the processes
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Steps to saving structs in Redis](img/Figure_8.6_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Steps to saving structs in Redis
  prefs: []
  type: TYPE_NORMAL
- en: The difference between PostgreSQL and Redis
  prefs: []
  type: TYPE_NORMAL
- en: Redis is a database, but it is different from a PostgreSQL database. Redis is
    closer to a key-value store. Redis is also fast due to the data being in memory.
    While Redis is not as complete as PostgreSQL for managing tables and how they
    relate to each other, Redis does have advantages. Redis supports useful data structures
    such as lists, sets, hashes, queues, and channels. You can also set expiry times
    on the data that you insert into Redis. You also do not need to handle data migrations
    with Redis. This makes Redis an ideal database for caching data that you need
    quick access to, but you are not too concerned about persistence. For the channels
    and queues, Redis is also ideal for facilitating communications between subscribers
    and publishers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can achieve the process in *Figure 8**.6* by carrying out the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Define Redis service for Docker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update Rust dependencies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update our config file for a Redis connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a counter struct that can be saved and loaded in a Redis database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement the counter for each request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s go over each step in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to spinning up a Redis Docker service, we need to use a standard
    Redis container with standard ports. After we have implemented our Redis service,
    our `docker-compose.yml` file should have the current state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that we now have the Redis service and the database service running
    on the local machine. Now that Redis can be run, we need to update our dependencies
    in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recalling *Figure 8**.6*, we need to serialize the Rust struct into bytes before
    inserting it into Redis. With these steps in mind, we need the following dependencies
    in our `Cargo.toml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are using the `redis` crate to connect to the Redis database. Now that our
    dependencies are defined, we can start defining our config file in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to our `config.yml` file, we must add the URL for the Redis database
    connection. At this point in time of the book, our `config.yml` file should have
    the following form:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have not added the port number for our `REDIS_URL` parameter. This is because
    we are using the standard port in our Redis service, which is `6379`, so we do
    not have to define the port. We now have all the data ready to define a struct
    that can connect to Redis, which we will do in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will define our `Counter` struct in the `src/counter.rs` file. First, we
    will have to import the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will be using the `Config` instance to get the Redis URL, and the `Deserialize`
    and `Serialize` traits to enable conversion to bytes. Our `Counter` struct takes
    the following form:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have our `Counter` struct defined with all the traits, we need
    to define the functions that are required for our operations with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the preceding functions defined, we can load and save our `Counter` struct
    into the Redis database. When it comes to building our `get_redis_url` function,
    it should be no surprise that it can take the following form:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have the Redis URL, we can save our `Counter` struct with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we can see that we can serialize our `Counter` struct to a `Vec<u8>`.
    We will then define the client for the Redis and insert our serialized `Counter`
    struct under the key `"COUNTER"`. There are more features to Redis, however, you
    can utilize Redis for this chapter by thinking about Redis as being a big scalable
    in-memory hashmap. With the hashmap concept in mind, how do you think we could
    get the `Counter` struct from the Redis database? You probably guessed it; we
    use the `GET` command with the `"COUNTER"` key and then deserialize it with the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have now defined our `Counter` struct. We have everything in-line to implement
    in our `main.rs` file in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to increasing the count by one every time a request comes in,
    we need to carry out the following code in the `main.rs` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see that we define the `counter` module. Before we spin up the
    server, we need to create the new `Counter` struct and insert it into Redis. We
    then get the `Counter` from Redis, increase the count, then save it for every
    request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now when we run our server, we can see that our counter is increasing every
    time we hit our server with a request. Our printout should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have integrated another storage option, our app essentially functions
    the way we want it to. If we wanted to ship our application now, there is nothing
    really stopping us from configuring the build with Docker and deploying it on
    a server with a database and **NGINX**.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency issues
  prefs: []
  type: TYPE_NORMAL
- en: If two servers request the counter at the same time, there is a risk of missing
    a request. The counter example was explored to demonstrate how to store serialized
    structs in Redis. If you need to implement a simple counter in a Redis database
    and concurrency is a concern, it is suggested that you use the `INCR` command.
    The `INCR` command increases the number under the key you select by one in the
    Redis database, returning the new increased number as a result. Seeing as the
    counter is increased in the Redis database, we have reduced the risk of concurrency
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are always things we can add. In the next section, we’ll investigate
    logging requests.
  prefs: []
  type: TYPE_NORMAL
- en: Logging our server traffic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, our application does not log anything. This does not directly affect
    the running of the app. However, there are some advantages to logging. Logging
    enables us to debug our applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Right now, as we are developing locally, it may not seem like logging is really
    needed. However, in a production environment, there are many reasons why an application
    can fail, including Docker container orchestration issues. Logs that note what
    processes have happened can help us to pinpoint an error. We can also use logging
    to see when edge cases and errors arise for us to monitor the general health of
    our application. When it comes to logging, there are four types of logs that we
    can build:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Informational (info)**: This is general logging. If we want to track a general
    process and how it is progressing, we use this type of log. Examples of using
    this are starting and stopping the server and logging certain checkpoints that
    we want to monitor, such as HTTP requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Verbose**: This is information such as the type defined in the previous point.
    However, it is more granular to inform us of a more detailed flow of a process.
    This type of log is mainly used for debugging purposes and should generally be
    avoided when it comes to production settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Warning**: We use this type when we are logging a process that is failing
    and should not be ignored. However, we can use this instead of raising an error
    because we do not want the service to be interrupted or the user to be aware of
    the specific error. The logs themselves are for us to be alerted of the problem
    to allow us to then act. Problems such as calls to another server failing are
    appropriate for this category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error**: This is where the process is interrupted due to an error and we
    need to sort it out as quickly as possible. We also need to inform the user that
    the transaction did not go through. A good example of this is a failure to connect
    or insert data into a database. If this happens, there is no record of the transaction
    happening and it cannot be solved retroactively. However, it should be noted that
    the process can continue running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a warning comes up about the server failing to send an email, connect to
    another server to dispatch a product for shipping, and so on. Once we have sorted
    out the problem, we can retroactively make a database call to transactions in
    this timeframe and make the calls to the server with the right information.
  prefs: []
  type: TYPE_NORMAL
- en: In the worst case, there will be a delay. With the error type, we will not be
    able to make the database call as the server was interrupted by an error before
    the order was even entered in the database. Considering this, it is clear why
    error logging is highly critical, as the user needs to be informed that there
    is a problem and their transaction did not go through, prompting them to try again
    later.
  prefs: []
  type: TYPE_NORMAL
- en: We could consider the option of including enough information in the error logs
    to retroactively go back and update the database and complete the rest of the
    process when the issue is resolved, removing the need to inform the user. While
    this is tempting, we must consider two things. Log data is generally unstructured.
  prefs: []
  type: TYPE_NORMAL
- en: There is no quality control for what goes into a log. Therefore, once we have
    finally managed to manipulate the log data into the right format, there is still
    a chance that corrupt data could find its way into the database.
  prefs: []
  type: TYPE_NORMAL
- en: The second issue is that logs are not considered secure. They get copied and
    sent to other developers in a crisis and they can be plugged into other pipelines
    and websites, such as *Bugsnag*, to monitor logs. Considering the nature of logs,
    it is not good practice to have any identifiable information in a log.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have understood the uses of logging, we can start configuring our
    own logger. When it comes to logging, we are going to use the Actix-web logger.
    This gives us flexibility on what we log while having the underlying mechanics
    of logging configured and working well with our Actix server. To build our logger,
    we must define a new crate in our `Cargo.toml` file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'This enables us to configure a logger using environment variables. We can now
    focus on `main.rs` as this is where our logger is configured and used. First,
    we will import our logger with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'With this import, we can define our logger in the `main` function with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are stating that our logger is going to log information to the info
    stream. With the logger configured, we can then wrap our server with the logger
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see in our logger that we passed in the `"&a %{User-Agent}I %r %s %D"`
    string. This string is interpreted by the logger, tell them what to log. The Actix
    logger can take the following inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`%%`: The percent sign'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%a`: The remote IP address (IP address of proxy if using reverse proxy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%t`: The time when the request was started to process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%P`: The process ID of the child that serviced the request'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%r`: The first line of request'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%s`: The response status code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%b`: The size of the response in bytes, including HTTP headers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%T`: The time taken to serve the request, in seconds with floating fraction
    in .06f format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%D`: The time taken to serve the request, in milliseconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%{``FOO}i`: `request.headers[''FOO'']`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%{``FOO}o`: `response.headers[''FOO'']`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%{``FOO}e`: `os.environ[''FOO'']`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these inputs, we can work out that we are going to log the remote IP address,
    user-agent, endpoint of the request, and the time taken to process the request.
    We will do this for every request that our Rust server has. Starting our Rust
    server with the logging gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that the time of the starting of the server with the number
    of workers is automatically logged. Then, if we start our frontend, we should
    be prompted to log in as the token should have expired by now. A full standard
    request log should look like the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the time, the fact that it is an `INFO`-level log, and what logger
    is logging it. We can also see my IP address, which is local as I am running my
    application locally, my computer/browser details, and the API call with a `401`
    response code. If we trim everything from the request log apart from the method,
    API endpoint, response code, and response time, our login prompt will look like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that I fail in getting items, with an unauthorized response. I then
    log in and get an `OK` response from the login. Here, we can see an `OPTIONS`
    and `POST` method. The `OPTIONS` method is for our CORS, which is why the `OPTIONS`
    calls processing time is a fraction of other API requests. We can see that we
    then get our items that are then rendered to the page. However, we can see what
    happens in the following logs when we refresh the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: We can see that there are two `GET` requests for the items. However, we have
    not altered the to-do items in the database. This is not an error, but it is wasteful.
    To optimize this, we can utilize the REST constraint of caching in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Caching is where we store data in the frontend to be reused. This enables us
    to reduce the number of API calls to the backend and reduce latency. Because the
    benefits are so clear, it can be tempting to cache everything. However, there
    are some things to consider.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency is a clear issue. The data could be outdated, leading to confusion
    and data corruption when sending the wrong information to the backend. There are
    also security concerns. If one user logs out and another user logs in on the same
    computer, there is a risk that the second user will be able to access the first
    user’s items. With this, there must be a couple of checks in place. The correct
    user needs to be logged in and the data needs to be timestamped so that if the
    cached data is accessed past a certain period, a `GET` request is made to refresh
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our application is fairly locked down. We cannot access anything unless we
    are logged in. The main process that we could cache in our application is the
    `GET` items call. All other calls that edit the state of the item list in the
    backend return the updated items. Considering this, our caching mechanism looks
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Our caching approach](img/Figure_8.7_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Our caching approach
  prefs: []
  type: TYPE_NORMAL
- en: The loop in the diagram can be executed as many times as we want when refreshing
    the page. However, this might not be a good idea. If a user logs onto our application
    on their phone when they are in the kitchen to update the list, then the user
    will have a problem when they go back to their computer to do some work, refreshing
    the page on the computer and updating the list. This caching system would expose
    the user to out-of-date data that will be sent to the backend. We can reduce the
    risk of this happening by referencing the time stamp. When the timestamp is older
    than the cutoff, we will then make another API call to refresh the data when the
    user refreshes the page.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to our caching logic, it will all be implemented in the `front_end/src/App.js`
    file under the `getItems` function. Our `getItems` function will take the following
    layout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we state that the time difference between the last cached items and the
    current time must be less than 120 seconds, which is 2 minutes. If the time difference
    is below 2 minutes, we will get our data from the cache. However, if the time
    difference is above 2 minutes, then we make a request to our API backend. If we
    get an unauthorized response, we will then log out. First, in this `getItems`
    function, we get the date that the items were cached and calculate the difference
    between then and now, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'If our time difference is 2 minutes, we will get our data from the local storage
    and update our state with that data with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we must parse the data from the local storage because the local storage
    only handles string data. Because local storage only handles strings, we must
    stringify the data that we are inserting into the local storage when we make the
    API request with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run our application, we only make one API call. If we refresh our application
    before the 2 minutes are up, we can see that there are no new API calls despite
    our frontend rendering all the items from the cache. However, if we create, edit,
    or delete an item, and then refresh the page before the 2 minutes is up, we will
    see that our view will revert to the previous out-of-date state. This is because
    the created, edited, and deleted items also return to their previous state but
    they are not stored in the local storage. This can be handled by updating our
    `handleReturnedState` function with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have it. We have managed to cache our data and reuse it to prevent
    our backend API from being hit excessively. This can be applied to other frontend
    processes too. For instance, a customer basket could be cached and used when the
    user checks out.
  prefs: []
  type: TYPE_NORMAL
- en: This takes our simple website one step closer to being a web app. However, we
    must acknowledge that as we use caching more, the complexity of the frontend increases.
    For our application, this is where the caching stops. Right now, there are no
    more alterations needed on our applications for the rest of the hour. However,
    there is one more concept that we should briefly cover, which is code on demand.
  prefs: []
  type: TYPE_NORMAL
- en: Code on demand
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Code on demand is where the backend server directly executes code on the frontend.
    This constraint is optional and not widely used. However, it can be useful as
    it gives the backend server the right to decide as and when code is executed on
    the frontend. We have already been doing this; in our logout view, we directly
    execute JavaScript on the frontend by simply returning it in a string. This is
    done in the `src/views/auth/logout.rs` file. We must remember that we have now
    added to-do items to our local storage. If we do not remove these items from our
    local storage when logging out, somebody else would be able to access our to-do
    items if they manage to log in to their own account on the same computer within
    2 minutes. While this is highly unlikely, we might as well be safe. Remember that
    our logout view in the `src/views/auth/logout.rs` file takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside the body of our response, we have the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: With this, we have not only removed the user token, but we have also removed
    all items and dates. With this, our data is safe once we have logged out.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have gone through the different aspects of RESTful design
    and implemented them into our application. We have assessed the layers of our
    application, enabling us to refactor the middleware to enable two different futures
    to be processed depending on the outcome. This doesn’t just stop at authorizing
    requests. Based on the parameters of the request, we could implement middleware
    to redirect requests to other servers, or directly respond with a code-on-demand
    response that makes some changes to the frontend and then makes another API call.
    This approach gives us another tool, custom logic with multiple future outcomes
    in the middleware before the view is hit.
  prefs: []
  type: TYPE_NORMAL
- en: We then refactored our path struct to make the interface uniform, preventing
    clashes between frontend and backend views.
  prefs: []
  type: TYPE_NORMAL
- en: We then explored the different levels of logging and logged all our requests
    to highlight silent yet undesirable behavior. After refactoring our frontend to
    rectify this, we then used our logging to assess whether our caching mechanism
    was working correctly when caching to-do items into the frontend to prevent excessive
    API calls. Now, our application is passable. We can always make improvements;
    however, we are not at the stage where if we were to deploy our application onto
    a server, we would be able to monitor it, check the logs when something is going
    wrong, manage multiple users with their own to-do lists, and reject unauthorized
    requests before they even hit the view. We also have caching, and our application
    is stateless, accessing and writing data on a PostgreSQL and Redis database.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be writing unit tests for our Rust structs and
    functional tests for our API endpoints, as well as cleaning the code up ready
    for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why can we not simply code multiple futures into the middleware and merely call
    and return the one that is right considering request parameters and authorization
    outcomes, but must wrap them in an enum instead?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we add a new version of views but still support the old views in case
    our API is serving mobile apps and third parties that might not update instantly?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is the stateless constraint becoming more important in the era of elastic
    cloud computing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could we enable another service to be incorporated utilizing the properties
    of the JWT?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A warning log message hides the fact that an error has happened from the user
    but still alerts us to fix it. Why do we ever bother telling the user that an
    error has occurred and to try again with an error log?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the advantages of logging all requests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we sometimes have to use `async move`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rust’s strong typing system will complain. This is because `async` blocks behave
    like closures meaning that every `async` block is its own type. Pointing to multiple
    futures is like pointing to multiple types, and thus it will look like we are
    returning multiple different types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We add a new module in the views directory with the new views. These have the
    same endpoints and views with new parameters that are needed. We can then add
    a version parameter in the factory function. These new views will have the same
    endpoints with `v2` in them. This enables users to use the new and old API endpoints.
    We then notify users when the old version will no longer be supported, giving
    them time to update. At a specific time, we will move our version in the build
    to `v2`, cutting all requests that make `v1` calls and responding with a helpful
    message that `v1` is no longer supported. For this transition to work, we will
    have to update the allowed versions in the `build` config to a list of supported
    versions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With orchestration tools, microservices, and elastic computing instances on
    demand, spinning up and shutting down elastic computing instances due to demand
    is becoming more common practice. If we store data on the instance itself, when
    the user makes another API call, there is no guarantee that the user will hit
    the same instance, getting inconsistent data read and writes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The JWT token enables us to store the user ID. If the second service has the
    same secret key, we can merely pass requests to the other service with the JWT
    in the header. The other service does not have to have the login views or access
    to the user database and can still function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When an error happens that prevents us from retroactively going back and sorting
    out the issue, then we must raise an error instead of a warning. A classic example
    of an error is not being able to write to a database. A good example of a warning
    is another service not responding. When the other service is up and running, we
    can do a database call and call the service to finish off the process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In production, it is needed to assess the state of a server when troubleshooting.
    For instance, if a user is not experiencing an update, we can quickly check the
    logs to see if the server is in fact receiving the request or if there is an error
    with the caching in the frontend. We can also use it to see if our app is behaving
    the way we expect it to.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There could be a possibility that the lifetime of the variable that we are referencing
    in the `async` block might not live long enough to see the end of the `async`
    block. To resolve this, we can shift the ownership of the variable to the block
    with an `async` `move` block.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Part 4:Testing and Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When an application is built, we need to deploy it on a server so others can
    use it. We also need to test it to ensure it works to our expectations before
    we deploy it. In this part, we will cover unit and end-to-end testing using tools
    such as Postman. We’ll build our own build and testing pipelines to automate the
    testing, building, and deployment processes. We’ll cover how HTTP requests route
    to servers and what the HTTPS protocol is so we can implement it on AWS. We will
    also route traffic to our frontend and backend with NGINX, balance traffic between
    two individual servers on AWS, and lock down traffic to these servers and load
    balancer with AWS security groups. We will automate the AWS infrastructure using
    Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part includes the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18722_09.xhtml#_idTextAnchor182), *Testing Our Application Endpoints
    and Components*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B18722_10.xhtml#_idTextAnchor200), *Deploying Our Application
    on AWS*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B18722_11.xhtml#_idTextAnchor222), *Configuring HTTPS with NGINX
    on AWS*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

<html><head></head><body><div><p>&#13;
			<h1 id="_idParaDest-47"><em class="italic"><a id="_idTextAnchor046"/>Chapter 3</em>: Understanding Concurrency</h1>&#13;
			<p>Speeding up our code with Rust is useful. However, understanding concurrency and utilizing threads and processes can take our ability to speed up our code to the next level. In this chapter, we will go through what processes and threads are. We then go through the practical steps of spinning up threads and processes in Python and Rust. However, while this can be exciting, we also must acknowledge that reaching for threads and processes without thinking about our approach can end up tripping us up. To avoid this, we also explore algorithm complexity and how this affects our computation time. </p>&#13;
			<p>In this chapter, we will cover the following topics:</p>&#13;
			<ul>&#13;
				<li>Introducing concurrency</li>&#13;
				<li>Basic asynchronous programming with threads</li>&#13;
				<li>Running multiple processes</li>&#13;
				<li>Customizing threads and processes safely </li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Technical requirements</h1>&#13;
			<p>The code for this chapter can be accessed via the following GitHub link:</p>&#13;
			<p><a href="https://github.com/PacktPublishing/Speed-up-your-Python-with-Rust/tree/main/chapter_three">https://github.com/PacktPublishing/Speed-up-your-Python-with-Rust/tree/main/chapter_three</a></p>&#13;
			<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>Introducing concurrency </h1>&#13;
			<p>As we <a id="_idIndexMarker162"/>explored in the introduction of <a href="B17720_01_Final_SK_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Introduction to Rust from a Python Perspective</em>, Moore's law is now failing, and therefore we have to consider other ways in which we can speed up our processing. This is where concurrency comes in. Concurrency is essentially running multiple computations at the same time. Concurrency is everywhere, and to give the concept full justice, we would have to write a whole book on it. </p>&#13;
			<p>However, for the scope of this book, understanding the basics of concurrency (and when to use it) can <a id="_idIndexMarker163"/>add an extra tool to our belt that enables us to speed up computations. Furthermore, threads and processes are how we can break up our program into computations that run at the same time. To start our concurrency tour, we will cover threads.</p>&#13;
			<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/>Threads </h2>&#13;
			<p><strong class="bold">Threads</strong> are the smallest unit of computation that we can process and manage independently. Threads <a id="_idIndexMarker164"/>are used to break a program into computational parts that can be run at the same time. It also has to be noted that threads can be run out of sequence. This brings forward an important distinction between concurrency and parallelism. <strong class="bold">Concurrency</strong> is the <a id="_idIndexMarker165"/>task of running and managing multiple computations at the same time, while <strong class="bold">parallelism</strong> is the task of running multiple <a id="_idIndexMarker166"/>computations at the same time. Concurrency has a non-deterministic control flow, while parallelism has a deterministic control flow. Threads share resources such as memory and processing power; however, they also block each other. For instance, if we spin off a thread that requires constant processing power, we will merely block the other thread, as seen in the following diagram:</p>&#13;
			<p>&#13;
				<div>&#13;
					<img src="img/Figure_3.01_B17720.jpg" alt="Figure 3.1 – Two threads over time&#13;&#10;" width="1135" height="223"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 3.1 – Two threads over time</p>&#13;
			<p>Here, we can see that <strong class="bold">Thread A</strong> stops running when <strong class="bold">Thread B</strong> is running. This is demonstrated in Pan Wu's 2020 article on understanding multithreading through simulations where different types of tasks were timed. The results in the article are summed up in the following chart:</p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Figure_3.02_B17720.jpg" alt="Figure 3.2 – Times of different tasks" width="841" height="491"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 3.2 – Times of different tasks [Source: Pan Wu, https://towardsdatascience.com/understanding-python-multithreading-and-multiprocessing-via-simulation-3f600dbbfe31]</p>&#13;
			<p>Here, we can <a id="_idIndexMarker167"/>see that the times decrease as the number of workers decreases, <em class="italic">apart from</em> the <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>)-heavy multithreaded tasks. This is because, as demonstrated in <em class="italic">Figure 3.1</em>, the CPU-intensive threads are blocking, so only one worker can process at a time. It does not matter how many more <a id="_idIndexMarker168"/>workers you add. It must be noted that this is because of Python's <strong class="bold">global interpreter lock</strong> (<strong class="bold">GIL</strong>), which is covered in <a href="B17720_06_Final_SK_ePub.xhtml#_idTextAnchor100"><em class="italic">Chapter 6</em></a>, <em class="italic">Working with Python Objects in Rust</em>. In other contexts, such as Rust, they can be executed on <a id="_idIndexMarker169"/>different CPU cores and generally will not block each other. </p>&#13;
			<p>We can also see in <em class="italic">Figure 3.2</em> that the <strong class="bold">input/output</strong> (<strong class="bold">I/O</strong>)-heavy tasks do reduce in time taken when <a id="_idIndexMarker170"/>the workers increase. This is because there is idle time in I/O-heavy tasks. This is where we can really utilize threads. Let's say our task is making a call to a server. There is some idle time when waiting for a response, therefore utilizing threads to make multiple calls to servers will speed up the time. We also must note that processes work for CPU- and I/O-heavy tasks. Because of this, it is beneficial for us to explore what processes are. </p>&#13;
			<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Processes </h2>&#13;
			<p><strong class="bold">Processes</strong> are more <a id="_idIndexMarker171"/>expensive to produce compared to threads. In fact, a process can host multiple threads. This is usually depicted in the following classic multithreading diagram, as seen everywhere (including the multiprocessing <em class="italic">Wikimedia</em> page):</p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Figure_3.03_B17720.jpg" alt="Figure 3.3 – Relationship between threads and processes" width="434" height="417"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 3.3 – Relationship between threads and processes [Source: Cburnett (2007) (https://commons.wikimedia.org/wiki/File:Multithreaded_process.svg), CC BY-SA 3.0]</p>&#13;
			<p>This is a classic diagram because it encapsulates the relationship between processes and threads so well. Here, we can see that threads are a subset of a process. We can also see why threads share memory, and as a result, we must note that processes are typically independent and do not share memory. We also must note that context switches are more expensive when using processes. A context switch refers to when the state of a process (or thread) is stored so that it can be restored and resumed at a later state. An example of <a id="_idIndexMarker172"/>this would be waiting for an <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) response. The state can be saved, and another process/thread can run while we wait for the API response.  </p>&#13;
			<p>Now that we understand the basic concepts behind threads and processes, we need to learn how to practically use threads in our programs.</p>&#13;
			<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/>Basic asynchronous programming with threads </h1>&#13;
			<p>To utilize threading, we need to be able to start threads, allow them to run, and then join them. We <a id="_idIndexMarker173"/>can see the stages of practically <a id="_idIndexMarker174"/>managing our threads in the following diagram: </p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Figure_3.04_B17720.jpg" alt="Figure 3.4 – Stages of threads&#13;&#10;" width="690" height="659"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 3.4 – Stages of threads</p>&#13;
			<p>We start the threads, we then let them run, and once they have run, we join them. If we did not join them, the program would continue to run before the threads had finished. In Python, we create a thread by inheriting the <code>Thread</code> object, as follows: </p>&#13;
			<pre>from threading import Thread </pre>&#13;
			<pre>from time import sleep</pre>&#13;
			<pre>from typing import Optional</pre>&#13;
			<pre>class ExampleThread(Thread):</pre>&#13;
			<pre>    </pre>&#13;
			<pre>    def __init__(self, seconds: int, name: str) -&gt; None:</pre>&#13;
			<pre>        super().__init__()</pre>&#13;
			<pre>        self.seconds: int = seconds</pre>&#13;
			<pre>        self.name: str = name</pre>&#13;
			<pre>        self._return: Optional[int] = None</pre>&#13;
			<pre>        </pre>&#13;
			<pre>    def run(self) -&gt; None:</pre>&#13;
			<pre>        print(f"thread {self.name} is running")</pre>&#13;
			<pre>        sleep(self.seconds)</pre>&#13;
			<pre>        print(f"thread {self.name} has finished")</pre>&#13;
			<pre>        self._return = self.seconds</pre>&#13;
			<pre>    </pre>&#13;
			<pre>    def join(self) -&gt; int:</pre>&#13;
			<pre>        Thread.join(self)</pre>&#13;
			<pre>        return self._return</pre>&#13;
			<p>Here, we can see that we have overwritten the <code>run</code> function in the <code>Thread</code> class. This function <a id="_idIndexMarker175"/>runs when the thread is running. We <a id="_idIndexMarker176"/>then overwrite the <code>join</code> method. However, we must note that in the <code>join</code> function, there is extra functionality going on under the hood; therefore, we must call the <code>Thread</code> class's <code>join</code> method, and then return whatever we want at the end. We do not have to return anything if we do not want to. If this is the case, then there is no point overwriting the <code>join</code> function. We can then implement the threads by running the following code:</p>&#13;
			<pre>one: ExampleThread = ExampleThread(seconds=5, name="one")</pre>&#13;
			<pre>two: ExampleThread = ExampleThread(seconds=5, name="two")</pre>&#13;
			<pre>three: ExampleThread = ExampleThread(seconds=5, </pre>&#13;
			<pre>  name="three")</pre>&#13;
			<p>We then <a id="_idIndexMarker177"/>have to time the process of starting, running, and <a id="_idIndexMarker178"/>joining the outcomes, like so:</p>&#13;
			<pre>import time</pre>&#13;
			<pre>start = time.time()</pre>&#13;
			<pre>one.start()</pre>&#13;
			<pre>two.start()</pre>&#13;
			<pre>three.start()</pre>&#13;
			<pre>print("we have started all of our threads")</pre>&#13;
			<pre>one_result = one.join()</pre>&#13;
			<pre>two_result = two.join()</pre>&#13;
			<pre>three_result = three.join()</pre>&#13;
			<pre>finish = time.time()</pre>&#13;
			<pre>print(f"{finish - start} has elapsed")</pre>&#13;
			<pre>print(one_result)</pre>&#13;
			<pre>print(two_result)</pre>&#13;
			<pre>print(three_result)</pre>&#13;
			<p>When we run this code, we get the following console printout:</p>&#13;
			<pre>thread one is running</pre>&#13;
			<pre>thread two is running</pre>&#13;
			<pre>thread three is running</pre>&#13;
			<pre>we have started all of our threads</pre>&#13;
			<pre>thread one has finished</pre>&#13;
			<pre>thread three has finished</pre>&#13;
			<pre>thread two has finished</pre>&#13;
			<pre>5.005641937255859 has elapsed</pre>&#13;
			<pre>5</pre>&#13;
			<pre>5</pre>&#13;
			<pre>5</pre>&#13;
			<p>Straight away, we can see that it took just over 5 seconds to execute the whole process. If we were running our program sequentially, it would take 15 seconds. This shows that our threads are working! </p>&#13;
			<p>It also must <a id="_idIndexMarker179"/>be noted that thread <code>three</code> finished before thread <code>two</code>, even though thread <code>two</code> started before. Don't worry if you <a id="_idIndexMarker180"/>get a finishing sequence of <code>one</code>, <code>two</code>, <code>three</code>; this is because threads finish in an indeterminate order. Even though the scheduling is deterministic, there are thousands of events and processes running under the hood of the CPU when the program is running. As a result, the exact time slices that each thread gets are never the same. These tiny changes add up over time, and as a result, we cannot guarantee that the threads will finish in a determinate order if the executions are close and the durations are roughly the same. </p>&#13;
			<p>Now we have the basics of Python threads, we can move on to spinning off threads in Rust. However, before we start doing this, we must understand the concept of <code>main</code> function or inside other scopes including other functions. A simple example of a building closure is to print an input, like this:</p>&#13;
			<pre>fn main() {</pre>&#13;
			<pre>    let example_closure: fn(&amp;str) = |string_input: &amp;str| {</pre>&#13;
			<pre>        println!("{}", string_input);</pre>&#13;
			<pre>    };</pre>&#13;
			<pre>    example_closure("this is a closure");</pre>&#13;
			<pre>}</pre>&#13;
			<p>With this approach, we can exploit scopes. It also must be noted that as closures are scope-sensitive, we can also utilize the existing variables around a closure. To demonstrate this, we can create a closure that calculates the amount of interest we have to pay on a loan due to the external base rate. We will also define it in an inner scope, as seen here:</p>&#13;
			<pre>fn main() {</pre>&#13;
			<pre>        let base_rate: f32 = 0.03;</pre>&#13;
			<pre>        let calculate_interest = |loan_amount: &amp;f32| {</pre>&#13;
			<pre>            return loan_amount * &amp;base_rate</pre>&#13;
			<pre>        };</pre>&#13;
			<pre>        println!("the total interest to be paid is: {}", </pre>&#13;
			<pre>          calculate_interest(&amp;32567.6));</pre>&#13;
			<pre>}</pre>&#13;
			<p>Running <a id="_idIndexMarker181"/>this code would give us the following printout in the console:</p>&#13;
			<pre>the total interest to be paid is: 977.02795</pre>&#13;
			<p>Here, we can <a id="_idIndexMarker182"/>see that closures can return values, but we have not defined the type for the closure. This is the case even though it is returning a float. In fact, if we set <code>calculate_interest</code> to <code>f32</code>, the compiler would complain, stating that the types were mismatched. This is because the closure is a unique anonymous type that cannot be written out. A closure is a struct generated by the compiler that houses captured variables. If we try to call the closure outside the inner scope, our application will fail to compile as the closure cannot be accessed outside the scope. </p>&#13;
			<p>Now that we have covered Rust closures, we can replicate the Python threading example that we covered earlier in the section. Initially, we must import the standard module crates that are required by running the following code:</p>&#13;
			<pre>use std::{thread, time};</pre>&#13;
			<pre>use std::thread::JoinHandle;</pre>&#13;
			<p>We are using <code>thread</code> to spawn off threads, <code>time</code> to keep track of how long our processes take, and the <code>JoinHandle</code> struct to join the threads. With these imports, we can build our own thread by running the following code:</p>&#13;
			<pre>fn simple_thread(seconds: i8, name: &amp;str) -&gt; i8 {</pre>&#13;
			<pre>    println!("thread {} is running", name);</pre>&#13;
			<pre>    let total_seconds = time::Duration::new(seconds as \</pre>&#13;
			<pre>      u64, 0);</pre>&#13;
			<pre>    thread::sleep(total_seconds);</pre>&#13;
			<pre>    println!("thread {} has finished", name);</pre>&#13;
			<pre>    return seconds</pre>&#13;
			<pre>}</pre>&#13;
			<p>Here, we can see that we create a <code>Duration</code> struct denoted as <code>total_seconds</code>. We then <a id="_idIndexMarker183"/>use the thread and <code>total_seconds</code> to put the function to sleep, returning the number of seconds when the whole <a id="_idIndexMarker184"/>process is finished. Right now, this is just a function, and running it by itself will not spin off different threads. Inside our <code>main</code> function, we start our timer and spawn off our three threads by running the following code:</p>&#13;
			<pre>let now = time::Instant::now();</pre>&#13;
			<pre>let thread_one: JoinHandle&lt;i8&gt; = thread::spawn(|| {</pre>&#13;
			<pre>    simple_thread(5, "one")});</pre>&#13;
			<pre>let thread_two: JoinHandle&lt;i8&gt; = thread::spawn(|| {</pre>&#13;
			<pre>    simple_thread(5, "two")});</pre>&#13;
			<pre>let thread_three: JoinHandle&lt;i8&gt; = thread::spawn(|| {</pre>&#13;
			<pre>    simple_thread(5, "three")});</pre>&#13;
			<p>Here, we spawn threads and pass our function in with the right parameters in the closure. Nothing is stopping us from putting any code in the closure. The final line in the closure would be what is returned to the <code>JoinHandle</code> struct to unwrap. Once this is done, we join all the threads to hold the program until all the threads have finished before moving on with this code:</p>&#13;
			<pre>let result_one = thread_one.join();</pre>&#13;
			<pre>let result_two = thread_two.join();</pre>&#13;
			<pre>let result_three = thread_three.join();</pre>&#13;
			<p>The <code>join</code> function returns a result with the <code>Result&lt;i8, Box&lt;dyn Any + Send&gt;&gt;</code> type.</p>&#13;
			<p>There are <a id="_idIndexMarker185"/>some new concepts here, but we <a id="_idIndexMarker186"/>can break them down, as follows:</p>&#13;
			<ul>&#13;
				<li>We remember that a <code>Result</code> struct in Rust either returns an <code>Ok</code> or an <code>Err</code> response. If the thread runs without any problems, then we will return the <code>i8</code> value that we are expecting. If not, then we have this rather ugly <code>Result&lt;i8, Box&lt;dyn Any + Send&gt;&gt;</code> output as the error. </li>&#13;
				<li>The first thing we must address here is the <code>Box</code> struct. This is one of the most basic forms of a pointer and allows us to store data on the heap rather than the stack. What remains on the stack is the pointer to the data in the heap. We are using this because we do not know how big the data is when coming out of the thread. </li>&#13;
				<li>The next expression that we must explain is <code>dyn</code>. This keyword is used to indicate that the type is a trait object. For instance, we might want to store a range of <code>Box</code> structs in an array. These <code>Box</code> structs might point to different structs. However, we can still ensure that they can be grouped together if they have a certain trait in common. For instance, if all the structs had to have <code>TraitA</code> implemented, we would denote this with <code>Box&lt;dyn TraitA&gt;</code>. </li>&#13;
				<li>The <code>Any</code> keyword is a trait for dynamic typing. This means that the data type can be anything. The <code>Any</code> trait is combined with <code>Send</code> by using the <code>Any + Send</code> expression. This means that both traits must be implemented. </li>&#13;
				<li>The <code>Send</code> trait is for types that can be transferred across thread boundaries. <code>Send</code> is implemented automatically by the compiler if it is deemed appropriate. With all this, we can confidently state that the join of a thread in Rust returns a result that can either be the integer that we desire or a pointer to anything else that can be transferred across threads. </li>&#13;
			</ul>&#13;
			<p>To process the results of the thread, we could just directly unwrap them. However, this would not be very useful when the demands of our multithreaded programs increase. We must be able to handle what potentially comes out of a thread, and to do this, we are going to have to downcast the outcome. Downcasting is Rust's method of converting a trait into <a id="_idIndexMarker187"/>a concrete type. In this context, we will be converting <code>PyO3</code> structs that denote Python types into concrete Rust data <a id="_idIndexMarker188"/>types such as strings or integers. To demonstrate this, let's build a function that handles the outcome of our thread, as follows:</p>&#13;
			<ol>&#13;
				<li>First, we are going to have to import everything we need, as seen in the following code snippet:<pre>use std::any::Any;
use std::marker::Send;</pre></li>&#13;
				<li>With these imports, we can create a function that unpacks the result and prints it using this code:<pre>fn process_thread(thread_result: Result&lt;i8, Box&lt;dyn \
  Any + Send&gt;&gt;, name: &amp;str) {
    match thread_result {
        Ok(result) =&gt; {
            println!("the result for {} is {}", \
              result, name);
        }
        Err(result) =&gt; {
            if let Some(string) = result.downcast \
              _ref::&lt;String&gt;() {
                println!("the error for {} is: {}", \
                  name, string);
            } else {
                println!("there error for {} does \
                  not have a message", name);
            }
        }
    }
}</pre></li>&#13;
				<li>Here, we merely print out the result if it is a success. However, if it is an error, as pointed out earlier, we do not know what data type the error is. However, we would still like to handle this. This is where we downcast. Downcasting returns an option, which is why we have the <code>if let Some(string) = result.downcast_ref::&lt;String&gt;()</code> condition. If the downcast is successful, we can move the string into the scope and print out the error string. If it is not successful, we can move on and state that although there was an error, an error string was not provided. We can use multiple conditional statements <a id="_idIndexMarker189"/>to account for a range of <a id="_idIndexMarker190"/>data types if we want. We can write a lot of Rust code without having to rely on downcasting, as Rust has a strict typing section. However, when interfacing with Python this can be useful, as we know that Python objects are dynamic and could essentially be anything. </li>&#13;
				<li>Now that we can process our threads when they have finished, we can stop the clock and process the outcomes by running the following code:<pre>println!("time elapsed {:?}", now.elapsed());
process_thread(result_one, "one");
process_thread(result_two, "two");
process_thread(result_three, "three");</pre></li>&#13;
				<li>This gives us the following printout:<pre><strong class="bold">thread one is running</strong>
<strong class="bold">thread three is running</strong>
<strong class="bold">thread two is running</strong>
<strong class="bold">thread one has finished</strong>
<strong class="bold">thread three has finished</strong>
<strong class="bold">thread two has finished</strong>
<strong class="bold">time elapsed 5.00525725s</strong>
<strong class="bold">the result for 5 is one</strong>
<strong class="bold">the result for 5 is two</strong>
<strong class="bold">the result for 5 is three</strong></pre></li>&#13;
			</ol>&#13;
			<p>And here we have it: we can run and process threads in Python and Rust. However, remember <a id="_idIndexMarker191"/>that if we try to run CPU-intensive <a id="_idIndexMarker192"/>tasks with the code that we have written, we will not get the speed up. However, it must be noted that in the Rust context of the code, there could be a speedup depending on the environment. For instance, if multiple <a id="_idIndexMarker193"/>CPU cores are available, the <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>) scheduler can put those threads onto those cores to be executed in parallel. To write code that will speed up our code in this context, we will have to learn how to practically spin up multiple processes, which we cover in the next section. </p>&#13;
			<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/>Running multiple processes </h1>&#13;
			<p>Technically <a id="_idIndexMarker194"/>with Python, we can simply switch the inheritance of our thread from <code>Thread</code> to <code>Process</code> by running the following code:</p>&#13;
			<pre>from multiprocessing import Process</pre>&#13;
			<pre>from typing import Optional</pre>&#13;
			<pre>class ExampleProcess(Process):</pre>&#13;
			<pre>    </pre>&#13;
			<pre>    def __init__(self, seconds: int, name: str) -&gt; None:</pre>&#13;
			<pre>        super().__init__()</pre>&#13;
			<pre>        self.seconds: int = seconds</pre>&#13;
			<pre>        self.name: str = name</pre>&#13;
			<pre>        self._return: Optional[int] = None</pre>&#13;
			<pre>        </pre>&#13;
			<pre>    def run(self) -&gt; None:</pre>&#13;
			<pre>        # do something demanding of the CPU</pre>&#13;
			<pre>        pass</pre>&#13;
			<pre>    </pre>&#13;
			<pre>    def join(self) -&gt; int:</pre>&#13;
			<pre>        Process.join(self)</pre>&#13;
			<pre>        return self._return</pre>&#13;
			<p>However, there are some compilations. If we refer to <em class="italic">Figure 3.3</em>, we can see that processes have <a id="_idIndexMarker195"/>their own memory. This is where things can get complicated. </p>&#13;
			<p>For instance, there is nothing wrong with the process defined previously if the process is not returning anything directly but writing to a database or file. On the other hand, the <code>join</code> function will not return anything directly and will just have <code>None</code> instead. This is because <code>Process</code> is not sharing the same memory space as the main process. We also must remember that spinning off processes is more expensive, so we must be more careful with this. </p>&#13;
			<p>Since we are getting more complex with the memory and the resources are getting more expensive, it makes sense to rein it in and keep it simple. This is where we utilize a <strong class="bold">pool</strong>. A pool is where <a id="_idIndexMarker196"/>we have several workers processing inputs simultaneously and then packaging them as an array, as seen here:</p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Figure_3.05_B17720.jpg" alt="Figure 3.5 – Pool of processes&#13;&#10;" width="898" height="301"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 3.5 – Pool of processes</p>&#13;
			<p>The advantage here is that we keep the expensive multiprocessing context to a small part of <a id="_idIndexMarker197"/>the program. We can also easily control the number of workers that we are willing to support. For Python, this means that we keep the interaction as lightweight as possible. As seen in the next diagram, we package an individual isolated function in a tuple with an array of inputs. This tuple gets processed in the pool by a worker, and then the result of the outcome is returned from the pool:</p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Figure_3.06_B17720.jpg" alt="Figure 3.6 – Pool data flow&#13;&#10;" width="825" height="221"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 3.6 – Pool data flow</p>&#13;
			<p>To demonstrate multiprocessing via a pool, we can utilize the <strong class="bold">Fibonacci sequence</strong>. This is <a id="_idIndexMarker198"/>where the next number of the sequence is the sum of the previous number in the sequence and the number before that, as illustrated here: </p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/B17720_03_001.jpg" alt="" width="508" height="67"/>&#13;
				</p>&#13;
			</p>&#13;
			<p>To calculate a <a id="_idIndexMarker199"/>number in the sequence, we will have to use <strong class="bold">recursion</strong>. There is a closed form of the Fibonacci sequence; however, this will not let us explore multiprocessing <a id="_idIndexMarker200"/>as the closed sequence by its very nature doesn't scale in computation as <em class="italic">n</em> increases. To calculate a Fibonacci number in Python, we can write an isolated function, as seen in the following code snippet:</p>&#13;
			<pre>def recur_fibo(n: int) -&gt; int:</pre>&#13;
			<pre>    if n &lt;= 1:</pre>&#13;
			<pre>        return n</pre>&#13;
			<pre>    else:</pre>&#13;
			<pre>        return (recur_fibo(n-1) + recur_fibo(n-2))</pre>&#13;
			<p>This function keeps going back until it hits the bottom of the tree at either 1 or 0. This function is terrible at scaling. To demonstrate this, let's look at the recursion tree shown here: </p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Figure_3.07_B17720.jpg" alt="Figure 3.7 – Fibonacci recursion tree&#13;&#10;" width="598" height="818"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 3.7 – Fibonacci recursion tree</p>&#13;
			<p>We can see that these are not perfect trees, and if you go online and search for <em class="italic">big O notation of the Fibonacci sequence</em>, there are debates, and some equations will equate the scaling factor to the golden ratio. While this is interesting, it is outside the scope of <a id="_idIndexMarker201"/>this book as we are focusing on computational complexity. As a result, we will simplify the math and treat this as a perfectly symmetrical tree. Recursion trees scale at the rate of <img src="img/B17720_03_002.png" alt="" width="50" height="38"/>, where <em class="italic">n</em> is the depth of the tree. Referring to <em class="italic">Figure 3.7</em>, we can see that if we treat the tree as perfectly symmetrical, a <em class="italic">n</em> value of 3 has a depth of 3, and a <em class="italic">n</em> value of 4 has a depth of 4. As <em class="italic">n</em> increases, the computation increases exponentially.</p>&#13;
			<p>We have taken a slight detour of complexity to highlight the importance of taking this into account before reaching for multiprocessing. The reason why you bought this book as opposed to searching online for multiprocessing code snippets to copy and paste into your code is that you want to be guided through these concepts with pointers for further reading and an appreciation of their context. In the case of this sequence, reaching for a closed form or caching answers would reduce the computation time greatly. If we have an ordered list of numbers, getting the highest number in the list and then creating a full sequence up to the highest number would be a lot quicker than repeatedly calculating the sequence again and again for each number we want to calculate. Avoiding recursion altogether is a better option than reaching for multiprocessing. </p>&#13;
			<p>To implement <a id="_idIndexMarker202"/>and test our multiprocessing pool, we first need to time how long it would take to calculate a range of numbers sequentially. This can be done like so:</p>&#13;
			<pre>import time</pre>&#13;
			<pre>start = time.time()</pre>&#13;
			<pre>recur_fibo(n=8)</pre>&#13;
			<pre>recur_fibo(n=12)</pre>&#13;
			<pre>recur_fibo(n=12)</pre>&#13;
			<pre>recur_fibo(n=20)</pre>&#13;
			<pre>recur_fibo(n=20)</pre>&#13;
			<pre>recur_fibo(n=20)</pre>&#13;
			<pre>recur_fibo(n=20)</pre>&#13;
			<pre>recur_fibo(n=28)</pre>&#13;
			<pre>recur_fibo(n=28)</pre>&#13;
			<pre>recur_fibo(n=28)</pre>&#13;
			<pre>recur_fibo(n=28)</pre>&#13;
			<pre>recur_fibo(n=36)</pre>&#13;
			<pre>finish = time.time()</pre>&#13;
			<pre>print(f"{finish - start} has elapsed")</pre>&#13;
			<p>We have introduced a pretty long list; however, this is essential to see the difference. If we just had two Fibonacci numbers to compute, then the cost of spinning up processes could eclipse the gain in multiprocessing.  </p>&#13;
			<p>Our multiple <a id="_idIndexMarker203"/>processing pool can be implemented as follows:</p>&#13;
			<pre>if __name__ == '__main__':</pre>&#13;
			<pre>    from multiprocessing import Pool</pre>&#13;
			<pre>    </pre>&#13;
			<pre>    start = time.time()</pre>&#13;
			<pre>    with Pool(4) as p:</pre>&#13;
			<pre>        print(p.starmap(recur_fibo, [(8,), (12,), (12,), \</pre>&#13;
			<pre>         (20,), (20,), (20,), (20,), (28,), (28,), (28,), \</pre>&#13;
			<pre>           (28,),(36,)]))</pre>&#13;
			<pre>    finish = time.time()</pre>&#13;
			<pre>    print(f"{finish - start} has elapsed")</pre>&#13;
			<p>Please note that we have nested this code under <code>if __name__ == </code>"<code>__main__</code>"<code>:</code>. This is because the whole script gets run again when spinning up another process, which can result in infinite loops. If the code is nested under <code>if __name__ == </code>"<code>__main__</code>"<code>:</code> then it will not run again as there is only one main process. It also must be noted that we defined a pool of four workers. This can be changed to whatever we feel fit but there are diminishing returns when increasing this, as we will explore later. The tuples in the list are the arguments for each computation. Running the whole script gives us the following output:</p>&#13;
			<pre>3.2531330585479736 has elapsed</pre>&#13;
			<pre>[21, 144, 144, 6765, 6765, 6765, 6765, 317811, </pre>&#13;
			<pre>317811, 317811, 317811, 14930352]</pre>&#13;
			<pre>3.100019931793213 has elapsed</pre>&#13;
			<p>We can see that the speed is not a quarter of the sequential calculations. However, the multiprocessing pool is slightly faster. If you run this multiple times, you will get some variance in the difference in times. However, the multiprocessing approach will always be faster. Now that we have run a multiprocessing tool in Python, we can implement our Fibonacci <a id="_idIndexMarker204"/>multithreading in the different context of a multiprocessing pool in Rust. Here's how we'll do this:</p>&#13;
			<ol>&#13;
				<li value="1">In our new Cargo project, we can code the following function in our <code>main.rs</code> file: <pre>pub fn fibonacci_recursive(n: i32) -&gt; u64 {
     if n &lt; 0 {
          panic!("{} is negative!", n);
     }
     match n {
           0     =&gt; panic!(
           "zero is not a right argument to 
           fibonacci_reccursive()!"),
           1 | 2 =&gt; 1,
           _     =&gt; fibonacci_reccursive(n - 1) + 
                    fibonacci_reccursive(n - 2)
}
}</pre><p>We can see that our Rust function is not more complex than our Python version. The extra lines of code are just to account for unexpected inputs. </p></li>&#13;
				<li>To run this and time it, we must import the time crate at the top of the <code>main.rs</code> file by running the following code:<pre>use std::time;</pre></li>&#13;
				<li>Then, we must compute the exact same Fibonacci numbers as we did in the Python implementation, as follows:<pre>fn main() {
    let now = time::Instant::now();
    fibonacci_reccursive(8);
    fibonacci_reccursive(12);
    fibonacci_reccursive(12);
    fibonacci_reccursive(20);
    fibonacci_reccursive(20);
    fibonacci_reccursive(20);
    fibonacci_reccursive(20);
    fibonacci_reccursive(28);
    fibonacci_reccursive(28);
    fibonacci_reccursive(28);
    fibonacci_reccursive(28);
    fibonacci_reccursive(36);
    println!("time elapsed {:?}", now.elapsed());
}</pre></li>&#13;
				<li>To run <a id="_idIndexMarker205"/>this, we are going to use the following command:<pre><strong class="bold">cargo run –release </strong></pre></li>&#13;
				<li>We are going to use the release version as that is what we will be using in production. Running it gives us the following output:<pre><strong class="bold">time elapsed 40.754875ms</strong></pre></li>&#13;
			</ol>&#13;
			<p>Running this several times will give us an average roundabout of 40 milliseconds. Considering that our multiprocessing Python code ran at roughly 3.1 seconds, our Rust single-threaded implementation runs 77 times faster than our Python multiprocessing code. Just let that sink in! The code was not more complex, and it is memory-safe. Therefore, fusing Rust with Python is such a quick win! Combining the aggressive typing with the compiler forces us to account for every input and outcome, and we are on the way to turbocharging our Python systems with safer, faster code. </p>&#13;
			<p>Now, we are going to see what happens to the speed when we run our numbers through a multithreading tool. Here's how we'll go about this:</p>&#13;
			<ol>&#13;
				<li value="1">To do this, we are going to use the <code>rayon</code> crate. We define this dependency in our <code>Cargo.toml</code> file by running the following code:<pre>[dependencies]
rayon="1.5.0"</pre></li>&#13;
				<li>Once this is done, we import it into our <code>main.rs</code> file, as follows:<pre>use rayon::prelude::*;</pre></li>&#13;
				<li>We can <a id="_idIndexMarker206"/>then run our multithreading pool in our <code>main</code> function below our sequential calculations by running the following code:<pre>rayon::ThreadPoolBuilder::new().num_threads(4) \
  .build_global().unwrap();
let now = time::Instant::now();
let numbers: Vec&lt;i32&gt; = vec![8, 12, 12, 20, 20, 20, \
  20, 28, 28, 28, 28, 36];
let outcomes: Vec&lt;u64&gt; = numbers.into_par_iter() \
  .map(|n| fibonacci_reccursive(n)).collect();
println!("{:?}", outcomes);
println!("time elapsed {:?}", now.elapsed());</pre></li>&#13;
				<li>Here, we define the number of threads that our pool builder has. We then execute the <code>into_par_iter</code> function on the vector. This is achieved by implementing the <code>IntoParallelIterator</code> trait onto the vector, which is done when the <code>rayon</code> crate is imported. If it were not imported, then the compiler would complain, stating that a vector does not have the <code>into_par_iter</code> function associated with it. </li>&#13;
				<li>We then map our Fibonacci function over the integers in the vector utilizing a closure and collect them. The calculated Fibonacci numbers are associated with the <code>outcomes</code> variable.</li>&#13;
				<li>We then <a id="_idIndexMarker207"/>print them and print the time elapsed. Running this via a release gives us the following printout in the console:<pre><strong class="bold">time elapsed 38.993791ms</strong>
<strong class="bold">[21, 144, 144, 6765, 6765, 6765, 6765, 317811, </strong>
<strong class="bold">317811, 317811, 317811, 14930352]</strong>
<strong class="bold">time elapsed 31.493291ms</strong></pre><p>Running this several times will give you roughly the times stated in the preceding console printout. Calculating this gives us a 20% increase in speed. Considering that the Python multiprocessing only gave us a 5% increase, we can deduce that Rust is also more efficient at multithreading when the right context is applied. </p><p>We can go a little further to really see the advantages of these pools. Remember that our sequence increases exponentially. In our Rust program, we can add three computations for <em class="italic">n</em> being 46 to our sequential calculations and pooled calculations, and we get the following output:</p><pre><strong class="bold">time elapsed 12.5856675s</strong>
<strong class="bold">[21, 144, 144, 6765, 6765, 6765, 6765, 317811, 317811, </strong>
<strong class="bold">317811, 317811, 14930352, 1836311903, 1836311903, </strong>
<strong class="bold">1836311903]</strong>
<strong class="bold">time elapsed 4.0485755s</strong></pre><p>First, we must acknowledge that the time went from milliseconds to double-digit seconds. Exponential scaling algorithms are painful, and just adding 10 to your calculation pushes it up greatly. We can also see that our savings have increased. Our pooled calculations are now 3.11 times faster as opposed to 1.2 times faster in the previous test!  </p></li>&#13;
				<li>If we add three extra computations for <em class="italic">n</em> being 46 for our Python implementation, we get the following console printout:<pre><strong class="bold">1105.5351197719574 has elapsed</strong>
<strong class="bold">[21, 144, 144, 6765, 6765, 6765, 6765, 317811, 317811, </strong>
<strong class="bold">317811, 317811, 14930352, 1836311903, 1836311903, </strong>
<strong class="bold">1836311903]</strong>
<strong class="bold">387.0687129497528 has elapsed</strong></pre></li>&#13;
			</ol>&#13;
			<p>Here, we can see that our Python pooled processing is 2.85 times faster than our Python sequential processing. We also must note here that our Rust sequential processing is roughly 95 times faster than our Python sequential processing, and our Rust pool multithreading is roughly 96 times faster than our Python pool processing. As the number of <a id="_idIndexMarker208"/>points that need processing increases, so will the difference. This highlights even more motivation for plugging Rust into Python. </p>&#13;
			<p>It must be noted that we got our speed increase in our Rust program through multithreading as opposed to multiprocessing. Multiprocessing in Rust is not as straightforward as in Python—this is mainly down to Rust being a newer language. For instance, there is a crate called <code>mitosis</code> that will enable us to run functions in a separate process; however, this crate only has four contributors, and the last contribution at the time of writing this book was 13 months ago. Considering this, we should approach multiprocessing in Rust without any third-party crates. To achieve this, we need to code a Fibonacci calculation program and a multiprocessing program that will call this in different processes, as seen in the following diagram:</p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Figure_3.08_B17720.jpg" alt="Figure 3.8 – Multiprocessing in Rust&#13;&#10;" width="1265" height="533"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 3.8 – Multiprocessing in Rust</p>&#13;
			<p>We are going to pass our data into these processes and parse the outputs handling them in our <code>multiprocessing.rs</code> file. To carry this out in the simplest way, we code both files in the same directory. First, we build our <code>fib_process.rs</code> file. We must import <a id="_idIndexMarker209"/>what we are going to do by running the following code:</p>&#13;
			<pre>use std::env;</pre>&#13;
			<pre>use std::vec::Vec;</pre>&#13;
			<p>We want our processes to accept a list of integers to calculate, so we define Fibonacci <code>number</code> and <code>numbers</code> functions, as follows:</p>&#13;
			<pre>pub fn fibonacci_number(n: i32) -&gt; u64 {</pre>&#13;
			<pre>      if n &lt; 0 {</pre>&#13;
			<pre>           panic!("{} is negative!", n);</pre>&#13;
			<pre>      }</pre>&#13;
			<pre>      match n {</pre>&#13;
			<pre>           0     =&gt; panic!("zero is not a right argument \</pre>&#13;
			<pre>                             to fibonacci_number!"),</pre>&#13;
			<pre>           1 | 2 =&gt; 1,</pre>&#13;
			<pre>           _     =&gt; fibonacci_number(n - 1) +</pre>&#13;
			<pre>                    fibonacci_number(n - 2)</pre>&#13;
			<pre>      }</pre>&#13;
			<pre>}</pre>&#13;
			<pre>pub fn fibonacci_numbers(numbers: Vec&lt;i32&gt;) -&gt; Vec&lt;u64&gt; {</pre>&#13;
			<pre>    let mut vec: Vec&lt;u64&gt; = Vec::new();</pre>&#13;
			<pre>    for n in numbers.iter() {</pre>&#13;
			<pre>        vec.push(fibonacci_number(*n));</pre>&#13;
			<pre>    }</pre>&#13;
			<pre>    return vec</pre>&#13;
			<pre>}</pre>&#13;
			<p>We have seen <a id="_idIndexMarker210"/>these functions before as they have become the standard way to calculate Fibonacci numbers in this book. We now must take a list of integers from arguments, parse them into integers, pass them into our calculation function, and return the results, as follows:</p>&#13;
			<pre>fn main() {</pre>&#13;
			<pre>    let mut inputs: Vec&lt;i32&gt; = Vec::new(); </pre>&#13;
			<pre>    let args: Vec&lt;String&gt; = env::args().collect();</pre>&#13;
			<pre>    for i in args {</pre>&#13;
			<pre>        match i.parse::&lt;i32&gt;() {</pre>&#13;
			<pre>            Ok(result) =&gt; inputs.push(result),</pre>&#13;
			<pre>            Err(_) =&gt; (),</pre>&#13;
			<pre>        }</pre>&#13;
			<pre>    }</pre>&#13;
			<pre>    let results = fibonacci_numbers(inputs);</pre>&#13;
			<pre>    for i in results {</pre>&#13;
			<pre>        println!("{}", i);</pre>&#13;
			<pre>    }</pre>&#13;
			<pre>}</pre>&#13;
			<p>Here, we can see that we collect the input from the environment. Once the input integers have been parsed into <code>i32</code> integers and used to calculate the Fibonacci numbers, we merely print them out. Printing out to the console generally acts as <code>stdout</code>. Our process file is fully coded, so we can compile it with the following command:</p>&#13;
			<pre>rustc fib_process.rs</pre>&#13;
			<p>This creates <a id="_idIndexMarker211"/>a binary of our file. Now that this is done, we can move on to our <code>multiprocessing.rs</code> file that will spawn multiple processes. We import what we need by running the following code:</p>&#13;
			<pre>use std::process::{Command, Stdio, Child};</pre>&#13;
			<pre>use std::io::{BufReader, BufRead};</pre>&#13;
			<p>The <code>Command</code> struct is going to be used to spawn off a new process, the <code>Stdio</code> struct is going to be used to define the piping of data back from the process, and the <code>Child</code> struct is returned when the process is spawned. We will use them to access the output data and get the process to wait to finish. The <code>BufReader</code> struct is used to read the data from the child process. Now that we have imported everything we need, we can define a function that accepts an array of integers as strings and spins off the process, returning the <code>Child</code> struct, as follows:</p>&#13;
			<pre>fn spawn_process(inputs: &amp;[&amp;str]) -&gt; Child {</pre>&#13;
			<pre>    return Command::new("./fib_process").args(inputs)</pre>&#13;
			<pre>    .stdout(Stdio::piped())</pre>&#13;
			<pre>    .spawn().expect("failed to execute process")</pre>&#13;
			<pre>}</pre>&#13;
			<p>Here, we can see that we just must call our binary and pass in our array of strings in the <code>args</code> function. We then define the <code>stdout</code> and spawn the process, returning the <code>Child</code> struct. Now that this is done, we can fire off three processes in our <code>main</code> function and wait for them to complete by running the following code:</p>&#13;
			<pre>fn main() {</pre>&#13;
			<pre>    let mut one = spawn_process(&amp;["5", "6", "7", "8"]);</pre>&#13;
			<pre>    let mut two = spawn_process(&amp;["9", "10", "11", "12"]);</pre>&#13;
			<pre>    let mut three = spawn_process(&amp;["13", "14", "15", \</pre>&#13;
			<pre>      "16"]);</pre>&#13;
			<pre>    one.wait();</pre>&#13;
			<pre>    two.wait();</pre>&#13;
			<pre>    three.wait();</pre>&#13;
			<pre>}</pre>&#13;
			<p>We can now <a id="_idIndexMarker212"/>start extracting the data from these processes inside our <code>main</code> function by running the following code:</p>&#13;
			<pre>    let one_stdout = one.stdout.as_mut().expect(</pre>&#13;
			<pre>        "unable to open stdout of child");</pre>&#13;
			<pre>    let two_stdout = two.stdout.as_mut().expect(</pre>&#13;
			<pre>        "unable to open stdout of child");</pre>&#13;
			<pre>    let three_stdout = three.stdout.as_mut().expect</pre>&#13;
			<pre>    ("unable to open stdout of child");</pre>&#13;
			<pre>    let one_data = BufReader::new(one_stdout);</pre>&#13;
			<pre>    let two_data = BufReader::new(two_stdout);</pre>&#13;
			<pre>    let three_data = BufReader::new(three_stdout);</pre>&#13;
			<p>Here, we can see that we have accessed the data using the <code>stdout</code> field, and then processed it using the <code>BufReader</code> struct. We can then loop through our extracted data, appending it to an empty vector and then printing it out by running the following code:</p>&#13;
			<pre>    let mut results = Vec::new(); </pre>&#13;
			<pre>    for i in three_data.lines() {</pre>&#13;
			<pre>        results.push(i.unwrap().parse::&lt;i32&gt;().unwrap());</pre>&#13;
			<pre>    }</pre>&#13;
			<pre>    for i in one_data.lines() {</pre>&#13;
			<pre>        results.push(i.unwrap().parse::&lt;i32&gt;().unwrap());</pre>&#13;
			<pre>    }</pre>&#13;
			<pre>    for i in two_data.lines() {</pre>&#13;
			<pre>        results.push(i.unwrap().parse::&lt;i32&gt;().unwrap());</pre>&#13;
			<pre>    }</pre>&#13;
			<pre>    println!("{:?}", results);</pre>&#13;
			<p>This code is a little repetitive, but it illustrates how to spawn and manage multiple processes in Rust. We then compile the file with the following command:</p>&#13;
			<pre>rustc fib_multiprocessing.rs     </pre>&#13;
			<p>We can <a id="_idIndexMarker213"/>then run our multiprocessing code with the following command:</p>&#13;
			<pre>./multiprocessing</pre>&#13;
			<p>We then get the output, as follows:</p>&#13;
			<pre>[233, 377, 610, 987, 5, 8, 13, 21, 34, 55, 89, 144] we have </pre>&#13;
			<pre>  it, our multiprocessing code in Rust works.</pre>&#13;
			<p>We have now covered all we need to know about running processes and threads to speed up our computations. However, we need to be mindful and investigate how to customize our threads and processes safely to avoid pitfalls. </p>&#13;
			<h1 id="_idParaDest-54"><a id="_idTextAnchor053"/>Customizing threads and processes safely</h1>&#13;
			<p>In this section, we <a id="_idIndexMarker214"/>will cover some of the pitfalls that we have to <a id="_idIndexMarker215"/>avoid when being creative with threads and processes. We will not cover the concepts in depth, as advanced multiprocessing and concurrency is a big topic and there are books completely dedicated to this. However, it is important to understand what to look out for and which topics to read if you want to increase your knowledge of multiprocessing/threading. </p>&#13;
			<p>Looking back at our Fibonacci sequences, it might be tempting to spin off extra threads inside our <a id="_idIndexMarker216"/>thread to speed up the individual computations in the <a id="_idIndexMarker217"/>thread pool. However, to truly understand if this is a good idea, we need to understand <strong class="bold">Amdahl's law</strong>. </p>&#13;
			<h2 id="_idParaDest-55"><a id="_idTextAnchor054"/>Amdahl's law </h2>&#13;
			<p>Amdahl's law lets us describe the trade-off on adding more threads. If we spin off threads inside <a id="_idIndexMarker218"/>the threads, we will have exponential growth of threads. You may be forgiven for thinking this to be a good idea; however, Amdahl's law states that there are diminishing returns when increasing the cores. Have a look at the following formula: </p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/B17720_03_003.jpg" alt="" width="626" height="138"/>&#13;
				</p>&#13;
			</p>&#13;
			<p>Here, the following applies:</p>&#13;
			<ul>&#13;
				<li><em class="italic">Speed</em>: This is the theoretical speedup of the execution of the whole task. </li>&#13;
				<li><em class="italic">s</em>: This is the speedup of the part of the task that benefits from improved system resources. </li>&#13;
				<li><em class="italic">p</em>: This is the proportion of execution time that the part benefiting from improved resources originally occupied. </li>&#13;
			</ul>&#13;
			<p>In general, increasing the cores does have an impact; however, the diminishing returns can be seen in the following screenshot: </p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Figure_3.09_B17720.jpg" alt="Figure 3.9 – Diminishing returns through Amdahl's law" width="811" height="659"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 3.9 – Diminishing returns through Amdahl's law [Source: Daniels220 (https://commons.wikimedia.org/w/index.php?curid=6678551), CC BY-SA 3.0]</p>&#13;
			<p>Considering this, we might <a id="_idIndexMarker219"/>want to investigate using a broker to manage our multiprocessing. However, this <a id="_idIndexMarker220"/>can lead to using <strong class="bold">clogging</strong> up the broker, resulting in <strong class="bold">deadlock</strong>. To understand the gravity of this situation, we will explore deadlocks in the next section.</p>&#13;
			<h2 id="_idParaDest-56"><a id="_idTextAnchor055"/>Deadlocks</h2>&#13;
			<p>Deadlocks can <a id="_idIndexMarker221"/>arise when it comes to bigger applications, where it is common to manage the multiprocessing through a task broker. This is usually managed via a database or caching mechanism such as Redis. This consists of a queue where the tasks are added, as illustrated here:</p>&#13;
			<p class="figure-caption"><img src="img/Figure_3.10_B17720.png" alt="Figure 3.10 – The flow of tasks when multiprocessing with a broker or queue&#13;&#10;" width="576" height="488"/></p>&#13;
			<p class="figure-caption">Figure 3.10 – The flow of tasks when multiprocessing with a broker or queue</p>&#13;
			<p>Here, we can see that new tasks can be added to the queue. As time goes on, the oldest tasks get taken <a id="_idIndexMarker222"/>off the queue and passed into the pool. Throughout the application, our code can send functions and parameters to the queue anywhere in the application. </p>&#13;
			<p>In Python, the library <a id="_idIndexMarker223"/>that does this is called <strong class="bold">Celery</strong>. There is also a Celery crate for Rust. This approach is also utilized for multiple server setups. Considering this, we could be tempted to send tasks to the queue inside another task. However, we can see here that this approach can lock up our queue:</p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Figure_3.11_B17720.jpg" alt="Figure 3.11 – Deadlock with task broker&#13;&#10;" width="688" height="784"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 3.11 – Deadlock with task broker</p>&#13;
			<p>In <em class="italic">Figure 3.11</em>, we can see that the tasks in the pool have sent tasks to the queue. However, they cannot complete until their dependencies have been executed. The thing is, they will <a id="_idIndexMarker224"/>never execute because the pool is full of tasks waiting for their dependency to complete and the pool is full, so they cannot be processed. The issue with this problem is that there are no errors raised with this—the pool will just hang. Deadlock is not the only problem that will arise without helpful warnings. Considering this, we must cover our last concept that we should be aware of before being creative: <strong class="bold">race conditions</strong>. </p>&#13;
			<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/>Race conditions </h2>&#13;
			<p>Race conditions <a id="_idIndexMarker225"/>occur when two or more threads access shared data that they both try to change. As we have noted when we were building and running our threads, they sometimes ran out of order. We can demonstrate this with a simple concept, as follows:</p>&#13;
			<ul>&#13;
				<li>If we were to have <em class="italic">thread one</em> calculate a price and write to a file and <em class="italic">thread two</em> to also calculate a price, read the price calculated from the <em class="italic">thread one</em> file, and add them together, there could be a chance that the price will not be written to the file before <em class="italic">thread two</em> reads it. What is even worse is that there could be an old price in the file. If this is the case, we will never know that the error occurred. The term <em class="italic">race conditions</em> is built upon the fact that both threads are racing to the data. </li>&#13;
			</ul>&#13;
			<p>As a solution to race conditions, we can introduce <strong class="bold">locks</strong>. Locks can be utilized for stopping other <a id="_idIndexMarker226"/>threads from accessing certain things such as a file until your thread has finished with it. However, it has to be noted that these locks only work inside the process; therefore, other processes can access the file. Caching solutions such as Redis and general databases have already implemented these safeguards, and locks will not protect against <a id="_idIndexMarker227"/>the race condition described in this section. In my experience, when we get creative with thread concepts such as locks, it is usually a sign that we must take a step back and rethink our design. </p>&#13;
			<p>Even an SQLite database file will manage our data race issues when reading and writing to a file, and if the data race condition described at the start of this section looks like it might happen, it is best to just not have them running at the same time at all. Sequential programming is safer and more useful.  </p>&#13;
			<h1 id="_idParaDest-58"><a id="_idTextAnchor057"/>Summary</h1>&#13;
			<p>In this chapter, we went through the basics of multiprocessing and multithreading. We then went through practical ways to utilize threads and processes. We then explored the Fibonacci sequence to explore how processes can speed up our computations. We also saw through the Fibonacci sequence that how we compute our problems is the biggest factor over threads and processes. Algorithms that scale exponentially should be avoided before reaching for multiprocessing for speed gains. We must remember that while it might be tempting to reach for more complex approaches to multiprocessing, this can lead to problems such as deadlock and data races. We kept our multiprocessing tight by keeping it contained within a processing pool. If we keep these principles in mind and keep all our multiprocessing contained to a pool, we will keep our hard-to-diagnose problems to a minimum. This does not mean that we should never be creative with multiprocessing but it is advised to do further reading on this field, as there are books entirely dedicated to concurrency (as noted in the <em class="italic">Further reading</em> section, with particular chapters to focus on). This is just an introduction to enable us to use concurrency in our Python packages if needed. In the next chapter, we will be building our own Python packages so that we can distribute our Python code across multiple projects and reuse code. </p>&#13;
			<h1 id="_idParaDest-59"><a id="_idTextAnchor058"/>Questions</h1>&#13;
			<ol>&#13;
				<li value="1">What is the difference between a process and a thread?</li>&#13;
				<li>Why wouldn't multithreading speed up our Python Fibonacci sequence calculations?</li>&#13;
				<li>Why is a multiprocessing pool used?</li>&#13;
				<li>Our threads in Rust return <code>Result&lt;i8, Box&lt;dyn Any + Send&gt;&gt;</code>. What does this mean?</li>&#13;
				<li>Why should we avoid using a recursion tree if we can?</li>&#13;
				<li>Should you just spin up more processes when you need a faster runtime?</li>&#13;
				<li>Why should you avoid complex multiprocessing if you can?</li>&#13;
				<li>What does <code>join</code> do for our program in multithreading?</li>&#13;
				<li>Why does <code>join</code> not return anything in a process?</li>&#13;
			</ol>&#13;
			<h1 id="_idParaDest-60"><a id="_idTextAnchor059"/>Answers</h1>&#13;
			<ol>&#13;
				<li value="1">Threads are lightweight and enable multithreading, where we can run multiple tasks that could have idle time. A process is more expensive, enabling us to run multiple CPU-heavy tasks at the same time. Processes do not share memory, while threads do. </li>&#13;
				<li>Multithreading would not speed up our Fibonacci sequence calculations because calculating Fibonacci numbers is a CPU-heavy task that does not have any idle time; therefore, the threads would run sequentially in Python. However, we did demonstrate that Rust can run multiple threads at the same time, getting a significant speed increase.</li>&#13;
				<li>Multiprocessing is expensive and the processes do not share memory, making the implementation potentially more complex. A processing pool keeps the multiprocessing part of a program to a minimum. This approach also enables us to easily control the different numbers of workers we need as they're all in one place, and we can also return all the outcomes in the same sequence as they are returned from the multiprocessing pool.</li>&#13;
				<li>Our Rust thread could fail. If it doesn't, then it will return an integer. If it fails, it could return anything of any size, which is why it's on the heap. It also has the <code>Send</code> trait, which means that it can be sent across threads.  </li>&#13;
				<li>Recursion trees scale exponentially. Even if we are multithreading, our computation time will quickly scale, pushing our milliseconds into seconds once we've crossed a boundary.</li>&#13;
				<li>No—as demonstrated in Amdahl's law, increasing the workers will give us some speedup, but we will have diminishing returns as the number of workers increases. </li>&#13;
				<li>Complex multiprocessing/multithreading can introduce a range of silent errors such as deadlock and data races that can be hard to diagnose and solve. </li>&#13;
				<li><code>join</code> blocks the program until the thread has completed. It can also return the result of the thread if we overwrite Python's <code>join</code> function. </li>&#13;
				<li>Processes do not share the same memory space, therefore they cannot be accessed. We can, however, access data from other processes by saving data to files for our main process to access or pipe data via <code>stdin</code> and <code>stdout</code>, as we did in our Rust multiprocessing example.</li>&#13;
			</ol>&#13;
			<h1 id="_idParaDest-61"><a id="_idTextAnchor060"/>Further reading</h1>&#13;
			<ul>&#13;
				<li><em class="italic">Pan Wu</em> (2020). <em class="italic">Understanding Python Multithreading and Multiprocessing via Simulation:</em> https://towardsdatascience.com/understanding-python-multithreading-and-multiprocessing-via-simulation-3f600dbbfe31 </li>&#13;
				<li><em class="italic">Brian Troutwine</em> (2018). <em class="italic">Hands-On Concurrency with Rust</em>  </li>&#13;
				<li><em class="italic">Gabriele Lanaro</em> and <em class="italic">Quan Nguyen</em> (2019). <em class="italic">Learning Path Advanced Python Programming</em>: <a href="B17720_08_Final_SK_ePub.xhtml#_idTextAnchor142"><em class="italic">Chapter 8</em></a> (<em class="italic">Advanced Introduction to Concurrent and Parallel Programming</em>)</li>&#13;
				<li><em class="italic">Andrew Johnson</em> (2018). <em class="italic">Hands-On Functional Programming in Rust</em>: <a href="B17720_08_Final_SK_ePub.xhtml#_idTextAnchor142"><em class="italic">Chapter 8</em></a> (<em class="italic">Implementing Concurrency</em>)</li>&#13;
				<li><em class="italic">Rahul Sharma and Vesa Kaihlavirta</em> (2018). <em class="italic">Mastering Rust</em>: <a href="B17720_08_Final_SK_ePub.xhtml#_idTextAnchor142"><em class="italic">Chapter 8</em></a> (<em class="italic">Concurrency</em>) </li>&#13;
			</ul>&#13;
		</div>&#13;
	</div></body></html>
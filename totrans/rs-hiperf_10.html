<html><head></head><body>
        

                            
                    <h1 class="header-title">Multithreading</h1>
                
            
            
                
<p class="mce-root">So far, we have seen how to make our code faster and faster by optimizing various aspects of how we code, but there is still one point left to optimize: making our code work in parallel. In this chapter, you will learn how <strong>fearless concurrency</strong> works in Rust by using threads to process your data.</p>
<p>During this chapter, you will learn the following:</p>
<ul>
<li><kbd>Send</kbd> and <kbd>Sync</kbd> traits—how does Rust achieve memory safety?</li>
<li>Basic threading in Rust—creating and managing threads</li>
<li>Moving data between threads</li>
<li>Crates to make multithreading easier and faster</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Concurrency in Rust</h1>
                
            
            
                
<p>For a long time, it has not made sense to perform all tasks sequentially in a computer. Of course, sometimes you need to perform some tasks before others, but in most real-world applications, you will want to run some tasks in parallel.</p>
<p>You might, for example, want to respond to HTTP requests. If you do one after the other, the overall server will be slow. Especially when you get many requests per second and some of them take time to complete. You probably want to start responding to others before you finish with the current one.</p>
<p>Furthermore, we now have multiple processors in almost any computer or server, even in most mobile phones. This means that not only can we process other tasks in parallel while our main task is idle, we can really use one processor for each task by using threads. This is a feature that we must use to our advantage when developing high-performance applications.</p>
<p>The main issue with concurrency is that it's hard. We are not used to thinking in parallel, and as programmers we make mistakes. We only have to check some of the security vulnerabilities or bugs in our most-used systems, developed by the greatest programmers, to see that it's difficult to make it right.</p>
<p>Sometimes, we try to change a variable without remembering that another task might be reading it, or even changing it at the same time. Imagine a request counter in the HTTP example. If we separate the load between two processors, and each processor receives a request, the shared counter should go up by two, right?</p>
<p>Each thread wants to add 1 to the counter. For that, they load the current counter in the CPU, they add one to it and then save it again in the RAM. This takes some time, especially loading it from RAM, which means that if they both load the counter at the same time, they will both have the current counter in the CPU.</p>
<p>If both add one to the counter and save it back, the value in the RAM will only add one request, instead of two, because both processors will save the new <kbd>+1</kbd> value in the RAM. This is what we call a data race. There are some tools that avoid this behavior, such as atomic variables, semaphores, and mutexes, but we sometimes forget to use them.</p>
<p>One of the best-known features in Rust is the fearless concurrency. This means that as long as we use safe Rust, we shouldn't be able to create a data race. This solves our issue but, how do they do it?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding the Send and Sync traits</h1>
                
            
            
                
<p>The secret ingredients for this to work are the <kbd>Send</kbd> and <kbd>Sync</kbd> traits. They are traits known to the compiler, so it will check whether they are the types we use want to use to implement them and act accordingly. You cannot implement <kbd>Send</kbd> or <kbd>Sync</kbd> for your types directly. The compiler will know whether your types are <kbd>Send</kbd> or <kbd>Sync</kbd> by checking whether the contained fields are <kbd>Sync</kbd> or <kbd>Send</kbd>, in the case of structures or enumerations with fields.</p>
<p>Let's now understand how they work. First of all, you should note that neither <kbd>Send</kbd> nor <kbd>Sync</kbd> traits add methods to a given type. This means that, once compiled, they will not occupy any memory or add any extra overhead to your binary. They will only be checked at compile time to make sure that multithreading is safe. You cannot directly implement <kbd>Send</kbd> or <kbd>Sync</kbd> for your types unless you are using an unsafe block, so the compiler will do it for you where appropriate.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Send trait</h1>
                
            
            
                
<p>A structure implementing the <kbd>Send</kbd> trait is safe to be moved between threads. This means that you can safely transfer ownership of a <kbd>Send</kbd> type between threads. The standard library implements <kbd>Send</kbd> for the types that can actually be moved across thread boundaries, and the compiler will automatically implement it for your types if they can also be moved between threads. If a type is only composed of <kbd>Send</kbd> types, it will be a <kbd>Send</kbd> type.</p>
<p>Most types in the standard library implement the <kbd>Send</kbd> trait. You can safely move ownership of a <kbd>u32</kbd> to another thread, for example. This means that the previous thread will not be able to use it again and that the new thread will be in charge of dropping it once it gets out of scope.</p>
<p>There are some exceptions, though. Raw pointers cannot be safely moved to another thread, since they have no safety guards. You can copy a raw pointer multiple times, and it could happen that one gets to one thread and the other stays in the current one. If both try to manipulate the same memory at the same time, it will create undefined behavior.</p>
<p>The other exception is the reference-counted pointer or <kbd>Rc</kbd> type. This type can easily and efficiently create shared pointers to a given memory location. It will be safe since the type itself has some memory guarantees to make sure that if a mutable borrow exists, no other borrows can be made, and that if one or more non-mutable borrows exists, no mutable borrow can be made. The information pointed by the pointer will be dropped at the same time the last reference gets out of scope.</p>
<p>This works by having a counter that adds <kbd>1</kbd> each time a reference gets created by calling the <kbd>clone()</kbd> method and that subtracts <kbd>1</kbd> once a reference gets dropped. You might have already realized the issue that will arise when sharing it between threads: if two threads drop a reference at the same time, the reference count might only subtract <kbd>1</kbd>. This means that when the last reference gets dropped, the counter won't be zero, and it will not drop the <kbd>Rc</kbd>, creating a memory leak.</p>
<p>Since Rust cannot allow memory leaks, the <kbd>Rc</kbd> type is not <kbd>Send</kbd>. There is an equivalent shared pointer that can be shared between threads, the atomically reference-counted pointer or <kbd>Arc</kbd>. This type makes sure that each addition or subtraction to the reference count gets performed atomically, so that if a new thread wants to add or subtract one reference, it will need to wait for the other threads to finish updating that counter. This makes it thread-safe, but it will be slower than an <kbd>Rc</kbd> due to the checks that need to be performed. So, you should use <kbd>Rc</kbd> if you don't need to send a reference to another thread.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Sync trait</h1>
                
            
            
                
<p>The <kbd>Sync</kbd> trait, on the other hand, represents a type that can be shared between threads. This refers to actually sharing the variable without transferring its ownership to the new thread.</p>
<p>As with the <kbd>Send</kbd> trait, raw pointers and <kbd>Rc</kbd> are not <kbd>Sync</kbd>, but there is another family of types that implement not <kbd>Send</kbd> but not <kbd>Sync</kbd>. A <kbd>Cell</kbd> can be safely sent between threads, but it cannot be shared. Let's review how a <kbd>Cell</kbd> works.</p>
<p>A cell that can be found in the <kbd>std::cell</kbd> module is a container that will have some inner data. This data will be another type. Cells are used for interior mutability, but what is that? Interior mutability is the option to change the contents of a variable without it being mutable. This might sound counter-intuitive, especially in Rust, but it's possible.</p>
<p>The two safe types of cells are <kbd>Cell</kbd> and <kbd>RefCell</kbd>. The first ones implement interior mutability by moving values in and out of the <kbd>Cell</kbd>. This means that you will be able to insert a new value in the cell or get the current cell value if it's a <kbd>Copy</kbd> type, but you won't be able to use its mutable methods if you are using a complex type, such as a vector or a <kbd>HashMap</kbd>. It's useful for small types such as integers, for example. An <kbd>Rc</kbd> will use a <kbd>Cell</kbd> to store a count of the references so that you can call the <kbd>clone()</kbd> method on a non-mutable <kbd>Rc</kbd> and still update the count of references. Let's see an example:</p>
<pre>use std::cell::Cell;<br/><br/>fn main() {<br/>    let my_cell = Cell::new(0);<br/>    println!("Initial cell value: {}", my_cell.get());<br/><br/>    my_cell.set(my_cell.get() + 1);<br/>    println!("Final cell value: {}", my_cell.get());<br/>}</pre>
<p>Note that the <kbd>my_cell</kbd> variable is not mutable, but the program still compiles and the output is the following:</p>
<div><img src="img/ff7ab900-af0b-48de-acb8-efae1884c3b8.png" style="width:12.67em;height:2.50em;"/></div>
<p>A <kbd>RefCell</kbd> does a similar thing, but it can be used with any kind of type, and you can get mutable references to the value inside if there are no other references to it. This internally uses unsafe code, of course, since Rust does not allow this. For this to work, it has a flag that lets the <kbd>RefCell</kbd> know whether it's currently borrowed or not. If it's borrowed for read, more read-only borrows can be generated with the <kbd>borrow()</kbd> method, but no mutable borrow can be done. If it's mutably borrowed with the <kbd>borrow_mut()</kbd> method, you will not be able to borrow it mutably or non-mutably.</p>
<p>These two methods will check the current borrow status at runtime, not at compile time, which is standard for Rust rules, and panic if the current state is not correct. They have non-panicking alternatives named <kbd>try_borrow()</kbd> and <kbd>try_borrow_mut()</kbd>. Since all the checks are done at runtime, they will be slower than the usual Rust rules, but they allow for this interior mutability. Let's see an example:</p>
<pre>use std::cell::RefCell;<br/>use std::collections::HashMap;<br/><br/>fn main() {<br/>    let hm = HashMap::new();<br/>    let my_cell = RefCell::new(hm);<br/>    println!("Initial cell value: {:?}", my_cell.borrow());<br/><br/>    my_cell.borrow_mut().insert("test_key", "test_value");<br/>    println!("Final cell value: {:?}", my_cell.borrow());<br/>}</pre>
<p>Once again, note that the <kbd>my_cell</kbd> variable is not mutable, and yet this code compiles and we get a mutable borrow to it, which allows us to insert a new key/value pair into the hash map. The output, as expected, is the following:</p>
<div><img src="img/55222331-d28c-418d-8773-5a1a5e240546.png" style="width:26.17em;height:2.50em;"/></div>
<p>On the other hand, since the borrow flag is not thread safe, the whole <kbd>RefCell</kbd> structure will not be <kbd>Sync</kbd>. You can safely send the complete ownership of the cell to a new thread, but you cannot have shared references to it. If you want to better understand how <kbd>Rc</kbd>, Cells, and RefCells work, we talked about them in <a href="71d38dd3-1f0b-408e-b454-3d342b413f7c.xhtml" target="_blank">Chapter 3</a>, <em>Memory Management in Rust</em>.</p>
<p>There are thread-safe alternatives that allow interior mutability, called Mutexes. A <kbd>Mutex</kbd> stores the guard in an actual system <kbd>Mutex</kbd>, which synchronizes the threads before accessing the data. This makes it <kbd>Sync</kbd> but also slower. We will see how they work in this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Other types of concurrency in Rust</h1>
                
            
            
                
<p>There are other ways of achieving parallel computing in Rust and in many other languages. In this chapter, we will talk about multithreading, where each thread has access to shared memory and creates its own stacks so that it can work independently. Ideally, you should have about the same number of threads working at the same time as the number of virtual CPUs in your PC/server.</p>
<p>This is usually twice the number of CPU cores, thanks to hyperthreading, where one core can run two threads at the same time by using its own hardware scheduler to decide which parts of each thread run at a given point in time.</p>
<p>The main issue with threads is that if you don't put a limit and run too many of them, maybe because some of them are idle and your CPU should be able to run the others, you will consume a lot of RAM. This is because of all the stacks that need to be created per thread. It is not uncommon for some web servers to create one thread per request. This will make things much slower when the load is high, since it will require a lot of RAM.</p>
<p>Another approach to concurrency is asynchronous programming. Rust has great tools for this kind of use and we will see them in the next chapter. The best improvement that asynchronous programming brings is the possibility for one thread to run multiple I/O requests while not blocking the actual thread.</p>
<p>Not only that, if the thread goes idle, it will not need to sleep for some time and then it will poll for new requests. The underlying operating system will wake the thread up when there is new information for it. This approach will, therefore, use the minimum possible resources for I/O operations.</p>
<p>But what about programs that do not need I/O? In those cases, things can be executed in parallel further than using threads. Most processors nowadays allow vectorization. Vectorization uses some special CPU instructions and registers where you can enter more than one variable and perform the same operation in all of them at the same time. This is extremely useful for high-performance computing, where you need to apply a certain algorithm multiple times to different datasets. With this approach, you can perform multiple additions, subtractions, multiplications, and divisions at the same time.</p>
<p>The special instructions used for vectorization are called the <strong>SIMD</strong> family, from <strong>Single Instruction Multiple Data</strong>. You can use them by running the assembly directly with the <kbd>asm!{}</kbd> macro in nightly Rust, and the compiler will try to automatically vectorize your code, even though this is not usually as good as professionals can achieve manually. There are multiple proposals to stabilize SIMD intrinsics in 2018. This way, you will be able to use this instruction with some abstraction from assembly. There is some effort going on in the <kbd>faster</kbd> crate (<a href="https://crates.io/crates/faster">https://crates.io/crates/faster</a>).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding multithreading</h1>
                
            
            
                
<p>Now that we understand the different approaches for concurrency in Rust, we can start with the most basic one: creating threads. If you have previously used languages such as Java or C++, you will probably be familiar with the <kbd>new Thread()</kbd> syntax in the former or the <kbd>std::thread</kbd> in the latter. In both cases, you will need to specify some code that the new thread will run, and some extra information the thread will have. In both cases, you can start threads and wait for them to finish.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating threads</h1>
                
            
            
                
<p>In Rust, things are similar to the C++ approach, where we have the <kbd>std::thread</kbd> module with the <kbd>spawn()</kbd> function. This function will receive a closure or a pointer to a function and execute it. It will return a handle to the thread, and we will be able to manage it from outside. Let's see how this works:</p>
<pre>use std::thread;<br/><br/>fn main() {<br/>    println!("Before the thread!");<br/><br/>    let handle = thread::spawn(|| {<br/>        println!("Inside the thread!");<br/>    });<br/>    println!("After thread spawn!");<br/><br/>    handle.join().expect("the thread panicked");<br/>    println!("After everything!");<br/>}</pre>
<p>This will output something similar to this:</p>
<div><img src="img/6dbb166c-a42f-42e3-ac6d-0ba22f5e440e.png" style="width:17.50em;height:4.83em;"/></div>
<p>The <strong>Inside the thread!</strong> and <strong>After thread spawn!</strong> messages could be ordered in any way, in theory, even though in this simple example it is easy to see that spawning the thread will take more time than printing in the screen buffer.</p>
<p>Nevertheless, this example shows some valuable information on how to work with threads. First, when the <strong>Before the thread!</strong> message gets printed, there is only one thread in execution: the main thread, running the <kbd>main()</kbd> function.</p>
<p>Then, we spawn a new thread with the <kbd>std::thread::spawn()</kbd> function, and we pass a simple closure to it. This closure will just print the <strong>Inside the thread!</strong> message in the console. This happens at the same time as the printing of the <strong>After thread spawn!</strong> message. In fact, in some programming languages, you might see that the characters of both messages get mixed and the final message is just a lot of incomprehensible characters.</p>
<p>Rust avoids this by only accessing the standard output file descriptor with a <kbd>Mutex</kbd>. The <kbd>println!()</kbd> macro will lock <kbd>stdout</kbd> while it writes the message, and if a new message wants to be written, it will have to wait until the first write finishes.</p>
<p>This has both advantages and disadvantages. As a clear advantage, the printed messages are clearly readable, since one of the threads (the main thread or the second thread) will always arrive before the other. On the other hand, it means that while the second thread is waiting for the first one to finish printing on the screen, it will be blocked and won't be able to do any computation.</p>
<p>You will need to make sure that you take that into account, and don't print frequently from many threads while performing computations. In fact, since Rust is a thread-safe language, it will happen with any shared resource, so you will need to be careful to avoid overhead.</p>
<p>You might think that this is a bad approach for performance, since it will make things slower, but actually, it's the only possible approach if the integrity of the data needs to be preserved. In other languages, you will need to implement the solution yourself, or use existing solutions explicitly to avoid memory corruption.</p>
<p>Before the end of the example code, we can see that we call the <kbd>join()</kbd> method in the thread handle. This will make the current thread wait for the other one to finish. You might note that I added a call to the <kbd>expect()</kbd> method after it. This is because the <kbd>join()</kbd> method returns a <kbd>Result</kbd> because it might have panicked before finishing.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Panicking in Rust</h1>
                
            
            
                
<p>Let's first understand what a thread panic is. You might already know that you can cause a panic by calling the <kbd>unwrap()</kbd> or <kbd>expect()</kbd> methods in an <kbd>Option</kbd> or <kbd>Result</kbd>, or even by calling directly to <kbd>panic!()</kbd>. There are multiple ways of panicking: the <kbd>unimplemented!()</kbd> macro panics, letting the user know that the feature is not implemented, the <kbd>assert!()</kbd> macro family will panic if the conditions are not satisfied, and indexing a slice out of bounds will also panic, but, what is a panic?</p>
<p>When talking about a single-threaded application, you might think that a panic is like exiting the program with an error, similar to the <kbd>exit()</kbd> function in C/C++. What might sound new to you is that a panic is something that happens at the thread level. If the main thread panics, the whole program exits, but if a non-main thread panics, you can recover from it.</p>
<p>But, is a panic really a simple program end? Actually, it's much more than that. In C/C++, when you exit a program, the memory just gets handed back to the kernel and then it just ends. Rust, on the other hand, due to its memory safety guarantees, makes sure that it calls all the destructors in the current stack. This means that all the variables will be dropped gracefully.</p>
<p>This is what is called <strong>stack unwinding</strong>, but it's not the only option. As we saw in the first chapter, where we explained how to configure the behavior in the <kbd>Cargo.toml</kbd> file, you can also opt to abort panics, which will mimic the standard C/C++ behavior.</p>
<p>The main advantage of the unwinding panic is, of course, that you can perform cleaning operations if things go bad. You can, for example, close files, write last minute logs, and update some databases just by implementing the <kbd>Drop</kbd> trait in your structures.</p>
<p>The main disadvantage, though, as we already mentioned in <a href="ad672e4d-0f5e-4c59-b823-249da183abc8.xhtml" target="_blank">Chapter 1</a>, <em>Common Performance Pitfalls</em>, is that each time we call the <kbd>unwrap()</kbd> or <kbd>expect()</kbd> methods, for example, a new branch appears. Either things go wrong and the thread panics or things go as they should. If they panic, the compiler needs to add the whole code for the stack unwinding, which makes executables noticeably bigger.</p>
<p>Now that you know how panics work, let's look at how we can recover from them:</p>
<pre>use std::thread;<br/><br/>fn main() {<br/>    println!("Before the thread!");<br/><br/>    let handle = thread::Builder::new()<br/>        .name("bad thread".to_owned())<br/>        .spawn(|| {<br/>            panic!("Panicking inside the thread!");<br/>        })<br/>        .expect("could not create the thread");<br/>    println!("After thread spawn!");<br/><br/>    if handle.join().is_err() {<br/>        println!("Something bad happened :(");<br/>    }<br/>    println!("After everything!");<br/>}</pre>
<p>As you can see, I added some more boilerplate code. I added a name to the thread, for example, which is a good practice so that we know what each thread is called if something goes wrong.  I changed the console print inside the second thread for an explicit panic, and then I checked if things were wrong when joining the thread. What is important here is that you should never just call <kbd>expect()</kbd> or <kbd>unwrap()</kbd> when joining a thread, since it could make your whole program fail.</p>
<p>For this example, the output should be similar to the following:</p>
<div><img src="img/cb9f8ba2-7f2c-4aa5-8ed1-6abb46ec9bad.png" style="width:40.67em;height:6.00em;"/></div>
<p>There is an extra tip when working with panicking threads. If you have a structure that implements the <kbd>Drop</kbd> trait, the <kbd>drop()</kbd> method will be called when panicking or when going out of scope.</p>
<p>You can find out whether the current thread is panicking by calling the <kbd>std::thread::panicking()</kbd> function. Let's see how it works:</p>
<pre>use std::thread;<br/><br/>struct MyStruct {<br/>    name: String,<br/>}<br/><br/>impl Drop for MyStruct {<br/>    fn drop(&amp;mut self) {<br/>        if thread::panicking() {<br/>            println!("The thread is panicking with the {} struct!", self.name);<br/>        } else {<br/>            println!("The {} struct is out of scope :(", self.name);<br/>        }<br/>    }<br/>}<br/><br/>fn main() {<br/>    let my_struct = MyStruct {<br/>        name: "whole program".to_owned(),<br/>    };<br/><br/>    {<br/>        let scoped_struct = MyStruct {<br/>            name: "scoped".to_owned(),<br/>        };<br/>    }<br/><br/>    let handle = thread::Builder::new()<br/>        .name("bad thread".to_owned())<br/>        .spawn(|| {<br/>            let thread_struct = MyStruct {<br/>                name: "thread".to_owned(),<br/>            };<br/>            panic!("Panicking inside the thread!");<br/>        })<br/>        .expect("could not create the thread");<br/>    println!("After thread spawn!");<br/><br/>    if handle.join().is_err() {<br/>        println!("Something bad happened :(");<br/>    }<br/>    println!("After everything!");<br/>}</pre>
<p>Let's first see what this code does. It adds a new <kbd>MyStruct</kbd> structure, which has a name in it and that implements the Drop trait. Then, it creates one instance of the structure with the <kbd>whole program</kbd> name. This structure will be dropped at the end of the <kbd>main()</kbd> function.</p>
<p>Then, in an artificial scope, it adds a scoped instance of the structure that will be dropped just at the end of that inner scope. Finally, inside the thread, it creates a new structure that should be dropped at the end of the thread, which will be unwound.</p>
<p>The Drop implementation of the <kbd>MyStruct</kbd> structure uses the <kbd>std::thread::panicking()</kbd> function to check whether it's being dropped while panicking or simply because it went out of scope. Here we have the output of this example:</p>
<div><img src="img/c37d2d98-8759-4105-8682-95c54944722a.png" style="width:47.00em;height:9.33em;"/></div>
<p>As we can see, the first message is the drop of the inner scope binding. Then, the new thread spawns, it panics, and the binding inside the thread is dropped while unwinding the stack. Finally, after the last message in the <kbd>main()</kbd> function, the first binding we created gets dropped.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Moving data between threads</h1>
                
            
            
                
<p>We saw with the <kbd>Send</kbd> and <kbd>Sync</kbd> traits that the first one allows for a variable to be sent between threads, but how does that work? Can we just use a variable created in the main thread inside our secondary thread? Let's try it:</p>
<pre>use std::thread;<br/><br/>fn main() {<br/>    let my_vec = vec![10, 33, 54];<br/><br/>    let handle = thread::Builder::new()<br/>        .name("my thread".to_owned())<br/>        .spawn(|| {<br/>            println!("This is my vector: {:?}", my_vec);<br/>        })<br/>        .expect("could not create the thread");<br/><br/>    if handle.join().is_err() {<br/>        println!("Something bad happened :(");<br/>    }<br/>}</pre>
<p>What we did was create a vector outside the thread and then use it from inside. But it seems it does not work. Let's see what the compiler tells us:</p>
<div><img src="img/b2e48acd-7fe8-4108-9167-a6d87885dba4.png"/></div>
<p>That's interesting. The compiler noticed that the <kbd>my_vec</kbd> binding would be dropped at the end of the <kbd>main()</kbd> function, and that the inner thread could live longer. This is not the case in our example, since we <kbd>join()</kbd> both threads before the end of the <kbd>main()</kbd> function, but it could happen in a scenario where a thread is creating more threads and then ending itself. This would make the reference inside the thread invalid, and Rust does not allow that.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The move keyword</h1>
                
            
            
                
<p>Nevertheless, it gives a great explanation of something we can do. We have two options: share the binding between the threads or send it to the second one. Since we won't use it in the main thread, we can add the <kbd>move</kbd> keyword before the closure and send the vector to the new thread, since it's <kbd>Send</kbd>. Let's see how we can make it work:</p>
<pre>use std::thread;<br/><br/>fn main() {<br/>    let my_vec = vec![10, 33, 54];<br/><br/>    let handle = thread::Builder::new()<br/>        .name("my thread".to_owned())<br/>        .spawn(move || {<br/>            println!("This is my vector: {:?}", my_vec);<br/>        })<br/>        .expect("could not create the thread");<br/><br/>    if handle.join().is_err() {<br/>        println!("Something bad happened :(");<br/>    }<br/>}</pre>
<p>This now compiles and shows the list of numbers in the console, perfect! But, what if we want to be able to see it in the main thread, too? Trying to print the vector after the spawning of the second thread won't work, since the variable has been moved to the new thread, and we already saw that if we don't move the vector we cannot use it inside the thread. What can we do?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Sharing data between threads</h1>
                
            
            
                
<p>There is one special reference-counted pointer that can be shared between threads that we already mentioned: <kbd>std::sync::Arc</kbd>. The main difference from the <kbd>Rc</kbd> is that the <kbd>Arc</kbd> counts the references with an atomic counter. This means that the kernel will make sure that all updates to the reference count will happen one by one, making it thread-safe. Let's see it with an example:</p>
<pre>use std::thread;<br/>use std::sync::Arc;<br/><br/>fn main() {<br/>    let my_vec = vec![10, 33, 54];<br/>    let pointer = Arc::new(my_vec);<br/><br/>    let t_pointer = pointer.clone();<br/>    let handle = thread::Builder::new()<br/>        .name("my thread".to_owned())<br/>        .spawn(move || {<br/>            println!("Vector in second thread: {:?}", t_pointer);<br/>        })<br/>        .expect("could not create the thread");<br/><br/>    println!("Vector in main thread: {:?}", pointer);<br/><br/>    if handle.join().is_err() {<br/>        println!("Something bad happened :(");<br/>    }<br/>}</pre>
<p>As you can see, the vector is used both inside the second thread and in the main thread. You might be wondering what <kbd>clone()</kbd> means in the pointer. Are we cloning the vector? Well, that would be the easy solution, right? The real deal is that we are just getting a new reference to the vector. That's because the <kbd>Clone</kbd> trait is not a normal clone in the <kbd>Arc</kbd>. It will return a new <kbd>Arc</kbd>, yes, but it will also increase the reference count. And since both instances of <kbd>Arc</kbd> will have the same pointers to the reference counter and the vector, we will be effectively sharing the vector.</p>
<p>How is it possible to simply debug the vector pointer inside the <kbd>Arc</kbd>? This is an interesting trick. <kbd>Arc&lt;T&gt;</kbd> implements <kbd>Deref&lt;T&gt;</kbd>, which means that it will automatically dereference to the vector it's pointing to when calling the debug. Interestingly enough, there are two traits that allow that automatic dereference: <kbd>Deref</kbd> and <kbd>DerefMut</kbd>. As you might guess, the former gives you an immutable borrow of the contained value, while the latter gives you a mutable borrow.</p>
<p><kbd>Arc</kbd> only implements <kbd>Deref</kbd>, not <kbd>DerefMut</kbd>, so we are not able to mutate what we have inside of it. But wait, we have cells that can mutate while being immutable, right? Well, there is an issue with them. The behavior we have seen from the <kbd>Arc</kbd>, of being able to be shared among threads, is only thanks to implementing the <kbd>Sync</kbd> trait, and it will only implement it if the inner value implements <kbd>Sync</kbd> and <kbd>Send</kbd>. Cells can be sent between threads, they implement <kbd>Send</kbd>, but they do not implement <kbd>Sync</kbd>. <kbd>Vec</kbd>, on the other hand, implements whatever the inside values implement, so in this case, it was both <kbd>Send</kbd> and <kbd>Sync</kbd>.</p>
<p>So, is that it? Can't we mutate anything inside an <kbd>Arc</kbd>? As you might have guessed, that's not the case. If what we want to share between threads is an integer or a Boolean, we can use any of the <kbd>std::sync::atomic</kbd> integers and Booleans, even though some are not stable yet. They implement <kbd>Sync</kbd> and they have interior mutability with their <kbd>load()</kbd> and <kbd>store()</kbd> methods. You will only need to specify the memory ordering of the operation. Let's see how that works:</p>
<pre>use std::thread;<br/>use std::sync::Arc;<br/>use std::sync::atomic::{AtomicUsize, Ordering};<br/><br/>fn main() {<br/>    let my_val = AtomicUsize::new(0);<br/>    let pointer = Arc::new(my_val);<br/><br/>    let t_pointer = pointer.clone();<br/>    let handle = thread::Builder::new()<br/>        .name("my thread".to_owned())<br/>        .spawn(move || {<br/>            for _ in 0..250_000 {<br/>                let cur_value = t_pointer.load(Ordering::Relaxed);<br/>                let sum = cur_value + 1;<br/>                t_pointer.store(sum, Ordering::Relaxed);<br/>            }<br/>        })<br/>        .expect("could not create the thread");<br/><br/>    for _ in 0..250_000 {<br/>        let cur_value = pointer.load(Ordering::Relaxed);<br/>        let sum = cur_value + 1;<br/>        pointer.store(sum, Ordering::Relaxed);<br/>    }<br/><br/>    if handle.join().is_err() {<br/>        println!("Something bad happened :(");<br/>    }<br/><br/>    let a_int = Arc::try_unwrap(pointer).unwrap();<br/>    println!("Final number: {}", a_int.into_inner());<br/>}</pre>
<p>If you run this program multiple times, you will see that the final number will be different each time, and none of them will be 500,000 (it could happen, but it's almost impossible). What we have is similar to a data race:</p>
<div><img src="img/84daab71-a9b4-4fd9-b233-bd78e7090d5f.png" style="width:30.00em;height:15.33em;"/></div>
<p>But wait, can't Rust prevent all data races? Well, this is not exactly a data race. When we save the integer, we don't check whether it has changed, so we are overriding whatever was written there. We are not using the advantages Rust gives us. It will make sure that the state of the variable is consistent, but it won't prevent logic errors.</p>
<p>The issue is that when we store it back, that value has already changed. To avoid it, atomics have the great <kbd>fetch_add()</kbd> function and its friends <kbd>fetch_sub()</kbd>, <kbd>fetch_and()</kbd>, <kbd>fetch_or()</kbd>, and <kbd>fetch_xor()</kbd>. They will perform the complete operation atomically. They also have the great <kbd>compare_and_swap()</kbd> and <kbd>compare_exchange()</kbd> functions, which can be used to create locks. Let's see how that would work:</p>
<pre>use std::thread;<br/>use std::sync::Arc;<br/>use std::sync::atomic::{AtomicUsize, Ordering};<br/><br/>fn main() {<br/>    let my_val = AtomicUsize::new(0);<br/>    let pointer = Arc::new(my_val);<br/><br/>    let t_pointer = pointer.clone();<br/>    let handle = thread::Builder::new()<br/>        .name("my thread".to_owned())<br/>        .spawn(move || {<br/>            for _ in 0..250_000 {<br/>                t_pointer.fetch_add(1, Ordering::Relaxed);<br/>            }<br/>        })<br/>        .expect("could not create the thread");<br/><br/>    for _ in 0..250_000 {<br/>        pointer.fetch_add(1, Ordering::Relaxed);<br/>    }<br/><br/>    if handle.join().is_err() {<br/>        println!("Something bad happened :(");<br/>    }<br/><br/>    let a_int = Arc::try_unwrap(pointer).unwrap();<br/>    println!("Final number: {}", a_int.into_inner());<br/>}</pre>
<p>As you can see now, the result is 500,000 every time you run it. If you want to perform more complex operations, you will need a lock. You can do that with an <kbd>AtomicBool</kbd>, for example, where you can wait for it to be <kbd>false</kbd>, then swap it with <kbd>true</kbd> and then perform operations. You would need to make sure that all your threads only change values when the lock is set to <kbd>true</kbd> by them, by using some memory ordering. Let's see an example:</p>
<pre>use std::thread;<br/>use std::sync::Arc;<br/>use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};<br/><br/>fn main() {<br/>    let my_val = AtomicUsize::new(0);<br/>    let pointer = Arc::new(my_val);<br/>    let lock = Arc::new(AtomicBool::new(false));<br/><br/>    let t_pointer = pointer.clone();<br/>    let t_lock = lock.clone();<br/>    let handle = thread::Builder::new()<br/>        .name("my thread".to_owned())<br/>        .spawn(move || {<br/>            for _ in 0..250_000 {<br/>                while t_lock.compare_and_swap(<br/>                        false, true, Ordering::Relaxed) {}<br/>                let cur_value = t_pointer.load(Ordering::Relaxed);<br/>                let sum = cur_value + 1;<br/>                t_pointer.store(sum, Ordering::Relaxed);<br/>                t_lock.store(false, Ordering::Relaxed);<br/>            }<br/>        })<br/>        .expect("could not create the thread");<br/><br/>    for _ in 0..250_000 {<br/>        while lock.compare_and_swap(<br/>            false, true, Ordering::Relaxed) {}<br/>        let cur_value = pointer.load(Ordering::Relaxed);<br/>        let sum = cur_value + 1;<br/>        pointer.store(sum, Ordering::Relaxed);<br/>        lock.store(false, Ordering::Relaxed);<br/>    }<br/><br/>    if handle.join().is_err() {<br/>        println!("Something bad happened :(");<br/>    }<br/><br/>    let a_int = Arc::try_unwrap(pointer).unwrap();<br/>    println!("Final number: {}", a_int.into_inner());<br/>}</pre>
<p>If you run it, you will see that it works perfectly. But this only works because in both threads we only change the value between the lock acquisition and the lock release. In fact, this is so safe that we could avoid using an atomic integer altogether, even though Rust won't allow us to do so in safe code.</p>
<p>Now that we have seen how to mutate integers shared between threads, you might be wondering if something similar can be done with other types of bindings. As you can probably guess, it can. You will need to use <kbd>std::sync::Mutex</kbd>, and it will be much more expensive in performance terms than using atomic operations, so use them with caution. Let's see how they work:</p>
<pre>use std::thread;<br/>use std::sync::{Arc, Mutex};<br/><br/>fn main() {<br/>    let my_vec = Arc::new(Mutex::new(Vec::new()));<br/><br/>    let t_vec = my_vec.clone();<br/>    let handle = thread::Builder::new()<br/>        .name("my thread".to_owned())<br/>        .spawn(move || {<br/>            for i in 0..50 {<br/>                t_vec.lock().unwrap().push(i);<br/>            }<br/>        })<br/>        .expect("could not create the thread");<br/><br/>    for i in 0..50 {<br/>        my_vec.lock().unwrap().push(i);<br/>    }<br/><br/>    if handle.join().is_err() {<br/>        println!("Something bad happened :(");<br/>    }<br/><br/>    let vec_mutex = Arc::try_unwrap(my_vec).unwrap();<br/>    let f_vec = vec_mutex.into_inner().unwrap();<br/>    println!("Final vector: {:?}", f_vec);<br/>}</pre>
<p>It will output something similar to this:</p>
<div><img src="img/b66f3067-2acb-443f-a7b2-0f95faaf398a.png"/></div>
<p>If you analyze the output closely, you will see that it will first add all numbers from 0 to 49 and then do the same again. If both threads were running in parallel, shouldn't all numbers be randomly distributed? Maybe two 1s first, then two 2s, and so on?</p>
<p>The main issue with sharing information between threads is that when the <kbd>Mutex</kbd> locks, it requires synchronization from both threads. This is perfectly fine and safe, but it takes a lot of time to switch from one thread to another to write in the vector. This is why the kernel scheduler allows for one of the threads to work for some time before locking the <kbd>Mutex</kbd>. If it was locking and unlocking the <kbd>Mutex</kbd> for each iteration, it would take ages to finish.</p>
<p>This means that if your loops were more than 50 iterations, maybe something like 1 million per loop, you would see that after some time, one of the threads would stop to give priority to the second one. In small numbers of iterations, though, you will see that one runs after the other.</p>
<p>A <kbd>Mutex</kbd> gets locked when you call <kbd>lock()</kbd> and gets unlocked when it goes out of scope. In this case, since there is no binding to it, it will go out of scope after calling <kbd>push(i)</kbd>, so we could add more computation after it and it would be done without requiring synchronization between threads. Sometimes, it might even be useful to create artificial scopes to unlock the <kbd>Mutex</kbd> as soon as possible if our work involves more than one line and we need a binding.</p>
<p>There is an extra issue we have to take into account when working with Mutexes: thread panicking. If your thread panics while the <kbd>Mutex</kbd> is locked, the <kbd>lock()</kbd> function in another thread will return a <kbd>Result::Err(_)</kbd>, so if we call <kbd>unwrap()</kbd> every time we <kbd>lock()</kbd> our <kbd>Mutex</kbd>, we can get into big trouble, since all threads would panic. This is called <kbd>Mutex</kbd> poisoning and there is a way to avoid it.</p>
<p>When a <kbd>Mutex</kbd> is poisoned because a thread panicked while having it locked, the error result of calling the <kbd>lock()</kbd> method will return the poisoning error. We can recover from it by calling the <kbd>into_inner()</kbd> method. Let's see an example of how this would work:</p>
<pre>use std::thread;<br/>use std::sync::{Arc, Mutex};<br/>use std::time::Duration;<br/><br/>fn main() {<br/>    let my_vec = Arc::new(Mutex::new(Vec::new()));<br/><br/>    let t_vec = my_vec.clone();<br/>    let handle = thread::Builder::new()<br/>        .name("my thread".to_owned())<br/>        .spawn(move || {<br/>            for i in 0..10 {<br/>                let mut vec = t_vec.lock().unwrap();<br/>                vec.push(i);<br/>                panic!("Panicking the secondary thread");<br/>            }<br/>        })<br/>        .expect("could not create the thread");<br/><br/>    thread::sleep(Duration::from_secs(1));<br/><br/>    for i in 0..10 {<br/>        let mut vec = match my_vec.lock() {<br/>            Ok(g) =&gt; g,<br/>            Err(e) =&gt; {<br/>                println!("The secondary thread panicked, recovering…");<br/>                e.into_inner()<br/>            }<br/>        };<br/>        vec.push(i);<br/>    }<br/><br/>    if handle.join().is_err() {<br/>        println!("Something bad happened :(");<br/>    }<br/><br/>    let vec_mutex = Arc::try_unwrap(my_vec).unwrap();<br/>    let f_vec = match vec_mutex.into_inner() {<br/>        Ok(g) =&gt; g,<br/>        Err(e) =&gt; {<br/>            println!("The secondary thread panicked, recovering…");<br/>            e.into_inner()<br/>        }<br/>    };<br/>    println!("Final vector: {:?}", f_vec);<br/>}</pre>
<p>As you can see in the code, the second thread will panic after inserting the first number in the vector. I added a small 1-second sleep in the main thread to make sure that the secondary thread would execute before the main one. If you run it, you will get something similar to this:</p>
<div><img src="img/174110c7-bce9-4437-b31e-2aa46881b193.png" style="width:40.92em;height:15.00em;"/></div>
<p>As you can see, once a <kbd>Mutex</kbd> has been poisoned, it will stay poisoned for all of its life. You should therefore try to avoid any behavior that could lead to a panic once you get the lock in a <kbd>Mutex</kbd>. In any case, you can still use it and as you can see, the final vector will contain values from both threads; only the <kbd>0</kbd> from the secondary thread, until the panic, and then the rest from the main thread. Make sure not to <kbd>unwrap()</kbd> a <kbd>Mutex</kbd> in a critical application, as it will make all your threads panic if you do it in all your threads after the first panic.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Channels between threads</h1>
                
            
            
                
<p>There is an extra way of sending information between two or more threads. They are called channels, or more specifically, multi-producer, single-consumer FIFO communication primitives.</p>
<p>Let's first analyze what that means. Since they are multi-producer channels, you can send the data to the receiver from multiple threads at the same time. In the same way, since they are single-consumers, only one receiver will receive data from all the associated senders in the channel. Finally, FIFO comes from <strong>first input, first output</strong>, which means that the messages in the channel will be ordered by their creation timestamp, and you will only be able to read the second <kbd>message</kbd> after reading the first one in the receiver.</p>
<p>These channels are located in the <kbd>std::sync::mpsc</kbd> module, and they are really useful for logging or telemetry, for example. One thread can manage the I/O interface with the communication or logging mechanism, while the others can send this thread the information they want to log or communicate. An approach using these channels is being studied for OpenStratos, for a stratospheric balloon-control software being written in Rust.</p>
<p class="mce-root">A channel consists of a <kbd>Sender</kbd> and a <kbd>Receiver</kbd>. <kbd>Sender</kbd> implements <kbd>Clone</kbd>, so that cloning the sender, multiple threads can send information to the associated receiver. There are two types of senders: <kbd>Sender</kbd> and <kbd>SyncSender</kbd>. The former will just send the message to the receiver without checking anything extra, while the latter will send the message only when the receiver's buffer has enough space. It will block the current thread until the message is sent.</p>
<p class="mce-root">Channels are created using the <kbd>channel()</kbd> and <kbd>sync_channel()</kbd> functions in the <kbd>std::sync::mpsc</kbd> module. They will return a tuple with a <kbd>Sender</kbd> or <kbd>SyncSender</kbd>, respectively, as the first element and a <kbd>Receiver</kbd> as the second one. Since <kbd>Sender</kbd> and <kbd>Receiver</kbd> implement <kbd>Send</kbd>, they can safely be sent to another thread with the <kbd>move</kbd> keyword. In the case of a synchronous channel, the <kbd>sync_channel()</kbd> will require a <kbd>usize</kbd> to set the buffer size. The <kbd>Sender</kbd> will block if the buffer is full. On the other hand, asynchronous channels work as if they had an infinite buffer, they will always accept sending new data.</p>
<p>Each channel can only send or receive one particular type of data, so if a channel is configured to send <kbd>u32</kbd>, only one <kbd>u32</kbd> per message can be sent. You can configure it to send your own types, though, such as a custom <kbd>Frame</kbd> type with all the information you might want to send. Let's see how a channel works:</p>
<pre>use std::thread;<br/>use std::sync::mpsc::*;<br/>use std::time::Duration;<br/><br/>fn main() {<br/>    let (sender, receiver) = channel();<br/><br/>    let handles: Vec&lt;_&gt; = (1..6)<br/>        .map(|i| {<br/>            let t_sender = sender.clone();<br/>            thread::Builder::new()<br/>                .name(format!("sender-{}", i))<br/>                .spawn(move || {<br/>                    t_sender.send(<br/>                        format!("Hello from sender {}!", i)<br/>                    ).unwrap();<br/>                })<br/>                .expect("could not create the thread")<br/>        })<br/>        .collect();<br/><br/>    while let Ok(message) = receiver.recv_timeout(Duration::from_secs(1)) {<br/>        println!("{}", message);<br/>    }<br/>    for handle in handles {<br/>        handle.join().unwrap();<br/>    }<br/>    println!("Finished");<br/>}</pre>
<p>As you can see in the code, five threads get created with an iterator, and their handles are collected in a vector. These threads will have a name containing the thread number, and will send the <kbd>Hello from sender {}!</kbd> message to the receiver. For each thread, the sender gets cloned so that the clone can be moved to the thread closure.</p>
<p>Then, a <kbd>while</kbd> loop will check with a 1-second timeout for the messages. It should be enough, since messages will be sent as soon as the threads start. In the case that no message gets received for one second (or all the senders get out of scope), the <kbd>while</kbd> loop will stop printing the messages and the threads will be joined. Finally, a completion message will be printed.</p>
<p>If you run this example, you will see an output similar to this one:</p>
<div><img src="img/b6376638-1397-4e17-9632-110b3f7d6fd5.png" style="width:14.17em;height:8.67em;"/></div>
<p>As you can see, the threads are not executed in any particular order. They will send the message to the receiver, and the receiver will read them in the received order. Since this example is asynchronous, we don't need to wait for the receiver to empty the buffer to send new messages, so it's really lightweight. In fact, we could join the threads before reading any messages from the receiver.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Multithreading crates</h1>
                
            
            
                
<p>Until now, we have only been using the standard library to manipulate threads, but thanks to the great <em>crates.io</em> ecosystem, we can make use of more approaches that will improve our development speed as well as the performance of our code.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Non-blocking data structures</h1>
                
            
            
                
<p>One of the issues we saw earlier was that if we wanted to share something more complex than an integer or a Boolean between threads and if we wanted to mutate it, we needed to use a <kbd>Mutex</kbd>. This is not entirely true, since one crate, Crossbeam, allows us to use great data structures that do not require locking a <kbd>Mutex</kbd>. They are therefore much faster and more efficient.</p>
<p>Often, when we want to share information between threads, it's usually a list of tasks that we want to work on cooperatively. Other times, we want to create information in multiple threads and add it to a list of information. It's therefore not so usual for multiple threads to be working with exactly the same variables, since as we have seen, that requires synchronization and it will be slow.</p>
<p>This is where Crossbeam shows all its potential. Crossbeam gives us some multithreaded queues and stacks, where we can insert data and consume data from different threads. We can, in fact, have some threads doing an initial processing of the data and others performing a second phase of the processing. Let's see how we can use these features. First, add <kbd>crossbeam</kbd> to the dependencies of the crate in the <kbd>Cargo.toml</kbd> file. Then, we start with a simple example:</p>
<pre>extern crate crossbeam;<br/><br/>use std::thread;<br/>use std::sync::Arc;<br/><br/>use crossbeam::sync::MsQueue;<br/><br/>fn main() {<br/>    let queue = Arc::new(MsQueue::new());<br/><br/>    let handles: Vec&lt;_&gt; = (1..6)<br/>        .map(|_| {<br/>            let t_queue = queue.clone();<br/>            thread::spawn(move || {<br/>                for _ in 0..1_000_000 {<br/>                    t_queue.push(10);<br/>                }<br/>            })<br/>        })<br/>        .collect();<br/><br/>    for handle in handles {<br/>        handle.join().unwrap();<br/>    }<br/><br/>    let final_queue = Arc::try_unwrap(queue).unwrap();<br/>    let mut sum = 0;<br/>    while let Some(i) = final_queue.try_pop() {<br/>        sum += i;<br/>    }<br/><br/>    println!("Final sum: {}", sum);<br/>}</pre>
<p>Let's first understand what this example does. It will iterate 1,000,000 times in 5 different threads, and each time it will push a 10 to a queue. Queues are FIFO lists, first input, first output. This means that the first number entered will be the first one to <kbd>pop()</kbd> and the last one will be the last to do so. In this case, all of them are a 10, so it doesn't matter.</p>
<p>Once the threads finish populating the queue, we iterate over it and we add all the numbers. A simple computation should make you able to guess that if everything goes perfectly, the final number should be 50,000,000. If you run it, that will be the result, and that's not all. If you run it by executing <kbd>cargo run --release</kbd>, it will run blazingly fast. On my computer, it took about one second to complete. If you want, try to implement this code with the standard library <kbd>Mutex</kbd> and vector, and you will see that the performance difference is amazing.</p>
<p>As you can see, we still needed to use an <kbd>Arc</kbd> to control the multiple references to the queue. This is needed because the queue itself cannot be duplicated and shared, it has no reference count.</p>
<p>Crossbeam not only gives us FIFO queues. We also have LIFO stacks. LIFO comes from last input, first output, and it means that the last element you inserted in the stack will be the first one to <kbd>pop()</kbd>. Let's see the difference with a couple of threads:</p>
<pre>extern crate crossbeam;<br/><br/>use std::thread;<br/>use std::sync::Arc;<br/>use std::time::Duration;<br/><br/>use crossbeam::sync::{MsQueue, TreiberStack};<br/><br/>fn main() {<br/>    let queue = Arc::new(MsQueue::new());<br/>    let stack = Arc::new(TreiberStack::new());<br/><br/>    let in_queue = queue.clone();<br/>    let in_stack = stack.clone();<br/>    let in_handle = thread::spawn(move || {<br/>        for i in 0..5 {<br/>            in_queue.push(i);<br/>            in_stack.push(i);<br/>            println!("Pushed :D");<br/>            thread::sleep(Duration::from_millis(50));<br/>        }<br/>    });<br/><br/>    let mut final_queue = Vec::new();<br/>    let mut final_stack = Vec::new();<br/><br/>    let mut last_q_failed = 0;<br/>    let mut last_s_failed = 0;<br/><br/>    loop {<br/>        // Get the queue<br/>        match queue.try_pop() {<br/>            Some(i) =&gt; {<br/>                final_queue.push(i);<br/>                last_q_failed = 0;<br/>                println!("Something in the queue! :)");<br/>            }<br/>            None =&gt; {<br/>                println!("Nothing in the queue :(");<br/>                last_q_failed += 1;<br/>            }<br/>        }<br/><br/>        // Get the stack<br/>        match stack.try_pop() {<br/>            Some(i) =&gt; {<br/>                final_stack.push(i);<br/>                last_s_failed = 0;<br/>                println!("Something in the stack! :)");<br/>            }<br/>            None =&gt; {<br/>                println!("Nothing in the stack :(");<br/>                last_s_failed += 1;<br/>            }<br/>        }<br/><br/>        // Check if we finished<br/>        if last_q_failed &gt; 1 &amp;&amp; last_s_failed &gt; 1 {<br/>            break;<br/>        } else if last_q_failed &gt; 0 || last_s_failed &gt; 0 {<br/>            thread::sleep(Duration::from_millis(100));<br/>        }<br/>    }<br/><br/>    in_handle.join().unwrap();<br/><br/>    println!("Queue: {:?}", final_queue);<br/>    println!("Stack: {:?}", final_stack);<br/>}</pre>
<p>As you can see in the code, we have two shared variables: a queue and a stack. The secondary thread will push new values to each of them, in the same order, from 0 to 4. Then, the main thread will try to get them back. It will loop indefinitely and use the <kbd>try_pop()</kbd> method. The <kbd>pop()</kbd> method can be used, but it will block the thread if the queue or the stack is empty. This will happen in any case once all values get popped, since no new values are being added, so the <kbd>try_pop()</kbd> method will help not to block the main thread and end gracefully.</p>
<p>The way it checks whether all the values were popped is by counting how many times it failed to pop a new value. Every time it fails, it will wait for 100 milliseconds, while the push thread only waits for 50 milliseconds between pushes. This means that if it tries to pop new values two times and there are no new values, the pusher thread has already finished.</p>
<p>It will add values as they are popped to two vectors and then print the result. In the meantime, it will print messages about pushing and popping new values. You will understand this better by seeing the output:</p>
<div><img src="img/a9446abb-afee-4432-a2f2-7f8d33925c1f.png" style="width:16.00em;height:29.08em;"/></div>
<p>Note that the output can be different in your case, since threads don't need to be executed in any particular order.</p>
<p>In this example output, as you can see, it first tries to get something from the queue and the stack but there is nothing there, so it sleeps. The second thread then starts pushing things, two numbers actually. After this, the queue and the stack will be <kbd>[0, 1]</kbd>. Then, it pops the first item from each of them. From the queue, it will pop the <kbd>0</kbd> and from the stack it will pop the <kbd>1</kbd> (the last one), leaving the queue as <kbd>[1]</kbd> and the stack as <kbd>[0]</kbd>.</p>
<p>It will go back to sleep and the secondary thread will insert a <kbd>2</kbd> in each variable, leaving the queue as <kbd>[1, 2]</kbd> and the stack as <kbd>[0, 2]</kbd>. Then, the main thread will pop two elements from each of them. From the queue, it will pop the <kbd>1</kbd> and the <kbd>2</kbd>, while from the stack it will pop the <kbd>2</kbd> and then the <kbd>0</kbd>, leaving both empty.</p>
<p>The main thread then goes to sleep, and for the next two tries, the secondary thread will push one element and the main thread will pop it, twice.</p>
<p>It might seem a little bit complex, but the idea is that these queues and stacks can be used efficiently between threads without requiring a <kbd>Mutex</kbd>, and they accept any <kbd>Send</kbd> type. This means that they are great for complex computations, and even for multi-staged complex computations.</p>
<p>The Crossbeam crate also has some helpers to deal with epochs and even some variants of the mentioned types. For multithreading, Crossbeam also adds a great utility: scoped threads.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Scoped threads</h1>
                
            
            
                
<p>In all our examples, we have used standard library threads. As we have discussed, these threads have their own stack, so if we want to use variables that we created in the main thread we will need to <em>send</em> them to the thread. This means that we will need to use things such as <kbd>Arc</kbd> to share non-mutable data. Not only that, having their own stack means that they will also consume more memory and eventually make the system slower if they use too much.</p>
<p>Crossbeam gives us some special threads that allow sharing stacks between them. They are called scoped threads. Using them is pretty simple and the crate documentation explains them perfectly; you will just need to create a <kbd>Scope</kbd> by calling <kbd>crossbeam::scope()</kbd>. You will need to pass a closure that receives the Scope. You can then call <kbd>spawn()</kbd> in that scope the same way you would do it in <kbd>std::thread</kbd>, but with one difference, you can share immutable variables among threads if they were created inside the scope or moved to it.</p>
<p>This means that for the queues or stacks we just talked about, or for atomic data, you can simply call their methods without requiring an <kbd>Arc</kbd>! This will improve the performance even further. Let's see how it works with a simple example:</p>
<pre>extern crate crossbeam;<br/><br/>fn main() {<br/>    let all_nums: Vec&lt;_&gt; = (0..1_000_u64).into_iter().collect();<br/>    let mut results = Vec::new();<br/><br/>    crossbeam::scope(|scope| {<br/>        for num in &amp;all_nums {<br/>            results.push(scope.spawn(move || num * num + num * 5 + 250));<br/>        }<br/>    });<br/><br/>    let final_result: u64 = results.into_iter().map(|res| res.join()).sum();<br/>    println!("Final result: {}", final_result);<br/>}</pre>
<p>Let's see what this code does. It will first just create a vector with all the numbers from 0 to 1000. Then, for each of them, in a <kbd>crossbeam</kbd> scope, it will run one scoped thread per number and perform a supposedly complex computation. This is just an example, since it will just return a result of a simple second-order function.</p>
<p>Interestingly enough, though, the <kbd>scope.spawn()</kbd> method allows returning a result of any type, which is great in our case. The code will add each result to a vector. This won't directly add the resulting number, since it will be executed in parallel. It will add a result guard, which we will be able to check outside the scope.</p>
<p>Then, after all the threads run and return the results, the scope will end. We can now check all the results, which are guaranteed to be ready for us. For each of them, we just need to call <kbd>join()</kbd> and we will get the result. Then, we sum it up to check that they are actual results from the computation.</p>
<p>This <kbd>join()</kbd> method can also be called inside the scope and get the results, but it will mean that if you do it inside the <kbd>for</kbd> loop, for example, you will block the loop until the result is generated, which is not efficient. The best thing is to at least run all the computations first and then start checking the results. If you want to perform more computations after them, you might find it useful to run the new computation in another loop or iterator inside the <kbd>crossbeam</kbd> scope.</p>
<p>But, how does <kbd>crossbeam</kbd> allow you to use the variables outside the scope freely? Won't there be data races? Here is where the magic happens. The scope will join all the inner threads before exiting, which means that no further code will be executed in the main thread until all the scoped threads finish. This means that we can use the variables of the main thread, also called <strong>parent stack</strong>, due to the main thread being the parent of the scope in this case without any issue.</p>
<p>We can actually check what is happening by using the <kbd>println!()</kbd> macro. If we remember from previous examples, printing to the console after spawning some threads would usually run even before the spawned threads, due to the time it takes to set them up. In this case, since we have <kbd>crossbeam</kbd> preventing it, we won't see it. Let's check the example:</p>
<pre>extern crate crossbeam;<br/><br/>fn main() {<br/>    let all_nums: Vec&lt;_&gt; = (0..10).into_iter().collect();<br/><br/>    crossbeam::scope(|scope| {<br/>        for num in all_nums {<br/>            scope.spawn(move || {<br/>                println!("Next number is {}", num);<br/>            });<br/>        }<br/>    });<br/><br/>    println!("Main thread continues :)");<br/>}<br/><br/></pre>
<p>If you run this code, you will see something similar to the following output:</p>
<div><img src="img/23ccb780-ea03-42ca-a70c-c597251ed1dd.png" style="width:15.75em;height:14.00em;"/></div>
<p>As you can see, scoped threads will run without any particular order. In this case, it will first run the <kbd>1</kbd>, then the <kbd>0</kbd>, then the <kbd>2</kbd>, and so on. Your output will probably be different. The interesting thing, though, is that the main thread won't continue executing until all the threads have finished. Therefore, reading and modifying variables in the main thread is perfectly safe.</p>
<p>There are two main performance advantages with this approach; <kbd>Arc</kbd> will require a call to <kbd>malloc()</kbd> to allocate memory in the heap, which will take time if it's a big structure and the memory is a bit full. Interestingly enough, that data is already in our stack, so if possible, we should try to avoid duplicating it in the heap. Moreover, the <kbd>Arc</kbd> will have a reference counter, as we saw. And it will even be an atomic reference counter, which means that every time we clone the reference, we will need to atomically increment the count. This takes time, even more than incrementing simple integers.</p>
<p>Most of the time, we might be waiting for some expensive computations to run, and it would be great if they just gave all the results when finished. We can still add some more chained computations, using scoped threads, that will only be executed after the first ones finish, so we should use scoped threads more often than normal threads, if possible.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Thread pooling</h1>
                
            
            
                
<p>So far, we have seen multiple ways of creating new threads and sharing information between them. Nevertheless, as we saw at the beginning of the chapter, the ideal number of threads we should spawn to do all the work should be around the number of virtual processors in the system. This means we should not spawn one thread for each chunk of work. Nevertheless, controlling what work each thread does can be complex, since you have to make sure that all threads have work to do at any given point in time.</p>
<p>Here is where thread pooling comes in handy. The <kbd>Threadpool</kbd> crate will enable you to iterate over all your work and for each of your small chunks, you can call something similar to a <kbd>thread::spawn()</kbd>. The interesting thing is that each task will be assigned to an idle thread, and no new thread will be created for each task. The number of threads is configurable and you can get the number of CPUs with other crates. Not only that, if one of the threads panics, it will automatically add a new one to the pool.</p>
<p>To see an example, first, let's add <kbd>threadpool</kbd> and <kbd>num_cpus</kbd> as dependencies in our <kbd>Cargo.toml</kbd> file.  Then, let's see an example code:</p>
<pre>extern crate num_cpus;<br/>extern crate threadpool;<br/><br/>use std::sync::atomic::{AtomicUsize, Ordering};<br/>use std::sync::Arc;<br/><br/>use threadpool::ThreadPool;<br/><br/>fn main() {<br/>    let pool = ThreadPool::with_name("my worker".to_owned(), num_cpus::get());<br/>    println!("Pool threads: {}", pool.max_count());<br/><br/>    let result = Arc::new(AtomicUsize::new(0));<br/><br/>    for i in 0..1_0000_000 {<br/>        let t_result = result.clone();<br/>        pool.execute(move || {<br/>            t_result.fetch_add(i, Ordering::Relaxed);<br/>        });<br/>    }<br/><br/>    pool.join();<br/><br/>    let final_res = Arc::try_unwrap(result).unwrap().into_inner();<br/>    println!("Final result: {}", final_res);<br/>}</pre>
<p>This code will create a thread pool of threads with the number of logical CPUs in your computer. Then, it will add a number from 0 to 1,000,000 to an atomic <kbd>usize</kbd>, just to test parallel processing. Each addition will be performed by one thread. Doing this with one thread per operation (1,000,000 threads) would be really inefficient. In this case, though, it will use the appropriate number of threads, and the execution will be really fast. There is another crate that gives thread pools an even more interesting parallel processing feature: Rayon.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Parallel iterators</h1>
                
            
            
                
<p>If you can see the big picture in these code examples, you'll have realized that most of the parallel work has a long loop, giving work to different threads. It happened with simple threads and it happens even more with scoped threads and thread pools. It's usually the case in real life, too. You might have a bunch of data to process, and you can probably separate that processing into chunks, iterate over them, and hand them over to various threads to do the work for you.</p>
<p>The main issue with that approach is that if you need to use multiple stages to process a given piece of data, you might end up with lots of boilerplate code that can make it difficult to maintain. Not only that, you might find yourself not using parallel processing sometimes due to the hassle of having to write all that code.</p>
<p>Luckily, Rayon has multiple data parallelism primitives around iterators that you can use to parallelize any iterative computation. You can almost forget about the <kbd>Iterator</kbd> trait and use Rayon's <kbd>ParallelIterator</kbd> alternative, which is as easy to use as the standard library trait!</p>
<p>Rayon uses a parallel iteration technique called <strong>work stealing</strong>. For each iteration of the parallel iterator, the new value or values get added to a queue of pending work. Then, when a thread finishes its work, it checks whether there is any pending work to do and if there is, it starts processing it. This, in most languages, is a clear source of data races, but thanks to Rust, this is no longer an issue, and your algorithms can run extremely fast and in parallel.</p>
<p>Let's look at how to use it for an example similar to those we have seen in this chapter. First, add <kbd>rayon</kbd> to your <kbd>Cargo.toml</kbd> file and then let's start with the code:</p>
<pre>extern crate rayon;<br/><br/>use rayon::prelude::*;<br/><br/>fn main() {<br/>    let result = (0..1_000_000_u64)<br/>        .into_par_iter()<br/>        .map(|e| e * 2)<br/>        .sum::&lt;u64&gt;();<br/><br/>    println!("Result: {}", result);<br/>}</pre>
<p>As you can see, this works just as you would write it in a sequential iterator, yet, it's running in parallel. Of course, running this example sequentially will be faster than running it in parallel thanks to compiler optimizations, but when you need to process data from files, for example, or perform very complex mathematical computations, parallelizing the input can give great performance gains.</p>
<p>Rayon implements these parallel iteration traits to all standard library iterators and ranges. Not only that, it can also work with standard library collections, such as <kbd>HashMap</kbd> and <kbd>Vec</kbd>. In most cases, if you are using the <kbd>iter()</kbd> or <kbd>into_iter()</kbd> methods from the standard library in your code, you can simply use <kbd>par_iter()</kbd> or <kbd>into_par_iter()</kbd> in those calls and your code should now be parallel and work perfectly.</p>
<p>But, beware, sometimes parallelizing something doesn't automatically improve its performance. Take into account that if you need to update some shared information between the threads, they will need to synchronize somehow, and you will lose performance. Therefore, multithreading is only great if workloads are completely independent and you can execute one without any dependency on the rest.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we saw how our sequential algorithms can easily gain performance by running in parallel. This parallelism can be obtained in multiple ways, and in this chapter, we learned about multithreading. We saw how multithreading is really safe in Rust, and how we can take advantage of the crate ecosystem to improve our performance even more.</p>
<p>We learned about some performance enhancements we can develop for our multithreaded code, and how to use all the available tools to our advantage. You can now develop a high-performance concurrent application in Rust using multiple threads.</p>
<p>In the next chapter, we will look at asynchronous programming. The primitives we will look at enable us to write concurrent programs that won't lock our threads if we are waiting for some computation, without even requiring us to spawn new threads!</p>


            

            
        
    </body></html>
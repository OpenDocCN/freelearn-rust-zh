<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Asynchronous Programming</h1>
                </header>
            
            <article>
                
<p class="mce-root">Until now, the only way we have seen to achieve concurrency in Rust is to create multiple threads, <span>one way or another,</span> to share the work. Nevertheless, those threads sometimes need to stop and look for something, such as a file or a network response. In those cases, the whole thread will be blocked and it will need to wait for the response.</p>
<p>This means that if we want to achieve a low latency for things such as an HTTP server, one way to do it is by spawning one thread per request, so that each request can be served as quickly as possible even if others block.</p>
<p>As we have seen, spawning hundreds of threads is not scalable, since each thread will have its own memory and will consume resources even if it's blocked. In this chapter, you will learn a new way of doing things by using asynchronous programming.</p>
<p>In this chapter, you will learn about the following:</p>
<ul>
<li>Asynchronous primitives with <kbd>mio</kbd></li>
<li>Using <kbd>futures</kbd></li>
<li>The new <kbd>async</kbd>/<kbd>await</kbd> syntax and generators</li>
<li>Asynchronous I/O with <kbd>tokio</kbd> and <kbd>websockets</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to asynchronous programming</h1>
                </header>
            
            <article>
                
<p>If you want to achieve high performance in computing, you will need to run tasks concurrently. Whether you are running complex computations that take days, such as machine learning training, or you are running a web server that needs to respond to thousands of requests per second, you will need to do more than one thing at the same time.</p>
<p>Thankfully, as we have already seen, our processors and operating systems are prepared for concurrency, and in fact, multithreading is a great way to achieve it. The main issue is that as we saw in the previous chapter, we should not be using more threads than logical CPUs in our computer.</p>
<p>We can, of course, but some threads will be waiting for others to execute, and the kernel will be orchestrating how much time each thread gets in the CPU. This will consume even more resources and make the overall process slower. It can sometimes be useful, though, to have more threads than the number of cores. Maybe some of them only wake up once every few seconds to do small tasks, or we know that most of them will block due to some I/O operation.</p>
<div class="packt_infobox">When a thread blocks, the execution stops. No further instruction will run in the CPU until it gets unblocked. This can happen when we read a file, for example. Until the reading ends, no further instruction will be executed. This of course, depends on how we read the file.</div>
<p>But in the latter case, instead of creating more threads we can do better—asynchronous programming. When programming asynchronously, we let the code continue being executed while we are still waiting for a certain result. That will avoid blocking the thread and let you use less threads for the same task, while still being concurrent. You can also use asynchronous programming for tasks not related to I/O, but if they are CPU bound (their bottleneck is the CPU), you won't get speed improvements, since the CPU will always be running at its best. To learn how asynchronous I/O works in Rust, let's first dive into how the CPU handles I/O.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding I/O in the CPU</h1>
                </header>
            
            <article>
                
<p>The <kbd>std::io</kbd> module in Rust handles all input/output operations. These operations can vary from keyboard/mouse input to reading a file, or from using TCP/IP sockets to command-line utilities (<kbd>stdio</kbd>/<kbd>stderr</kbd>). But how does it work internally?</p>
<p>Instead of understanding how the Rust standard library does it, we will dig some levels deeper to understand how it works at CPU level. We will later go back to see how the kernel provides this functionality to Rust. This will be based mostly in the x86_64 platform and Linux kernel, but other platforms handle these things similarly.</p>
<p>There are two main types of I/O architecture: channel-based I/O and memory-mapped I/O. Channel-based I/O is really niche and is not used in modern PCs or most servers. In CPU architectures such as x86/x86_64 (most modern day Intel and AMD CPUs), memory-mapped I/O is used. But what does it mean?</p>
<p>As you should know by now, the CPU gets all the required information for its work from the RAM memory. As we saw in previous chapters, this information will later be cached in the CPU cache, and it won't be used until it gets to the CPU registers, but this is not so relevant for now. So, if the CPU wants to get information about what key was pressed on the keyboard, or what TCP frames the website you are visiting is sending, it needs to either have some extra hardware channel to those input/output interfaces, or those interfaces have to change something in the RAM.</p>
<p>The first option is the channel-based I/O. CPUs that use channel-based I/O have dedicated channels and hardware for I/O operations. This usually increases the price of the CPUs a lot. On the other hand, in memory-mapped I/O the second option gets used—the memory gets somehow modified when an I/O operation happens.</p>
<p>Here, we have to pause a little bit to understand this better. Even though we may think that all our memory is in our RAM sticks, it's not exactly like that. Memory is divided into virtual and physical memory. Each program has one virtual memory address available for each addressable byte with the size of a CPU word. This means that a 32-bit CPU will have 2<sup>32</sup> virtual memory addresses available for each of its programs and a 64-bit CPU will have 2<sup>64</sup> addresses. This would mean having 4 GiB of RAM in the case of 32-bit computers and 16 EiB of RAM in the case of a 64-bit CPU. <strong>EiBs</strong> are <strong>exbibytes</strong>, or 1,014 <strong>PiB</strong> (<strong>pebibytes</strong>). Each PiB is 1024 <strong>GiB</strong> (<strong>gibibytes</strong>). Remember that gibibytes are the two-power version of <strong>gigabytes</strong> (<strong>GB</strong>). And all of this is true for each of the processes in the CPU.</p>
<p>There are some issues with this. First, if we have two processes, we would need double the amount of memory, right? But the kernel can only address that amount of memory (it's a process itself). So we need <strong>translation tables</strong> (<strong><span>TLBs</span></strong>), that tell each process where their memory is. But even though we may have 4 GiB of RAM for 32-bit CPUs, we don't have 16 EiB of RAM anywhere. Not only that, 32-bit CPUs have existed long before we were able to create PCs with 4 GiB of RAM. How can a process address more RAM than what we have installed?</p>
<p>The solution is simple—we call that address space the virtual memory space, and the real RAM the physical memory space. If a process requires more memory than the available physical memory, two things can happen—either our kernel can move some memory addresses out of RAM into the disk and allocate some more RAM for this process, or it will receive an out-of-memory error. The first option is called page swapping, and it's really common in Unix systems, where you sometimes even decide how much space in the disk you want to provide for that.</p>
<p>Moving information from the RAM to the disk will slow things down a lot, since the disk itself is really slow compared to the RAM (even modern day SSDs are much slower than RAM). Nevertheless, here we find that there is some I/O happening to swap that memory information to the disk, right? How does that happen?</p>
<p>Well, we said that the virtual memory space was specific for each process, and we said that the kernel was another process. This means that the kernel also has the whole memory space available for it to use. This is where memory-mapped I/O comes in. The CPU will decide to map new devices to some addresses. This means that the kernel will be able to read information about the I/O interface just by reading some concrete positions in its virtual address space.</p>
<p>In this regard, there are some variants as to how to read that information. Two main ways exist—port-mapped I/O and direct memory access or DMA. Port-mapped I/O is used, of course, for TCP/IP, serial, and other kinds of peripheral communication. It will have some certain addresses allocated to it. These addresses will be a buffer, which means that as the input comes, it will write one by one the next memory address. Once it gets to the end, it will start from the beginning again, so the kernel has to be fast enough to read the information before it gets rewritten. It can also block the port, stopping the communication.</p>
<p>In the case of DMA, the memory space of the device will be directly mapped in the virtual memory. This enables accessing that memory as if it were part of the virtual address space of the current PC. Which approach is used depends on the task and the device we want to communicate with. You may now be wondering how the kernel handles all this for your programs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the kernel to control the I/O</h1>
                </header>
            
            <article>
                
<p>When a new TCP/IP connection gets established, or when a new key is pressed on the keyboard, the kernel must know about it so that it can act accordingly. There are two ways of doing this—the kernel could be looking to those ports or memory addresses once and again to look for changes, which would make the CPU work for nothing most of the time, or the kernel could be notified by a CPU interrupt.</p>
<p>As you can imagine, most kernels decide to go for the second option. They are idle, letting other processes use the CPU until there's a change in some I/O port or address. This makes the CPU interrupt at a hardware level, and gives control to the kernel. The kernel will check what happened and decide accordingly. If there is some process waiting for that interrupt, it will wake that process and let it know that there is some new information for it.</p>
<p>It may be, though, that the process waiting for the information is already awake. This happens in asynchronous programming. The process will keep on performing some computations while it's still waiting for the I/O transaction. In this case, the process will have registered some callback function in the kernel, so that the kernel knows what to call once the I/O operation is ready.</p>
<p>This means that while the I/O operation was being performed, the process was doing useful things, instead of being blocked and waiting for the kernel to return the result of the I/O operation. This enables you to use the CPU almost all the time, without pausing the execution, making your code perform better.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Asynchronous programming from the programmer's perspective</h1>
                </header>
            
            <article>
                
<p>Until now, we have seen how I/O works from a hardware and software perspective. We mentioned that it is possible to have our process working while waiting for the I/O, but how do we do it?</p>
<p>The kernel has some things to help us with this. In the case of Linux, it has the <kbd>epoll()</kbd> system call, which lets the kernel know that our code wants to receive some information from an I/O interface, but that it doesn't need to lock itself until the information is available. The kernel will know what callback to run when the information is ready, and meanwhile, our program can do a lot of computations.</p>
<p>This is very useful, for example, if we are processing some data and we know that in the future we will need some information from a file. We can ask the kernel to get the information from the file while we continue the computation, and as soon as we need the information from the file, we won't need to wait for the file reading operation—the information will just be there. This reduces the disk read latency a lot, since it will be almost as fast as reading from the RAM instead of the disk.</p>
<p>We can use this approach for TCP/IP connections, serial connections, and, in general, anything that requires I/O access. This <kbd>epoll()</kbd> system call comes directly from the Linux C API, but in Rust we have great wrappers that make all of this much easier without overhead. Let's check them out.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding futures</h1>
                </header>
            
            <article>
                
<p>If we use the <kbd>std::io::Read</kbd> and <kbd>std::io::Write</kbd> traits in our code, we will be able to easily read and write data from I/O interfaces, but every time we do it, the thread doing the call will block until the data is received. Luckily, the great crate ecosystem Rust has brings us great opportunities to improve this situation.</p>
<p>In many programming languages, you can find the concept of <em>not yet available data</em>. In JavaScript, for example, they are called promises, and in Rust, we call them futures. A future represents any data that will be available at some point in the future but may not be available yet. You can check whether a future has a value at any time, and get it if it does. If not, you can either perform some computation in the meantime or block the current thread until the value gets there.</p>
<p>Rust futures not only give us this feature, but they even give us tons of helpful APIs that we can use to improve the readability and reduce the amount of code written. The <kbd>futures</kbd> crate does all this with <em>zero-cost</em> abstractions. This means that it will not require extra allocations and the code will be as close as possible to the best assembly code you could write to make all this possible.</p>
<p>Futures not only work for I/O; they can be used with any kind of computation or data. We will be using the 0.2.x version of the futures crate in these examples. At the time of writing, that version is still in the alpha development phase, but it's expected to be released really soon. Let's see an example of how futures work. We will first need to add the <kbd>futures</kbd> crate as a dependency in the <kbd>Cargo.toml</kbd> file, and then we can start writing some code in the <kbd>main.rs</kbd> file of our project:</p>
<pre>extern crate futures;<br/><br/>use futures::prelude::*;<br/>use futures::future::{self, FutureResult};<br/>use futures::executor::block_on;<br/><br/>fn main() {<br/>    let final_result = some_complex_computation().map(|res| (res - 10) / 7);<br/><br/>    println!("Doing some other things while our result gets generated");<br/><br/>    match block_on(final_result) {<br/>        Ok(res) =&gt; println!("The result is {}", res),<br/>        Err(e) =&gt; println!("Error: {}", e),<br/>    }<br/>}<br/><br/>fn some_complex_computation() -&gt; FutureResult&lt;u32, String&gt; {<br/>    use std::thread;<br/>    use std::time::Duration;<br/><br/>    thread::sleep(Duration::from_secs(5));<br/><br/>    future::ok(150)<br/>}</pre>
<p>In this example, we have a simulated complex computation that takes around 5 seconds. This computation returns a <kbd>Future</kbd>, and we can therefore use useful methods to modify the result once it gets generated. These methods come from the <kbd>FutureExt</kbd> trait.</p>
<p>Then, the <kbd>block_on()</kbd> function will wait until the given future is no longer pending. You may think that this is exactly the same as when we were working with threads, but the interesting thing is that we are only using one thread here. The future will be computed once the main thread has some spare time, or when we call the <kbd>block_on()</kbd> function.</p>
<p>This, of course, does not make much sense for computationally intense applications, since we will in any case have to do the computation in the main thread, but it makes a lot of sense for I/O access. We can think of a <kbd>Future</kbd> as the asynchronous version of a <kbd>Result</kbd>.</p>
<p>As you can see in the <kbd>FutureExt</kbd> trait documentation at <a href="https://docs.rs/futures/0.2.0-alpha/futures/trait.FutureExt.html">https://docs.rs/futures/0.2.0-alpha/futures/trait.FutureExt.html</a>, we have tons of combinators to use. In this case, we used the <kbd>map()</kbd> method, but we can also use other methods such as <kbd>and_then()</kbd>, <kbd>map_err()</kbd>, <kbd>or_else()</kbd>, or even joins between futures. All these methods will run asynchronously one after the other. Once you call the <kbd>block_on()</kbd> function, you will get the <kbd>Result</kbd> of the final future.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Future combinators</h1>
                </header>
            
            <article>
                
<p>And now that we mentioned joins, it is actually possible to have two co-dependent futures. Maybe we have information from two files, we generate one future reading from each file, and then we want to combine the information from them. We don't need to block the thread for that; we can use the <kbd>join()</kbd> method, and the logic behind it will make sure that once the closure we write gets called, both futures will have received the final value.</p>
<p>This is really useful when creating concurrency dependency graphs. If you have many small computations that you want to parallelise, you can create a closure or a function for each of the parts, and then use <kbd>join()</kbd> and other methods, such as <kbd>and_then()</kbd>, to decide which computations need to run some of them in parallel while still receiving all the required data for each step. The <kbd>join()</kbd> method comes in five variants depending on how many futures you need for your next computation.</p>
<p>But simple futures is not the only thing this crate gives us. We can also use the <kbd>Stream</kbd> trait, which works similarly to the <kbd>Iterator</kbd> trait, but asynchronously. This is extremely useful for inputs that come one by one and are not just a one-time value. This happens with TCP, serial, or any connection that uses byte streams, for example.</p>
<p>With this trait, and especially with the <kbd>StreamExt</kbd> trait, we have almost the same API as with iterators, and we can create a complete iterator that can, for example, retrieve HTTP data from a TCP connection byte by byte and asynchronously. This has many applications in web servers, and we have already seen crates in the community migrating to asynchronous APIs.</p>
<p>The crate also offers an asynchronous version of the <kbd>Write</kbd> trait. With the <kbd>Sink</kbd> and the <kbd>SinkExt</kbd> traits you can send data to any output object. This could be a file, a connection, or even some kind of streaming computation. <kbd>Sink</kbd> and <kbd>Stream</kbd> work great together, since the <kbd>send_all()</kbd> method in the <kbd>SinkExt</kbd> trait lets you send a whole <kbd>Stream</kbd> to the <kbd>Sink</kbd>. You could, for example, asynchronously read a file byte by byte, do some computation for each of them or in chunks, and then write the result in another file just by using these combinators.</p>
<p>Let's see an example. We will be using the <kbd>futures-timer</kbd> crate, and unfortunately it's not yet available for futures 0.2.0. So, let's update our <kbd>Cargo.toml</kbd> file with the following <kbd>[dependencies]</kbd> section:</p>
<pre>[dependencies]<br/>futures = "0.1"<br/>futures-timer = "0.1"</pre>
<p>Then, let's write the following code in our <kbd>main.rs</kbd> file:</p>
<pre>extern crate futures;<br/>extern crate futures_timer;<br/><br/>use std::time::Duration;<br/><br/>use futures::prelude::*;<br/>use futures_timer::Interval;<br/>use futures::future::ok;<br/><br/>fn main() {<br/>    Interval::new(Duration::from_secs(1))<br/>        .take(5)<br/>        .for_each(|_| {<br/>            println!("New interval");<br/>            ok(())<br/>        })<br/>        .wait()<br/>        .unwrap();<br/>}</pre>
<p>If you execute cargo run for this example, it will generate five new lines with the <kbd>New interval</kbd> text, one every second. The Interval just returns a <kbd>()</kbd> every time the configured interval times out. We then only take the first five and run the closure inside the <kbd>for_each</kbd> loop. As you can see, the <kbd>Stream</kbd> and <kbd>StreamExt</kbd> traits works almost the same way as the <kbd>Iterator</kbd> trait.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Asynchronous I/O in Rust</h1>
                </header>
            
            <article>
                
<p>When it comes to I/O operations, there is a go-to crate. It's called <kbd>tokio</kbd>, and it handles asynchronous input and output operations seamlessly. This crate is based in MIO. MIO, from Metal IO, is a base crate that provides a really low-level interface to asynchronous programming. It generates an event queue, and you can use a loop to gather all the events one by one, asynchronously.</p>
<p>As we saw earlier, these events can be anything from <em>a TCP message was received</em> to <em>the file you requested is partially ready</em>. There are tutorials to create small TCP servers in MIO, for example, but the idea of MIO is not using the crate directly, but using a facade. The most known and useful facade is the <kbd>tokio</kbd> crate. This crate, by itself, only gives you some small primitives, but it opens the doors to many asynchronous interfaces. You have, for example, <kbd>tokio-serial</kbd>, <kbd>tokio-jsonrpc</kbd>, <kbd>tokio-http2</kbd>, <kbd>tokio-imap</kbd>, and many, many more.</p>
<p>Not only that, you have also utilities such as <kbd>tokio-retry</kbd> that will automatically retry the I/O operation if an error happens. Tokio is really easy to use, it has an extremely low footprint, and it enables you to create incredibly fast services with its asynchronous operations. As you probably have already noticed, it is mostly centred around communication. This is due to all the helpers and capabilities it provides for these cases. The core crate also has file reading capabilities, so you should be covered for any I/O-bound operation, as we will see.</p>
<p>We will see first how to develop a small TCP echo server using Tokio. You can find similar tutorials on the Tokio website (<a href="https://tokio.rs/">https://tokio.rs/</a>), and it is worthwhile to follow all of them. Let's therefore start by adding <kbd>tokio</kbd> as a dependency to the <kbd>Cargo.toml</kbd> file. Then, we will use the <kbd>TcpListener</kbd> from the <kbd>tokio</kbd> crate to create a small server. This structure binds a TCP socket listener to a given address, and it will asynchronously execute a given function for each of the incoming connections. In that function, we will asynchronously read any potential data that we could find in the socket and return it, doing an <kbd>echo</kbd>. Let's see what it looks like:</p>
<pre>extern crate tokio;<br/><br/>use tokio::prelude::*;<br/>use tokio::net::TcpListener;<br/>use tokio::io;<br/><br/>fn main() {<br/>    let address = "127.0.0.1:8000".parse().unwrap();<br/>    let listener = TcpListener::bind(&amp;address).unwrap();<br/><br/>    let server = listener<br/>        .incoming()<br/>        .map_err(|e| eprintln!("Error accepting connection: {:?}", e))<br/>        .for_each(|socket| {<br/>            let (reader, writer) = socket.split();<br/>            let copied = io::copy(reader, writer);<br/><br/>            let handler = copied<br/>                .map(|(count, _reader, _writer)| println!("{} bytes <br/>                  received", count))<br/>                .map_err(|e| eprintln!("Error: {:?}", e));<br/><br/>            tokio::spawn(handler)<br/>        });<br/><br/>    tokio::run(server);<br/>}</pre>
<p>Let's analyze the code. The listener creates an asynchronous stream of incoming connections with the <kbd>incoming()</kbd> method. For each of them, we check whether it was an error and print a message accordingly, and then, for the correct ones, we get the socket and get a writer and a reader by using the <kbd>split()</kbd> method. Then, Tokio gives us a <kbd>Copy</kbd> future that gets created with the <kbd>tokio::io::copy()</kbd> function. This future represents data that gets copied from a reader to a writer asynchronously.</p>
<p>We could have written that future ourselves by using the <kbd>AsyncRead</kbd> and <kbd>AsyncWrite</kbd> traits, but it's great to see that Tokio already has that example future. Since the behavior we want is to return back whatever the connection was sending, this will work perfectly. We then add some extra code that will be executed after the reader returns <strong>End of File</strong> or <strong>EOF</strong> (when the connection gets closed). It will just print the number of bytes that were copied, and it will handle any potential errors that may appear.</p>
<p>Then, in order for the future to perform its task, something needs to execute it. This is where Tokio executors come in—we call <kbd>tokio::spawn()</kbd>, which will execute the future in the default executor. What we just created is a stream of things to do when a connection comes, but we now need to actually run the code. For that, Tokio has the <kbd>tokio::run()</kbd> function, which starts the whole Tokio runtime process and starts accepting connections.</p>
<p>The main future we created, the stream of incoming connections, will be executed at that point and will block the main thread. Since the server is always waiting for new connections, it will just block indefinitely. Still, this does not mean that the execution of the futures is synchronous. The thread will go idle without consuming CPU, and when a connection comes, the thread will be awakened and the future executed. In the future itself, while sending the received data back, it will not block the execution if there is no more data. This enables the running of many connections in only one thread. In a production environment, you will probably want to have similar behavior in multiple threads, so that each thread can handle multiple connections.</p>
<p>It's now time to test it. You can start the server by running <kbd>cargo run</kbd> and you can connect to it with a TCP tool such as Telnet. In the case of Telnet, it buffers the sent data line by line, so you will need to send a whole line to receive the echo back. There is another area where Tokio is especially useful—parsing frames. If you want to create your own communication protocol, for example, you may want to get chunks of those TCP bytes as frames, and then convert them to your type of data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating Tokio codecs</h1>
                </header>
            
            <article>
                
<p>In Tokio, we have the concept of a codec. A codec is a type that divides a slice of bytes into frames. Each frame will contain certain information parsed from the stream of bytes. In our case, we will read the input of the TCP connection and divide it into chunks each time we find the <kbd>a</kbd> letter. A production-ready codec will probably be more complex, but this example will give us a good enough base to implement our own codecs. We will need to implement two traits from the <kbd>tokio-io</kbd> crate, so we will need to add it to the <kbd>[dependencies]</kbd> section of our <kbd>Cargo.toml</kbd> file and import it with <kbd>extern crate tokio_io;</kbd>. We will need to do the same with the <kbd>bytes</kbd> crate. Now, let's start writing the code:</p>
<pre>extern crate bytes;<br/>extern crate tokio;<br/>extern crate tokio_io;<br/><br/>use std::io;<br/><br/>use tokio_io::codec::{Decoder, Encoder};<br/>use bytes::BytesMut;<br/><br/>#[derive(Debug, Default)]<br/>struct ADividerCodec {<br/>    next_index: usize,<br/>}<br/><br/>impl Decoder for ADividerCodec {<br/>    type Item = String;<br/>    type Error = io::Error;<br/><br/>    fn decode(&amp;mut self, buf: &amp;mut BytesMut)<br/>    -&gt; Result&lt;Option&lt;Self::Item&gt;, Self::Error&gt; {<br/>        if let Some(new_offset) = <br/>          buf[self.next_index..].iter().position(|b| *b == b'a') {<br/>            let new_index = new_offset + self.next_index;<br/>            let res = buf.split_to(new_index + 1);<br/>            let res = &amp;res[..res.len() - 1];<br/>            let res: Vec&lt;_&gt; = res.into_iter()<br/>                .cloned()<br/>                .filter(|b| *b != b'\r' &amp;&amp; *b != b'\n')<br/>                .collect();<br/>            let res = String::from_utf8(res).map_err(|_| {<br/>                io::Error::new(<br/>                    io::ErrorKind::InvalidData,<br/>                    "Unable to decode input as UTF8"<br/>                )<br/>            })?;<br/>            self.next_index = 0;<br/>            Ok(Some(res))<br/>        } else {<br/>            self.next_index = buf.len();<br/>            Ok(None)<br/>        }<br/>    }<br/><br/>    fn decode_eof(&amp;mut self, buf: &amp;mut BytesMut)<br/>    -&gt; Result&lt;Option&lt;String&gt;, io::Error&gt; {<br/>        Ok(match self.decode(buf)? {<br/>            Some(frame) =&gt; Some(frame),<br/>            None =&gt; {<br/>                // No terminating 'a' - return remaining data, if any<br/>                if buf.is_empty() {<br/>                    None<br/>                } else {<br/>                    let res = buf.take();<br/>                    let res: Vec&lt;_&gt; = res.into_iter()<br/>                        .filter(|b| *b != b'\r' &amp;&amp; *b != b'\n')<br/>                        .collect();<br/>                    let res = String::from_utf8(res).map_err(|_| {<br/>                        io::Error::new(<br/>                            io::ErrorKind::InvalidData,<br/>                            "Unable to decode input as UTF8"<br/>                        )<br/>                    })?;<br/>                    self.next_index = 0;<br/>                    Some(res)<br/>                }<br/>            }<br/>        })<br/>    }<br/>}</pre>
<p>This is a lot of code; let's analyse it carefully. We created a structure, named <kbd>ADividerCodec</kbd>, and we implemented the <kbd>Decode</kbd> trait for it. This code has two methods. The first and most important one is the <kbd>decode()</kbd> method. It receives a buffer containing data coming from the connection and it needs to return either some data or none. In this case, it will try to find the position of the <kbd>a</kbd> letter, in lower case. If it finds it, it will return all the bytes that were read until then. It also removes new lines, just to make the printing more clear.</p>
<p>It creates a string with those bytes, so it will fail if we send non-UTF-8 bytes. Once we take bytes from the front of the buffer, the next index should point to the first element in the buffer. If there was no <kbd>a</kbd> in the buffer, it will just update the index to the last element that was read, and just return <kbd>None</kbd>, since there isn't a full frame ready. The <kbd>decode_eof()</kbd> method will do a similar thing when the connection gets closed. We use strings as the output of the codec, but you can use any structure or enumeration to represent your data or commands, for example.</p>
<p>We also need to implement the <kbd>Encode</kbd> trait so that we can use the <kbd>framed()</kbd> method from Tokio. This just represents how the data would be encoded in a new byte array if we wanted to use bytes again. We will just get the bytes of the strings and append an <kbd>a</kbd> to it. We will lose new line information, though. Let's see what it looks like:</p>
<pre>impl Encoder for ADividerCodec {<br/>    type Item = String;<br/>    type Error = io::Error;<br/><br/>    fn encode(&amp;mut self, chunk: Self::Item, buf: &amp;mut BytesMut)<br/>    -&gt; Result&lt;(), io::Error&gt; {<br/>        use bytes::BufMut;<br/><br/>        buf.reserve(chunk.len() + 1);<br/>        buf.put(chunk);<br/>        buf.put_u8(b'a');<br/>        Ok(())<br/>    }<br/>}</pre>
<p>To see how it works, let's implement a simple <kbd>main()</kbd> function and use Telnet to send some text with <kbd>a</kbd> letters in it:</p>
<pre>use tokio::prelude::*;<br/>use tokio::net::TcpListener;<br/><br/>fn main() {<br/>    let address = "127.0.0.1:8000".parse().unwrap();<br/>    let listener = TcpListener::bind(&amp;address).unwrap();<br/><br/>    let server = listener<br/>        .incoming()<br/>        .map_err(|e| eprintln!("Error accepting connection: {:?}", e))<br/>        .for_each(|socket| {<br/>            tokio::spawn(<br/>                socket<br/>                    .framed(ADividerCodec::default())<br/>                    .for_each(|chunk| {<br/>                        println!("{}", chunk);<br/>                        Ok(())<br/>                    })<br/>                    .map_err(|e| eprintln!("Error: {:?}", e)),<br/>            )<br/>        });<br/><br/>    println!("Running Tokio server...");<br/>    tokio::run(server);<br/>}</pre>
<p>We could send this text, for example:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/10b83c36-d695-4ec2-885d-b104891e1a15.png" style="width:43.25em;height:7.08em;"/></div>
<p>The output in the server will be similar to this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/63cb8680-f15c-4fe7-a3a1-afee297e94cc.png" style="width:15.67em;height:11.92em;"/></div>
<p>Note that I didn't close the connection, so the last part of the last sentence was still in the buffer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">WebSockets in Rust</h1>
                </header>
            
            <article>
                
<p>If you work in web development, you know that WebSockets are one of the most useful protocols to speed up communication with the client. Using them allows your server to send information to the client without the latter requesting it, therefore avoiding one extra request. Rust has a great crate that allows the implementation of WebSockets, named <kbd>websocket</kbd>.</p>
<p>We will analyze a small, asynchronous WebSocket echo server example to see how it works. We will need to add <kbd>websocket</kbd>, <kbd>futures</kbd>, and <kbd>tokio-core</kbd> to the <kbd>[dependencies]</kbd> section of our <kbd>Cargo.toml</kbd> file. The following example has been retrieved and adapted from the asynchronous server example in the <kbd>websocket</kbd> crate. It uses the Tokio reactor core, which means that it requires a core object and its handle. The WebSocket requires this behavior since it's not a simple I/O operation, which means that it requires some wrappers, such as connection upgrades to WebSockets. Let's see how it works:</p>
<pre>extern crate futures;<br/>extern crate tokio_core;<br/>extern crate websocket;<br/><br/>use websocket::message::OwnedMessage;<br/>use websocket::server::InvalidConnection;<br/>use websocket::async::Server;<br/><br/>use tokio_core::reactor::Core;<br/>use futures::{Future, Sink, Stream};<br/><br/>fn main() {<br/>    let mut core = Core::new().unwrap();<br/>    let handle = core.handle();<br/>    let server = Server::bind("127.0.0.1:2794", &amp;handle).unwrap();<br/><br/>    let task = server<br/>        .incoming()<br/>        .map_err(|InvalidConnection { error, .. }| error)<br/>        .for_each(|(upgrade, addr)| {<br/>            println!("Got a connection from: {}", addr);<br/><br/>            if !upgrade.protocols().iter().any(|s| s == "rust-websocket") {<br/>                handle.spawn(<br/>                    upgrade<br/>                        .reject()<br/>                        .map_err(|e| println!("Error: '{:?}'", e))<br/>                        .map(|_| {}),<br/>                );<br/>                return Ok(());<br/>            }<br/><br/>            let fut = upgrade<br/>                .use_protocol("rust-websocket")<br/>                .accept()<br/>                .and_then(|(client, _)| {<br/>                    let (sink, stream) = client.split();<br/><br/>                    stream<br/>                        .take_while(|m| Ok(!m.is_close()))<br/>                        .filter_map(|m| match m {<br/>                            OwnedMessage::Ping(p) =&gt; {<br/>                                Some(OwnedMessage::Pong(p))<br/>                            }<br/>                            OwnedMessage::Pong(_) =&gt; None,<br/>                            _ =&gt; Some(m),<br/>                        })<br/>                        .forward(sink)<br/>                        .and_then(|(_, sink)| {<br/>                            sink.send(OwnedMessage::Close(None))<br/>                        })<br/>                });<br/><br/>            handle.spawn(<br/>                fut.map_err(|e| {<br/>                    println!("Error: {:?}", e)<br/>                }).map(|_| {}));<br/>            Ok(())<br/>        });<br/><br/>    core.run(task).unwrap();<br/>}</pre>
<p>Most of the code, as you can see, is really similar to the code used in the previous examples. The first change that we see is that for each connection, before actually accepting the connection, it will check if the socket can be upgraded to the <kbd>rust-websocket</kbd> protocol. Then, it will upgrade the connection protocol to that protocol and accept the connection. For each connection, it will receive a handle to the client and some headers. All this is done asynchronously, of course.</p>
<p>We discard the headers, and we divide the client into a sink and a stream. A sink is the asynchronous equivalent to a synchronous writer, in <kbd>futures</kbd> terminology. It starts taking bytes from the stream until it closes, and, for each of them, it replies with the same message. It will then call the <kbd>forward()</kbd> method, which consumes all the messages in the stream, and then it sends a connection closed message. The future we just created is then spawned using the handle we took from the core. This means that, for each connection, this whole future will be run. The Tokio core then runs the whole server task.</p>
<p>If you get the example client implementation from the crate's Git repository (<a href="https://github.com/cyderize/rust-websocket/blob/master/examples/async-client.rs">https://github.com/cyderize/rust-websocket/blob/master/examples/async-client.rs</a>), you will be able to see how the server replies to whatever the client sends. Once you understand this code, you will be able to create any WebSocket server you need.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the new Generators</h1>
                </header>
            
            <article>
                
<p>A new feature is coming to Rust in 2018—asynchronous generators. Generators are functions that can yield elements before returning from the function and resume executing later. This is great for the loops that we have seen in this chapter. With generators, we could directly replace many of the callbacks with the new <kbd>async</kbd>/<kbd>await</kbd> syntax.</p>
<p>This is still an unstable feature that can only be used in nightly, so it may be that the code you write becomes obsolete before stabilization. Let's see a simple example of a generator:</p>
<pre>#![feature(generators, generator_trait)]<br/><br/>use std::ops::{Generator, GeneratorState};<br/><br/>fn main() {<br/>    let mut generator = || {<br/>        for i in 0..10 {<br/>            yield i;<br/>        }<br/>        return "Finished!";<br/>    };<br/><br/>    loop {<br/>        match generator.resume() {<br/>            GeneratorState::Yielded(num) =&gt; println!("Yielded {}", num),<br/>            GeneratorState::Complete(text) =&gt; {<br/>                println!("{}", text);<br/>                break;<br/>            }<br/>        }<br/>    }<br/>}</pre>
<p>You will need to execute <kbd>rustup override add nightly</kbd> to run the example. If you run it, you will see this output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8beec889-41e0-4c17-beb8-c0946385aa98.png" style="width:6.33em;height:13.67em;"/></div>
<p>The interesting thing here is that the generator function can perform any computation, and you can resume the computation once a partial result gets yielded, without needing buffers. You can test this by doing the following—instead of yielding something from the generator, just use it to print in the console. Let's see an example:</p>
<pre>#![feature(generators, generator_trait)]<br/><br/>use std::ops::Generator;<br/><br/>fn main() {<br/>    let mut generator = || {<br/>        println!("Before yield");<br/>        yield;<br/>        println!("After yield");<br/>    };<br/><br/>    println!("Starting generator...");<br/>    generator.resume();<br/>    println!("Generator started");<br/>    generator.resume();<br/>    println!("Generator finished");<br/>}</pre>
<p>If you run this example, you will see the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/842e17d8-1283-4ec7-b691-b31ef64df46b.png" style="width:13.58em;height:6.50em;"/></div>
<p>As you can see, the function pauses its execution when it gets to a <kbd>yield</kbd> statement. If there is any data in that yield statement, the caller will be able to retrieve it. Once the generator is resumed, the rest of the function gets executed, until a <kbd>yield</kbd> or a <kbd>return</kbd> statement.</p>
<p>This, of course, is of great advantage for the <kbd>futures</kbd> we saw earlier. This is why the <kbd>futures-await</kbd> crate was created. This crate uses generators to make the implementation of asynchronous futures much easier. Let's rewrite the TCP echo server we created before using this crate. We will need to add the <kbd>0.2.0</kbd> version of the <kbd>futures-await</kbd> to the <kbd>[dependencies]</kbd> section of our <kbd>Cargo.toml</kbd> file and then start using a bunch of nightly features. Let's see some example code:</p>
<pre>#![feature(proc_macro, conservative_impl_trait, generators)]<br/><br/>extern crate futures_await as futures;<br/><br/>use futures::prelude::*;<br/>use futures::executor::block_on;<br/><br/>#[async]<br/>fn retrieve_data_1() -&gt; Result&lt;i32, i32&gt; {<br/>    Ok(1)<br/>}<br/><br/>#[async]<br/>fn retrieve_data_2() -&gt; Result&lt;i32, i32&gt; {<br/>    Ok(2)<br/>}<br/><br/>#[async_move]<br/>fn add_data() -&gt; Result&lt;i32, i32&gt; {<br/>    Ok(await!(retrieve_data_1())? + await!(retrieve_data_2())?)<br/>}<br/><br/>fn main() {<br/>    println!("Result: {:?}", block_on(add_data()));<br/>}</pre>
<p>This example will have two asynchronous functions that could, for example, be retrieving information from the network. They get called by the <kbd>add_data()</kbd> function, which will wait for them to return before adding them up and returning a result. If you run it, you will see that the result is <kbd>Ok(3)</kbd>. The line importing the <kbd>futures_await</kbd> crate as <kbd>futures</kbd> makes sense because the <kbd>futures-await</kbd> crate is just a small wrapper around the futures crate, and all the usual structures, functions, and traits are available.</p>
<p>The whole generators and <kbd>async</kbd>/<kbd>await</kbd> syntax is still being heavily worked on, but the Rust 2018 roadmap says it should be stabilized before the end of the year.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this last chapter of the book, you learned to use asynchronous programming to avoid creating too many threads. You can now use just the right amount of threads and still run the workload in parallel and efficiently in networking applications. To be able to do that, you first learned about the futures crate, which give us the minimum primitives to use when working with asynchronous programming in Rust. You then learned how the MIO-based Tokio works, and created your first servers.</p>
<p>Before understanding external crates, you learned about WebSockets and grasped the Tokio core reactor syntax. Finally, you learned about the new generators syntax and how the <kbd>futures</kbd> crate is being adapted to make use of this new syntax. Make sure to stay up to date about the news on when this great compiler feature will be stabilized.</p>
<p>Now that the book came to an end, we can see that high performance can be achieved in Rust in multiple and complimentary ways. We can first start by improving our sequential code as we saw in the first chapters. These improvements come from various techniques, starting from a proper compiler configuration and ending in small tips and tricks with the code. As we saw, some tools will help us in this labour.</p>
<p>We can then use metaprogramming to improve both the maintainability of the code and the performance, by reducing the amount of work the software has to do at runtime. We saw that new ways of metaprogramming are arriving this year to Rust.</p>
<p>Finally, the last step to make things faster is to run tasks concurrently, as we saw in the last two chapters. Depending on the requirements of our project, we will use multithreading or/and asynchronous programming.</p>
<p>You should now be able to improve the performance of your Rust applications and even to start learning deeper concepts of high performance programming. It has been a pleasure to guide you through these topics in the Rust programming language, and I hope you enjoyed the read.</p>


            </article>

            
        </section>
    </body></html>
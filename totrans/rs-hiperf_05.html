<html><head></head><body>
        

                            
                    <h1 class="header-title">Profiling Your Rust Application</h1>
                
            
            
                
<p class="mce-root">One of the elementary steps to understand why your application is going slower than expected is to check what your application is doing at a low level. In this chapter, you will understand the importance of low-level optimization, and learn about some tools that will help you find where your bottlenecks are.</p>
<p>In this chapter, you will learn about the following topics:</p>
<ul>
<li>How the processor works at low level</li>
<li>CPU caches</li>
<li>Branch prediction</li>
<li>How to fix some of the most common bottlenecks</li>
<li>How to use Callgrind to find your most-used code</li>
<li>How to use Cachegrind to see where your code might be performing poorly in the cache</li>
<li>Learn how to use OProfile to know where your program spends most of its execution time</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding the hardware</h1>
                
            
            
                
<p>To understand what our software is doing, we should first understand how the compiled code is running in our system. We will, therefore, start with how the <strong>Central Processing Unit</strong> (<strong>CPU</strong>) works.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding how the CPU works</h1>
                
            
            
                
<p>The CPU is in charge of running the central logic of your application. Even if your application is a graphics application running most of its workload in the GPU, the CPU is still going to be governing all that process. There are many different CPUs, some faster than others for certain things, others that are more efficient and consume less power, sacrificing their computing power. In any case, Rust can compile for most CPUs, since it knows how they work.</p>
<p>But our job here is to figure out how they work by ourselves, since sometimes the compiler won't be as efficient at improving our machine code as we are. So, let's get to the center of the processing, where things get done.</p>
<p>The processor has a set of instructions it knows how to execute. We can ask it to perform any kind of instruction in that set, but we have a limitation: in most cases, it can only work with what's called a <strong>register</strong>. A register is a small location near the <strong>arithmetic and logical unit</strong> (<strong>ALU</strong>) inside the CPU. It can contain one variable, as big as the processor word size; nowadays, that is 64 bits most of the time, but it can be 32, 16, or even 8 in some embedded processors.</p>
<p>Those registers go as fast as the processor itself, so information can be modified in them without having to wait for anything (well, just for the actual instructions to execute). This is great; in fact, you might be wondering, why do we even need RAM if we have registers?</p>
<p>Well, the answer is simple: price. Having more registers in a CPU is very costly. You cannot have much more than a couple of dozen registers (in the best case scenario), since the processor wiring would get very complicated. Taking into account that for each instruction and register you will have dedicated data lines and control lines, this is very costly.</p>
<p>So, an external memory is required, one that can be freely accessed without requiring you to read all the memory sequentially, and still being very fast. <strong>Random Access Memory</strong> (<strong>RAM</strong>) is there for you. Your program will be loaded from RAM and will use RAM to store data that will need to be manipulated during the software execution. Of course, that software in RAM has to be loaded from the hard or solid state disk to RAM before being usable.</p>
<p>The main issue with RAM is that even if it's much, much faster than even the fastest SSD out there, it's still not as fast as the processor. The processor can execute tens of instructions while waiting for the RAM to get some data into one of the processor's registers to be able to operate with them. So, to avoid having the processor waiting every time it needs to load or store something in RAM, we have caches.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Speeding up memory access with the cache</h1>
                
            
            
                
<p>The first level of cache, also known as the L1 cache, is a cache located almost as close as the registers to the ALU. The L1 cache is almost as fast as a register (about three times slower), and it has a very interesting property. Its internal structure can be represented as a lookup table. It will have two columns: the first will contain a memory address in the RAM, while the second will contain the contents of that memory address. When we need to load something from the RAM to a register, if it's already in the L1 cache, it will be almost immediate.</p>
<p>And not only that, this cache is usually divided in two very differentiated areas: the data cache and the instructions cache. The first will contain information about variables that the program is using, while the second will contain the instructions the program will execute. This way, since we know how the program is being executed, we can preload the next instructions in the instructions cache and execute them without having to wait for the RAM. In the same way, since we know what variables will be required in the execution of the program, we can preload them in the cache.</p>
<p>But this cache has some problems too. Even if it's three times slower than the processor, it is still too expensive. Also, there is not much physical space so close to the processor, so its size is limited usually to about 32 KiB. Since most software requires more than that size, and we want it to execute fast without having to wait for the RAM, we usually have a second level of cache, called the L2 cache, that also runs at the processor's clock speed, but being farther away from the L1 cache makes its signal arrives with a higher latency.</p>
<p>The L2 cache is thus almost as fast as the L1 cache and usually has up to 4 MiB of space. Instructions and data are combined. This is still not enough for many operations your software might do. Remember you will require all the data you are using, and in image processing, that might be millions of pixels with their value. So, for that, some high-end processors have an L3 cache in this case, farther away and with a slower clock, but still much faster than the RAM. Sometimes, this cache can be up to 32 MiB at the time of writing.</p>
<p>But still, we know that even in processors such as the Ryzen processors, with more than 40 MiB of combined caches, we will need more space. We have the RAM, of course, since in the end, the cache is just a copy of the RAM we have close to the processor for faster use. For every new piece of information, we will need to load it from RAM to the L3 cache, then the L2, then the L1, and finally a register, making the process slower.</p>
<p>For this, processors have highly complex algorithms programmed in pure silicon, in hardware, that are able to predict what memory locations are going to be accessed next, and preload the locations in bursts. This means that if the processor knows you will access variables stored from address 1,000 to 2,000 in the RAM, it will request the RAM to load the whole batch of the memory in the L3 cache, and when the time to use them approaches, the L2 cache will copy the data from the L3, and so will the L1 from the L2. When your program asks for that memory location's value, it will magically be in the L1 cache already, and be extremely fast to retrieve.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Cache misses</h1>
                
            
            
                
<p>But how well does this work? Is it possible to have 100% efficiency? Well, no. Sometimes, your software will do something the processor wasn't built to predict, and the data will not be in the L1 cache. This means that the processor will ask the L1 cache for the data, and this L1 cache will see that it doesn't have it, losing time. It will then ask the L2, the L2 will ask the L3, and so on. If you are lucky, it will be on the second level of cache, but it might not even be on the L3 and thus your program will need to wait for the RAM, after waiting for the three caches.</p>
<p>This is what is called a cache miss. How often does this happen? Depending on how optimized your code and the CPU are, it might be between 2% and 5% of the time. It seems low, but when it happens, the whole processor stops to wait, which means that even if it doesn't happen many times, it has a huge performance impact, making things much slower. And as we saw, it's not only the loss of time of having to wait for the slower storage, it's also the lookup time in the previous storage that is lost, so in cache misses, it would have been faster to just ask the RAM directly (if the value wasn't in any cache).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How can you fix it?</h1>
                
            
            
                
<p>It is not easy to fix this situation. Cache misses happen sometimes because you use big arrays only once, for example, in scenarios such as video buffers. If you are streaming some data, it might happen that the data is not yet in the cache and that it will no longer be used after using it once. This creates two problems. First, the time you need it, it's still in some area of the RAM, creating a cache miss. And second, once you load it into the cache, it will occupy most of the cache, forgetting about other variables you might require and creating more cache misses. This last effect is called <strong>cache pollution</strong>.</p>
<p>The best way to avoid this kind of behavior is to use smaller buffers, but this creates other problems, such as the data requiring constant buffering, so you will need to see what is best for your particular situation. If it's not caused by buffers, it might be that you are creating too many variables and only use them once. Try to find out whether you can reuse information, or whether you can change some executions for loops. But be careful, since some loops can affect branch prediction, as we will see later.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Cache invalidation</h1>
                
            
            
                
<p>There is another big issue with caches, called <strong>cache invalidation</strong>. Since, usually, in new processors, you use multithreaded applications, it sometimes happens that one thread changes some information in memory and that the other threads need to check it. As you might know, or as you will see in <a href="03028198-9025-4bfd-8677-215147e9400d.xhtml" target="_blank">Chapter 10</a>, <em>Multithreading</em>, Rust makes this perfectly safe at compile time. Data races won't happen, but it won't prevent cache invalidation performance issues.</p>
<p>A cache invalidation happens in the case that some information in the RAM gets changed by another CPU or thread. This means that if that memory location was cached in any L1 to L3 caches, somehow it will need to be removed from there, since it will have old values. This is usually done by the storage mechanism. Whenever a memory address gets changed, any cache pointing to that memory address gets invalidated. This way, the next instruction trying to read the data from that address will create a cache miss, thus making the cache refresh and get data from the RAM.</p>
<p>This is pretty inefficient, in any case, since every time you change a shared variable, that variable will require a cache refresh in the rest of the threads it gets used. In Rust, for that, you will be using an <kbd>Arc</kbd>. To try to avoid this kind of performance pitfall, you should try to share as little as possible between threads, and if messages have to be delivered to them, it might sometimes make sense to use structures in the <kbd>std::sync::mpsc</kbd> module, as we will see in <a href="03028198-9025-4bfd-8677-215147e9400d.xhtml" target="_blank">Chapter 10</a>, <em>Multithreading</em>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">CPU pipeline</h1>
                
            
            
                
<p>Instruction sets get more and more complex, and processors have faster and faster clock speeds, which sometimes makes most CPU instructions require more than one clock tick to execute. This is usually because the CPU needs to first understand what instruction is being executed, understand its operands, produce the meaningful signals to get those operands, perform the operations, and then save those operations. And no more than one step can be done per clock tick.</p>
<p>This is usually solved in processors by creating a CPU pipeline. This means that when a new instruction comes in, while that instruction gets analyzed and executed, the next instruction comes to the CPU to get analyzed. This has some complications, as you might imagine.</p>
<p>First, if an instruction requires the output of a previous instruction, it might need to sometimes wait for the result of another instruction. It might also sometimes happen that the instruction being executed is a jump to another place in the memory so that new instructions need to be fetched from the RAM and the pipeline needs to be removed.</p>
<p>Overall, what this technique achieves is to be able to execute one instruction per clock cycle in ideal conditions (once the pipeline is full). Most new processors do this, since it enables much faster execution without requiring a clock speed improvement, as we can see in this diagram:</p>
<div><img src="img/a2973ad4-d32a-45a5-b3e0-9fe358416125.png" style="width:24.58em;height:19.58em;"/></div>
<p>Another extra benefit from this approach is that dividing the instruction processing makes each step easier to implement, and not only that. Since each section will be physically smaller, electrons at light speed will be able to synchronize the whole step circuit in less time, making it possible for the clock to run faster.</p>
<p>In any case, though, dividing each instruction execution into more steps increases the complexity of the CPU wiring, since it has to fix potential concurrency issues. In the event that an instruction requires the output of the previous one to work, four different things can happen. As a first, and bad, option, it could happen that the behavior gets undefined. This is not what we want, and fixing this complicates the wiring of the processor.</p>
<p>The most important wiring piece to fix this is to first detect it. This on its own will make the wiring more complex. Once the CPU can detect the potential issue, the easiest fix is to simply wait for the output without advancing the pipeline. This is called <strong>stalling</strong>, and will hurt the performance of the CPU, but it will work properly.</p>
<p>Some processors will handle this by adding some extra input paths that will contain previous results, in case they need to be used, but this will greatly increase the complexity of the pipeline. Another option would be to detect some safety instructions and make them run before the instruction that requires the output of the previous one. This last option is called <strong>out of order execution</strong> and will also increase the complexity of the CPU.</p>
<p>So, in conclusion, to improve the speed of a CPU, apart from making its clock run faster, we have the option to create a pipeline of instructions. This will make it possible to run one instruction per clock tick (ideally) and sometimes even increase the clock speed. It will increase the complexity of the CPU, though, making it much more expensive.</p>
<p>And what are the pipelines of current processors like, you might ask? Well, they come in different lengths and behaviors, but in the case of some high-end Intel chips, pipelines can be larger than 30 steps. This will make them run really fast, but greatly increase their complexity and price.</p>
<p>When you develop applications, a way to avoid slowing down the pipeline will be to try to perform operations that do not require previous results first, and then use the generated results, even though this, in practice, is very difficult to do, and some compilers will actually do it for you.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Branch prediction</h1>
                
            
            
                
<p>There is one situation that we didn't see how to solve, though. When our processor receives a conditional jump instruction, depending on the current state of the processor flags, the next instruction to execute will be one or another. This means that we cannot anticipate some instructions and load them into the pipeline. Or can we?</p>
<p>There are multiple ways of somehow predicting what code will be run next without doing the computation of the last instructions. The simplest way, and one used in old processors, was to statically decide which branches will load the next instructions and which will load instructions at the jump address.</p>
<p>Some processors would do that by deciding that some type of instructions were more likely to jump than others. Other processors would look into the jump address. If the address was lower, they would load the instructions at the target address into the pipeline, if not, they would load the ones at the next address. For example, in loops, it's much more likely to loop back to the beginning of the loop more times than continuing the flow of the program.</p>
<p>In both preceding cases, the decision was made statically, when developing the processor, and the actual program execution wouldn't change the way pipeline loading would work. A much-improved approach, a dynamic one, was to count how many times the processor would jump for a given conditional jump.</p>
<p>The first time the processor gets to the branch, it won't know whether the code will jump or not, so it will probably load the next instruction to the pipeline. If it jumps, the next instruction will be canceled and new instructions will be loaded in the pipeline. This will make the processor wait for as many cycles as the pipeline has stages.</p>
<p>In this old method, we would have put the counter of jumps for those instructions to <kbd>1</kbd>, and the counter of no jumps to <kbd>0</kbd>. The next time the program gets to the same jump, seeing the counter, the processor will start loading the instructions that come from the jump into the pipeline, instead of the next instructions.</p>
<p>Once the calculation has been done, if the processor actually needs to jump, it already has the instructions in the pipeline. If not, it will need to load the next instructions into it. In both cases, the respective counter would go up by 1.</p>
<p>This means that, for example, in a long <kbd>for</kbd> loop, the jump counter will increase to a high number, since it will, most of the time, have to jump back to the beginning of the loop, and only after the last iteration will it continue the flow of the application. This means that for all except the first and the last iteration, there will be no empty pipeline and the branch will be predicted properly.</p>
<p>These counters are actually a bit more complex since they would saturate at 1 or 2 bits, meaning that the counter could indicate whether the last time the branch was taken or not, or how sure the processor was that the next time the branch would be taken. The counter could be <kbd>0</kbd>, if it usually never takes the branch, 1, if it might take it, 2, if it many times takes the branch or 3 if it takes it almost always. This means that a branch that gets taken only some of the time will have a better prediction. Some benchmarks have shown that the accuracy can be as high as 93.5%.</p>
<p>It's amazing how a simple counter will make branch prediction much more efficient, right? Well, of course, this has big limitations. In code, that branching depends on some condition, but where patterns can be seen (an <kbd>if</kbd> condition that returns true on almost every second call, for example), counters will fail enormously, since they will have no clue of the pattern.</p>
<p>For this kind of behavior, complex adaptive prediction tables get used. It will store the last <em>n</em> occurrences of the <kbd>jump</kbd> instruction in a table, and see whether there is a pattern. If there is, it will group the outcomes in groups of the number of elements in the pattern, and better predict this kind of behavior. This increases the accuracy up to 97% in some cases.</p>
<p>There are many different branch prediction techniques, and depending on the pipeline size, it will make more sense to use more complex predictors or simpler ones. If the processor has a 30-stage pipeline, failing to predict a branch will end up in a 30-cycle delay for the next instruction. If it has 2 stages, it will only lose 2 cycles. This means that more complex and expensive pipelines will also require more complex and expensive branch predictors.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The relevance of branch prediction for our code</h1>
                
            
            
                
<p>This book is not about creating processors, so you might think that all this branch prediction theory does not make sense for our use case of improving the efficiency of our Rust code. But the reality is that this theory can make us develop more efficient applications.</p>
<p>First, knowing that patterns in conditional execution will probably be detected after two passes by new and expensive processors will make us try to use those patterns if we know that our code will be mainly used by newer processors. On the other hand, if we know that our code will run in a cheaper or older processor, we might optimize its execution by maybe writing the result of the pattern condition sequentially if possible (in a loop) or by trying to group conditions in other ways.</p>
<p>Also, we have to take into account compiler optimizations. Rust will often optimize loops to the point of copying some code 12 times if it knows it will always be executed that number of times, to avoid branching. It will also lose some optimization prediction if we have many branches in the same code generation unit (a function, for example).</p>
<p>This is where Clippy lints such as <strong>cyclomatic complexity</strong> enter into play. They will show as functions where we are adding too many branches. This can be fixed by dividing such functions into smaller ones. The Rust compiler will better optimize the given function, and if we have link-time optimizations enabled, it might even end up in the same function, in the end, making the processor branchless.</p>
<p>We shouldn't completely rely on hardware branch prediction, especially if our code is performance-critical, and we should develop taking into account how the processor will optimize it too. If we know for sure which processor will be running our code, we might even decide to learn the branch prediction techniques of the processor from the developer manual and write our code accordingly.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Profiling tools</h1>
                
            
            
                
<p>You might be wondering how we will detect bottlenecks such as these ones in our application. We all know that not all developers will take such low-level details into account, and even if they do, they might forget to do it in some critical code that the program needs to run many times in a row. We cannot check the whole code base manually but, fortunately, there are some profiling tools that will give us information about our software.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Valgrind</h1>
                
            
            
                
<p>Let's first start with a tool that will help you find where your software spends more time. <strong>Valgrind</strong> is a tool that helps to find bottlenecks. Two main tools inside Valgrind will give us the statistics we need to find out where to improve our code. It's included in most Linux distributions. There are Windows alternatives, but if you have access to a Linux machine (even if it's a virtual one), Valgrind will really make the difference when getting results.</p>
<p>The easiest way to use it is to use <kbd>cargo-profiler</kbd>. This tool is in <kbd>crates.io</kbd>, but it's no longer updated, and the version in GitHub has some much-needed fixes. You can install it by running the following command:</p>
<pre><strong>cargo install --git https://github.com/kernelmachine/cargo-profiler.git</strong></pre>
<p>Once installed, you can use it by running <kbd>cargo profiler callgrind</kbd> or <kbd>cargo profiler cachegrind</kbd>, depending on the Valgrind tool you want to use. Nevertheless, <kbd>cargo-profiler</kbd> does not compile with source annotations by default, so it might make sense to use the <kbd>cargo rustc</kbd> command to compile with a <kbd>-g</kbd> flag, and then run Valgrind directly in those binaries:</p>
<pre><strong>cargo rustc --release --bin {binary_name} -- -g</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Callgrind</h1>
                
            
            
                
<p>Callgrind will show statistics about the most-used functions in your program. To run it, you will need to run <kbd>cargo profiler callgrind {args}</kbd>, where <kbd>args</kbd> are the arguments to your executable. There is an interesting issue here though. Rust uses <kbd>jemalloc</kbd> as the default allocator, but Valgrind will try to use its own allocator to detect calls. There is a way to use Valgrind's allocator, but it will only work in nightly Rust.</p>
<p>You will need to add the following lines to your <kbd>main.rs</kbd> file:</p>
<pre>#![feature(alloc_system)]<br/>extern crate alloc_system;</pre>
<p>This will force Rust to use the system allocator. You might need to add <kbd>#![allow(unused_extern_crates)]</kbd> to the file so that it doesn't alert you for an unused crate. An interesting flag for Valgrind is <kbd>-n {num}</kbd>.  This will limit the results to the <kbd>num</kbd> most relevant ones. In the case of Callgrind, it will only show the most-used functions. An optional <kbd>--release</kbd> flag will tell <kbd>cargo profiler</kbd> if you want to profile the application in release mode instead of doing it in debug mode.</p>
<p>Let's see an output of the Callgrind tool:</p>
<div><img src="img/73d0a237-8dfa-4030-aa2d-d9261b106d2d.png" style="width:37.92em;height:25.25em;"/></div>
<p>Let's analyze what this means. I only selected the top functions, but we can already see lots of information. The most-used function is a system call for a memory copy. This means that this program is copying lots of data in memory between one position and another. Fortunately, at least it uses an efficient system call, but maybe we should check whether we need so much copying for its job.</p>
<p>The second most-used function is something called <kbd>Executor</kbd> in the <kbd>abxml</kbd> module/crate. You might think the <kbd>regex</kbd> reference is more used because it's in the second position, but the <kbd>Executor</kbd> seems to be divided since it seems that some of the references lost their initial <kbd>lib.rs</kbd> (third and fifth elements seem the same). It seems that we use that function a lot, or that at least it's taking most of our CPU time.</p>
<p>We should ask ourselves if this is normal. Should it spend so much time on that function? In the case of this program, the SUPER Android Analyzer (<a href="http://superanalyzer.rocks/">http://superanalyzer.rocks/</a>), it uses that function to get the resources of the application. It makes sense that most of the time it would be actually analyzing the application instead of decompressing it (and in fact, we are not being shown the use of a Java dependency that takes 80% of the time). But it seems that the decompression of the resources of the <kbd>apk</kbd> file takes a lot of time.</p>
<p>We could check if there is something that could be optimized in that function or in descendant functions. It makes sense that if we could optimize 10% of that function, we would gain a lot of speed in the application.</p>
<p>Another option would be to check our regular expression usage, as we can see that many instructions are used to check regular expressions and compile them. An improvement in the regular expression engine would also make a difference.</p>
<p>We finally see that the SHA-1 and SHA-256 algorithm execution take a lot of time too. Do we need them? Could they be optimized? Maybe by using the native algorithm implementations often found in newer processors, we could speed up the execution. It might make sense to create a pull request in the upstream crate.</p>
<p>As you can see, Callgrind gives us tons of valuable information about the execution of our program, and we can at least see where it makes sense to spend time trying to optimize the code. In this particular case, hashing algorithms, regular expressions, and resource decompression take most of the time; we should try to optimize those functions. On the other hand, for example, one of the lesser-used functions is the XML emitter. So even if we find out how to optimize that function by 90%, it won't really make a difference. If it's an easy optimization, we can do it (better something than nothing), but if it will take us a long time to implement, it probably makes no sense to do it.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Cachegrind</h1>
                
            
            
                
<p>We have talked at length about caches in this chapter, but is there a way to see how our application is performing in this sense? There actually is. It's called <strong>Cachegrind</strong>, and it's part of Valgrind. It's used in the same way as Callgrind, with <kbd>cargo profiler</kbd>. In the case of the same preceding application, the <kbd>cargo profiler</kbd> failed to parse Cachegrind's response, so I had to run Valgrind directly, as you can see in the following screenshot:</p>
<div><img src="img/368b2b62-6548-4978-bd60-c97ac8f8d4ac.png"/></div>
<p>This was my second or third run, so some information might have already been cached. But still, as you can see, 2.1% of the first-level data cache missed, which is not so bad, given that the second-level cache had that data most of the time (it only missed 0.1% of the time).</p>
<p>Instruction data was fetched properly at the level 1 cache almost all the time, except for 0.04% of the time. There was almost no miss at level 2. Cachegrind can also give us more valuable information with some flags though.</p>
<p>Using <kbd>--branch-sim=yes</kbd> as an argument, we can see how the branch prediction worked:</p>
<div><img src="img/01bda42e-d40f-4878-85df-42c635d27442.png" style="width:41.75em;height:3.67em;"/></div>
<p>As we can see, 3.3% of the branches were not predicted properly. This means that an interesting improvement could be done if branches were more predictable, or if some loops were unrolled, as we saw previously.</p>
<p>This, by itself, tells us nothing about where we could improve our code. But using the tool creates a <kbd>cachegrind.out</kbd> file in the current directory. This file can be used by another tool, <kbd>cg_anotate</kbd>, that will show improved stats. Running it, you will see the various stats on a per-function basis, where you can see which functions are giving the cache more trouble, and go there to try to fix them.</p>
<p>In the case of SUPER, it seems that the resource decompress is giving more cache misses. It might make sense, though, since it's reading new data from a file and reading that data almost all the time from memory for the first time. But maybe we can check those functions and try to improve the fetching, by using buffers, for example, if they are not being used.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">OProfile</h1>
                
            
            
                
<p>OProfile is another great tool that can give us interesting information about our program. It's also only available for Linux, but you will find a similar tool for Windows too. Once again, if you can get a Linux partition to check this, your results will probably be more in line with what you will read next. To install it, install the <kbd>oprofile</kbd> package of your distribution. You might also need to install the generic Linux tools (<kbd>linux-tools-generic</kbd> in Ubuntu).</p>
<p>OProfile is not of much help without source annotations, so you should first compile your binary with them by using the following command:</p>
<pre><strong>cargo rustc --release --bin super-analyzer -- -g</strong></pre>
<p>You will need to be root to do the profiling, as it will directly get the kernel counters for it. Don't worry; once you profile the application, you can stop being root. To profile the application, simply run <kbd>operf</kbd> with the binary and the arguments to profile:</p>
<div><img src="img/2db4a3e0-5be1-4dab-a13e-e91811cb6588.png"/></div>
<p>This will create an <kbd>oprofile_data</kbd> directory in the current path. To make some sense from it, you can use the <kbd>opannotate</kbd> command. This will show a bunch of stats, with some of the source code present, with how much time the CPU spends in each place. In the case of our Android analyzer, we can see that the rule processing takes quite a lot of time:</p>
<div><img src="img/78b6a992-02a9-4c3d-a429-4554900a0f0b.png"/></div>
<p>In this case, this is probably reasonable. It makes sense that a piece of software that is supposed to analyze a file with rules would spend lots of time analyzing that file with those rules. But, nevertheless, it also means that in that code, we could find some optimization that could make a difference.</p>
<p>With OProfile, we could find some areas where the program is spending more time than it should. Maybe we would find a bottleneck in an unexpected area. That is why it's important to use these kinds of tools.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, you learned how the processor really works. You understood the multiple hacks we have in the hardware so that everything runs much, much faster than it would if the CPU was always waiting for the RAM. You also got a grasp of the most common performance issues and some information on how to fix them.</p>
<p>Finally, you learned about the Callgrind, Cachegrind, and OProfile tools, which will help you find those bottlenecks so that you can fix them easily. They will even show where in your source code you can find the slowdowns.</p>
<p>In <a href="4a4cf14d-1c2e-4a33-9856-6ef520591a44.xhtml" target="_blank">Chapter 6</a>, <em>Benchmarking</em>, you will learn how to benchmark your application. It is especially interesting to compare it to other applications or to a previous version of your own application. You will learn how to spot changes that make your application slower.</p>
<p class="mce-root"/>


            

            
        
    </body></html>
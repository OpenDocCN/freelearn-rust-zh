<html><head></head><body>
		<div id="_idContainer156">
			<h1 id="_idParaDest-221" class="chapter-number"><a id="_idTextAnchor222"/>11</h1>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor223"/>Configuring HTTPS with NGINX on AWS</h1>
			<p>When it comes to deploying our applications, a lot of tutorials and books go through simple deployments and smooth over the concept of encrypting traffic to and from the server using HTTPS. However, HTTPS is essential and usually the biggest hurdle that a developer must overcome to get their website or API out into the world. While this book’s title is <em class="italic">Rust Web Programming</em>, it is essential to dedicate a chapter to truly understanding how HTTPS works so that you can implement HTTPS locally and then on the <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) cloud. This chapter will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>What <span class="No-Break">is HTTPS?</span></li>
				<li>Implementing HTTPS locally <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">docker-compose</strong></span></li>
				<li>Attaching a URL to our deployed application <span class="No-Break">on AWS</span></li>
				<li>Enforcing HTTPS on our application <span class="No-Break">on AWS</span></li>
			</ul>
			<p>By the end of this chapter, you will be able to build infrastructure in Terraform code that can encrypt traffic and lock down unwanted traffic to our <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) instances so that they can only<a id="_idIndexMarker1035"/> accept traffic from the load balancer, which will be explained later in the chapter. What is best is that most of this is automated, and seeing as we are using Docker to deploy our applications, you will be able to transfer this skill to any web project that you want to deploy in the future. While this is not the best implementation as there are entire books dedicated to cloud computing, you will be able to implement a solid, safe deployment that will continue to serve users even if an EC2 instance is out of service as the load balancer can route to other instances. It will also be able to scale if traffic <span class="No-Break">demand increases.</span></p>
			<h1 id="_idParaDest-223"><a id="_idTextAnchor224"/>Technical requirements</h1>
			<p>In this chapter, we’ll build on the code built in <a href="B18722_10.xhtml#_idTextAnchor200"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Deploying Our Application on AWS</em>. This can be found at the following <span class="No-Break">URL:</span><a href=" https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter10/deploying_our_application_on_aws"><span class="No-Break"> </span></a><a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter10/deploying_our_application_on_aws"><span class="No-Break">https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter10/deploying_our_application_on_aws</span></a><span class="No-Break">.</span></p>
			<p>The code for this chapter can be found <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter11"><span class="No-Break">https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter11</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor225"/>What is HTTPS?</h1>
			<p>So far, our frontend and backend applications<a id="_idIndexMarker1036"/> have been running through HTTP. However, this is not secure and has some drawbacks. To secure the traffic between our browser and our NGINX server, we are going to have to ensure that our application is using the HTTP/2 protocol. The HTTP/2 protocol has the following differences from the standard <span class="No-Break">HTTP/1 protocol:</span></p>
			<ul>
				<li><span class="No-Break">Binary protocol</span></li>
				<li><span class="No-Break">Compressed headers</span></li>
				<li><span class="No-Break">Persistent connections</span></li>
				<li><span class="No-Break">Multiplex streaming</span></li>
			</ul>
			<p>We can go through the preceding laid-out points and discuss <span class="No-Break">the differences.</span></p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor226"/>Binary protocol</h2>
			<p>HTTP uses a text-based<a id="_idIndexMarker1037"/> protocol whereas HTTP/2 uses<a id="_idIndexMarker1038"/> a binary protocol. A binary protocol uses bytes to transmit data as opposed to human-readable<a id="_idIndexMarker1039"/> characters, which are encoded using the <strong class="bold">American Standard Code for Information Interchange</strong> (<strong class="bold">ASCII</strong>). Using bytes reduces the number of possible errors and the size needed to transfer data. It will also enable us to encrypt our data stream, which is the basis <span class="No-Break">of HTTPS.</span></p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor227"/>Compressed headers</h2>
			<p>HTTP/2 compresses headers when sending<a id="_idIndexMarker1040"/> requests. Compressing headers has similar benefits to the binary protocol, which results in a lower size of data needed to transfer the same request. The HTTP/2 protocol uses the <span class="No-Break">HPACK format.</span></p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor228"/>Persistent connections</h2>
			<p>When using HTTP, our browser must make<a id="_idIndexMarker1041"/> a request every time a resource is needed. For instance, we could get our NGINX server to serve an HTML file. This will result in one request to get the HTML file. Inside the HTML file, there could be a reference to a CSS file, and this would result in another request to the NGINX server. It is also not uncommon to have a reference to a JavaScript file in the HTML file. This would result in another request. Therefore, to load a standard web page, our browser requires up to three requests. This does not scale well when we are running a server with multiple users. With HTTP/2, we can have persistent connections. This means that we can make three requests for the HTML, CSS, and JavaScript files in <span class="No-Break">one connection.</span></p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor229"/>Multiplex streaming</h2>
			<p>Making requests using HTTP/1 means that we must send<a id="_idIndexMarker1042"/> our requests sequentially. This means that we make one request, wait for this request to be resolved, and then send another request. With HTTP/2, we use multiplex streaming, which means that we can send multiple requests at the same time and resolve them when the responses are returned. Combining multiplex streaming with a persistent connection results in a faster loading time. Any reader who used to use the internet in the 1990s will remember having to wait a long time for a simple page to be loaded. Granted—the internet connection back then was not as fast, but it also was a result of making multiple HTTP sequential requests with different connections to load multiple pictures, HTML, <span class="No-Break">and CSS.</span></p>
			<p>Now that we have explored the difference between HTTP and HTTP/2, we can explore HTTPS, which is built on top of HTTP/2. Before we go forward, however, it must be stated that security is a field in itself. Learning the high-level concepts around HTTPS is enough to make us understand the importance of what we are implementing and why we take certain steps. However, it does not make us <span class="No-Break">security experts.</span></p>
			<p>Before we explore the steps of HTTPS, we need to understand what a middleman attack is because this attack inspires the steps of HTTPS. A middleman attack<a id="_idIndexMarker1043"/> is exactly what it sounds like: a malicious eavesdropper that can intercept communication packets between the user and the server. This also means that the eavesdropper can also obtain encryption if they are passed over a network. Simply googling “middleman attacks” will result in loads of tutorials and software that can be downloaded to implement such attacks. There are more caveats to security that are beyond the scope of this book, but to conclude, if you are hosting a website that you expect users to connect and log in to, there is no excuse to not <span class="No-Break">use HTTPS.</span></p>
			<p>When it comes to HTTPS, there<a id="_idIndexMarker1044"/> is a range of steps that need to take place. First, before any requests are made to the server, the owner of the server and domain must get a certificate from a trusted central authority. There are not many trusted central authorities around. What these authorities do is get some identification of the person who owns the domain and some proof that the person applying for the certificate owns the domain. This might sound like a headache, but a lot of URL providers such as AWS have streamlined the process using information, such as payment details, to send to the central trusted authority in the backend when you are pointing and clicking to buy the domain. We will have to fill in some extra forms, but if you have a working AWS account, this will not be too taxing. These central authorities are limited because anyone with a computer can create a digital certificate. For instance, if we were intercepting traffic between a server and a user, we could generate our own digital certificates with a key and forward them to the user. Because of this, mainstream browsers only recognize certificates that have been issued by a small number of recognized authorities. If the browser gets a certificate that is not recognized, these browsers will give a warning, such as in the following <span class="No-Break">Chrome example:</span></p>
			<p class="IMG---Figure">  </p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/Figure_11.1_B18722.jpg" alt="Figure 11.1 – Chrome blocking access due to unrecognized certificate"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Chrome blocking access due to unrecognized certificate</p>
			<p>It is not advised to click on <strong class="bold">ADVANCED</strong> <span class="No-Break">to proceed.</span></p>
			<p>Once the owner of the domain and server has the certificate from the central authority, the user can make requests. Before<a id="_idIndexMarker1045"/> any meaningful data to the application is exchanged, the server sends over the certificate with the public key to the user. The user then creates a session key and encrypts this session key and the public key of the certificate, which is then sent back to the server. The server can then decrypt the key using the private key that was not sent over the network. Therefore, even if the eavesdropper manages to intercept the messages and get the session key, it is encrypted, so they cannot use it. Both the server and client can check the validity of mutually encrypted messages. We can use the session key to send encrypted messages to and from the server and the user, as seen in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/Figure_11.2_B18722.jpg" alt="Figure 11.2 – Steps required when making an HTTPS connection"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Steps required when making an HTTPS connection</p>
			<p>Do not worry—there are packages and tools that will help us manage the HTTPS process; we will not have to implement our own protocol. You will understand why we must carry out certain steps <a id="_idIndexMarker1046"/>and how to troubleshoot problems when they arise. In the next section, we will implement a basic HTTPS protocol with <span class="No-Break">NGINX locally.</span></p>
			<h1 id="_idParaDest-229"><a id="_idTextAnchor230"/>Implementing HTTPS locally with docker-compose</h1>
			<p>When it comes<a id="_idIndexMarker1047"/> to implementing HTTPS, most<a id="_idIndexMarker1048"/> of the work is going to be achieved through NGINX. Although we have worked a little with NGINX, the NGINX configuration is a powerful tool. You can implement conditional logic, pull variables and data from the request and act on them, redirect traffic, and much more. In this chapter, we are going to do enough to implement HTTPS, but it is advised if you have time to read up on the fundamentals of NGINX configurations; reading material is provided in the <em class="italic">Further reading</em> section. For our <strong class="source-inline">deployment/nginx_config.yml</strong> file, we need to have the <span class="No-Break">following layout:</span></p>
			<pre class="source-code">
worker_processes  auto;
error_log  /var/log/nginx/error.log warn;
events {
    worker_connections  512;
}
http {
    server {
        . . .
    }
    server {
        . . .
    }
}</pre>
			<p>Here, we can see<a id="_idIndexMarker1049"/> that we have two <strong class="source-inline">server</strong> scopes<a id="_idIndexMarker1050"/> in our <strong class="source-inline">http</strong> scope. This is because we need to enforce HTTPS. We must remember that our outside port is <strong class="source-inline">80</strong>. However, if we want to carry out an HTTPS connection, we instead need to connect to port <strong class="source-inline">443</strong>, which is the standard for HTTPS. Typing <strong class="source-inline">https://</strong> into the browser will target port <strong class="source-inline">443</strong>, and typing <strong class="source-inline">http://</strong> into the browser will target port <strong class="source-inline">80</strong>. If we allow port <strong class="source-inline">80</strong> to be open, loads of users will access our site in an unsecured way because some people will type <strong class="source-inline">http://</strong> into the browser. Hackers would also spread the HTTP links around because they want as many people as possible to not use a secure network. However, if we block port <strong class="source-inline">80</strong>, people who put <strong class="source-inline">http://</strong> into the browser will just get blocked from accessing the website. It is unlikely that the average user is going to understand the differences in ports, look at what they typed in, and correct it. Instead, they are just going to think that the site is down. Therefore, we are<a id="_idIndexMarker1051"/> going to have to listen<a id="_idIndexMarker1052"/> to both ports <strong class="source-inline">443</strong> and <strong class="source-inline">80</strong>. However, when a request is made to port <strong class="source-inline">80</strong>, we going to redirect the request to port <strong class="source-inline">443</strong> instead. Our first server scope can redirect with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
server {
    listen 80;
    return 301 https://$host$request_uri;
}</pre>
			<p>Here, we can see that we listen to port <strong class="source-inline">80</strong>, and then return the same request that was sent but with the HTTPS protocol, which means that it will hit our <strong class="source-inline">443</strong> port. We can also see that we reference <strong class="source-inline">$host</strong> and <strong class="source-inline">$request_uri</strong> variables. These variables are standard variables in NGINX that are automatically populated. We can define our own variables with the following line <span class="No-Break">of code:</span></p>
			<pre class="source-code">
set $name 'Maxwell';</pre>
			<p>However, we want our NGINX instance to work on our server and localhost, so using the standard variables is the best choice here. Now that we have defined our rerouting rule, we can move on to the next server scope; we listen to port <strong class="source-inline">443</strong> with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
server {
    listen 443 ssl http2;
    ssl_certificate /etc/nginx/ssl/self.crt;
    ssl_certificate_key /etc/nginx/ssl/self.key;
    location /v1 {
        proxy_pass http://rust_app:8000/v1;
    }
    location / {
        proxy_pass http://front_end:4000/;
    }
}</pre>
			<p>Looking at the preceding code, it must be noted<a id="_idIndexMarker1053"/> that we are handling and directing traffic for both the frontend<a id="_idIndexMarker1054"/> and backend applications in the same NGINX instance. Alongside the port definition, we also declare that we are using the <strong class="source-inline">ssl</strong> and <strong class="source-inline">http2</strong> NGINX modules. This is not surprising as HTTPS is essentially SSL on top of HTTP/2. We then define where our server certificate is in the NGINX container. We will add these later in the <strong class="source-inline">docker-compose</strong> volume. We can also see that we pass our HTTPS request to the appropriate application via HTTP. If we try to change these proxies to the HTTPS protocol, then we would get a bad gateway error. This is because the handshake between NGINX and our services would fail. It is not essential because we must remember that the ports that the frontend and backend applications expose are not available to anyone outside of localhost. Yes—on our local machine we can access them, but this is because they are running on our local machine. If we were to deploy our application server, it will look <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/Figure_11.3_B18722.jpg" alt="Figure 11.3 – Traffic flow if deployed on a server"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Traffic flow if deployed on a server</p>
			<p>Our NGINX configuration<a id="_idIndexMarker1055"/> is not optimum. There are settings that can be tweaked in terms of ciphers, caching, and managing<a id="_idIndexMarker1056"/> timeouts. However, this is enough to get an HTTPS protocol working. If you get to the point that you need to optimize the caching and encryption methods of your NGINX configuration, it is suggested that you seek further education materials on DevOps <span class="No-Break">and NGINX.</span></p>
			<p>Now that we have defined our NGINX configuration, we must define <span class="No-Break">our certificates.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">To define our own certificates, we must install the <strong class="source-inline">openssl</strong> package by following the steps using the <span class="No-Break">next links:</span></p>
			<p class="callout"><span class="No-Break"><strong class="bold">Linux</strong></span><span class="No-Break">:</span></p>
			<p class="callout"><a href="https://fedingo.com/how-to-install-openssl-in-ubuntu/"><span class="No-Break">https://fedingo.com/how-to-install-openssl-in-ubuntu/</span></a></p>
			<p class="callout"><span class="No-Break"><strong class="bold">Windows</strong></span><span class="No-Break">:</span></p>
			<p class="callout"><a href="https://linuxhint.com/install-openssl-windows/"><span class="No-Break">https://linuxhint.com/install-openssl-windows/</span></a></p>
			<p class="callout"><span class="No-Break"><strong class="bold">Mac</strong></span><span class="No-Break">:</span></p>
			<p class="callout"><a href="https://yasar-yy.medium.com/installing-openssl-library-on-macos-catalina-6777a2e238a6"><span class="No-Break">https://yasar-yy.medium.com/installing-openssl-library-on-macos-catalina-6777a2e238a6</span></a></p>
			<p>This can be done<a id="_idIndexMarker1057"/> with the<a id="_idIndexMarker1058"/> <span class="No-Break">following</span><span class="No-Break"><a id="_idIndexMarker1059"/></span><span class="No-Break"> command:</span></p>
			<pre class="console">
openssl req -x509 -days 10 -nodes -newkey rsa:2048
-keyout ./self.key -out ./self.crt</pre>
			<p>This creates a key with <strong class="source-inline">x509</strong>, which is the international<a id="_idIndexMarker1060"/> telecommunication union<a id="_idIndexMarker1061"/> standard. We state that the certificate will expire in 10 days and that the key and certificate will have the name <strong class="source-inline">self</strong>. They can be called anything; however, for us, it makes sense to call the certificate <strong class="source-inline">self</strong> because it is a self-issued certificate. The command shown in the previous code snippet will push several prompts. It does not matter what you say to these prompts as we will just be making localhost requests with them, meaning that they will never go anywhere outside of our local computer. We can now stash the key and certificate anywhere within the <strong class="source-inline">deployment</strong> directory if you can reference the key and certificate placed in the <strong class="source-inline">docker-compose.yml</strong> file. In our <strong class="source-inline">docker-compose.yml</strong> file, our NGINX service now takes the <span class="No-Break">following form:</span></p>
			<pre class="source-code">
nginx:
container_name: 'nginx-rust'
image: "nginx:latest"
ports:
  - "80:80"
  - 443:443
links:
  - rust_app
  - front_end
volumes:
  - ./nginx_config.conf:/etc/nginx/nginx.conf
  - ./nginx_configs/ssl/self.crt:/etc/nginx/ssl/self.crt
  - ./nginx_configs/ssl/self.key:/etc/nginx/ssl/self.key</pre>
			<p>Here, we can see that I chose to store<a id="_idIndexMarker1062"/> the key and certificate inside<a id="_idIndexMarker1063"/> a directory called <strong class="source-inline">nginx_configs/ssl/</strong>. This is because I have added several simple NGINX configs into the GitHub repo under the <strong class="source-inline">nginx_configs</strong> directory if you want some easy quick references on handling variables, conditional logic, and serving HTML files directly from NGINX. While where you get your key and certificate from may vary, it is important that you put the key and the certificate inside the <strong class="source-inline">etc/nginx/ssl/</strong> directory inside the <span class="No-Break">NGINX container.</span></p>
			<p>We are now at the point where you can test our application to see if the local HTTPS is working. If you spin up your <strong class="source-inline">docker-compose</strong> instance and then go to the <strong class="source-inline">https://localhost</strong> URL in your browser, you should get a warning that it is not secure, and you will not be able to instantly connect to the frontend. This is reassuring because we are not a central authority, so our browser will not recognize our certificate. There is a multitude of browsers, and we would waste a lot of space in this book describing how to get past this for every browser. Considering browsers are free to download, we can get around the blocking of our application in Chrome by going to the <strong class="source-inline">flags</strong> URL, as <span class="No-Break">seen here:</span></p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/Figure_11.4_B18722.jpg" alt="Figure 11.4 – Allowing our application certificate to pass in Chrome"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Allowing our application certificate to pass in Chrome</p>
			<p>Here, we can see that I have<a id="_idIndexMarker1064"/> allowed invalid certificates<a id="_idIndexMarker1065"/> from localhost. Now that our invalid certificate is enabled in our browser, we can access our application, as <span class="No-Break">seen here:</span></p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/Figure_11.5_B18722.jpg" alt="Figure 11.5 – Accessing our application through HTTPS"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Accessing our application through HTTPS</p>
			<p>Here, we are using the HTTPS<a id="_idIndexMarker1066"/> protocol; however, as we<a id="_idIndexMarker1067"/> can see in the preceding screenshot, Chrome is complaining stating that it is not secure. We can inspect the reason why by clicking on the <strong class="bold">Not Secure</strong> statement, giving the <span class="No-Break">following view:</span></p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/Figure_11.6_B18722.jpg" alt="Figure 11.6 – Explaining why the connection is not secure"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Explaining why the connection is not secure</p>
			<p>Here, we can see that our certificate is not valid. We expected the certificate to not be valid because we issued it, which makes<a id="_idIndexMarker1068"/> it not officially recognized. However, our HTTPS connection is working! This is interesting<a id="_idIndexMarker1069"/> to see how HTTPS works; however, it is not useful with a self-signed certificate running on our localhost. If we want to utilize HTTPS, we are going to have to apply it to our application on AWS. There are a couple of steps we need to carry out before we implement HTTPS on AWS. In the next section, we will assign a URL to <span class="No-Break">our application.</span></p>
			<h1 id="_idParaDest-230"><a id="_idTextAnchor231"/>Attaching a URL to our deployed application on AWS</h1>
			<p>In the previous chapter, we managed<a id="_idIndexMarker1070"/> to deploy our to-do application<a id="_idIndexMarker1071"/> onto a server on AWS and access this application directly by putting the IP address of the server into our browser. When it comes to registering our URL, you will be exposed to multiple acronyms. To feel comfortable when navigating AWS routing, it makes sense to be familiar with URL<a id="_idIndexMarker1072"/> acronyms by reading the <span class="No-Break">following</span><span class="No-Break"><a id="_idIndexMarker1073"/></span><span class="No-Break"> diagram:</span></p>
			<p class="IMG---Figure">  </p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/Figure_11.7_B18722.jpg" alt="Figure 11.7 – Anatomy of a URL"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – Anatomy of a URL</p>
			<p>When we are associating a URL with our application, we are going to be configuring a <strong class="bold">Domain Name System</strong> (<strong class="bold">DNS</strong>). A DNS is a system that translates<a id="_idIndexMarker1074"/> a user-friendly URL to an IP address. For a DNS system to work, we will need the <span class="No-Break">following components:</span></p>
			<ul>
				<li><strong class="bold">Domain registrar</strong>: An organization such as AWS, Google<a id="_idIndexMarker1075"/> Cloud, Azure, GoDaddy, and so on that will register a domain if it receives payment for the domain and personal details of the person responsible for the domain. This organization will also handle reports of abuse if the URL is used in <span class="No-Break">illegal activity.</span></li>
				<li><strong class="bold">DNS record</strong>: A registered URL can have multiple DNS records. A DNS record<a id="_idIndexMarker1076"/> essentially defines a routing rule for the URL. For instance, a simple DNS record will forward the URL to an IP address of <span class="No-Break">a server.</span></li>
				<li><strong class="bold">Zone file</strong>: A container for DNS<a id="_idIndexMarker1077"/> records (in our case, the zone file will be managed <span class="No-Break">by AWS).</span></li>
			</ul>
			<p>The DNS records and registrars are essential for our URL to work. Even though we can directly connect to IP addresses, there are a couple of middlemen if we want to connect to a URL, as laid <span class="No-Break">out here:</span></p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/Figure_11.8_B18722.jpg" alt="Figure 11.8 – Steps required to connect a URL to an IP address"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – Steps required to connect a URL to an IP address</p>
			<p>As we can see from the preceding diagram, if we want to connect to a server, we send the URL to the local DNS server. This server then makes three calls in sequential order from top to bottom. By the end of the three requests, the local DNS server will have the IP address related to the URL. We can see that the registrar is responsible for part of the mapping. This is where our DNS<a id="_idIndexMarker1078"/> records are configured. If we remove<a id="_idIndexMarker1079"/> our DNS records, then the URL is no longer available on the internet. We do not have to make the calls laid out in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.8</em> every time we enter a URL. Our browsers and the local DNS server will cache the URL to the IP address mapped to reduce the number of calls to the other three servers. There is a problem, however; when we have been building our production server, you might have realized that the IP address has changed every time we tear down and spin up a production server. There’s nothing wrong going on here; when we create an EC2 instance, we must take a server that is available. A cloud provider such as AWS cannot hold the server aside just for us unless we want to pay for it. In the next section, we will keep our IP consistent with elastic <span class="No-Break">IP addresses.</span></p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor232"/>Attaching an elastic IP to our server</h2>
			<p>Elastic IP addresses<a id="_idIndexMarker1080"/> are essentially fixed IP addresses<a id="_idIndexMarker1081"/> that we keep. We can then attach these elastic IP addresses to any one EC2 instance at one point in time as we see fit. This is very helpful when it comes to routing. We can set up the routing of a URL to an elastic IP and then switch the allocation of the elastic IP to the server that we need. This means that we can deploy a new application to another server, test it, and then switch our elastic IP to the new deployment server without having to touch the routing for <span class="No-Break">our URL.</span></p>
			<p>We will not be creating<a id="_idIndexMarker1082"/> an elastic IP every time we spin<a id="_idIndexMarker1083"/> up a production server. Because of this, it is OK to point and click in the AWS console to create and attach the elastic IP address. Before we do this, however, we need to deploy our production server with the previous NGINX config file that does not have HTTPS defined, but instead has the <span class="No-Break">following form:</span></p>
			<pre class="source-code">
worker_processes  auto;
error_log  /var/log/nginx/error.log warn;
events {
    worker_connections  512;
}
http {
    server {
        listen 80;
        location /v1 {
            proxy_pass http://rust_app:8000/v1;
        }
        location / {
            proxy_pass http://front_end:4000/;
        }
    }
}</pre>
			<p>This should make sense to you now, as the NGINX<a id="_idIndexMarker1084"/> config merely listens to HTTP requests through the outside port <strong class="source-inline">80</strong> and then passes<a id="_idIndexMarker1085"/> them through to our applications. We also must remove our reference to our self-signed certificates because we will not need them, and we will also not be uploading those certificates to our server. Considering our lack of reference to our certificates, our <strong class="source-inline">docker-compose</strong> instance in our <strong class="source-inline">deployment</strong> directory should have the following <span class="No-Break">NGINX definition:</span></p>
			<pre class="source-code">
nginx:
  container_name: 'nginx-rust'
  image: "nginx:latest"
  ports:
    - "80:80"
  links:
    - rust_app
    - front_end
  volumes:
    - ./nginx_config.conf:/etc/nginx/nginx.conf</pre>
			<p>We are now ready to deploy our build on our production server. Remember—we can do this using the <strong class="source-inline">deployment/run_build.py</strong> Python script that we set up in the previous chapter. Once the server is built, we know that there is an EC2 instance live with the <strong class="source-inline">"to-do production server"</strong> tag. We are now ready to allocate an elastic IP address to our <span class="No-Break">EC2 instance.</span></p>
			<p>To allocate elastic IPs, we first need to navigate to the EC2 service by searching for EC2 in the search bar in the AWS dashboard at the top and clicking on the service, resulting in the <span class="No-Break">following view:</span></p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/Figure_11.9_B18722.jpg" alt="Figure 11.9 – EC2 dashboard view"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – EC2 dashboard view</p>
			<p>We can see that <strong class="bold">Elastic IPs</strong> can be accessed on the right of the <strong class="bold">Resources</strong> panel and on the left<a id="_idIndexMarker1086"/> of the screen. Once we are in the elastic IP address<a id="_idIndexMarker1087"/> dashboard, we will have a list of elastic IP addresses that you have. On the top right of the screen, there will be an orange button labeled <strong class="bold">Allocate Elastic IP address</strong>. If you click this button, you will get the following <span class="No-Break">creation form:</span></p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/Figure_11.10_B18722.jpg" alt="Figure 11.10 – Allocating (creating) an elastic IP address"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – Allocating (creating) an elastic IP address</p>
			<p>What we are doing here is grabbing<a id="_idIndexMarker1088"/> an elastic IP address from a pool of IP<a id="_idIndexMarker1089"/> addresses for the area you are working on. There is a limit of five elastic IP addresses per account. If you think this is not enough for you, you will need to get more creative with your networking infrastructure. You can also investigate creating sub-accounts for the main account. As with clean code, there are benefits to having clean accounts that only work on one project at a time. This will help you keep track of costs, and shutting down all infrastructure for a project will be clean as you can be sure that everything for that project has been cleared by deleting <span class="No-Break">the account.</span></p>
			<p>Moving on to our allocation<a id="_idIndexMarker1090"/> of the elastic IP<a id="_idIndexMarker1091"/> to our EC2 server, we can allocate our elastic IP by highlighting the desired elastic IP address in the elastic IP dashboard and clicking on the <strong class="bold">Actions</strong> button at the top right of the dashboard, as <span class="No-Break">seen here:</span></p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/Figure_11.11_B18722.jpg" alt="Figure 11.11 – Actions on our elastic IP"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11 – Actions on our elastic IP</p>
			<p>Under <strong class="bold">Actions</strong>, we must click on the <strong class="bold">Associate Elastic IP address</strong> option, giving us the <span class="No-Break">following display:</span></p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/Figure_11.12_B18722.jpg" alt="Figure 11.12 – Associating an elastic IP address"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12 – Associating an elastic IP address</p>
			<p>The <strong class="bold">Instance</strong> option will provide<a id="_idIndexMarker1092"/> a drop-down menu of the EC2 instances<a id="_idIndexMarker1093"/> that we have running. Luckily, we have the helpful <strong class="source-inline">"to-do production server"</strong> tag that we defined in Terraform. However, if we do not have a tag, we can still choose an instance from the drop-down menu. We can then click the <strong class="bold">Associate</strong> button. Once this is done, we can go to our elastic IP address in our browser, and we should be able to access our to-do application, as <span class="No-Break">seen here:</span></p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/Figure_11.13_B18722.jpg" alt="Figure 11.13 – Accessing our to-do application using a static IP"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13 – Accessing our to-do application using a static IP</p>
			<p>And here we have it—our application can be accessed with an elastic IP!! We can now spin up a new server if we want, test it, and redirect our elastic IP to the new server if we are happy, providing seamless<a id="_idIndexMarker1094"/> updates without our users<a id="_idIndexMarker1095"/> knowing. However, getting users to type in the raw IP address is not desirable. In the next section, we will register a domain and connect it to our elastic <span class="No-Break">IP address.</span></p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor233"/>Registering a domain name</h2>
			<p>When it comes to registering<a id="_idIndexMarker1096"/> a domain, it can all be handled in AWS with the Route 53 service. First, we navigate to Route 53, which is the service that handles routing and URL registration. On the left side of the <strong class="bold">Route 53</strong> dashboard web page, we can click on the <strong class="bold">Registered domains</strong> section, as seen in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/Figure_11.14_B18722.jpg" alt="Figure 11.14 – Navigation to registered domains in the Route 53 dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.14 – Navigation to registered domains in the Route 53 dashboard</p>
			<p>We will then see a list of registered<a id="_idIndexMarker1097"/> domains that we already own and the option to register a domain, as shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/Figure_11.15_B18722.jpg" alt="Figure 11.15 – Registered domains view"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.15 – Registered domains view</p>
			<p>If you click on the <strong class="bold">Register Domain</strong> button, you will be led through a straightforward series of forms to register your domain. Screenshotting these steps would be excessive. The forms ask you for the domain you want to register. They will then tell you if the domain and other domains such as this are available. Domains cost about $12 a year on average at the time of writing this book. Once you have selected your domain and clicked on the checkout, you will be passed through a series of personal information forms. These forms include a contact address and whether the domain is for personal use or for a company. Screenshotting these forms would result in excessive pages with little educational advantage as these forms are personal and easy to fill in. It is recommended that you select the validation through DNS because this <span class="No-Break">is automated.</span></p>
			<p>Once your domain<a id="_idIndexMarker1098"/> is registered, you can go to the <strong class="bold">Hosted zones</strong> section in the main Route 53 dashboard. Here, we will see a list of hosted zones for each URL that you own. A hosted zone is essentially a collection of DNS records. If we click on a hosted zone for a URL, there will be two DNS records: NS and SOA (NS—name server; SOA—start of authority). These records should not be deleted, and if they are, somebody else who knows what these records are could hijack your URL by implementing those records themselves. DNS records are essentially records on how to route traffic for a domain name. Each DNS record<a id="_idIndexMarker1099"/> has the <span class="No-Break">following attributes:</span></p>
			<ul>
				<li><strong class="bold">Domain/subdomain name</strong>: The name of the URL the record <span class="No-Break">belongs to</span></li>
				<li><strong class="bold">Record type</strong>: The type of record (A, AAAA, CNAME, <span class="No-Break">or NS)</span></li>
				<li><strong class="bold">Value</strong>: Target <span class="No-Break">IP address</span></li>
				<li><strong class="bold">Routing policy</strong>: How Route 53 responds <span class="No-Break">to queries</span></li>
				<li><strong class="bold">TLL (Time to Live)</strong>: The amount of time the record is cached in the DNS resolvers in the client so that we do not have to query the Route 53 servers too often, with the trade-off of reducing traffic to DNS servers versus time for updates to roll <span class="No-Break">onto clients</span></li>
			</ul>
			<p>The attributes just defined are self-explanatory, apart from the record type. There are advanced record types that we can build in Route 53. However, the following record types are needed for us to route our domain to <span class="No-Break">our IP:</span></p>
			<ul>
				<li><strong class="bold">A</strong>: The simplest record type. Type A<a id="_idIndexMarker1100"/> merely routes traffic from the URL to an IPv4 <span class="No-Break">IP address.</span></li>
				<li><strong class="bold">AAAA</strong>: Routes traffic from the URL to an <span class="No-Break">IPv6 address.</span></li>
				<li><strong class="bold">CNAME</strong>: Maps a hostname to another hostname (target must be an A or AAAA <span class="No-Break">record type).</span></li>
				<li><strong class="bold">NS</strong>: Name server for the hosted zone (controls how the traffic <span class="No-Break">is routed).</span></li>
			</ul>
			<p>For us, we are going<a id="_idIndexMarker1101"/> to create an DNS <strong class="source-inline">A</strong> record by clicking on the <strong class="bold">Create record</strong> button, as seen in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/Figure_11.16_B18722.jpg" alt="Figure 11.16 – Creating a DNS record"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.16 – Creating a DNS record</p>
			<p>Once we have clicked on this, we get the <span class="No-Break">following layout:</span></p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/Figure_11.17_B18722.jpg" alt="Figure 11.17 – Creating a DNS record form"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.17 – Creating a DNS record form</p>
			<p>We can see that record<a id="_idIndexMarker1102"/> type <strong class="source-inline">A</strong> is the default. We can also see that we can add a subdomain. This gives us some flexibility. For instance, if we wanted, the <strong class="source-inline">api.freshcutswags.com</strong> URL could point to a different IP address from <strong class="source-inline">freshcutswags.com</strong>. For now, we are going to just leave the subdomain empty. We are then going to put the elastic IP address that we set up in the previous section in the <strong class="bold">Value</strong> section and click <strong class="bold">Create records</strong>. We are then going to create the exact same DNS record<a id="_idIndexMarker1103"/> but with the subdomain of <strong class="source-inline">www</strong>. Once this is done, we should have two <strong class="source-inline">A</strong> records. We can then check where our URL is mapping the traffic using the <a href="https://www.digwebinterface.com">www.digwebinterface.com</a> website. Here, we can enter URLs, and the website will tell us where the URLs are being mapped. We can see here that our URLs are both mapping to the correct <span class="No-Break">elastic IP:</span></p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/Figure_11.18_B18722.jpg" alt="Figure 11.18 – Inspecting our URL mapping"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.18 – Inspecting our URL mapping</p>
			<p>With the mapping result confirmed, as seen in the preceding screenshot, we can visit our URL and expect to see <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/Figure_11.19_B18722.jpg" alt="Figure 11.19 – Accessing our application through the registered URL"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.19 – Accessing our application through the registered URL</p>
			<p>We can see that our URL is now working. However, the connection is not secure. In the next section, we will enforce<a id="_idIndexMarker1104"/> an HTTPS protocol for our application and lock it down, as right now, even though we can access our application through the URL, there is nothing stopping us from accessing the IP of the <span class="No-Break">server directly.</span></p>
			<h1 id="_idParaDest-233"><a id="_idTextAnchor234"/>Enforcing HTTPS on our application on AWS</h1>
			<p>Right now, our application<a id="_idIndexMarker1105"/> kind of works, but it is a nightmare<a id="_idIndexMarker1106"/> in terms of security. By the end of this section, we will not have the most secure application, as further reading of a networking and DevOps textbook is suggested to achieve gold-standard security. However, we will have configured security groups, locked down our EC2 instances so that they cannot be directly accessed by outsiders, and enforced encrypted traffic through a load balancer that will then direct traffic to our EC2 instances. The result of our efforts will be the <span class="No-Break">following system:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/Figure_11.20_B18722.jpg" alt="Figure 11.20 – Layout of our desired system to achieve HTTPS"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.20 – Layout of our desired system to achieve HTTPS</p>
			<p>To achieve the system<a id="_idIndexMarker1107"/> shown in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.20</em>, we need to carry<a id="_idIndexMarker1108"/> out the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Get certificates approved for our URL <span class="No-Break">and variations.</span></li>
				<li>Create multiple EC2 instances to distribute traffic and ensure that the service <span class="No-Break">survives outages.</span></li>
				<li>Create a load balancer to handle <span class="No-Break">incoming traffic.</span></li>
				<li>Create <span class="No-Break">security groups.</span></li>
				<li>Update our Python build script to support multiple <span class="No-Break">EC2 instances.</span></li>
				<li>Attach our URL to the load balancer using the Route <span class="No-Break">53 wizard.</span></li>
			</ol>
			<p>In the previous section on attaching a URL to our application on AWS, we did a lot of pointing and clicking. As said in the previous chapter, pointing and clicking should be avoided if possible as it is not repeatable and we as humans forget what we did. Sadly, with URL approvals pointing and clicking is the best option. In this section, only the first and sixth steps will require pointing and clicking. The rest will be achieved with Terraform and Python. We are going<a id="_idIndexMarker1109"/> to be making some big changes to our Terraform config, so it is advised<a id="_idIndexMarker1110"/> that you run a <strong class="source-inline">terraform destroy</strong> command before altering your Terraform config. Before we can do any of the coding, however, we are going to have to get our certificates for <span class="No-Break">our URL.</span></p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor235"/>Getting certificates for our URL</h2>
			<p>Because we brought<a id="_idIndexMarker1111"/> our URL through<a id="_idIndexMarker1112"/> Route 53, which is handled by AWS, and our servers are running in AWS, the certification and implementation of those certificates is a straightforward process. We need to navigate to Certificate Manager by typing <strong class="source-inline">certificate manager</strong> into the services search bar and clicking on it. Once there, we are displayed with a page that only has one orange button labeled <strong class="bold">Request a certificate</strong>. Click on this, and we will be transported to the <span class="No-Break">following page:</span></p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/Figure_11.21_B18722.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.21 – Certificate Manager: start of journey</p>
			<p>We are going to want our certificate to be public facing; therefore, we are happy with the default selection, and we click <strong class="bold">Next</strong>. We are then presented with the <span class="No-Break">following form:</span></p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/Figure_11.22_B18722.jpg" alt="Figure 11.22 – Defining certificate request"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.22 – Defining certificate request</p>
			<p>Here, we type in the URL<a id="_idIndexMarker1113"/> we want to associate<a id="_idIndexMarker1114"/> with the certificate. We could add another, but we will do a separate certificate for our URL that has prefixes as we want to explore how to attach multiple certificates in Terraform. We can also see that DNS validation is already highlighted, and this is recommended as we have our servers on AWS, meaning we will have to take no more action to get the certificate issued. We can then click on the button labeled <strong class="bold">Request</strong>, and we will be redirected to a page with the list of certificates. I find that the new certificate request is not present nearly every time I have done this. My guess is that there is a delay. Do not worry—merely refresh <a id="_idIndexMarker1115"/>the page, and you will see the pending certificate <a id="_idIndexMarker1116"/>request listed. Click on this listing, and you will be directed to a detailed view of this certificate request. In the middle of the screen on the right, you need to click the <strong class="bold">Create records in Route 53</strong> button, as <span class="No-Break">seen here:</span></p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/Figure_11.23_B18722.jpg" alt="Figure 11.23 – Creating records for DNS confirmation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.23 – Creating records for DNS confirmation</p>
			<p>Follow the prompts after clicking the button, and the CNAME record will be created. If you do not do this, then the pending status for the ticket will go on indefinitely because the cloud provider needs the route to issue the certificate, considering we selected DNS validation. After a few minutes, the certificate should be issued. Once this is done, carry out the same steps for the prefix wildcard. Once this is done, your certificate lists should look something like <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/Figure_11.24_B18722.jpg" alt="Figure 11.24 – Issued certificates"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.24 – Issued certificates</p>
			<p>In the preceding screenshot, we can see that I have two certificates: one for if a user directly types in the URL with no prefix, and the wildcard that covers all prefixes. We are ready to use these<a id="_idIndexMarker1117"/> certificates, but before we do this, we are going<a id="_idIndexMarker1118"/> to have to carry out some other steps. Before we define rules around the traffic, we are going to have to build the infrastructure where the traffic is going. In the next section, we will build two <span class="No-Break">EC2 instances.</span></p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor236"/>Creating multiple EC2 instances</h2>
			<p>We will be using a load <a id="_idIndexMarker1119"/>balancer. Because of this, we need a minimum of two EC2 instances. This means that if one EC2 instance is down, we can still use the other EC2 instance. We can also scale our application. For instance, if everyone in the world suddenly realized that they needed a to-do app to sort out their lives, there is nothing stopping us from increasing the number of EC2 instances to distribute the traffic across. We can increase our EC2 instances to two by going into our <strong class="source-inline">deployment/main.tf</strong> file and having the following definition of our <span class="No-Break">EC2 instances:</span></p>
			<pre class="source-code">
resource "aws_instance" "production_server" {
    ami = "ami-0fdbd8587b1cf431e"
    instance_type = "t2.medium"
    count = 2
    key_name = "remotebuild"
    user_data = file("server_build.sh")
    tags = {
      Name = "to-do prod ${count.index}"
    }
    # root disk
    root_block_device {
      volume_size = "20"
      volume_type = "gp2"
      delete_on_termination = true
    }
}</pre>
			<p>Here, we can see that we have<a id="_idIndexMarker1120"/> added a <strong class="source-inline">count</strong> parameter and defined it as <strong class="source-inline">2</strong>. We have also altered the tag. We can also see that we access the number of EC2 instances being created with <strong class="source-inline">index</strong>. The index starts at zero and increases by one every time a resource is made. Now that we have two instances, we must update the outputs at the bottom of the <strong class="source-inline">deployment/main.tf</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
output "ec2_global_ips" {
  value = ["${aws_instance.production_server.*.public_ip}"]
}
output "db_endpoint" {
  value = "${aws_db_instance.main_db.*.endpoint}"
}
output "public_dns" {
  value =
      ["${aws_instance.production_server.*.public_dns}"]
}
output "instance_id" {
    value = ["${aws_instance.production_server.*.id}"]
}</pre>
			<p>Here, we can see that apart from the database endpoint, all the other outputs have been changed to lists. This is because they all reference our multiple EC2 instances. Now that we have our EC2 instances defined, we can route traffic to our instances with a <span class="No-Break">load balancer.</span></p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor237"/>Creating a load balancer for our traffic</h2>
			<p>There is a range of different load balancers<a id="_idIndexMarker1121"/> that we can pick from. We have already discussed NGINX, which is a popular load balancer. For this chapter, we are going to use the application load balancer to route the traffic to our EC2 instances and implement the HTTPS<a id="_idIndexMarker1122"/> protocol. There are multiple features that load balancers can offer, and they protect against <strong class="bold">distributed denial-of-service</strong> (<strong class="bold">DDoS</strong>) attacks whereby the attacker will try to overload the server with excessive requests. We will create our load balancer in the <strong class="source-inline">deployment/load_balancer.tf</strong> file. First, we collect the data that we need with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
data "aws_subnet_ids" "subnet" {
    vpc_id = aws_default_vpc.default.id
}
data "aws_acm_certificate" "issued_certificate" {
    domain   = "*.freshcutswags.com"
    statuses = ["ISSUED"]
}
data "aws_acm_certificate" "raw_cert" {
    domain   = "freshcutswags.com"
    statuses = ["ISSUED"]
}</pre>
			<p>We can see that instead of <strong class="source-inline">resource</strong>, we used the <strong class="source-inline">data</strong> declaration. This is where we make queries to AWS for specific types of data to be used in the rest of the Terraform script. We get the <strong class="bold">virtual private cloud</strong> (<strong class="bold">VPC</strong>) ID. In Terraform, we can define and build a VPC, but throughout<a id="_idIndexMarker1123"/> this book, we have been using the default VPC. We can get the default VPC ID for our load balancer. We then get the data for the certificates that we have defined in the previous section with the <span class="No-Break">preceding code.</span></p>
			<p>We now must define a target for our load balancer. This is done in the form of a target group where we can bunch a group of instances together for the load balancer to target with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
resource "aws_lb_target_group" "target-group" {
    health_check {
        interval = 10
        path = "/"
        protocol = "HTTP"
        timeout = 5
        healthy_threshold = 5
        unhealthy_threshold = 2
    }
    name = "ToDoAppLbTg"
    port = 80
    protocol = "HTTP"
    target_type = "instance"
    vpc_id   = aws_default_vpc.default.id
}</pre>
			<p>Here, we can see that we define the parameters<a id="_idIndexMarker1124"/> for a health check. The parameters for the health check are self-explanatory. The health check will alert a service targeting the health status of the target group. We would not want to route traffic to a target group that is down. We then define the protocol and port of the traffic, the type of resource in the target group, and the ID of the VPC. Now that our target group is defined, we can attach our EC2 instances to it with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
resource "aws_lb_target_group_attachment" "ec2_attach" {
    count = length(aws_instance.production_server)
    target_group_arn = aws_lb_target_group.target-group.arn
    target_id =
        aws_instance.production_server[count.index].id
}</pre>
			<p>We can see that we get the IDs<a id="_idIndexMarker1125"/> of the EC2 servers for the target ID. With this, our EC2 instances can be targeted by the load balancer. Now that we have a target, we can create our load balancer with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
resource "aws_lb" "application-lb" {
    name = "ToDoApplicationLb"
    internal = false
    ip_address_type = "ipv4"
    load_balancer_type = "application"
    security_groups = ["${aws_security_group.
        alb-security-group.id}"]
        subnets = data.aws_subnet_ids.subnet.ids
    tags = {
        name = "todo load balancer"
    }
}</pre>
			<p>We can see that the parameters in the load balancer definition are straightforward. However, you may have noticed the security group definition. We are referencing a security group even though we have not defined any security groups. If you do not know what a security group is, do not worry—we will cover and build all the security groups we need in the next section. Before we do that, however, we might as well define listening and routing rules for the load balancer. First, we can define the HTTP listener for port <strong class="source-inline">80</strong>. If you remember the first section of this chapter when getting HTTPS working on our localhost, what do you think we need to do for the HTTP traffic? You don’t have to know the specific Terraform code, but what is the general behavior that we want to facilitate? With this in mind, we can achieve that behavior with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
resource "aws_lb_listener" "http-listener" {
    load_balancer_arn = aws_lb.application-lb.arn
    port = 80
    protocol = "HTTP"
    default_action {
        type = "redirect"
        redirect {
            port        = "443"
            protocol    = "HTTPS"
            status_code = "HTTP_301"
        }
    }
}</pre>
			<p>That’s right! We receive HTTPS<a id="_idIndexMarker1126"/> traffic from port <strong class="source-inline">80</strong> and we then redirect it to port <strong class="source-inline">443</strong> with the HTTPS protocol. We can see<a id="_idIndexMarker1127"/> that we have attached this listener using the <strong class="bold">Amazon Resource Name</strong> (<strong class="bold">ARN</strong>) of the load balancer that we created. We can now define our HTTPS listener with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
resource "aws_lb_listener" "https-listener" {
    load_balancer_arn = aws_lb.application-lb.arn
    port = 443
    protocol = "HTTPS"
    certificate_arn = data.aws_acm_certificate.
                      issued_certificate.arn
    default_action {
        target_group_arn = aws_lb_target_group.target-
            group.arn
        type = "forward"
    }
}</pre>
			<p>Here, we can see that we accept HTTPS traffic and then forward HTTP traffic to our target group that we defined using the ARN of the target group. We can also see that we have attached one of the certificates to the listener. However, this does not cover all our URL combinations. Remember—we have another certificate that we want to attach. We can attach our second certificate with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
resource "aws_lb_listener_certificate" "extra_certificate" {
  listener_arn = "${aws_lb_listener.https-listener.arn}"
  certificate_arn =
      "${data.aws_acm_certificate.raw_cert.arn}"
}</pre>
			<p>This connection should be easy to understand. Here, we merely reference the ARN of the HTTPS listener and the ARN of the certificate that we want to attach. We have now defined everything we need for the resources of the load balancer. However, what of the traffic? We have defined the load balancer, EC2<a id="_idIndexMarker1128"/> instances, and the routing for HTTPS for the load balancer. However, what is stopping someone just directly connecting to the EC2 instance, bypassing the load balancer and HTTPS completely? This is where security groups come in. In the next section, we will lock down the traffic by creating security groups so that users cannot bypass our <span class="No-Break">load balancer.</span></p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor238"/>Creating security groups to lock down and secure traffic</h2>
			<p>Security groups are essentially<a id="_idIndexMarker1129"/> firewalls. We can define the traffic<a id="_idIndexMarker1130"/> to and from a resource that has a security group implemented. The rules of the traffic can be fine-grained. A single security group can have multiple rules defining the origin (even specific IP addresses if needed) and the protocol of the traffic. When it comes to our security groups, we are going to need two. One will accept HTTP and HTTPS traffic from all IPs anywhere in the world. This is going to be for our load balancer because we want our application to be available to everyone. The other security group will be implemented by our EC2 instances; this one blocks all HTTP traffic apart from the first security group. We will also enable SSH inbound traffic because we need to SSH into the servers to deploy the applications, giving us the following <span class="No-Break">traffic layout:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/Figure_11.25_B18722.jpg" alt="Figure 11.25 – Security groups for our deployment system"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.25 – Security groups for our deployment system</p>
			<p>This is where you must be careful with online tutorials. There is no shortage of <em class="italic">YouTube</em> videos and <em class="italic">Medium</em> articles that will get a load balancer up and running with some pointing and clicking. However, they leave the EC2 instances exposed and do not bother exploring security groups. Even with this section, we are going to leave the database exposed. I am doing this because it is a good question to be asked in the <em class="italic">Questions</em> section. However, I’m highlighting it here because you need to be warned that it is exposed. The way to lock <a id="_idIndexMarker1131"/>down the database will be covered<a id="_idIndexMarker1132"/> in the <em class="italic">Answers</em> section of this chapter. When it comes to our security groups, we can define them in the <strong class="source-inline">deployment/security_groups.tf</strong> file. We can start with the load balancer security group with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
resource "aws_security_group" "alb-security-group" {
    name = "to-do-LB"
    description = "the security group for the
                   application load balancer"
    ingress {
        . . .
    }
    ingress {
        . . .
    }
    egress {
        . . .
    }
    tags = {
        name: "to-do-alb-sg"
    }
}</pre>
			<p>Here, we have two inbound rules under the <strong class="source-inline">ingress</strong> tag and one outbound rule under the <strong class="source-inline">egress</strong> tag. Our first inbound rule is to allow HTTP data from anywhere with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
ingress {
    description = "http access"
    from_port = 80
    to_port = 80
    protocol = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
}</pre>
			<p>The <strong class="source-inline">cidr</strong> blocks of all zeros means <span class="No-Break"><em class="italic">from anywhere</em></span><span class="No-Break">.</span></p>
			<p>Our second inbound rule<a id="_idIndexMarker1133"/> is HTTPS traffic from anywhere. How do you think<a id="_idIndexMarker1134"/> this will be defined? It can be defined with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
ingress {
    description = "https access"
    from_port = 443
    to_port = 443
    protocol = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
}</pre>
			<p>Now for our outbound rule. We must allow all traffic and protocols out of the load balancer as they are coming from our resources. This can be achieved with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
egress {
    from_port = 0
    to_port = 0
    protocol = "-1"
    cidr_blocks = ["0.0.0.0/0"]
}</pre>
			<p>It must be noted that <strong class="source-inline">from_port</strong> and <strong class="source-inline">to_port</strong> are zero, meaning that we allow outgoing traffic from all ports. We also<a id="_idIndexMarker1135"/> have set <strong class="source-inline">protocol</strong> to <strong class="source-inline">-1</strong>, meaning we are allowing all protocols as outgoing<a id="_idIndexMarker1136"/> traffic. We have now defined the security group for the load balancer. We can now move on to defining our security group for the EC2 instances with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
resource "aws_security_group" "webserver-security-group" {
    name = "to-do-App"
    description = "the security group for the web server"
    ingress {
        . . .
    }
    ingress {
        . . .
    }
    egress {
        from_port = 0
        to_port = 0
        protocol = "-1"
        cidr_blocks = ["0.0.0.0/0"]
    }
    tags = {
        name: "to-do-webserver-sg"
    }
}</pre>
			<p>The outbound rules are going to be the same as the load balancer because we want to return data to anywhere that is available<a id="_idIndexMarker1137"/> to request it. When it comes to our HTTP inbound<a id="_idIndexMarker1138"/> rules, we only want to accept traffic from the load balancer with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
ingress {
    description = "http access"
    from_port = 80
    to_port = 80
    protocol = "tcp"
    security_groups = ["${aws_security_group.
                          alb-security-group.id}"]
}</pre>
			<p>Here, we can see that instead of defining the <strong class="source-inline">cidr</strong> blocks, we rely on the security group of the load balancer. Now that all the user traffic is defined, we only need to define the SSH traffic for deployment with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
ingress {
    description = "SSH access"
    from_port = 22
    to_port = 22
    protocol = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
}</pre>
			<p>Here, we access SSH through port <strong class="source-inline">22</strong>. This will enable us to SSH into our servers and deploy our applications. Nearly everything is done. We only must attach our EC2 instances to our EC2 security group, which can be done with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
resource "aws_network_interface_sg_attachment"
    "sg_attachment_webserver" {
         count = length(aws_instance.production_server)
             security_group_id = aws_security_group.
                 webserver-security-group.id
  network_interface_id = aws_instance.
                         production_server[count.index].
                         primary_network_interface_id
}</pre>
			<p>Our Terraform scripts are now complete<a id="_idIndexMarker1139"/> and will be able to spin up multiple EC2 instances, a database, and a load<a id="_idIndexMarker1140"/> balancer while locking down traffic. This is a good template for other projects if you want to get a basic web app with HTTPS and access to a database off the ground with <span class="No-Break">restricted traffic.</span></p>
			<p>Now that we have two different EC2 instances, we are going to have to change our deployment scripts so that both have applications installed in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor239"/>Updating our Python deployment script for multiple EC2 instances</h2>
			<p>There are ways in which<a id="_idIndexMarker1141"/> we can optimize the deployment<a id="_idIndexMarker1142"/> process, such as running multiple processes at the same time. This will speed up your deployment the more EC2 instances that we have. However, we must remember that this is a book on Rust and web programming, with some deployment chapters included so that you can use what you create. We could write an entire book on optimizing and improving our deployment pipeline. When it comes to supporting multiple EC2 instances in our <strong class="source-inline">deployment/run_build.py</strong> file, at the end of the file, we merely loop through the list of global IPs from the output with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
for server_ip in data["ec2_global_ips"]["value"][0]:
    print("waiting for server to be built")
    time.sleep(5)
    print("attempting to enter server")
    build_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp; sh
        ./run_build.sh
                            {server_ip} {args.u} {args.p}",
                            shell=True)
    build_process.wait()</pre>
			<p>This is it. Multiple servers are now supported. Here, we can see the effectiveness of separating the logic behind managing the data around the deployment in the Python file and the individual Bash script for deploying the applications on the individual server. Keeping things<a id="_idIndexMarker1143"/> isolated keeps technical debt<a id="_idIndexMarker1144"/> down, enabling easy refactoring. Now, all our code infrastructure is done! We can run this Python script and deploy our build onto AWS. Everything is nearly done; all we must do is connect our URL to our load balancer in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor240"/>Attaching our URL to the load balancer</h2>
			<p>This is the home<a id="_idIndexMarker1145"/> run. We are finally<a id="_idIndexMarker1146"/> near the end. I appreciate you sticking with me on this chapter as it is not as exciting as coding in Rust. However, it is important if you want to use your Rust server. To connect our URL to the load balancer, we must navigate to the hosted zone for your URL. Once there, click on the <strong class="bold">Create record</strong> button. When in the <strong class="bold">Create record</strong> display, if you are not using the wizard, click on the <strong class="bold">Switch to wizard</strong> link at the top right of the <strong class="bold">Create record</strong> display to get the following <span class="No-Break">wizard view:</span></p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/Figure_11.26_B18722.jpg" alt="Figure 11.26 – Create record wizard view"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.26 – Create record wizard view</p>
			<p>Here, we can see a range of fancy<a id="_idIndexMarker1147"/> ways to route traffic. However, we are going<a id="_idIndexMarker1148"/> to merely select <strong class="bold">Simple routing</strong> as we just need to pass traffic to the load balancer, which is doing the distribution of traffic between EC2 instances. Selecting <strong class="bold">Simple routing</strong> gives us the following form to <span class="No-Break">fill in:</span></p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/Figure_11.27_B18722.jpg" alt="Figure 11.27 – Define simple record wizard view"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.27 – Define simple record wizard view</p>
			<p>Here, we can see<a id="_idIndexMarker1149"/> that I have selected <strong class="bold">Alias to Application and Classic Load Balancer</strong> to route<a id="_idIndexMarker1150"/> traffic to. I have then selected the location where the load balancer is. This has given me a dropdown where I can see my <strong class="source-inline">ToDoApplicationLb</strong> load balancer to select. When you click the <strong class="bold">Define simple record</strong> button, you are then navigated to a list of records that are to be created. We carry out the creation wizard process one more time to account for all prefixes with the wildcard, and then confirm our creation of records. With this, our HTTPS now works with our application, as <span class="No-Break">seen here:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/Figure_11.28_B18722.jpg" alt="Figure 11.28 – HTTPS working with our application on the internet"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.28 – HTTPS working with our application on the internet</p>
			<p>With this, our chapter is complete. If you try to access<a id="_idIndexMarker1151"/> one of our EC2 instances directly through an IP<a id="_idIndexMarker1152"/> address, you will be blocked. Therefore, we cannot directly access our EC2 instances but can access them through HTTPS through our URL. If you give users any variation of your URL, even an HTTP link to the URL, your users will happily use your application with the <span class="No-Break">HTTPS protocol.</span></p>
			<h1 id="_idParaDest-240"><a id="_idTextAnchor241"/>Summary</h1>
			<p>For now, we have done all we need to do to deploy a robust and secure application on AWS with locked-down traffic and HTTPS enforced. We have covered a lot to get here, and the skillset that you have gained in this chapter can be applied to pretty much any other project you want to deploy on AWS if you can package it in Docker. You now understand the advantages of HTTPS and the steps needed to not only achieve the HTTPS protocol but also to map your URL to the IP address of a server or a load balancer. What is more, we automated the attachment of certificates that we created using Certificate Manager to our load balancer in Terraform using the powerful data query resource that Terraform has to offer. Finally, this all came together when we managed to access our application using HTTPS and only HTTPS. Not only have we developed some practical skills that will become useful in many future projects, but we have also explored the nature of how HTTPS and DNS work, giving us a deeper understanding and appreciation of how the internet generally works when we type in a URL to <span class="No-Break">the browser.</span></p>
			<p>In the next chapter, we will explore the Rocket framework. Due to how we have built our Rust modules in our Actix web application, we will be able to lift modules directly from the Actix web application and slot them into the Rocket application. Considering what we have done in this chapter, we will also be able to wrap our Rocket application in Docker and slot it into the build pipeline here by merely changing one line of code in the deployment <strong class="source-inline">docker-compose</strong> file. In the next chapter, you will see firsthand that when everything is well structured and isolated, changing features and frameworks is not going to be a headache and is, in fact, <span class="No-Break">fairly joyful.</span></p>
			<h1 id="_idParaDest-241"><a id="_idTextAnchor242"/>Further reading</h1>
			<ul>
				<li>Application load balancers <span class="No-Break">documentation: </span><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/load-balancer-types.html"><span class="No-Break">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/load-balancer-types.html</span></a></li>
				<li>Security group <span class="No-Break">documentation: </span><a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html"><span class="No-Break">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</span></a></li>
			</ul>
			<h1 id="_idParaDest-242"><a id="_idTextAnchor243"/>Questions</h1>
			<ol>
				<li value="1">Once we remember that our system still has the database exposed to the rest of the world, how would we lock <span class="No-Break">it down?</span></li>
				<li>Why should we use an elastic IP in <span class="No-Break">some cases?</span></li>
				<li>How could we automate the association of an existing elastic IP to an <span class="No-Break">EC2 instance?</span></li>
				<li>How do we utilize security groups to lock down traffic between a URL and <span class="No-Break">EC2 instances?</span></li>
				<li>Let’s say that the traffic for our application greatly increased and our two instances cannot handle the pressure. What could we do to enable our system to handle <span class="No-Break">the increase?</span></li>
				<li>What is the basic premise of a <span class="No-Break">DNS record?</span></li>
				<li>What is a URL <span class="No-Break">hosted zone?</span></li>
			</ol>
			<h1 id="_idParaDest-243"><a id="_idTextAnchor244"/>Answers</h1>
			<ol>
				<li value="1">We would create a security group for the database that only accepts traffic to and from the EC2 instance security group. If we were having to make a migration, we can SSH into an EC2 instance and use it as a proxy to connect to a database. You can also do this with database viewing software such as DataGrip. Sometimes you can have an EC2 instance that is just there for a user to use as a proxy to access the database. This is known as a <span class="No-Break">bastion server.</span></li>
				<li>When we destroy and create EC2 instances, the IP address of the EC2 instance will change. We can use an elastic IP to ensure that the IP address remains consistent, which can be helpful for automation pipelines as we can continue to point to that IP address. However, if we are using a load balancer, we do not need to use an elastic IP address as we will point our URL to the <span class="No-Break">load balancer.</span></li>
				<li>We can automate the association of an elastic IP to an EC2 instance using Terraform and its powerful data resource. This means we get the data of the existing elastic IP with a data query, and then attach this IP to an EC2 instance that we <span class="No-Break">are creating.</span></li>
				<li>Ensure that the security group for the EC2 instances in the target group that the load balancer is attached to can only accept HTTP traffic from the <span class="No-Break">load balancer.</span></li>
				<li>Considering that everything in our system is automated and piped into each other, all we must do is increase the count in our EC2 instance definition from 2 to 3. This will increase the number of EC2 instances handling the traffic from 2 <span class="No-Break">to 3.</span></li>
				<li>A DNS record is essentially a routing rule that will tell us how to route traffic from the URL to a server IP, URL namespace, or <span class="No-Break">AWS service.</span></li>
				<li>A URL hosted zone is a collection of DNS records for <span class="No-Break">a URL.</span></li>
			</ol>
		</div>
	

		<div id="_idContainer157" class="Content">
			<h1 id="_idParaDest-244"><a id="_idTextAnchor245"/>Part 5:Making Our Projects Flexible</h1>
			<p>We now have a fully working application deployed on AWS with HTTPS and a database. However, there are a lot of moving parts to get there. In this part, we transfer the application that we have built over the book into a Rocket application to see how structuring our code well will give us flexibility when choosing a web framework. We then cover practices on how to keep our web application repository clean and flexible with build/deployment pipelines and automated migration Docker containers that fire once to apply database migrations and then die. We also cover multi-stage builds and how to build distroless server Docker images that are roughly 50 MB.  </p>
			<p>This part includes the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B18722_12.xhtml#_idTextAnchor246"><em class="italic">Chapter 12</em></a>, <em class="italic">Recreating Our Application in Rocket</em></li>
				<li><a href="B18722_13.xhtml#_idTextAnchor264"><em class="italic">Chapter 13</em></a>, <em class="italic">Best Practices for a Clean Web App Repository</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer158">
			</div>
		</div>
		<div>
			<div id="_idContainer159">
			</div>
		</div>
	</body></html>
<html><head></head><body>
		<div><h1 id="_idParaDest-139" class="chapter-number"><a id="_idTextAnchor138"/>8</h1>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor139"/>Runtimes, Wakers, and the Reactor-Executor Pattern</h1>
			<p>In the previous chapter, we created our own pausable tasks (coroutines) by writing them as state machines. We created a common API for these tasks by requiring them to implement the <code>Future</code> trait. We also showed how we can create these coroutines using some keywords and programmatically rewrite them so that we don’t have to implement these state machines by hand, and instead write our programs pretty much the same way we normally would.</p>
			<p>If we stop for a moment and take a bird’s eye view over what we got so far, it’s conceptually pretty simple: we have an interface for pausable tasks (the <code>Future</code> trait), and we have two keywords (<code>coroutine/wait</code>) to indicate code segments we want rewritten as a state machine that divides our code into segments we can pause between.</p>
			<p>However, we have no event loop, and we have no scheduler yet. In this chapter, we’ll expand on our example and add a runtime that allows us to run our program efficiently and opens up the possibility to schedule tasks concurrently much more efficiently than what we do now.</p>
			<p>This chapter will take you on a journey where we implement our runtime in two stages, gradually making it more useful, efficient, and capable. We’ll start with a brief overview of what runtimes are and why we want to understand some of their characteristics.  We’ll build on what we just learned in <a href="B20892_07.xhtml#_idTextAnchor122"><em class="italic">Chapter 7</em></a>, and show how we can make it much more efficient and avoid continuously polling the future to make it progress by leveraging the knowledge we gained in <a href="B20892_04.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>.</p>
			<p>Next, we’ll show how we can get a more flexible and loosely coupled design by dividing the runtime into two parts: an <strong class="bold">executor</strong> and a <strong class="bold">reactor</strong>.</p>
			<p>In this chapter, you will learn about basic runtime design, reactors, executors, wakers, and spawning, and we’ll build on a lot of the knowledge we’ve gained throughout the book.</p>
			<p>This will be one of the big chapters in this book, not because the topic is too complex or difficult, but because we have quite a bit of code to write. In addition to that, I try to give you a good mental model of what’s happening by providing quite a few diagrams and explaining everything very thoroughly. It’s not one of those chapters you typically blaze through before going to bed, though, but I do promise it’s absolutely worth it in the end.</p>
			<p>The chapter will be divided into the following segments:</p>
			<ul>
				<li>Introduction to runtimes and why we need them</li>
				<li>Improving our base example</li>
				<li>Creating a proper runtime</li>
				<li>Step 1 – Improving our runtime design by adding a Reactor and a Waker</li>
				<li>Step 2 – Implementing a proper Executor</li>
				<li>Step 3 – Implementing a proper Reactor</li>
				<li>Experimenting with our new runtime</li>
			</ul>
			<p>So, let’s dive right in!</p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor140"/>Technical requirements</h1>
			<p>The examples in this chapter will build on the code from our last chapter, so the requirements are the same. The examples will all be cross-platform and work on all platforms that Rust (<a href="https://doc.rust-lang.org/beta/rustc/platform-support.html#tier-1-with-host-tools">https://doc.rust-lang.org/beta/rustc/platform-support.html#tier-1-with-host-tools</a>) and <code>mio</code> (<a href="https://github.com/tokio-rs/mio#platforms">https://github.com/tokio-rs/mio#platforms</a>) supports. The only thing you need is Rust installed and the repository that belongs to the book downloaded locally. All the code in this chapter will be found in the <code>ch08</code> folder.</p>
			<p>To follow the examples step by step, you’ll also need <code>corofy</code> installed on your machine. If you didn’t install it in <a href="B20892_07.xhtml#_idTextAnchor122"><em class="italic">Chapter 7</em></a>, install it now by going into the <code>ch08/corofy</code> folder in the repository and running this command:</p>
			<pre class="console">
cargo install --force --path .</pre>			<p>Alternatively, you can just copy the relevant files in the repository when we come to the points where we use <code>corofy</code> to rewrite our <code>coroutine/wait</code> syntax. Both versions will be available to you there as well.</p>
			<p>We’ll also use <code>delayserver</code> in this example, so you need to open a separate terminal, enter the <code>delayserver</code> folder at the root of the repository, and write <code>cargo run</code> so that it’s ready and available for the examples going forward.</p>
			<p>Remember to change the ports in the code if you for some reason have to change the port <code>delayserver</code> listens on.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor141"/>Introduction to runtimes and why we need them</h1>
			<p>As you know by now, you<a id="_idIndexMarker463"/> need to bring your own runtime for driving and scheduling asynchronous tasks in Rust.</p>
			<p>Runtimes come in many flavors, from the<a id="_idIndexMarker464"/> popular <strong class="bold">Embassy</strong> embedded runtime (<a href="https://github.com/embassy-rs/embassy">https://github.com/embassy-rs/embassy</a>), which centers more on general multitasking and <a id="_idIndexMarker465"/>can replace the need for a <strong class="bold">real-time operating system</strong> (<strong class="bold">RTOS</strong>) on many platforms, to <strong class="bold">Tokio</strong> (<a href="https://github.com/tokio-rs/tokio">https://github.com/tokio-rs/tokio</a>), which centers on non-blocking I/O on popular server and desktop operating systems.</p>
			<p>All runtimes in Rust need to do at least two things: schedule and drive objects implementing Rust’s <code>Future</code> trait to completion. Going forward in this chapter, we’ll mostly focus on runtimes for doing non-blocking I/O on popular desktop and server operating systems such as Windows, Linux, and macOS. This is also by far the most common type of runtime most programmers will encounter in Rust.</p>
			<p>Taking control over how tasks are scheduled is <em class="italic">very</em> invasive, and it’s pretty much a one-way street. If you rely on a userland scheduler to run your tasks, you cannot, at the same time, use the OS scheduler (without jumping through several hoops), since mixing them in your code will wreak havoc and might end up defeating the whole purpose of writing an asynchronous program.</p>
			<p>The following diagram illustrates the different schedulers:</p>
			<div><div><img src="img/B20892_09_1.jpg" alt="Figure 8.1 – Task scheduling in a single-threaded asynchronous system"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Task scheduling in a single-threaded asynchronous system</p>
			<p>An example of yielding to the OS<a id="_idIndexMarker466"/> scheduler is making a blocking call using the default <code>std::net</code><code> ::TcpStream</code> or <code>std::thread::sleep</code> methods. Even <em class="italic">potentially</em> blocking calls using primitives such as <code>Mutex</code> provided by the standard library might yield to the OS scheduler.</p>
			<p>That’s why you’ll often find that asynchronous programming tends to color everything it touches, and it’s tough to only run a part of your program using <code>async/await</code>.</p>
			<p>The consequence is that runtimes must use a non-blocking version of the standard library. In theory, you could make one non-blocking version of the standard library that all runtimes use, and that was one of the goals of the <code>async_std</code> initiative (<a href="https://book.async.rs/introduction">https://book.async.rs/introduction</a>). However, having the community agree upon one way to solve this task was a tall order and one that hasn’t really come to fruition yet.</p>
			<p>Before we start implementing our <a id="_idIndexMarker467"/>examples, we’ll discuss the overall design of a typical async runtime in Rust. Most runtimes such as Tokio, Smol, or async-std will divide their runtime into two parts.</p>
			<p>The part that tracks events we’re waiting on and makes sure to wait on notifications from the OS in an efficient<a id="_idIndexMarker468"/> manner is <a id="_idIndexMarker469"/>often called the <em class="italic">reactor</em> or <em class="italic">driver</em>.</p>
			<p>The part that schedules tasks and polls them to completion<a id="_idIndexMarker470"/> is called the <em class="italic">executor</em>.</p>
			<p>Let’s take a high-level look at this design so that we know what we’ll be implementing in our example.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor142"/>Reactors and executors</h2>
			<p>Dividing the runtime into two distinct parts makes a lot of sense when we take a look at how Rust models asynchronous tasks. If you read the documentation for <code>Future</code> (<a href="https://doc.rust-lang.org/std/future/trait.Future.html">https://doc.rust-lang.org/std/future/trait.Future.html</a>) and <code>Waker</code> (<a href="https://doc.rust-lang.org/std/task/struct.Waker.html">https://doc.rust-lang.org/std/task/struct.Waker.html</a>), you’ll see that Rust doesn’t only define a <code>Future</code> trait and a <code>Waker</code> type but also comes with important information on how they’re supposed to be used.</p>
			<p>One example of this is that <code>Future</code> traits are <em class="italic">inert</em>, as we covered in <a href="B20892_06.xhtml#_idTextAnchor113"><em class="italic">Chapter 6</em></a>. Another example is that a call to <code>Waker::wake</code> will guarantee <em class="italic">at least one call</em> to <code>Future::poll</code> on the corresponding task.</p>
			<p>So, already by reading the documentation, you will see that there is at least some thought put into how runtimes should behave.</p>
			<p>The reason for learning this pattern is that it’s almost a glove-to-hand fit for Rust’s asynchronous model.</p>
			<p>Since many readers, including me, will not have English as a first language, I’ll explain the names here at the start since, well, they seem to be easy to misunderstand.</p>
			<p>If the name <code>READABLE</code> event on <code>TcpStream</code>.</p>
			<p>You could have several kinds of reactors running in the same runtime.</p>
			<p>If the name <strong class="bold">executor</strong> gives you <a id="_idIndexMarker472"/>associations to <em class="italic">executioners</em> (the medieval times kind) or <em class="italic">executables</em>, drop that thought as well. If you look up what an executor is, it’s a person, often a lawyer, who administers a person’s will. Most often, since that person is dead. Which is also the point where whatever mental model the naming suggests to you falls apart since nothing, and no one, needs to come in harm’s way for the executor to have work to do in an asynchronous runtime, but I digress.</p>
			<p>The important point is that an executor simply decides who gets time on the CPU to progress and when they get it. The executor must also call <code>Future::poll</code> and advance the state machines to their next state. It’s a type of scheduler.</p>
			<p>It can be frustrating to get the wrong idea from the start since the subject matter is already complex enough without thinking about how on earth nuclear reactors and executioners fit in the whole picture.</p>
			<p>Since reactors will respond to events, they need some integration with the <em class="italic">source</em> of the event. If we continue using <code>TcpStream</code> as an example, something will call <em class="italic">read</em> or <em class="italic">write</em> on it, and at that point, the reactor needs to know that it should track certain events on that source.</p>
			<p>For this reason, non-blocking I/O primitives and reactors need tight integration, and depending on how you look at it, the I/O primitives will either have to bring their own reactor or you’ll have a reactor that provides I/O primitives such as sockets, ports, and streams.</p>
			<p>Now that we’ve covered some of the overarching design, we can start writing some code.</p>
			<p>Runtimes tend to get complex pretty quickly, so to keep this as simple as possible, we’ll avoid any error handling in our code and use <code>unwrap</code> or <code>expect</code> for everything. We’ll also choose simplicity over cleverness and readability over efficiency to the best of our abilities.</p>
			<p>Our first task will be to take the first example we wrote in <a href="B20892_07.xhtml#_idTextAnchor122"><em class="italic">Chapter 7</em></a> and improve it by avoiding having to actively poll it to make progress. Instead, we lean on what we learned about non-blocking I/O and <code>epoll</code> in the earlier chapters.</p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor143"/>Improving our base example</h1>
			<p>We’ll create a version of the first example in <a href="B20892_07.xhtml#_idTextAnchor122"><em class="italic">Chapter 7</em></a> since it’s the simplest one to start with. Our only focus is<a id="_idIndexMarker473"/> showing how to schedule and drive the runtimes more efficiently.</p>
			<p>We start with the following steps:</p>
			<ol>
				<li>Create a new project and name it <code>a-runtime</code> (alternatively, navigate to <code>ch08/a-runtime</code> in the book’s repository).</li>
				<li>Copy the <code>future.rs</code> and <code>http.rs</code> files in the <code>src</code> folder from the first project we created in <a href="B20892_07.xhtml#_idTextAnchor122"><em class="italic">Chapter 7</em></a>, named <code>a-coroutine</code> (alternatively, copy the files from <code>ch07/a-coroutine</code> in the book’s repository) to the <code>src</code> folder in our new project.</li>
				<li>Make sure to add <code>mio</code> as a dependency by adding the following to <code>Cargo.toml</code>:<pre class="source-code">
[dependencies]
mio = { version = "0.8", features = ["net", "os-poll"] }</pre></li>				<li>Create a new file in the <code>src</code> folder called <code>runtime.rs</code>.</li>
			</ol>
			<p>We’ll use <code>corofy</code> to change the following <code>coroutine/wait</code> program into its state machine representation that we can run.</p>
			<p>In <code>src/main.rs</code>, add the following code:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/a-runtime/src/main.rs</p>
			<pre class="source-code">
mod future;
mod http;
mod runtime;
use future::{Future, PollState};
use runtime::Runtime;
fn main() {
    let future = async_main();
    let mut runtime = Runtime::new();
    runtime.block_on(future);
}
coroutine fn async_main() {
    println!("Program starting");
    let txt = http::Http::get("/600/HelloAsyncAwait").wait;
    println!("{txt}");
    let txt = http::Http::get("/400/HelloAsyncAwait").wait;
    println!("{txt}");
}</pre>			<p>This program is basically the same<a id="_idIndexMarker474"/> one we created in <a href="B20892_07.xhtml#_idTextAnchor122"><em class="italic">Chapter 7</em></a>, only this time, we create it from our <code>coroutine/wait</code> syntax instead of writing the state machine by hand. Next, we need to transform this into code by using <code>corofy</code> since the compiler doesn’t recognize our own <code>coroutine/wait</code> syntax.</p>
			<ol>
				<li>If you’re in the root folder of <code>a-runtime</code>, run <code>corofy ./src/main.rs</code>.</li>
				<li>You should now have a file that’s called <code>main_corofied.rs</code>.</li>
				<li>Delete the code in <code>main.rs</code> and copy the contents of <code>main_corofied.rs</code> into <code>main.rs</code>.</li>
				<li>You can now delete <code>main_corofied.rs</code> since we won’t need it going forward.</li>
			</ol>
			<p>If everything is done right, the project structure should now look like this:</p>
			<pre class="console">
src
 |-- future.rs
 |-- http.rs
 |-- main.rs
 |-- runtime.rs</pre>			<p class="callout-heading">Tip</p>
			<p class="callout">You can always refer to the book’s repository to make sure everything is correct. The correct example is located in the <code>ch08/a-runtime</code> folder. In the repository, you’ll also find a file called <code>main_orig.rs</code> in the root folder that contains the <code>coroutine/wait</code> program if you want to <a id="_idIndexMarker475"/>rerun it or have problems getting everything working correctly.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor144"/>Design</h2>
			<p>Before we go any further, let’s visualize how <a id="_idIndexMarker476"/>our system is currently working if we consider it with two futures created by <code>coroutine/wait</code> and two calls to <code>Http::get</code>. The loop that polls our <code>Future</code> trait to completion in the <code>main</code> function takes the role of the executor in our visualization, and as you see, we have a chain of futures consisting of:</p>
			<ol>
				<li>Non-leaf futures created by <code>async/await</code> (or <code>coroutine/wait</code> in our example) that simply call <code>poll</code> on the next future until it reaches a leaf future</li>
				<li>Leaf futures that poll an actual source that’s either <code>Ready</code> or <code>NotReady</code></li>
			</ol>
			<p>The following diagram shows a simplified overview of our current design:</p>
			<div><div><img src="img/B20892_09_2.jpg" alt="Figure 8.2 – Executor and Future chain: current design"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Executor and Future chain: current design</p>
			<p>If we take a closer look at the future chain, we can see that when a future is polled, it polls all its child futures until it <a id="_idIndexMarker477"/>reaches a leaf future that represents something we’re actually waiting on. If that future returns <code>NotReady</code>, it will propagate that up the chain immediately. However, if it returns <code>Ready</code>, the state machine will advance all the way until the next time a future returns <code>NotReady</code>. The top-level future will not resolve until all child futures have returned <code>Ready</code>.</p>
			<p>The next diagram takes a closer look at the future chain and gives a simplified overview of how it works:</p>
			<div><div><img src="img/B20892_09_3.jpg" alt="Figure 8.3 – Future chain: a detailed view"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Future chain: a detailed view</p>
			<p>The first improvement we’ll make is to avoid the need for continuous polling of our top-level future to drive it <a id="_idIndexMarker478"/>forward.</p>
			<p>We’ll change our design so that it looks more like this:</p>
			<div><div><img src="img/B20892_09_4.jpg" alt="Figure 8.4 – Executor and Future chain: design 2"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Executor and Future chain: design 2</p>
			<p>In this design, we use the knowledge we gained in <a href="B20892_04.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>, but instead of simply relying on <code>epoll</code>, we’ll use <code>mio</code>’s cross-platform abstraction instead. The way it works should be well known to us by now <a id="_idIndexMarker479"/>since we already implemented a simplified version of it earlier.</p>
			<p>Instead of continuously looping and polling our top-level future, we’ll register interest with the <code>Poll</code> instance, and when we get a <code>NotReady</code> result returned, we wait on <code>Poll</code>. This will put the thread to sleep, and no work will be done until the OS wakes us up again to notify us that an event we’re waiting on is ready.</p>
			<p>This design will be much more efficient and scalable.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor145"/>Changing the current implementation</h2>
			<p>Now that we have an overview of our design and know what to do, we can go on and make the necessary changes to our program, so let’s go through each file we need to change. We’ll start with <code>main.rs</code>.</p>
			<h3>main.rs</h3>
			<p>We already made some <a id="_idIndexMarker480"/>changes to <code>main.rs</code> when we ran <code>corofy</code> on our updated <code>coroutine/wait</code> example. I’ll just point out the<a id="_idIndexMarker481"/> change here so that you don’t miss it since there is really nothing more we need to change here.</p>
			<p>Instead of polling the future in the <code>main</code> function, we created a new <code>Runtime</code> struct and passed the future as an argument to the <code>Runtime::block_on</code> method. There are no more changes that we need to in this file. Our <code>main</code> function changed to this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/a-runtime/src/main.rs</p>
			<pre class="source-code">
 fn main() {
    <strong class="bold">let future = async_main();</strong>
<strong class="bold">    let mut runtime = Runtime::new();</strong>
<strong class="bold">    runtime.block_on(future);</strong>
}</pre>			<p>The logic we had in the <code>main</code> function has now moved into the <code>runtime</code> module, and that’s also where we need to change the code that polls the future to completion from what we had earlier.</p>
			<p>The next step will, therefore, be to open <code>runtime.rs</code>.</p>
			<h3>runtime.rs</h3>
			<p>The first thing we do in <code>runtime.rs</code> is pull in the <a id="_idIndexMarker482"/>dependencies we need:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/a-runtime/src/runtime.rs</p>
			<pre class="source-code">
use crate::future::{Future, PollState};
use mio::{Events, Poll, Registry};
use std::sync::OnceLock;</pre>			<p>The next step is to create a static variable called <code>REGISTRY</code>. If you remember, <code>Registry</code> is the way we register interest in events with our <code>Poll</code> instance. We want to register interest in events on our <code>TcpStream</code> when making the actual HTTP <code>GET</code> request. We could have <code>Http::get</code> accept a <code>Registry</code> struct that it stored for later use, but we want to keep the API <a id="_idIndexMarker483"/>clean, and instead, we want to access <code>Registry</code> inside <code>HttpGetFuture</code> without having to pass it around as a reference:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/a-runtime/src/runtime.rs</p>
			<pre class="source-code">
static REGISTRY: OnceLock&lt;Registry&gt; = OnceLock::new();
pub fn registry() -&gt; &amp;'static Registry {
    REGISTRY.get().expect("Called outside a runtime context")
}</pre>			<p>We use <code>std::sync::OnceLock</code> so that we can initialize <code>REGISTRY</code> when the runtime starts, thereby preventing anyone (including ourselves) from calling <code>Http::get</code> without having a <code>Runtime</code> instance running. If we did call <code>Http::get</code> without having our runtime initialized, it would panic since the only public way to access it outside the <code>runtime</code> module is through the <code>pub fn registry(){…}</code> function, and that call would fail.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We might as well have used a thread-local static variable using the <code>thread_local!</code> macro from the standard library, but we’ll need to access this from multiple threads when we expand the example later in this chapter, so we start the design with this in mind.</p>
			<p>The next thing we <a id="_idIndexMarker484"/>add is a <code>Runtime</code> struct:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/a-runtime/src/runtime.rs</p>
			<pre class="source-code">
pub struct Runtime {
    poll: Poll,
}</pre>			<p>For now, our runtime will only store a <code>Poll</code> instance. The interesting part is in the implementation of <code>Runtime</code>. Since it’s not too long, I’ll present the whole implementation here and explain it next:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/a-runtime/src/runtime.rs</p>
			<pre class="source-code">
impl Runtime {
    pub fn new() -&gt; Self {
        let poll = Poll::new().unwrap();
        let registry = poll.registry().try_clone().unwrap();
        REGISTRY.set(registry).unwrap();
        Self { poll }
    }
    pub fn block_on&lt;F&gt;(&amp;mut self, future: F)
    where
        F: Future&lt;Output = String&gt;,
    {
        let mut future = future;
        loop {
            match future.poll() {
                PollState::NotReady =&gt; {
                    println!("Schedule other tasks\n");
                    let mut events = Events::with_capacity(100);
                    self.poll.poll(&amp;mut events, None).unwrap();
                }
                PollState::Ready(_) =&gt; break,
            }
        }
    }
}</pre>			<p>The first thing we do is create a <code>new</code> function. This will initialize our runtime and set everything we need up. We create a new <code>Poll</code> instance, and from the <code>Poll</code> instance, we get an owned version of <code>Registry</code>. If you remember from <a href="B20892_04.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>, this is one of the methods we mentioned but didn’t<a id="_idIndexMarker485"/> implement in our example. However, here, we take advantage of the ability to split the two pieces up.</p>
			<p>We store <code>Registry</code> in the <code>REGISTRY</code> global variable so that we can access it from the <code>http</code> module later on without having a reference to the runtime itself.</p>
			<p>The next function is the <code>block_on</code> function. I’ll go through it step by step:</p>
			<ol>
				<li>First of all, this function takes a generic argument and will block on anything that implements our <code>Future</code> trait with an <code>Output</code> type of <code>String</code> (remember that this is currently the only kind of <code>Future</code> trait we support, so we’ll just return an empty string if there is no data to return).</li>
				<li>Instead of having to take <code>mut future</code> as an argument, we define a variable that we declare as <code>mut</code> in the function<a id="_idIndexMarker486"/> body. It’s just to keep the API slightly cleaner and avoid us having to make minor changes later on.</li>
				<li>Next, we create a loop. We’ll loop until the top-level future we received returns <code>Ready</code>.<p class="list-inset">If the future returns <code>NotReady</code>, we write out a message letting us know that at this point we could do other things, such as processing something unrelated to the future or, more likely, polling another top-level future if our runtime supported multiple top-level futures (don’t worry – it will be explained later on).</p><p class="list-inset">Note that we need to pass in an <code>Events</code> collection to <code>mio</code>’s <code>Poll::poll</code> method, but since there is only one top-level future to run, we don’t really care which event happened; we only care that something happened and that it most likely means that data is ready (remember – we always have to account for false wakeups anyway).</p></li>
			</ol>
			<p>That’s all the changes we need to make to the <code>runtime</code> module for now.</p>
			<p>The last thing we need to do is register <em class="italic">interest</em> for <em class="italic">read</em> events after we’ve written the request to the server in our <code>http</code> module.</p>
			<p>Let’s open <code>http.rs</code> and make some changes.</p>
			<h3>http.rs</h3>
			<p>First of all, let’s adjust our<a id="_idIndexMarker487"/> dependencies so that we pull in everything we need:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/a-runtime/src/http.rs</p>
			<pre class="source-code">
use crate::{future::PollState, <strong class="bold">runtime,</strong> Future};
<strong class="bold">use mio::{Interest, Token};</strong>
use std::io::{ErrorKind, Read, Write};</pre>			<p>We need to add a dependency on our <code>runtime</code> module as well as a few types from <code>mio</code>.</p>
			<p>We only need to make one more change in this file, and that’s in our <code>Future::poll</code> implementation, so let’s go ahead and locate that:</p>
			<p>We made one important change here that I’ve highlighted for you. The implementation is exactly the same, with one<a id="_idIndexMarker488"/> important difference:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/a-runtime/src/http.rs</p>
			<pre class="source-code">
impl Future for HttpGetFuture {
  type Output = String;
  fn poll(&amp;mut self) -&gt; PollState&lt;Self::Output&gt; {
    if self.stream.is_none() {
      println!("FIRST POLL - START OPERATION");
      self.write_request();
      <strong class="bold">runtime::registry()</strong>
<strong class="bold">        .register(self.stream.as_mut().unwrap(), Token(0), Interest::READABLE)</strong>
<strong class="bold">                .unwrap();</strong>
        }
        let mut buff = vec![0u8; 4096];
        loop {
            match self.stream.as_mut().unwrap().read(&amp;mut buff) {
                Ok(0) =&gt; {
                    let s = String::from_utf8_lossy(&amp;self.buffer);
                    break PollState::Ready(s.to_string());
                }
                Ok(n) =&gt; {
                    self.buffer.extend(&amp;buff[0..n]);
                    continue;
                }
                Err(e) if e.kind() == ErrorKind::WouldBlock =&gt; {
                    break PollState::NotReady;
                }
                Err(e) =&gt; panic!("{e:?}"),
            }
        }
    }
}</pre>			<p>On the first poll, after we’ve written the request, we register interest in <code>READABLE</code> events on this <code>TcpStream</code>. We <a id="_idIndexMarker489"/>also removed the line:</p>
			<pre class="source-code">
return PollState::NotReady;</pre>			<p>By removing his line, we’ll poll <code>TcpStream</code> immediately, which makes sense since we don’t really want to return control to our scheduler if we get the response immediately. You wouldn’t go wrong either way here since we registered our <code>TcpStream</code> as an event source with our reactor and would get a wakeup in any case. These changes were the last piece we needed to get our example back up and running.</p>
			<p>If you remember the version from <a href="B20892_07.xhtml#_idTextAnchor122"><em class="italic">Chapter 7</em></a>, we got<a id="_idIndexMarker490"/> the following output:</p>
			<pre class="console">
Program starting
FIRST POLL - START OPERATION
Schedule other tasks
Schedule other tasks
Schedule other tasks
Schedule other tasks
Schedule other tasks
Schedule other tasks
Schedule other tasks
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Thu, 16 Nov xxxx xx:xx:xx GMT
HelloWorld1
FIRST POLL - START OPERATION
Schedule other tasks
Schedule other tasks
Schedule other tasks
Schedule other tasks
Schedule other tasks
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Thu, 16 Nov xxxx xx:xx:xx GMT
HelloWorld2</pre>			<p>In our new and improved version, we<a id="_idIndexMarker491"/> get the following output if we run it with <code>cargo run</code>:</p>
			<pre class="console">
Program starting
FIRST POLL - START OPERATION
Schedule other tasks
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Thu, 16 Nov xxxx xx:xx:xx GMT
HelloAsyncAwait
FIRST POLL - START OPERATION
Schedule other tasks
HTTP/1.1 200 OK
content-length: 11
connection: close
content-type: text/plain; charset=utf-8
date: Thu, 16 Nov xxxx xx:xx:xx GMT
HelloAsyncAwait</pre>			<p class="callout-heading">Note</p>
			<p class="callout">If you run the example on Windows, you’ll see that you get two <code>Schedule other tasks</code> messages after each other. The reason for that is that Windows emits an extra event when the <code>TcpStream</code> is dropped on the server end. This doesn’t happen on Linux. Filtering out these events is quite simple, but we won’t focus on doing that in our example since it’s more of an optimization that we don’t really need for our example to work.</p>
			<p>The thing to make a note of here is how<a id="_idIndexMarker492"/> many times we printed <code>Schedule other tasks</code>. We print this message every time we poll and get <code>NotReady</code>. In the first version, we printed this every 100 ms, but that’s just because we had to delay on each sleep to not get overwhelmed with printouts. Without it, our CPU would work 100% on polling the future.</p>
			<p>If we add a delay, we also add latency even if we make the delay much shorter than 100 ms since we won’t be able to respond to events immediately.</p>
			<p>Our new design makes sure that we respond to events as soon as they’re ready, and we do no unnecessary work.</p>
			<p>So, by making these minor changes, we have already created a much better and more scalable version than we had before.</p>
			<p>This version is fully single-threaded, which keeps things simple and avoids the complexity and overhead synchronization. When you use Tokio’s <code>current-thread</code> scheduler, you get a scheduler that is based on the same idea as we showed here.</p>
			<p>However, there are also some drawbacks to our current implementation, and the most noticeable one is that it requires a very tight integration between the <em class="italic">reactor part</em> and the <em class="italic">executor part</em> of the runtime centered on <code>Poll</code>.</p>
			<p>We want to yield to the OS scheduler <em class="italic">when there is no work to do</em> and have the OS wake us up when an event has happened so that we can progress. In our current design, this is done through blocking on <code>Poll::poll</code>.</p>
			<p>Consequently, both the executor (scheduler) and the reactor must know about <code>Poll</code>. The downside is, then, that if you’ve created an executor that suits a specific use case perfectly and want to allow users to use a different reactor that doesn’t rely on <code>Poll</code>, you can’t.</p>
			<p><em class="italic">More importantly, you might want to run multiple different reactors that wake up the executor for different reasons.</em> You might find that there is something that <code>mio</code> doesn’t support, so you create a different reactor for those tasks. How are they supposed to wake up the executor<a id="_idIndexMarker493"/> when it’s blocking on <code>mio::Poll::poll(...)</code>?</p>
			<p>To give you a few examples, you could use a separate reactor for handling timers (for example, when you want a task to sleep for a given time), or you might want to implement a thread pool for handling CPU-intensive or blocking tasks as a reactor that wakes up the corresponding future when the task is ready.</p>
			<p>To solve these problems, we need a loose coupling between the reactor and executor part of the runtime by having a way to wake up the executor that’s not tightly coupled to a single reactor implementation.</p>
			<p>Let’s look at how we can solve this problem by creating a better runtime design.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor146"/>Creating a proper runtime</h1>
			<p>So, if we visualize the degree of dependency<a id="_idIndexMarker494"/> between the different parts of our runtime, our current design could be described this way:</p>
			<div><div><img src="img/B20892_09_5.jpg" alt="Figure 8.5 – Tight coupling between reactor and executor"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Tight coupling between reactor and executor</p>
			<p>If we want a loose coupling between<a id="_idIndexMarker495"/> the reactor and executor, we need an interface provided to signal the executor that it should wake up when an event that allows a future to progress has occurred. It’s no coincidence that this type is called <code>Waker</code> (<a href="https://doc.rust-lang.org/stable/std/task/struct.Waker.html">https://doc.rust-lang.org/stable/std/task/struct.Waker.html</a>) in Rust’s standard library. If we change our visualization to reflect this, it will look something like this:</p>
			<div><div><img src="img/B20892_09_6.jpg" alt="Figure 8.6 – A loosely coupled reactor and executor"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – A loosely coupled reactor and executor</p>
			<p>It’s no coincidence that we land on the same design as what we have in Rust today. It’s a minimal design from Rust’s <a id="_idIndexMarker496"/>point of view, but it allows for a wide variety of runtime designs without laying too many restrictions for the future.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Even though the design is pretty minimal today from a language perspective, there are plans to stabilize more async-related traits and interfaces in the future.</p>
			<p class="callout">Rust has a working group tasked with including widely used traits and interfaces in the standard library, which you can find more information about here: <a href="https://rust-lang.github.io/wg-async/welcome.html">https://rust-lang.github.io/wg-async/welcome.html</a>. You can also get an overview of items they work on and track their progress here: <a href="https://github.com/orgs/rust-lang/projects/28/views/1">https://github.com/orgs/rust-lang/projects/28/views/1</a>.</p>
			<p class="callout">Maybe you even want to get involved (<a href="https://rust-lang.github.io/wg-async/welcome.html#-getting-involved">https://rust-lang.github.io/wg-async/welcome.html#-getting-involved</a>) in making async Rust better for everyone after reading this book?</p>
			<p>If we change our system <a id="_idIndexMarker497"/>diagram to reflect the changes we need to make to our runtime going forward, it will look like this:</p>
			<div><div><img src="img/B20892_09_7.jpg" alt="Figure 8.7 – Executor and reactor: final design"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Executor and reactor: final design</p>
			<p>We have two parts that have no direct dependency on each other. We have an <code>Executor</code> that schedules tasks and passes on a <code>Waker</code> when polling a future that eventually will be caught and <a id="_idIndexMarker498"/>stored by the <code>Reactor</code>. When the <code>Reactor</code> receives a notification that an event is ready, it locates the <code>Waker</code> associated with that task and calls <code>Wake::wake</code> on it.</p>
			<p>This enables us to:</p>
			<ul>
				<li>Run several OS threads that each have their own executor, but share the same reactor</li>
				<li>Have multiple reactors that handle different kinds of leaf futures and make sure to wake up the correct executor when it can progress</li>
			</ul>
			<p>So, now that we have an idea of what to do, it’s time to start writing it in code.</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor147"/>Step 1 – Improving our runtime design by adding a Reactor and a Waker</h1>
			<p>In this step, we’ll make the<a id="_idIndexMarker499"/> following changes:</p>
			<ol>
				<li>Change the project structure so that it reflects our new design.</li>
				<li>Find a way for the executor to sleep and wake up that does not rely directly on <code>Poll</code> and create a <code>Waker</code> based on this that allows us to wake up the executor and identify which task is ready to progress.</li>
				<li>Change the trait definition for <code>Future</code> so that poll takes a <code>&amp;Waker</code> as an argument.</li>
			</ol>
			<p class="callout-heading">Tip</p>
			<p class="callout">You’ll find this example in the <code>ch08/b-reactor-executor</code> folder. If you follow along by writing the examples from the book, I suggest that you create a new project called <code>b-reactor-executor</code> for this example by following these steps:</p>
			<p class="callout">     1. Create a new folder called <code>b-reactor-executor</code>.</p>
			<p class="callout">     2. Enter the newly created folder and write <code>cargo init</code>.</p>
			<p class="callout">     3. Copy everything in the <code>src</code> folder in the previous example, <code>a-runtime</code>, into the <code>src</code> folder of a new project.</p>
			<p class="callout">     4. Copy the <code>dependencies</code> section of the <code>Cargo.toml</code> file into the <code>Cargo.toml</code> file in the new project.</p>
			<p>Let’s start by making some<a id="_idIndexMarker500"/> changes to our project structure to set it up so that we can build on it going forward. The first thing we do is divide our <code>runtime</code> module into two submodules, <code>reactor</code> and <code>executor</code>:</p>
			<ol>
				<li>Create a new subfolder in the <code>src</code> folder called <code>runtime</code>.</li>
				<li>Create two new files in the <code>runtime</code> folder called <code>reactor.rs</code> and <code>executor.rs</code>.</li>
				<li>Just below the imports in <code>runtime.rs</code>, declare the two new modules by adding these lines:<pre class="source-code">
mod executor;
mod reactor;</pre></li>			</ol>
			<p>You should now have a folder structure that looks like this:</p>
			<pre class="console">
src
 |-- runtime
        |-- executor.rs
        |-- reactor.rs
 |-- future.rs
 |-- http.rs
 |-- main.rs
 |-- runtime.rs</pre>			<p>To set everything up, we start<a id="_idIndexMarker501"/> by deleting everything in <code>runtime.rs</code> and replacing it with the following lines of code:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime.rs</p>
			<pre class="source-code">
pub use executor::{spawn, Executor, Waker};
pub use reactor::reactor;
mod executor;
mod reactor;
pub fn init() -&gt; Executor {
    reactor::start();
    Executor::new()
}</pre>			<p>The new content of <code>runtime.rs</code> first declares two submodules called <code>executor</code> and <code>reactor</code>. We then declare one function called <code>init</code> that starts our <code>Reactor</code> and creates a new <code>Executor</code> that it returns to the caller.</p>
			<p>The next point on our list is to find a way for our <code>Executor</code> to sleep and wake up when needed without relying on <code>Poll</code>.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor148"/>Creating a Waker</h2>
			<p>So, we need to find a different way for our <a id="_idIndexMarker502"/>executor to sleep and get woken up that doesn’t rely directly on <code>Poll</code>.</p>
			<p>It turns out that this is quite easy. The standard library gives us what we need to get something working. By calling <code>std::thread::current()</code>, we can get a <code>Thread</code> object. This object is a handle to the current thread, and it gives us access to a few methods, one of which is <code>unpark</code>.</p>
			<p>The standard library also gives us a method called <code>std::thread::park()</code>, which simply asks the OS scheduler to park our thread until we ask for it to get <em class="italic">unparked</em> later on.</p>
			<p>It turns out that if we combine these, we have a way to both <em class="italic">park</em> and <em class="italic">unpark</em> the executor, which is exactly what we need.</p>
			<p>Let’s create a <code>Waker</code> type based on this. In our example, we’ll define the <code>Waker</code> inside the <code>executor</code> module <a id="_idIndexMarker503"/>since that’s where we create this exact type of <code>Waker</code>, but you could argue that it belongs to the <code>future</code> module since it’s a part of the <code>Future</code> trait.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Our <code>Waker</code> relies on calling <code>park/unpark</code> on the <code>Thread</code> type from the standard library. This is OK for our example since it’s easy to understand, but given that any part of the code (including any libraries you use) can get a handle to the same thread by calling <code>std::thread::current()</code> and call <code>park/unpark</code> on it, it’s not a robust solution. If unrelated parts of the code call <code>park/unpark</code> on the same thread, we can miss wakeups or end up in deadlocks. Most production libraries create their own <code>Parker</code> type or rely on something such as <code>crossbeam::sync::Parker</code> (<a href="https://docs.rs/crossbeam/latest/crossbeam/sync/struct.Parker.html">https://docs.rs/crossbeam/latest/crossbeam/sync/struct.Parker.html</a>) instead.</p>
			<p>We won’t implement <code>Waker</code> as a trait since passing trait objects around will significantly increase the complexity of our example, and it’s not in line with the current design of <code>Future</code> and <code>Waker</code> in Rust either.</p>
			<p>Open the <code>executor.rs</code> file located inside the <code>runtime</code> folder, and let’s add all the imports we’re going to need right from the start:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/executor.rs</p>
			<pre class="source-code">
use crate::future::{Future, PollState};
use std::{
    cell::{Cell, RefCell},
    collections::HashMap,
    sync::{Arc, Mutex},
    thread::{self, Thread},
};</pre>			<p>The next thing we add is our <code>Waker</code>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/executor.rs</p>
			<pre class="source-code">
#[derive(Clone)]
pub struct Waker {
    thread: Thread,
    id: usize,
    ready_queue: Arc&lt;Mutex&lt;Vec&lt;usize&gt;&gt;&gt;,
}</pre>			<p>The <code>Waker</code> will hold three things<a id="_idIndexMarker504"/> for us:</p>
			<ul>
				<li><code>thread</code> – A handle to the <code>Thread</code> object we mentioned earlier.</li>
				<li><code>id</code> – An <code>usize</code> that identifies which task this <code>Waker</code> is associated with.</li>
				<li><code>ready_queue</code> – This is a reference that can be shared between threads to a <code>Vec&lt;usize&gt;</code>, where <code>usize</code> represents the ID of a task that’s in the ready queue. We share this object with the executor so that we can push the task ID associated with the <code>Waker</code> onto that queue when it’s ready.</li>
			</ul>
			<p>The implementation of our <code>Waker</code> will be quite simple:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/executor.rs</p>
			<pre class="source-code">
impl Waker {
    pub fn wake(&amp;self) {
        self.ready_queue
            .lock()
            .map(|mut q| q.push(self.id))
            .unwrap();
        self.thread.unpark();
    }
}</pre>			<p>When <code>Waker::wake</code> is called, we first take a lock on the <code>Mutex</code> that protects the ready queue we share with the executor. We then push the <code>id</code> value that identifies the task that this <code>Waker</code> is associated <a id="_idIndexMarker505"/>with onto the ready queue.</p>
			<p>After that’s done, we call <code>unpark</code> on the executor thread and wake it up. It will now find the task associated with this <code>Waker</code> in the ready queue and call <code>poll</code> on it.</p>
			<p>It’s worth mentioning that many designs take a <em class="italic">shared reference (for example, an Arc&lt;…&gt;)</em> to the <em class="italic">future/task itself</em>, and push that onto the queue. By doing so, they skip a level of indirection that we get here by representing the task as a <code>usize</code> instead of passing in a reference to it.</p>
			<p>However, I personally think this way of doing it is easier to understand and reason about, and the end result will be the same.</p>
			<p class="callout-heading">How does this Waker compare to the one in the standard library?</p>
			<p class="callout">The <code>Waker</code> we create here will take the same role as<a id="_idIndexMarker506"/> the <code>Waker</code> type from the standard library. The biggest difference is that the <code>std::task::Waker</code> method is wrapped in a <code>Context</code> struct and requires us to jump through a few hoops when we create it ourselves. Don’t worry – we’ll do all this at the end of this book, but neither of these differences is important for understanding the role it plays, so that’s why we stick to our own simplified version of asynchronous Rust for now.</p>
			<p>The last thing we need to do is to change the definition of the <code>Future</code> trait so that it takes <code>&amp;Waker</code> as an argument.</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor149"/>Changing the Future definition</h2>
			<p>Since our <code>Future</code> definition is in the <code>future.rs</code> file, we start by opening that file.</p>
			<p>The first thing we need to<a id="_idIndexMarker507"/> change is to pull in the <code>Waker</code> so that we can use it. At the top of the file, add the following code:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/future.rs</p>
			<pre class="source-code">
use crate::runtime::Waker;</pre>			<p>The next thing we do is to change our <code>Future</code> trait so that it takes <code>&amp;Waker</code> as an argument:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/future.rs</p>
			<pre class="source-code">
pub trait Future {
    type Output;
    fn poll(&amp;mut self<strong class="bold">, waker: &amp;Waker</strong>) -&gt; PollState&lt;Self::Output&gt;;
}</pre>			<p>At this point, you have a choice. We won’t be using the <code>join_all</code> function or the <code>JoinAll&lt;F: Future&gt;</code> struct going forward.</p>
			<p>If you don’t want to keep them, you can just delete everything related to <code>join_all</code>, and that’s all you need to do in <code>future.rs</code>.</p>
			<p>If you want to keep them for further experimentation, you need to change the <code>Future</code> implementation for <code>JoinAll</code> so that it accepts a <code>waker: &amp;Waker</code> argument, and remember to pass the <code>Waker</code> when polling the joined futures in <code>match fut.poll(waker)</code>.</p>
			<p>The remaining things to do in <em class="italic">step 1</em> are to make some minor changes where we implement the <code>Future</code> trait.</p>
			<p>Let’s start in <code>http.rs</code>. The first thing we do is adjust our dependencies a little to reflect the changes we made to our <code>runtime</code> module, and we add a dependency on our new <code>Waker</code>. Replace the <code>dependencies</code> section at the top of the file with this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/http.rs</p>
			<pre class="source-code">
use crate::{future::PollState, runtime::{self, reactor, Waker}, Future};
use mio::Interest;
use std::io::{ErrorKind, Read, Write};</pre>			<p>The compiler will complain about not<a id="_idIndexMarker508"/> finding the reactor yet, but we’ll get to that shortly.</p>
			<p>Next, we have to navigate to the <code>impl Future for HttpGetFuture</code> block, where we need to change the <code>poll</code> method so that it accepts a <code>&amp;</code><code>Waker</code> argument:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/http.rs</p>
			<pre class="source-code">
impl Future for HttpGetFuture {
    type Output = String;
    fn poll(&amp;mut self<strong class="bold">, waker: &amp;Waker</strong>) -&gt; PollState&lt;Self::Output&gt; {
…</pre>			<p>The last file we need to change is <code>main.rs</code>. Since <code>corofy</code> doesn’t know about <code>Waker</code> types, we need to change a few lines in the coroutines it generated for us in <code>main.rs</code>.</p>
			<p>First of all, we have to add a dependency on our new <code>Waker</code>, so add this at the start of the file:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/main.rs</p>
			<pre class="source-code">
use runtime::Waker;</pre>			<p>In the <code>impl Future for Coroutine</code>block, change the following three lines of code that I’ve highlighted:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/main.rs</p>
			<pre class="source-code">
fn poll(&amp;mut self<strong class="bold">, waker: &amp;Waker</strong>)
match f1.poll(<strong class="bold">waker</strong>)
match f2.poll(<code>Waker</code>.</p>
			<p>The next step will be to create a proper <code>Executor</code>.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor150"/>Step 2 – Implementing a proper Executor</h1>
			<p>In this step, we’ll create an executor that will:</p>
			<ul>
				<li>Hold many top-level futures and <a id="_idIndexMarker510"/>switch between them</li>
				<li>Enable us to spawn new top-level futures from anywhere in our asynchronous program</li>
				<li>Hand out <code>Waker</code> types so that they can sleep when there is nothing to do and wake up when one of the top-level futures can progress</li>
				<li>Enable us to run several executors by having each run on its dedicated OS thread</li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">It’s worth mentioning that our executor won’t be fully multithreaded in the sense that tasks/futures can’t be sent from one thread to another, and the different <code>Executor</code> instances will not know of each other. Therefore, executors can’t steal work from each other (no work-stealing), and we can’t rely on executors picking tasks from a global task queue.</p>
			<p class="callout">The reason is that the <code>Executor</code> design will be much more complex if we go down that route, not only because of the added logic but also because we have to add constraints, such as requiring everything to be <code>Send + </code><code>Sync</code>.</p>
			<p class="callout">Some of the complexity in asynchronous Rust today can be attributed to the fact that many runtimes in Rust are multithreaded by default, which makes asynchronous Rust deviate more from “normal” Rust than it actually needs to.</p>
			<p class="callout">It’s worth mentioning that since <a id="_idIndexMarker511"/>most production runtimes in Rust are multithreaded by default, most of them also have a work-stealing executor. This will be similar to the last version of our bartender example in <a href="B20892_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, where we achieved a slightly increased efficiency by letting the bartenders “steal” tasks that are <em class="italic">in progress</em> from each other.</p>
			<p class="callout">However, this example should still give you an idea of how we can leverage all the cores on a machine to run asynchronous tasks, giving us both concurrency and parallelism, even though it will have limited capabilities.</p>
			<p>Let’s start by opening up <code>executor.rs</code> located in the <code>runtime</code> subfolder.</p>
			<p>This file should already contain our <code>Waker</code> and the dependencies we need, so let’s start by adding the following lines of code just below our dependencies:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/executor.rs</p>
			<pre class="source-code">
type Task = Box&lt;dyn Future&lt;Output = String&gt;&gt;;
thread_local! {
    static CURRENT_EXEC: ExecutorCore = ExecutorCore::default();
}</pre>			<p>The first line is a <em class="italic">type alias</em>; it simply lets us create an alias called <code>Task</code> that refers to the type: <code>Box&lt;dyn Future&lt;Output = String&gt;&gt;</code>. This will help keep our code a little bit cleaner.</p>
			<p>The next line might be new to some readers. We define a thread-local static variable by using the <code>thread_local!</code> macro.</p>
			<p>The <code>thread_local!</code> macro lets us define a static variable that’s unique to the thread it’s first called from. This means that all threads we create will have their own instance, and it’s impossible for one thread to access another thread’s <code>CURRENT_EXEC</code> variable.</p>
			<p>We call the variable <code>CURRENT_EXEC</code> since it holds the <code>Executor</code> that’s currently running on this thread.</p>
			<p>The next lines we add to this <a id="_idIndexMarker512"/>file is the definition of <code>ExecutorCore</code>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/executor.rs</p>
			<pre class="source-code">
#[derive(Default)]
struct ExecutorCore {
    tasks: RefCell&lt;HashMap&lt;usize, Task&gt;&gt;,
    ready_queue: Arc&lt;Mutex&lt;Vec&lt;usize&gt;&gt;&gt;,
    next_id: Cell&lt;usize&gt;,
}</pre>			<p><code>ExecutorCore</code> holds all the state for our <code>Executor</code>:</p>
			<ul>
				<li><code>tasks</code> – This is a <code>HashMap</code> with a <code>usize</code> as the <em class="italic">key</em> and a <code>Task</code> (remember the alias we created previously) as <em class="italic">data</em>. This will hold all the top-level futures associated with the executor on this thread and allow us to give each an <code>id</code> property to identify them. We can’t simply mutate a static variable, so we need internal mutability here. Since this will only be callable from one thread, a <code>RefCell</code> will do so since there is no need for synchronization.</li>
				<li><code>ready_queue</code> – This is a simple <code>Vec&lt;usize&gt;</code> that stores the IDs of tasks that should be polled by the executor. If we refer back to <em class="italic">Figure 8</em><em class="italic">.7</em>, you’ll see how this fits into the design we outlined there. As mentioned earlier, we could store something such as an <code>Arc&lt;dyn Future&lt;…&gt;&gt;</code> here instead, but that adds quite a bit of complexity to our example. The only downside with the current design is that instead of getting a reference to the task directly, we have to look it up in our <code>tasks</code> collection, which takes time. An <code>Arc&lt;…&gt;</code> (shared reference) to this collection will be given to each <code>Waker</code> that this executor creates. Since the <code>Waker</code> can (and will) be sent to a different thread and signal that a specific task is ready by <a id="_idIndexMarker513"/>adding the task’s ID to <code>ready_queue</code>, we need to wrap it in an <code>Arc&lt;Mutex&lt;…&gt;&gt;</code>.</li>
				<li><code>next_id</code> – This is a counter that gives out the next available I, which means that it should never hand out the same ID twice for this executor instance. We’ll use this to give each top-level future a unique ID. Since the executor instance will only be accessible on the same thread it was created, a simple <code>Cell</code> will suffice in giving us the internal mutability we need.</li>
			</ul>
			<p><code>ExecutorCore</code> derives the <code>Default</code> trait since there is no special initial state we need here, and it keeps the code short and concise.</p>
			<p>The next function is an important one. The <code>spawn</code> function allows us to register new top-level futures with our executor from anywhere in our program:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/executor.rs</p>
			<pre class="source-code">
pub fn spawn&lt;F&gt;(future: F)
where
    F: Future&lt;Output = String&gt; + 'static,
{
    CURRENT_EXEC.with(|e| {
        let id = e.next_id.get();
        e.tasks.borrow_mut().insert(id, Box::new(future));
        e.ready_queue.lock().map(|mut q| q.push(id)).unwrap();
        e.next_id.set(id + 1);
    });
}</pre>			<p>The <code>spawn</code> function does a few things:</p>
			<ul>
				<li>It gets the next available ID.</li>
				<li>It assigns the ID to the future it receives and stores it in a <code>HashMap</code>.</li>
				<li>It adds the ID that represents this task to <code>ready_queue</code> so that it’s polled at least once (remember that <code>Future</code> traits in Rust don’t do anything unless they’re polled at least once).</li>
				<li>It increases the ID counter by one.</li>
			</ul>
			<p>The unfamiliar syntax accessing <code>CURRENT_EXEC</code> by calling <code>with</code> and passing in a closure is just a consequence of how thread local statics is implemented in Rust. You’ll also notice that we must use a<a id="_idIndexMarker514"/> few special methods because we use <code>RefCell</code> and <code>Cell</code> for internal mutability for <code>tasks</code> and <code>next_id</code>, but there is really nothing inherently complex about this except being a bit unfamiliar.</p>
			<p class="callout-heading">A quick note about static lifetimes</p>
			<p class="callout">When a <code>'static</code> lifetime is used as<a id="_idIndexMarker515"/> a trait bound as we do here, it doesn’t actually mean that the lifetime of the <code>Future</code> trait we pass in <em class="italic">must be</em> static (meaning it will have to live until the end of the program). It means that it <em class="italic">must be able to</em> last until the end of the program, or, put another way, the lifetime can’t be constrained in any way.</p>
			<p class="callout">Most often, when you encounter something that requires a <code>'static</code> bound, it simply means that you’ll have to give ownership over the thing you pass in. If you pass in any references, they need to have a <code>'static</code> lifetime. It’s less difficult to satisfy this constraint than you might expect.</p>
			<p>The final part of <em class="italic">step 2</em> will be to define and implement the <code>Executor</code> struct itself.</p>
			<p>The <code>Executor</code> struct is very simple, and there is only one line of code to add:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/executor.rs</p>
			<pre class="source-code">
pub struct Executor;</pre>			<p>Since all the state we need for our example is held in <code>ExecutorCore</code>, which is a static thread-local variable, our <code>Executor</code> struct doesn’t need any state. This also means that we don’t strictly need a struct at all, but to keep the API somewhat familiar, we do it anyway.</p>
			<p>Most of the executor implementation is a handful of simple helper methods that end up in a <code>block_on</code> function, which is<a id="_idIndexMarker516"/> where the interesting parts really happen.</p>
			<p>Since these helper methods are short and easy to understand, I’ll present them all here and just briefly go over what they do:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We open the <code>impl Executor</code> block here but will not close it until we’ve finished implementing the <code>block_on</code> function.</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/executor.rs</p>
			<pre class="source-code">
impl Executor {
    pub fn new() -&gt; Self {
        Self {}
    }
    fn pop_ready(&amp;self) -&gt; Option&lt;usize&gt; {
        CURRENT_EXEC.with(|q| q.ready_queue.lock().map(|mut q| q.pop()).unwrap())
    }
    fn get_future(&amp;self, id: usize) -&gt; Option&lt;Task&gt; {
        CURRENT_EXEC.with(|q| q.tasks.borrow_mut().remove(&amp;id))
    }
    fn get_waker(&amp;self, id: usize) -&gt; Waker {
        Waker {
            id,
            thread: thread::current(),
            ready_queue: CURRENT_EXEC.with(|q| q.ready_queue.clone()),
        }
    }
    fn insert_task(&amp;self, id: usize, task: Task) {
        CURRENT_EXEC.with(|q| q.tasks.borrow_mut().insert(id, task));
    }
    fn task_count(&amp;self) -&gt; usize {
        CURRENT_EXEC.with(|q| q.tasks.borrow().len())
    }</pre>			<p>So, we have six methods here:</p>
			<ul>
				<li><code>new</code> – Creates a new <code>Executor</code> instance. For simplicity, we have no initialization here, and everything is done<a id="_idIndexMarker517"/> lazily by design in the <code>thread_local!</code> macro.</li>
				<li><code>pop_ready</code> – This function takes a lock on <code>read_queue</code> and pops off an ID that’s ready from the back of <code>Vec</code>. Calling <code>pop</code> here means that we also remove the item from the collection. As a side note, since <code>Waker</code> pushes its ID to the <em class="italic">back</em> of <code>ready_queue</code> and we pop off from the <em class="italic">back</em> as well, we essentially get a <code>VecDeque</code> from the standard library would easily allow us to choose the order in which we remove items from the queue if we wish to change that behavior.</li>
				<li><code>get_future</code> – This function <a id="_idIndexMarker519"/>takes the ID of a top-level future as an argument, removes the future from the <code>tasks</code> collection, and returns it (if the task is found). This means that if the task returns <code>NotReady</code> (signaling that we’re not done with it), we need to remember to add it back to the collection again.</li>
				<li><code>get_waker</code> – This function creates a new <code>Waker</code> instance.</li>
				<li><code>insert_task</code> – This function takes an <code>id</code> property and a <code>Task</code> property and inserts them into our <code>tasks</code> collection.</li>
				<li><code>task_count</code> – This function simply returns a count of how many tasks we have in the queue.</li>
			</ul>
			<p>The final and last part of the <code>Executor</code> implementation is the <code>block_on</code> function. This is also where we close the <code>impl </code><code>Executor</code> block:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/executor.rs</p>
			<pre class="source-code">
pub fn block_on&lt;F&gt;(&amp;mut self, future: F)
  where
      F: Future&lt;Output = String&gt; + 'static,
  {
      spawn(future);
      loop {
          while let Some(id) = self.pop_ready() {
        let mut future = match self.get_future(id) {
          Some(f) =&gt; f,
          // guard against false wakeups
          None =&gt; continue,
        };
        let waker = self.get_waker(id);
        match future.poll(&amp;waker) {
          PollState::NotReady =&gt; self.insert_task(id, future),
          PollState::Ready(_) =&gt; continue,
        }
      }
      let task_count = self.task_count();
      let name = thread::current().name().unwrap_or_default().to_string();
      if task_count &gt; 0 {
        println!("{name}: {task_count} pending tasks. Sleep until notified.");
        thread::park();
      } else {
        println!("{name}: All tasks are finished");
        break;
      }
    }
  }
}</pre>			<p><code>block_on</code> will be the entry point to our <code>Executor</code>. Often, you will pass in one top-level future first, and when the top-level future progresses, it will spawn new top-level futures onto our executor. Each new future can, of course, spawn new futures onto the <code>Executor</code> too, and that’s how an asynchronous program basically works.</p>
			<p>In many ways, you can view this first top-level future in the same way you view the <code>main</code> function in a normal Rust program. <code>spawn</code> is similar to <code>thread::spawn</code>, with the exception that the tasks stay on the same OS thread in this example. This means the tasks won’t be able to run in parallel, which in turn allows us to avoid any need for synchronization between tasks to <a id="_idIndexMarker520"/>avoid data races.</p>
			<p>Let’s go through the function step by step:</p>
			<ol>
				<li>The first thing we do is spawn the future we received onto ourselves. There are many ways this could be implemented, but this is the easiest way to do it.</li>
				<li>Then, we have a loop that will run as long as our asynchronous program is running.</li>
				<li>Every time we loop, we create an inner <code>while let Some(…)</code> loop that runs as long as there are tasks in <code>ready_queue</code>.</li>
				<li>If there is a task in <code>ready_queue</code>, we take ownership of the <code>Future</code> object by removing it from the collection. We guard against false wakeups by just continuing if there is no future there anymore (meaning that we’re done with it but still get a wakeup). This will, for example, happen on Windows since we get a <code>READABLE</code> event when the connection closes, but even though we could filter those events out, <code>mio</code> doesn’t guarantee that false wakeups won’t happen, so we have to handle that possibility anyway.</li>
				<li>Next, we create a new <code>Waker</code> instance that we can pass into <code>Future::poll()</code>. Remember that this <code>Waker</code> instance now holds the <code>id</code> property that identifies this specific <code>Future</code> trait and a handle to the thread we’re currently running on.</li>
				<li>The next step is to call <code>Future::poll</code>.</li>
				<li>If we get <code>NotReady</code> in return, we insert the task back into our <code>tasks</code> collection. I want to emphasize that when a <code>Future</code> trait returns <code>NotReady</code>, we know it will arrange it so that <code>Waker::wake</code> is called at a later point in time. It’s not the executor’s responsibility to track the readiness of this future.</li>
				<li>If the <code>Future</code> trait returns <code>Ready</code>, we simply continue to the next item in the ready queue. Since we took ownership over the <code>Future</code> trait, this will drop the object before we enter the next iteration of the <code>while </code><code>let</code> loop.</li>
				<li>Now that we’ve polled all the tasks in our ready queue, the first thing we do is get a task count to see how<a id="_idIndexMarker521"/> many tasks we have left.</li>
				<li>We also get the name of the current thread for future logging purposes (it has nothing to do with how our executor works).</li>
				<li>If the task count is larger than <code>0</code>, we print a message to the terminal and call <code>thread::park()</code>. Parking the thread will yield control to the OS scheduler, and our <code>Executor</code> does nothing until it’s woken up again.</li>
				<li>If the task count is <code>0</code>, we’re done with our asynchronous program and exit the main loop.</li>
			</ol>
			<p>That’s pretty much all there is to it. By this point, we’ve covered all our goals for <em class="italic">step 2</em>, so we can continue to the last and final step and implement a <code>Reactor</code> for our runtime that will wake up our executor when something happens.</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor151"/>Step 3 – Implementing a proper Reactor</h1>
			<p>The final part of our <a id="_idIndexMarker522"/>example is the <code>Reactor</code>. Our <code>Reactor</code> will:</p>
			<ul>
				<li>Efficiently wait and handle events that our runtime is interested in</li>
				<li>Store a collection of <code>Waker</code> types and make sure to wake the correct <code>Waker</code> when it gets a notification on a source it’s tracking</li>
				<li>Provide the necessary mechanisms for leaf futures such as <code>HttpGetFuture</code>, to register and deregister interests in events</li>
				<li>Provide a way for leaf futures to store the last received <code>Waker</code></li>
			</ul>
			<p>When we’re done with this step, we should have everything we need for our runtime, so let’s get to it.</p>
			<p>Start by opening the <code>reactor.rs</code> file.</p>
			<p>The first thing we do is<a id="_idIndexMarker523"/> add the dependencies we need:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/reactor.rs</p>
			<pre class="source-code">
use crate::runtime::Waker;
use mio::{net::TcpStream, Events, Interest, Poll, Registry, Token};
use std::{
    collections::HashMap,
    sync::{
        atomic::{AtomicUsize, Ordering},
        Arc, Mutex, OnceLock,
    },
    thread,
};</pre>			<p>After we’ve added our dependencies, we create a <em class="italic">type alias</em> called <code>Wakers</code> that aliases the type for our <code>wakers</code> collection:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/reactor.rs</p>
			<pre class="source-code">
type Wakers = Arc&lt;Mutex&lt;HashMap&lt;usize, Waker&gt;&gt;&gt;;</pre>			<p>The next line will declare a static variable called <code>REACTOR</code>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/reactor.rs</p>
			<pre class="source-code">
static REACTOR: OnceLock&lt;Reactor&gt; = OnceLock::new();</pre>			<p>This variable will hold a <code>OnceLock&lt;Reactor&gt;</code>. In contrast to our <code>CURRENT_EXEC</code> static variable, this will be possible to access from different threads. <code>OnceLock</code> allows us to define a static variable that we can write to once so that we can initialize it when we start our <code>Reactor</code>. By doing so, we also make sure that there can only be a single instance of this specific reactor<a id="_idIndexMarker524"/> running in our program.</p>
			<p>The variable will be private to this module, so we create a public function allowing other parts of our program to access it:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/reactor.rs</p>
			<pre class="source-code">
pub fn reactor() -&gt; &amp;'static Reactor {
    REACTOR.get().expect("Called outside an runtime context")
}</pre>			<p>The next thing we do is define our <code>Reactor</code> struct:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/reactor.rs</p>
			<pre class="source-code">
pub struct Reactor {
    wakers: Wakers,
    registry: Registry,
    next_id: AtomicUsize,
}</pre>			<p>This will be all the state our <code>Reactor</code> struct needs to hold:</p>
			<ul>
				<li><code>wakers</code> – A <code>HashMap</code> of <code>Waker</code> objects, each identified by an integer</li>
				<li><code>registry</code> – Holds a <code>Registry</code> instance so that we can interact with the event queue in <code>mio</code></li>
				<li><code>next_id</code> – Stores the next available ID so that we can track which event occurred and which <code>Waker</code> should be woken</li>
			</ul>
			<p>The implementation of <code>Reactor</code> is actually<a id="_idIndexMarker525"/> quite simple. It’s only four short methods for interacting with the <code>Reactor</code> instance, so I’ll present them all here and give a brief explanation next:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/reactor.rs</p>
			<pre class="source-code">
impl Reactor {
    pub fn register(&amp;self, stream: &amp;mut TcpStream, interest: Interest, id: usize) {
        self.registry.register(stream, Token(id), interest).unwrap();
    }
    pub fn set_waker(&amp;self, waker: &amp;Waker, id: usize) {
        let _ = self
            .wakers
            .lock()
            .map(|mut w| w.insert(id, waker.clone()).is_none())
            .unwrap();
    }
    pub fn deregister(&amp;self, stream: &amp;mut TcpStream, id: usize) {
        self.wakers.lock().map(|mut w| w.remove(&amp;id)).unwrap();
        self.registry.deregister(stream).unwrap();
    }
    pub fn next_id(&amp;self) -&gt; usize {
        self.next_id.fetch_add(1, Ordering::Relaxed)
    }
}</pre>			<p>Let’s briefly explain what these four methods do:</p>
			<ul>
				<li><code>register</code> – This method is a thin wrapper around <code>Registry::register</code>, which we know from <a href="B20892_04.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>. The one thing to make a note of here is that we pass in an <code>id</code> property so that we can identify which event has occurred when we receive a notification later on.</li>
				<li><code>set_waker</code> – This method<a id="_idIndexMarker526"/> adds a <code>Waker</code> to our <code>HashMap</code> using the provided <code>id</code> property as a key to identify it. If there is a <code>Waker</code> there already, we replace it and drop the old one. An important point to remember is that <code>Waker</code> associated with the <code>TcpStream</code>.</li>
				<li><code>deregister</code> – This function does two things. First, it removes the <code>Waker</code> from our <code>wakers</code> collection. Then, it deregisters the <code>TcpStream</code> from our <code>Poll</code> instance.</li>
				<li>I want to remind you at this point that while we only work with <code>TcpStream</code> in our examples, this could, in theory, be done with anything that implements <code>mio</code>’s <code>Source</code> trait, so the same thought process is valid in a much broader context than what we deal with here.</li>
				<li><code>next_id</code> – This simply gets the current <code>next_id</code> value and increments the counter atomically. We don’t care about any happens before/after relationships happening here; we only care about not handing out the same value twice, so <code>Ordering::Relaxed</code> will suffice here. Memory ordering in atomic operations is a complex topic that we won’t be able to dive into in this book, but if you want to know more about the different memory orderings in Rust and what they mean, the official documentation is the right place to start: <a href="https://doc.rust-lang.org/stable/std/sync/atomic/enum.Ordering.html">https://doc.rust-lang.org/stable/std/sync/atomic/enum.Ordering.html</a>.</li>
			</ul>
			<p>Now that our <code>Reactor</code> is set up, we<a id="_idIndexMarker527"/> only have two short functions left. The first one is <code>event_loop</code>, which will hold the logic for our event loop that waits and reacts to new events:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/reactor.rs</p>
			<pre class="source-code">
fn event_loop(mut poll: Poll, wakers: Wakers) {
    let mut events = Events::with_capacity(100);
    loop {
        poll.poll(&amp;mut events, None).unwrap();
        for e in events.iter() {
            let Token(id) = e.token();
            let wakers = wakers.lock().unwrap();
            if let Some(waker) = wakers.get(&amp;id) {
                waker.wake();
            }
        }
    }
}</pre>			<p>This function takes a <code>Poll</code> instance and a <code>Wakers</code> collection as arguments. Let’s go through it step by step:</p>
			<ul>
				<li>The first thing we do is create an <code>events</code> collection. This should be familiar since we did the exact same thing in <a href="B20892_04.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>.</li>
				<li>The next thing we do is create a <code>loop</code> that in our case will continue to loop for eternity. This makes our example short and simple, but it has the downside that we have no way of shutting our event loop down once it’s started. Fixing that is not especially difficult, but since it won’t be necessary for our example, we don’t cover this here.</li>
				<li>Inside the loop, we call <code>Poll::poll</code> with a timeout of <code>None</code>, which means it will never time out<a id="_idIndexMarker528"/> and block until it receives an event notification.</li>
				<li>When the call returns, we loop through every event we receive.</li>
				<li>If we receive an event, it means that something we registered interest in happened, so we get the <code>id</code> we passed in when we first registered an interest in events on this <code>TcpStream</code>.</li>
				<li>Lastly, we try to get the associated <code>Waker</code> and call <code>Waker::wake</code> on it. We guard ourselves from the fact that the <code>Waker</code> may have been removed from our collection already, in which case we do nothing.</li>
			</ul>
			<p>It’s worth noting that we can filter events if we want to here. Tokio provides some methods on the <code>Event</code> object to check several things about the event it reported. For our use in this example, we don’t need to filter events.</p>
			<p>Finally, the last function is the second public function in this module and the one that initializes and starts the runtime:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/runtime/runtime.rs</p>
			<pre class="source-code">
pub fn start() {
    use thread::spawn;
    let wakers = Arc::new(Mutex::new(HashMap::new()));
    let poll = Poll::new().unwrap();
    let registry = poll.registry().try_clone().unwrap();
    let next_id = AtomicUsize::new(1);
    let reactor = Reactor {
        wakers: wakers.clone(),
        registry,
        next_id,
    };
    REACTOR.set(reactor).ok().expect("Reactor already running");
    spawn(move || event_loop(poll, wakers));
}</pre>			<p>The <code>start</code> method should be fairly easy to understand. The first thing we do is create our <code>Wakers</code> collection and our <code>Poll</code> instance. From the <code>Poll</code> instance, we get an owned version of <code>Registry</code>. We initialize <code>next_id</code> to <code>1</code> (for debugging purposes, I wanted to initialize it to a different start value than our <code>Executor</code>) and create our <code>Reactor</code> object.</p>
			<p>Then, we set the static variable we named <code>REACTOR</code> by giving it our <code>Reactor</code> instance.</p>
			<p>The last thing is probably the <em class="italic">most important one to pay attention to</em>. We spawn a new OS thread and start our <code>event_loop</code> function on that one. This also means that we pass on our <code>Poll</code> instance to <a id="_idIndexMarker529"/>the event loop thread for good.</p>
			<p>Now, the best practice would be to store the <code>JoinHandle</code> returned from <code>spawn</code> so that we can join the thread later on, but our thread has no way to shut down the event loop anyway, so joining it later makes little sense, and we simply discard the handle.</p>
			<p>I don’t know if you agree with me, but the logic here is not that complex when we break it down into smaller pieces. Since we know how <code>epoll</code> and <code>mio</code> work already, the rest is pretty easy to understand.</p>
			<p>Now, we’re not done yet. We still have some small changes to make to our <code>HttpGetFuture</code> leaf future since it doesn’t register with the reactor at the moment. Let’s fix that.</p>
			<p>Start by opening the <code>http.rs</code> file.</p>
			<p>Since we already added the correct imports when we opened the file to adapt everything to the new <code>Future</code> interface, there are<a id="_idIndexMarker530"/> only a few places we need to change that so this leaf future integrates nicely with our reactor.</p>
			<p>The first thing we do is give <code>HttpGetFuture</code> an identity. It’s the source of events we want to track with our <code>Reactor</code>, so we want it to have the same ID until we’re done with it:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/http.rs</p>
			<pre class="source-code">
struct HttpGetFuture {
    stream: Option&lt;mio::net::TcpStream&gt;,
    buffer: Vec&lt;u8&gt;,
    path: String,
    <strong class="bold">id: usize,</strong>
}</pre>			<p>We also need to retrieve a new ID from the reactor when the future is created:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/http.rs</p>
			<pre class="source-code">
impl HttpGetFuture {
    fn new(path: String) -&gt; Self {
        <strong class="bold">let id = reactor().next_id();</strong>
        Self {
            stream: None,
            buffer: vec![],
            path,
            <strong class="bold">id,</strong>
        }
    }</pre>			<p>Next, we have to locate the <code>poll</code> implementation for <code>HttpGetFuture</code>.</p>
			<p>The first thing we need to do is make sure that we register interest with our <code>Poll</code> instance and register the <code>Waker</code> we receive with the <code>Reactor</code> the first time the future gets polled. Since we don’t register directly<a id="_idIndexMarker531"/> with <code>Registry</code> anymore, we remove that line of code and add these new lines instead:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/http.rs</p>
			<pre class="source-code">
if self.stream.is_none() {
            println!("FIRST POLL - START OPERATION");
            self.write_request();
            <strong class="bold">let stream = self.stream.as_mut().unwrap();</strong>
<strong class="bold">            runtime::reactor().register(stream, Interest::READABLE, self.id);</strong>
            <strong class="bold">runtime::reactor().set_waker(waker, self.id);</strong>
        }</pre>			<p>Lastly, we need to make some minor changes to how we handle the different conditions when reading from <code>TcpStream</code>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/http.rs</p>
			<pre class="source-code">
match self.stream.as_mut().unwrap().read(&amp;mut buff) {
                Ok(0) =&gt; {
                    let s = String::from_utf8_lossy(&amp;self.buffer);
                    <strong class="bold">runtime::reactor().deregister(self.stream.as_mut().unwrap(), self.id);</strong>
                    break PollState::Ready(s.to_string());
                }
                Ok(n) =&gt; {
                    self.buffer.extend(&amp;buff[0..n]);
                    continue;
                }
                Err(e) if e.kind() == ErrorKind::WouldBlock =&gt; {
                    <strong class="bold">runtime::reactor().set_waker(waker, self.id);</strong>
                    break PollState::NotReady;
                }
                Err(e) =&gt; panic!("{e:?}"),
            }</pre>			<p>The first change is to deregister the<a id="_idIndexMarker532"/> stream from our <code>Poll</code> instance when we’re done.</p>
			<p>The second change is a little more subtle. If you read the documentation for <code>Future::poll</code> in Rust (<a href="https://doc.rust-lang.org/stable/std/future/trait.Future.html#tymethod.poll">https://doc.rust-lang.org/stable/std/future/trait.Future.html#tymethod.poll</a>) carefully, you’ll see that it’s expected that the <code>Waker</code> from the <em class="italic">most recent call</em> should be scheduled to wake up. That means that every time we get a <code>WouldBlock</code> error, we need to make sure we store the most recent <code>Waker</code>.</p>
			<p>The reason is that the future could have moved to a different executor in between calls, and we need to wake up the correct one (it won’t be possible to move futures like those in our example, but let’s play by the same rules).</p>
			<p>And that’s it!</p>
			<p>Congratulations! You’ve now created a fully working runtime based on the reactor-executor pattern. Well done!</p>
			<p>Now, it’s time to test it and run a few experiments with it.</p>
			<p>Let’s go back to <code>main.rs</code> and change the <code>main</code> function so that we get our program running correctly with our new runtime.</p>
			<p>First of all, let’s remove the <a id="_idIndexMarker533"/>dependency on the <code>Runtime</code> struct and make sure our imports look like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/main.rs</p>
			<pre class="source-code">
mod future;
mod http;
mod runtime;
use future::{Future, PollState};
use runtime::Waker;</pre>			<p>Next, we need to make sure that we initialize our runtime and pass in our future to <code>executor.block_on</code>. Our <code>main</code> function should look like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch08/b-reactor-executor/src/main.rs</p>
			<pre class="source-code">
fn main() {
    let mut executor = runtime::init();
    executor.block_on(async_main());
}</pre>			<p>And finally, let’s try it out by running it:</p>
			<pre class="source-code">
<code>cargo run</code>.</pre>			<p>You should get the following output:</p>
			<pre class="console">
Program starting
FIRST POLL - START OPERATION
main: 1 pending tasks. Sleep until notified.
HTTP/1.1 200 OK
content-length: 15
connection: close
content-type: text/plain; charset=utf-8
date: Thu, xx xxx xxxx 15:38:08 GMT
HelloAsyncAwait
FIRST POLL - START OPERATION
main: 1 pending tasks. Sleep until notified.
HTTP/1.1 200 OK
content-length: 15
connection: close
content-type: text/plain; charset=utf-8
date: Thu, xx xxx xxxx 15:38:08 GMT
HelloAsyncAwait
main: All tasks are finished</pre>			<p>Great – it’s working just as expected!!!</p>
			<p>However, we’re not really<a id="_idIndexMarker534"/> using any of the new capabilities of our runtime yet so before we leave this chapter, let’s have some fun and see what it can do.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor152"/>Experimenting with our new runtime</h1>
			<p>If you remember from <a href="B20892_07.xhtml#_idTextAnchor122"><em class="italic">Chapter 7</em></a>, we implemented a <code>join_all</code> method to get our futures running concurrently. In libraries such as Tokio, you’ll find a <code>join_all</code> function too, and the slightly more versatile <code>FuturesUnordered</code> API that allows you to join a set of predefined futures and run them concurrently.</p>
			<p>These are convenient<a id="_idIndexMarker535"/> methods to have, but it does force you to know which futures you want to run concurrently in advance. If the futures you run using <code>join_all</code> want to spawn new futures that run concurrently with their “parent” future, there is no way to do that using only these methods.</p>
			<p>However, our newly created spawn functionality does exactly this. Let’s put it to the test!</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor153"/>An example using concurrency</h2>
			<p class="callout-heading">Note</p>
			<p class="callout">The exact same version of this <a id="_idIndexMarker536"/>program can be found in the <code>ch08/c-runtime-executor</code> folder.</p>
			<p>Let’s try a new program that looks like this:</p>
			<pre class="source-code">
fn main() {
    let mut executor = runtime::init();
    executor.block_on(async_main());
}
coro fn request(i: usize) {
    let path = format!("/{}/HelloWorld{i}", i * 1000);
    let txt = Http::get(&amp;path).wait;
    println!("{txt}");
}
coro fn async_main() {
    println!("Program starting");
    for i in 0..5 {
        let future = request(i);
        runtime::spawn(future);
    }
}</pre>			<p>This is pretty much the same example we used to show how <code>join_all</code> works in <a href="B20892_07.xhtml#_idTextAnchor122"><em class="italic">Chapter 7</em></a>, only this time, we spawn them as top-level futures instead.</p>
			<p>To run this example, follow these steps:</p>
			<ol>
				<li>Replace everything <em class="italic">below the imports</em> in <code>main.rs</code> with the preceding code.</li>
				<li>Run <code>corofy ./src/main.rs</code>.</li>
				<li>Copy everything from <code>main_corofied.rs</code> to <code>main.rs</code> and delete <code>main_corofied.rs</code>.</li>
				<li>Fix the fact that <code>corofy</code> doesn’t know we changed our futures to take <code>waker: &amp;Waker</code> as an argument. The easiest way is to simply run <code>cargo check</code> and let the compiler guide you to the places we need to change.</li>
			</ol>
			<p>Now, you can run the example and see<a id="_idIndexMarker537"/> that the tasks run concurrently, just as they did using <code>join_all</code> in <a href="B20892_07.xhtml#_idTextAnchor122"><em class="italic">Chapter 7</em></a>. If you measured the time it takes to run the tasks, you’d find that it all takes around 4 seconds, which makes sense if you consider that you just spawned 5 futures, and ran them concurrently. The longest wait time for a single future was 4 seconds.</p>
			<p>Now, let’s finish off this chapter with another interesting example.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor154"/>Running multiple futures concurrently and in parallel</h2>
			<p>This time, we spawn multiple threads and <a id="_idIndexMarker538"/>give each thread its own <code>Executor</code> so that we can run the previous example simultaneously in parallel using the same <code>Reactor</code> for all <code>Executor</code> instances.</p>
			<p>We’ll also make a small adjustment to the printout so that we don’t get overwhelmed with data.</p>
			<p>Our new program will look like this:</p>
			<pre class="source-code">
mod future;
mod http;
mod runtime;
use crate::http::Http;
use future::{Future, PollState};
use runtime::{Executor, Waker};
use std::thread::Builder;
fn main() {
    let mut executor = runtime::init();
    let mut handles = vec![];
    for i in 1..12 {
        let name = format!("exec-{i}");
        let h = Builder::new().name(name).spawn(move || {
            let mut executor = Executor::new();
            executor.block_on(async_main());
        }).unwrap();
        handles.push(h);
    }
    executor.block_on(async_main());
    handles.into_iter().for_each(|h| h.join().unwrap());
}
coroutine fn request(i: usize) {
    let path = format!("/{}/HelloWorld{i}", i * 1000);
    let txt = Http::get(&amp;path).wait;
    let txt = txt.lines().last().unwrap_or_default();
    println!(«{txt}»);
}
coroutine fn async_main() {
    println!("Program starting");
    for i in 0..5 {
        let future = request(i);
        runtime::spawn(future);
    }
}</pre>			<p>The machine I’m currently running has 12 cores, so when I create 11 new threads to run the same asynchronous tasks, I’ll use all the<a id="_idIndexMarker539"/> cores on my machine. As you’ll notice, we also give each thread a unique name that we’ll use when logging so that it’s easier to track what happens behind the scenes.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">While I use 12 cores, you should use the number of cores on your machine. If we increase this number too much, our OS will not be able to give us more cores to run our program in parallel on and instead start pausing/resuming the threads we create, which adds no value to us since we handle the concurrency aspect ourselves in an <code>a^tsync</code> runtime.</p>
			<p>You’ll have to do the same steps as we did in the last example:</p>
			<ol>
				<li>Replace the code that’s currently in <code>main.rs</code> with the preceding code.</li>
				<li>Run <code>corofy ./src/main.rs</code>.</li>
				<li>Copy everything from <code>main_corofied.rs</code> to <code>main.rs</code> and delete <code>main_corofied.rs</code>.</li>
				<li>Fix the fact that <code>corofy</code> doesn’t know we changed our futures to take <code>waker: &amp;Waker</code> as an argument. The<a id="_idIndexMarker540"/> easiest way is to simply run <code>cargo check</code> and let the compiler guide you to the places we need to change.</li>
			</ol>
			<p>Now, if you run the program, you’ll see that it still only takes around 4 seconds to run, but this time we made <strong class="bold">60 GET requests instead of 5</strong>. This time, we ran our futures both concurrently and in parallel.</p>
			<p>At this point, you can continue experimenting with shorter delays or more requests and see how many concurrent tasks you can have before the system breaks down.</p>
			<p>Pretty quickly, printouts to <code>stdout</code> will be a bottleneck, but you can disable those. Create a blocking version using OS threads and see how many threads you can run concurrently before the system breaks down compared to this version.</p>
			<p>Only imagination sets the limit, but do take the time to have some fun with what you’ve created before we continue with the next chapter.</p>
			<p>The only thing to be careful about is testing the concurrency limit of your system by sending these kinds of requests to a random server you don’t control yourself since you can potentially overwhelm it and cause problems for others.</p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor155"/>Summary</h1>
			<p>So, what a ride! As I said in the introduction for this chapter, this is one of the biggest ones in this book, but even though you might not realize it, you’ve already got a better grasp of how asynchronous Rust works than most people do. <strong class="bold">Great work!</strong></p>
			<p>In this chapter, you learned a lot about runtimes and why Rust designed the <code>Future</code> trait and the <code>Waker</code> the way it did. You also learned about reactors and executors, <code>Waker</code> types, <code>Futures</code> traits, and different ways of achieving concurrency through the <code>join_all</code> function and spawning new top-level futures on the executor.</p>
			<p>By now, you also have an idea of how we can achieve both concurrency and parallelism by combining our own runtime with OS threads.</p>
			<p>Now, we’ve created our own async universe consisting of <code>coro/wait</code>, our own <code>Future</code> trait, our own <code>Waker</code> definition, and our own runtime. I’ve made sure that we don’t stray away from the core ideas behind asynchronous programming in Rust so that everything is directly applicable to <code>async/await</code>, <code>Future</code> traits, <code>Waker</code> types, and runtimes in day-to-day programming.</p>
			<p>By now, we’re in the final stretch of this book. The last chapter will finally convert our example to use the real <code>Future</code> trait, <code>Waker</code>, <code>async/await</code>, and so on instead of our own versions of it. In that chapter, we’ll also reserve some space to talk about the state of asynchronous Rust today, including some of the most popular runtimes, but before we get that far, there is one more topic I want to cover: pinning.</p>
			<p>One of the topics that seems hardest to understand and most different from all other languages is the concept of pinning. When writing asynchronous Rust, you will at some point have to deal with the fact that <code>Future</code> traits in Rust must be pinned before they’re polled.</p>
			<p>So, the next chapter will explain pinning in Rust in a practical way so that you understand why we need it, what it does, and how to do it.</p>
			<p>However, you absolutely deserve a break after this chapter, so take some fresh air, sleep, clear your mind, and grab some coffee before we enter the last parts of this book.</p>
		</div>
	</body></html>
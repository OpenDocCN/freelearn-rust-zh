- en: Asynchronous Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, the only way we have seen to achieve concurrency in Rust is to create
    multiple threads, one way or another, to share the work. Nevertheless, those threads
    sometimes need to stop and look for something, such as a file or a network response.
    In those cases, the whole thread will be blocked and it will need to wait for
    the response.
  prefs: []
  type: TYPE_NORMAL
- en: This means that if we want to achieve a low latency for things such as an HTTP
    server, one way to do it is by spawning one thread per request, so that each request
    can be served as quickly as possible even if others block.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, spawning hundreds of threads is not scalable, since each thread
    will have its own memory and will consume resources even if it's blocked. In this
    chapter, you will learn a new way of doing things by using asynchronous programming.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous primitives with `mio`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `futures`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new `async`/`await` syntax and generators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asynchronous I/O with `tokio` and `websockets`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to asynchronous programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you want to achieve high performance in computing, you will need to run tasks
    concurrently. Whether you are running complex computations that take days, such
    as machine learning training, or you are running a web server that needs to respond
    to thousands of requests per second, you will need to do more than one thing at
    the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, as we have already seen, our processors and operating systems are
    prepared for concurrency, and in fact, multithreading is a great way to achieve
    it. The main issue is that as we saw in the previous chapter, we should not be
    using more threads than logical CPUs in our computer.
  prefs: []
  type: TYPE_NORMAL
- en: We can, of course, but some threads will be waiting for others to execute, and
    the kernel will be orchestrating how much time each thread gets in the CPU. This
    will consume even more resources and make the overall process slower. It can sometimes
    be useful, though, to have more threads than the number of cores. Maybe some of
    them only wake up once every few seconds to do small tasks, or we know that most
    of them will block due to some I/O operation.
  prefs: []
  type: TYPE_NORMAL
- en: When a thread blocks, the execution stops. No further instruction will run in
    the CPU until it gets unblocked. This can happen when we read a file, for example.
    Until the reading ends, no further instruction will be executed. This of course,
    depends on how we read the file.
  prefs: []
  type: TYPE_NORMAL
- en: But in the latter case, instead of creating more threads we can do better—asynchronous
    programming. When programming asynchronously, we let the code continue being executed
    while we are still waiting for a certain result. That will avoid blocking the
    thread and let you use less threads for the same task, while still being concurrent.
    You can also use asynchronous programming for tasks not related to I/O, but if
    they are CPU bound (their bottleneck is the CPU), you won't get speed improvements,
    since the CPU will always be running at its best. To learn how asynchronous I/O
    works in Rust, let's first dive into how the CPU handles I/O.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding I/O in the CPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `std::io` module in Rust handles all input/output operations. These operations
    can vary from keyboard/mouse input to reading a file, or from using TCP/IP sockets
    to command-line utilities (`stdio`/`stderr`). But how does it work internally?
  prefs: []
  type: TYPE_NORMAL
- en: Instead of understanding how the Rust standard library does it, we will dig
    some levels deeper to understand how it works at CPU level. We will later go back
    to see how the kernel provides this functionality to Rust. This will be based
    mostly in the x86_64 platform and Linux kernel, but other platforms handle these
    things similarly.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main types of I/O architecture: channel-based I/O and memory-mapped
    I/O. Channel-based I/O is really niche and is not used in modern PCs or most servers.
    In CPU architectures such as x86/x86_64 (most modern day Intel and AMD CPUs),
    memory-mapped I/O is used. But what does it mean?'
  prefs: []
  type: TYPE_NORMAL
- en: As you should know by now, the CPU gets all the required information for its
    work from the RAM memory. As we saw in previous chapters, this information will
    later be cached in the CPU cache, and it won't be used until it gets to the CPU
    registers, but this is not so relevant for now. So, if the CPU wants to get information
    about what key was pressed on the keyboard, or what TCP frames the website you
    are visiting is sending, it needs to either have some extra hardware channel to
    those input/output interfaces, or those interfaces have to change something in
    the RAM.
  prefs: []
  type: TYPE_NORMAL
- en: The first option is the channel-based I/O. CPUs that use channel-based I/O have
    dedicated channels and hardware for I/O operations. This usually increases the
    price of the CPUs a lot. On the other hand, in memory-mapped I/O the second option
    gets used—the memory gets somehow modified when an I/O operation happens.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have to pause a little bit to understand this better. Even though we
    may think that all our memory is in our RAM sticks, it's not exactly like that.
    Memory is divided into virtual and physical memory. Each program has one virtual
    memory address available for each addressable byte with the size of a CPU word.
    This means that a 32-bit CPU will have 2^(32) virtual memory addresses available
    for each of its programs and a 64-bit CPU will have 2^(64) addresses. This would
    mean having 4 GiB of RAM in the case of 32-bit computers and 16 EiB of RAM in
    the case of a 64-bit CPU. **EiBs** are **exbibytes**, or 1,014 **PiB** (**pebibytes**).
    Each PiB is 1024 **GiB** (**gibibytes**). Remember that gibibytes are the two-power
    version of **gigabytes** (**GB**). And all of this is true for each of the processes
    in the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: There are some issues with this. First, if we have two processes, we would need
    double the amount of memory, right? But the kernel can only address that amount
    of memory (it's a process itself). So we need **translation tables** (**TLBs**),
    that tell each process where their memory is. But even though we may have 4 GiB
    of RAM for 32-bit CPUs, we don't have 16 EiB of RAM anywhere. Not only that, 32-bit
    CPUs have existed long before we were able to create PCs with 4 GiB of RAM. How
    can a process address more RAM than what we have installed?
  prefs: []
  type: TYPE_NORMAL
- en: The solution is simple—we call that address space the virtual memory space,
    and the real RAM the physical memory space. If a process requires more memory
    than the available physical memory, two things can happen—either our kernel can
    move some memory addresses out of RAM into the disk and allocate some more RAM
    for this process, or it will receive an out-of-memory error. The first option
    is called page swapping, and it's really common in Unix systems, where you sometimes
    even decide how much space in the disk you want to provide for that.
  prefs: []
  type: TYPE_NORMAL
- en: Moving information from the RAM to the disk will slow things down a lot, since
    the disk itself is really slow compared to the RAM (even modern day SSDs are much
    slower than RAM). Nevertheless, here we find that there is some I/O happening
    to swap that memory information to the disk, right? How does that happen?
  prefs: []
  type: TYPE_NORMAL
- en: Well, we said that the virtual memory space was specific for each process, and
    we said that the kernel was another process. This means that the kernel also has
    the whole memory space available for it to use. This is where memory-mapped I/O
    comes in. The CPU will decide to map new devices to some addresses. This means
    that the kernel will be able to read information about the I/O interface just
    by reading some concrete positions in its virtual address space.
  prefs: []
  type: TYPE_NORMAL
- en: In this regard, there are some variants as to how to read that information.
    Two main ways exist—port-mapped I/O and direct memory access or DMA. Port-mapped
    I/O is used, of course, for TCP/IP, serial, and other kinds of peripheral communication.
    It will have some certain addresses allocated to it. These addresses will be a
    buffer, which means that as the input comes, it will write one by one the next
    memory address. Once it gets to the end, it will start from the beginning again,
    so the kernel has to be fast enough to read the information before it gets rewritten.
    It can also block the port, stopping the communication.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of DMA, the memory space of the device will be directly mapped in
    the virtual memory. This enables accessing that memory as if it were part of the
    virtual address space of the current PC. Which approach is used depends on the
    task and the device we want to communicate with. You may now be wondering how
    the kernel handles all this for your programs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the kernel to control the I/O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a new TCP/IP connection gets established, or when a new key is pressed
    on the keyboard, the kernel must know about it so that it can act accordingly.
    There are two ways of doing this—the kernel could be looking to those ports or
    memory addresses once and again to look for changes, which would make the CPU
    work for nothing most of the time, or the kernel could be notified by a CPU interrupt.
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, most kernels decide to go for the second option. They are
    idle, letting other processes use the CPU until there's a change in some I/O port
    or address. This makes the CPU interrupt at a hardware level, and gives control
    to the kernel. The kernel will check what happened and decide accordingly. If
    there is some process waiting for that interrupt, it will wake that process and
    let it know that there is some new information for it.
  prefs: []
  type: TYPE_NORMAL
- en: It may be, though, that the process waiting for the information is already awake.
    This happens in asynchronous programming. The process will keep on performing
    some computations while it's still waiting for the I/O transaction. In this case,
    the process will have registered some callback function in the kernel, so that
    the kernel knows what to call once the I/O operation is ready.
  prefs: []
  type: TYPE_NORMAL
- en: This means that while the I/O operation was being performed, the process was
    doing useful things, instead of being blocked and waiting for the kernel to return
    the result of the I/O operation. This enables you to use the CPU almost all the
    time, without pausing the execution, making your code perform better.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming from the programmer's perspective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we have seen how I/O works from a hardware and software perspective.
    We mentioned that it is possible to have our process working while waiting for
    the I/O, but how do we do it?
  prefs: []
  type: TYPE_NORMAL
- en: The kernel has some things to help us with this. In the case of Linux, it has
    the `epoll()` system call, which lets the kernel know that our code wants to receive
    some information from an I/O interface, but that it doesn't need to lock itself
    until the information is available. The kernel will know what callback to run
    when the information is ready, and meanwhile, our program can do a lot of computations.
  prefs: []
  type: TYPE_NORMAL
- en: This is very useful, for example, if we are processing some data and we know
    that in the future we will need some information from a file. We can ask the kernel
    to get the information from the file while we continue the computation, and as
    soon as we need the information from the file, we won't need to wait for the file
    reading operation—the information will just be there. This reduces the disk read
    latency a lot, since it will be almost as fast as reading from the RAM instead
    of the disk.
  prefs: []
  type: TYPE_NORMAL
- en: We can use this approach for TCP/IP connections, serial connections, and, in
    general, anything that requires I/O access. This `epoll()` system call comes directly
    from the Linux C API, but in Rust we have great wrappers that make all of this
    much easier without overhead. Let's check them out.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding futures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we use the `std::io::Read` and `std::io::Write` traits in our code, we will
    be able to easily read and write data from I/O interfaces, but every time we do
    it, the thread doing the call will block until the data is received. Luckily,
    the great crate ecosystem Rust has brings us great opportunities to improve this
    situation.
  prefs: []
  type: TYPE_NORMAL
- en: In many programming languages, you can find the concept of *not yet available
    data*. In JavaScript, for example, they are called promises, and in Rust, we call
    them futures. A future represents any data that will be available at some point
    in the future but may not be available yet. You can check whether a future has
    a value at any time, and get it if it does. If not, you can either perform some
    computation in the meantime or block the current thread until the value gets there.
  prefs: []
  type: TYPE_NORMAL
- en: Rust futures not only give us this feature, but they even give us tons of helpful
    APIs that we can use to improve the readability and reduce the amount of code
    written. The `futures` crate does all this with *zero-cost* abstractions. This
    means that it will not require extra allocations and the code will be as close
    as possible to the best assembly code you could write to make all this possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Futures not only work for I/O; they can be used with any kind of computation
    or data. We will be using the 0.2.x version of the futures crate in these examples.
    At the time of writing, that version is still in the alpha development phase,
    but it''s expected to be released really soon. Let''s see an example of how futures
    work. We will first need to add the `futures` crate as a dependency in the `Cargo.toml`
    file, and then we can start writing some code in the `main.rs` file of our project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we have a simulated complex computation that takes around 5
    seconds. This computation returns a `Future`, and we can therefore use useful
    methods to modify the result once it gets generated. These methods come from the
    `FutureExt` trait.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the `block_on()` function will wait until the given future is no longer
    pending. You may think that this is exactly the same as when we were working with
    threads, but the interesting thing is that we are only using one thread here.
    The future will be computed once the main thread has some spare time, or when
    we call the `block_on()` function.
  prefs: []
  type: TYPE_NORMAL
- en: This, of course, does not make much sense for computationally intense applications,
    since we will in any case have to do the computation in the main thread, but it
    makes a lot of sense for I/O access. We can think of a `Future` as the asynchronous
    version of a `Result`.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the `FutureExt` trait documentation at [https://docs.rs/futures/0.2.0-alpha/futures/trait.FutureExt.html](https://docs.rs/futures/0.2.0-alpha/futures/trait.FutureExt.html),
    we have tons of combinators to use. In this case, we used the `map()` method,
    but we can also use other methods such as `and_then()`, `map_err()`, `or_else()`,
    or even joins between futures. All these methods will run asynchronously one after
    the other. Once you call the `block_on()` function, you will get the `Result`
    of the final future.
  prefs: []
  type: TYPE_NORMAL
- en: Future combinators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And now that we mentioned joins, it is actually possible to have two co-dependent
    futures. Maybe we have information from two files, we generate one future reading
    from each file, and then we want to combine the information from them. We don't
    need to block the thread for that; we can use the `join()` method, and the logic
    behind it will make sure that once the closure we write gets called, both futures
    will have received the final value.
  prefs: []
  type: TYPE_NORMAL
- en: This is really useful when creating concurrency dependency graphs. If you have
    many small computations that you want to parallelise, you can create a closure
    or a function for each of the parts, and then use `join()` and other methods,
    such as `and_then()`, to decide which computations need to run some of them in
    parallel while still receiving all the required data for each step. The `join()`
    method comes in five variants depending on how many futures you need for your
    next computation.
  prefs: []
  type: TYPE_NORMAL
- en: But simple futures is not the only thing this crate gives us. We can also use
    the `Stream` trait, which works similarly to the `Iterator` trait, but asynchronously.
    This is extremely useful for inputs that come one by one and are not just a one-time
    value. This happens with TCP, serial, or any connection that uses byte streams,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: With this trait, and especially with the `StreamExt` trait, we have almost the
    same API as with iterators, and we can create a complete iterator that can, for
    example, retrieve HTTP data from a TCP connection byte by byte and asynchronously.
    This has many applications in web servers, and we have already seen crates in
    the community migrating to asynchronous APIs.
  prefs: []
  type: TYPE_NORMAL
- en: The crate also offers an asynchronous version of the `Write` trait. With the
    `Sink` and the `SinkExt` traits you can send data to any output object. This could
    be a file, a connection, or even some kind of streaming computation. `Sink` and
    `Stream` work great together, since the `send_all()` method in the `SinkExt` trait
    lets you send a whole `Stream` to the `Sink`. You could, for example, asynchronously
    read a file byte by byte, do some computation for each of them or in chunks, and
    then write the result in another file just by using these combinators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see an example. We will be using the `futures-timer` crate, and unfortunately
    it''s not yet available for futures 0.2.0\. So, let''s update our `Cargo.toml`
    file with the following `[dependencies]` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s write the following code in our `main.rs` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you execute cargo run for this example, it will generate five new lines with
    the `New interval` text, one every second. The Interval just returns a `()` every
    time the configured interval times out. We then only take the first five and run
    the closure inside the `for_each` loop. As you can see, the `Stream` and `StreamExt`
    traits works almost the same way as the `Iterator` trait.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous I/O in Rust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to I/O operations, there is a go-to crate. It's called `tokio`,
    and it handles asynchronous input and output operations seamlessly. This crate
    is based in MIO. MIO, from Metal IO, is a base crate that provides a really low-level
    interface to asynchronous programming. It generates an event queue, and you can
    use a loop to gather all the events one by one, asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, these events can be anything from *a TCP message was received* to
    *the file you requested is partially ready*. There are tutorials to create small
    TCP servers in MIO, for example, but the idea of MIO is not using the crate directly,
    but using a facade. The most known and useful facade is the `tokio` crate. This
    crate, by itself, only gives you some small primitives, but it opens the doors
    to many asynchronous interfaces. You have, for example, `tokio-serial`, `tokio-jsonrpc`,
    `tokio-http2`, `tokio-imap`, and many, many more.
  prefs: []
  type: TYPE_NORMAL
- en: Not only that, you have also utilities such as `tokio-retry` that will automatically
    retry the I/O operation if an error happens. Tokio is really easy to use, it has
    an extremely low footprint, and it enables you to create incredibly fast services
    with its asynchronous operations. As you probably have already noticed, it is
    mostly centred around communication. This is due to all the helpers and capabilities
    it provides for these cases. The core crate also has file reading capabilities,
    so you should be covered for any I/O-bound operation, as we will see.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will see first how to develop a small TCP echo server using Tokio. You can
    find similar tutorials on the Tokio website ([https://tokio.rs/](https://tokio.rs/)),
    and it is worthwhile to follow all of them. Let''s therefore start by adding `tokio`
    as a dependency to the `Cargo.toml` file. Then, we will use the `TcpListener`
    from the `tokio` crate to create a small server. This structure binds a TCP socket
    listener to a given address, and it will asynchronously execute a given function
    for each of the incoming connections. In that function, we will asynchronously
    read any potential data that we could find in the socket and return it, doing
    an `echo`. Let''s see what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let's analyze the code. The listener creates an asynchronous stream of incoming
    connections with the `incoming()` method. For each of them, we check whether it
    was an error and print a message accordingly, and then, for the correct ones,
    we get the socket and get a writer and a reader by using the `split()` method.
    Then, Tokio gives us a `Copy` future that gets created with the `tokio::io::copy()`
    function. This future represents data that gets copied from a reader to a writer
    asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: We could have written that future ourselves by using the `AsyncRead` and `AsyncWrite`
    traits, but it's great to see that Tokio already has that example future. Since
    the behavior we want is to return back whatever the connection was sending, this
    will work perfectly. We then add some extra code that will be executed after the
    reader returns **End of File** or **EOF** (when the connection gets closed). It
    will just print the number of bytes that were copied, and it will handle any potential
    errors that may appear.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in order for the future to perform its task, something needs to execute
    it. This is where Tokio executors come in—we call `tokio::spawn()`, which will
    execute the future in the default executor. What we just created is a stream of
    things to do when a connection comes, but we now need to actually run the code.
    For that, Tokio has the `tokio::run()` function, which starts the whole Tokio
    runtime process and starts accepting connections.
  prefs: []
  type: TYPE_NORMAL
- en: The main future we created, the stream of incoming connections, will be executed
    at that point and will block the main thread. Since the server is always waiting
    for new connections, it will just block indefinitely. Still, this does not mean
    that the execution of the futures is synchronous. The thread will go idle without
    consuming CPU, and when a connection comes, the thread will be awakened and the
    future executed. In the future itself, while sending the received data back, it
    will not block the execution if there is no more data. This enables the running
    of many connections in only one thread. In a production environment, you will
    probably want to have similar behavior in multiple threads, so that each thread
    can handle multiple connections.
  prefs: []
  type: TYPE_NORMAL
- en: It's now time to test it. You can start the server by running `cargo run` and
    you can connect to it with a TCP tool such as Telnet. In the case of Telnet, it
    buffers the sent data line by line, so you will need to send a whole line to receive
    the echo back. There is another area where Tokio is especially useful—parsing
    frames. If you want to create your own communication protocol, for example, you
    may want to get chunks of those TCP bytes as frames, and then convert them to
    your type of data.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Tokio codecs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Tokio, we have the concept of a codec. A codec is a type that divides a
    slice of bytes into frames. Each frame will contain certain information parsed
    from the stream of bytes. In our case, we will read the input of the TCP connection
    and divide it into chunks each time we find the `a` letter. A production-ready
    codec will probably be more complex, but this example will give us a good enough
    base to implement our own codecs. We will need to implement two traits from the
    `tokio-io` crate, so we will need to add it to the `[dependencies]` section of
    our `Cargo.toml` file and import it with `extern crate tokio_io;`. We will need
    to do the same with the `bytes` crate. Now, let''s start writing the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is a lot of code; let's analyse it carefully. We created a structure, named
    `ADividerCodec`, and we implemented the `Decode` trait for it. This code has two
    methods. The first and most important one is the `decode()` method. It receives
    a buffer containing data coming from the connection and it needs to return either
    some data or none. In this case, it will try to find the position of the `a` letter,
    in lower case. If it finds it, it will return all the bytes that were read until
    then. It also removes new lines, just to make the printing more clear.
  prefs: []
  type: TYPE_NORMAL
- en: It creates a string with those bytes, so it will fail if we send non-UTF-8 bytes.
    Once we take bytes from the front of the buffer, the next index should point to
    the first element in the buffer. If there was no `a` in the buffer, it will just
    update the index to the last element that was read, and just return `None`, since
    there isn't a full frame ready. The `decode_eof()` method will do a similar thing
    when the connection gets closed. We use strings as the output of the codec, but
    you can use any structure or enumeration to represent your data or commands, for
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to implement the `Encode` trait so that we can use the `framed()`
    method from Tokio. This just represents how the data would be encoded in a new
    byte array if we wanted to use bytes again. We will just get the bytes of the
    strings and append an `a` to it. We will lose new line information, though. Let''s
    see what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To see how it works, let''s implement a simple `main()` function and use Telnet
    to send some text with `a` letters in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We could send this text, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10b83c36-d695-4ec2-885d-b104891e1a15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output in the server will be similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63cb8680-f15c-4fe7-a3a1-afee297e94cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that I didn't close the connection, so the last part of the last sentence
    was still in the buffer.
  prefs: []
  type: TYPE_NORMAL
- en: WebSockets in Rust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you work in web development, you know that WebSockets are one of the most
    useful protocols to speed up communication with the client. Using them allows
    your server to send information to the client without the latter requesting it,
    therefore avoiding one extra request. Rust has a great crate that allows the implementation
    of WebSockets, named `websocket`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will analyze a small, asynchronous WebSocket echo server example to see
    how it works. We will need to add `websocket`, `futures`, and `tokio-core` to
    the `[dependencies]` section of our `Cargo.toml` file. The following example has
    been retrieved and adapted from the asynchronous server example in the `websocket`
    crate. It uses the Tokio reactor core, which means that it requires a core object
    and its handle. The WebSocket requires this behavior since it''s not a simple
    I/O operation, which means that it requires some wrappers, such as connection
    upgrades to WebSockets. Let''s see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Most of the code, as you can see, is really similar to the code used in the
    previous examples. The first change that we see is that for each connection, before
    actually accepting the connection, it will check if the socket can be upgraded
    to the `rust-websocket` protocol. Then, it will upgrade the connection protocol
    to that protocol and accept the connection. For each connection, it will receive
    a handle to the client and some headers. All this is done asynchronously, of course.
  prefs: []
  type: TYPE_NORMAL
- en: We discard the headers, and we divide the client into a sink and a stream. A
    sink is the asynchronous equivalent to a synchronous writer, in `futures` terminology.
    It starts taking bytes from the stream until it closes, and, for each of them,
    it replies with the same message. It will then call the `forward()` method, which
    consumes all the messages in the stream, and then it sends a connection closed
    message. The future we just created is then spawned using the handle we took from
    the core. This means that, for each connection, this whole future will be run.
    The Tokio core then runs the whole server task.
  prefs: []
  type: TYPE_NORMAL
- en: If you get the example client implementation from the crate's Git repository
    ([https://github.com/cyderize/rust-websocket/blob/master/examples/async-client.rs](https://github.com/cyderize/rust-websocket/blob/master/examples/async-client.rs)),
    you will be able to see how the server replies to whatever the client sends. Once
    you understand this code, you will be able to create any WebSocket server you
    need.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the new Generators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A new feature is coming to Rust in 2018—asynchronous generators. Generators
    are functions that can yield elements before returning from the function and resume
    executing later. This is great for the loops that we have seen in this chapter.
    With generators, we could directly replace many of the callbacks with the new
    `async`/`await` syntax.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is still an unstable feature that can only be used in nightly, so it may
    be that the code you write becomes obsolete before stabilization. Let''s see a
    simple example of a generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You will need to execute `rustup override add nightly` to run the example.
    If you run it, you will see this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8beec889-41e0-4c17-beb8-c0946385aa98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The interesting thing here is that the generator function can perform any computation,
    and you can resume the computation once a partial result gets yielded, without
    needing buffers. You can test this by doing the following—instead of yielding
    something from the generator, just use it to print in the console. Let''s see
    an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this example, you will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/842e17d8-1283-4ec7-b691-b31ef64df46b.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the function pauses its execution when it gets to a `yield`
    statement. If there is any data in that yield statement, the caller will be able
    to retrieve it. Once the generator is resumed, the rest of the function gets executed,
    until a `yield` or a `return` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'This, of course, is of great advantage for the `futures` we saw earlier. This
    is why the `futures-await` crate was created. This crate uses generators to make
    the implementation of asynchronous futures much easier. Let''s rewrite the TCP
    echo server we created before using this crate. We will need to add the `0.2.0`
    version of the `futures-await` to the `[dependencies]` section of our `Cargo.toml`
    file and then start using a bunch of nightly features. Let''s see some example
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This example will have two asynchronous functions that could, for example, be
    retrieving information from the network. They get called by the `add_data()` function,
    which will wait for them to return before adding them up and returning a result.
    If you run it, you will see that the result is `Ok(3)`. The line importing the
    `futures_await` crate as `futures` makes sense because the `futures-await` crate
    is just a small wrapper around the futures crate, and all the usual structures,
    functions, and traits are available.
  prefs: []
  type: TYPE_NORMAL
- en: The whole generators and `async`/`await` syntax is still being heavily worked
    on, but the Rust 2018 roadmap says it should be stabilized before the end of the
    year.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this last chapter of the book, you learned to use asynchronous programming
    to avoid creating too many threads. You can now use just the right amount of threads
    and still run the workload in parallel and efficiently in networking applications.
    To be able to do that, you first learned about the futures crate, which give us
    the minimum primitives to use when working with asynchronous programming in Rust.
    You then learned how the MIO-based Tokio works, and created your first servers.
  prefs: []
  type: TYPE_NORMAL
- en: Before understanding external crates, you learned about WebSockets and grasped
    the Tokio core reactor syntax. Finally, you learned about the new generators syntax
    and how the `futures` crate is being adapted to make use of this new syntax. Make
    sure to stay up to date about the news on when this great compiler feature will
    be stabilized.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the book came to an end, we can see that high performance can be achieved
    in Rust in multiple and complimentary ways. We can first start by improving our
    sequential code as we saw in the first chapters. These improvements come from
    various techniques, starting from a proper compiler configuration and ending in
    small tips and tricks with the code. As we saw, some tools will help us in this
    labour.
  prefs: []
  type: TYPE_NORMAL
- en: We can then use metaprogramming to improve both the maintainability of the code
    and the performance, by reducing the amount of work the software has to do at
    runtime. We saw that new ways of metaprogramming are arriving this year to Rust.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the last step to make things faster is to run tasks concurrently, as
    we saw in the last two chapters. Depending on the requirements of our project,
    we will use multithreading or/and asynchronous programming.
  prefs: []
  type: TYPE_NORMAL
- en: You should now be able to improve the performance of your Rust applications
    and even to start learning deeper concepts of high performance programming. It
    has been a pleasure to guide you through these topics in the Rust programming
    language, and I hope you enjoyed the read.
  prefs: []
  type: TYPE_NORMAL

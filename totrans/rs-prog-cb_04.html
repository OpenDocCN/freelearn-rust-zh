<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Fearless Concurrency</h1>
                </header>
            
            <article>
                
<p>Concurrency and parallelism are important parts of modern-day programming and Rust is perfectly equipped to deal with these challenges. The borrowing and ownership model is great for preventing data races (<strong>anomalies</strong>, as they are called in the database world) since variables are immutable by default and if mutability is required, there cannot be any other reference to the data. This makes any type of concurrency safe and less complex in Rust (compared to many other languages). </p>
<p>In this chapter, we will cover several ways of employing concurrency to solve problems and will even look at futures, which are—at the time of writing—not part of the language yet. If you are reading this in the future (no pun intended), this may be part of the core language already and you can check out the <em>Asynchronous programming with futures</em> recipe for historical reference.</p>
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Moving data into new threads</li>
<li>Managing multiple threads</li>
<li>Message passing between threads</li>
<li>Shared mutable states</li>
<li>Multiprocessing</li>
<li>Making sequential code parallel</li>
<li>Concurrent data processing in vectors</li>
<li>Shared immutable states</li>
<li>Actors and asynchronous messages</li>
<li>Async programming with futures</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Moving data into new threads</h1>
                </header>
            
            <article>
                
<p>Rust threads operate just like in any other language—in scopes. Any other scope (such as closures) can easily borrow the variables from the parent scope since it's easy to determine if and when variables are dropped. However, when spawning a thread, its lifetime, compared to its parent's lifetime, is impossible to know and therefore the reference can become invalid at any time.</p>
<p>To tackle this problem, the threaded scope can take ownership of its variables—the memory is <strong>moved</strong> into the thread's scope. Let's see how this is done!</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Follow these steps to see how to move memory between threads:</p>
<ol>
<li><span>Use </span><kbd>cargo new simple-threads</kbd><span> to create a new application project and open the directory in Visual Studio Code.</span></li>
<li>Edit <kbd>src/main.rs</kbd> and spawn a simple thread that does not move data into its scope. Since it's the simplest form of a thread, let's print something to the command line and wait:</li>
</ol>
<pre style="padding-left: 60px">use std::thread;<br/>use std::time::Duration;<br/><br/>fn start_no_shared_data_thread() -&gt; thread::JoinHandle&lt;()&gt; {<br/>    thread::spawn(|| {<br/>        // since we are not using a parent scope variable in here<br/>        // no move is required<br/>        println!("Waiting for three seconds.");<br/>        thread::sleep(Duration::from_secs(3)); <br/>        println!("Done")<br/>    })<br/>}</pre>
<ol start="3">
<li>Now, let's call the new function from within <kbd>fn main()</kbd>. Replace the <kbd>hello world</kbd> snippet with the following:</li>
</ol>
<pre style="padding-left: 60px">    let no_move_thread = start_no_shared_data_thread();<br/>    <br/>    for _ in 0..10 {<br/>        print!(":");<br/>    }<br/><br/>    println!("Waiting for the thread to finish ... {:?}", <br/>    no_move_thread.join());</pre>
<ol start="4">
<li>Let's run the code to see if it works:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo run</strong><br/> Compiling simple-threads v0.1.0 (Rust-Cookbook/Chapter05/simple-<br/> threads)<br/>    Finished dev [unoptimized + debuginfo] target(s) in 0.35s<br/>     Running `target/debug/simple-threads`<br/>::::::::::Waiting for three seconds.<br/>Done<br/>Waiting for the thread to finish ... Ok(())</pre>
<ol start="5">
<li>Now, let's get some outside data into a thread. Add another function to <kbd>src/main.rs</kbd>:</li>
</ol>
<pre style="padding-left: 60px">fn start_shared_data_thread(a_number: i32, a_vec: Vec&lt;i32&gt;) -&gt; thread::JoinHandle&lt;Vec&lt;i32&gt;&gt; {<br/>    // thread::spawn(move || {<br/>    thread::spawn(|| {<br/>        print!(" a_vec ---&gt; [");<br/>        for i in a_vec.iter() {<br/>            print!(" {} ", i);<br/>        }<br/>        println!("]");<br/>        println!(" A number from inside the thread: {}", a_number);<br/>        a_vec // let's return ownership<br/>    })<br/>}</pre>
<ol start="6">
<li>To demonstrate what's happening under the hood, we have left out the <kbd>move</kbd> keyword for now. Expand the <kbd>main</kbd> function with the following code: </li>
</ol>
<pre style="padding-left: 60px">    let a_number = 42;<br/>    let a_vec = vec![1,2,3,4,5];<br/><br/>    let move_thread = start_shared_data_thread(a_number, a_vec);<br/><br/>    println!("We can still use a Copy-enabled type: {}", a_number); <br/>    println!("Waiting for the thread to finish ... {:?}", <br/>    move_thread.join());</pre>
<ol start="7">
<li class="mce-root">Does it work<span>? Let's try</span> <kbd>cargo run</kbd><span>:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>$ cargo run</strong><br/>Compiling simple-threads v0.1.0 (Rust-Cookbook/Chapter04/simple-threads)<br/>error[E0373]: closure may outlive the current function, but it borrows `a_number`, which is owned by the current function<br/>  --&gt; src/main.rs:22:20<br/>   |<br/>22 | thread::spawn(|| {<br/>   |               ^^ may outlive borrowed value `a_number`<br/>...<br/>29 |    println!(" A number from inside the thread: {}", a_number);<br/>   |                                                     -------- `a_number` is borrowed here<br/>   |<br/>note: function requires argument type to outlive `'static`<br/>  --&gt; src/main.rs:22:6<br/>   |<br/>23 | /     thread::spawn(|| {<br/>24 | |        print!(" a_vec ---&gt; [");<br/>25 | |        for i in a_vec.iter() {<br/>...  |<br/>30 | |        a_vec // let's return ownership<br/>31 | |     })<br/>   | |______^<br/>help: to force the closure to take ownership of `a_number` (and any other referenced variables), use the `move` keyword<br/>   |<br/>23 | thread::spawn(move || {<br/>   |               ^^^^^^^<br/><br/>error: aborting due to previous error<br/><br/>For more information about this error, try `rustc --explain E0373`.<br/>error: Could not compile `simple-threads`.<br/><br/>To learn more, run the command again with --verbose.</pre>
<ol start="8">
<li>There we have it: to get any kind of data into a threaded scope, we need to transfer ownership by moving the value into the scope using the <kbd>move</kbd> keyword. Let's follow the compiler's instructions:</li>
</ol>
<pre style="padding-left: 60px">///<br/>/// Starts a thread moving the function's input parameters<br/>/// <br/>fn start_shared_data_thread(a_number: i32, a_vec: Vec&lt;i32&gt;) -&gt; thread::JoinHandle&lt;Vec&lt;i32&gt;&gt; {<br/>    thread::spawn(move || {<br/>    // thread::spawn(|| {<br/>        print!(" a_vec ---&gt; [");<br/>        for i in a_vec.iter() {<br/>            print!(" {} ", i);<br/>        }<br/>        println!("]");<br/>        println!(" A number from inside the thread: {}", a_number);<br/>        a_vec // let's return ownership<br/>    })<br/>}</pre>
<ol start="9">
<li>Let's try again with <kbd>cargo run</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo run</strong><br/>   Compiling simple-threads v0.1.0 (Rust-Cookbook/Chapter04/simple-<br/>   threads)<br/>    Finished dev [unoptimized + debuginfo] target(s) in 0.38s<br/>     Running `target/debug/simple-threads`<br/>::::::::::Waiting for three seconds.<br/>Done<br/>Waiting for the thread to finish ... Ok(())<br/>We can still use a Copy-enabled type: 42<br/>   a_vec ---&gt; [ 1 2 3 4 5 ]<br/>   A number from inside the thread: 42<br/>Waiting for the thread to finish ... Ok([1, 2, 3, 4, 5])</pre>
<p><span>Now, let's go behind the scenes to understand the code better.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Threads in Rust behave a lot like regular functions: they can take ownership and operate on the same syntax as closures (<kbd>|| {}</kbd> is an empty/<kbd>noop</kbd><span> function without parameters). Therefore,</span> we <span>have to treat them like we treat functions and think of them in terms of ownership and borrowing, or more specifically: lifetimes. Passing a reference (the default behavior) into this thread function makes it</span> impossible <span>for the compiler to keep track of the validity of the reference, which is a problem for code safety. Rust solves this by introducing the</span> <kbd>move</kbd> <span>keyword.</span></p>
<p>Using the <kbd>move</kbd> keyword changes the default behavior of borrowing to moving the ownership of every variable into the scope. Hence, unless these values implement the <kbd>Copy</kbd> trait (like <kbd>i32</kbd>), or have a longer lifetime than the thread when borrowing (like the <kbd>'static</kbd> lifetime for <kbd>str</kbd> literals), they become unavailable to the thread's parent scope.</p>
<p>Giving back ownership also works just like in a function—via the <kbd>return</kbd> statement. The thread that waits for the other (using <kbd>join()</kbd>) can then retrieve the return value by unwrapping the <kbd>join()</kbd> result.</p>
<p>Threads in Rust are native threads for each operating system and have their own local state and execution stack. When they panic, only the thread stops, not the entire program.</p>
<p><span>We've successfully gone through moving data into new threads. Now let's move on to the next recipe.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Managing multiple threads</h1>
                </header>
            
            <article>
                
<p>Single threads are great, but in reality, many use cases demand a wealth of threads to execute on a large-scale data set in parallel. This has been popularized by the map/reduce pattern, published several years ago, and is still a great way to process something distinct such as multiple files, rows in a database result, and many more in parallel. Whatever the source, as long as the processing is not inter-dependent, it can be chunked and <strong>mapped</strong>—both of which Rust can make easy and free of data-race conditions. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we'll add some more threads to do map-style data processing. Follow these steps:</p>
<ol>
<li><span>Run</span><span> </span><kbd>cargo new multiple-threads</kbd><span> </span><span>to create a new application project and open the directory in Visual Studio Code.</span></li>
<li>In <kbd>src/main.rs</kbd>, <span>add the following function on top of</span> <kbd>main()</kbd><span>:</span></li>
</ol>
<pre style="padding-left: 60px">use std::thread;<br/><br/><br/>///<br/>/// Doubles each element in the provided chunks in parallel and returns the results.<br/>/// <br/>fn parallel_map(data: Vec&lt;Vec&lt;i32&gt;&gt;) -&gt; Vec&lt;thread::JoinHandle&lt;Vec&lt;i32&gt;&gt;&gt; {<br/>    data.into_iter()<br/>        .map(|chunk| thread::spawn(move ||<br/>         chunk.into_iter().map(|c| <br/>         c * 2).collect()))<br/>        .collect()<br/>}</pre>
<ol start="3">
<li>In this function, we spawn a thread for each chunk that has been passed in. This thread only doubles the number and therefore the function returns <kbd>Vec&lt;i32&gt;</kbd> for each chunk containing the results of this transformation. Now we need to create input data and call the function. Let's extend <kbd>main</kbd> to do that:</li>
</ol>
<pre style="padding-left: 60px">fn main() {<br/><br/>    // Prepare chunked data<br/>    let data = vec![vec![1, 2, 3], vec![4, 4, 5], vec![6, 7, 7]];<br/><br/>    // work on the data in parallel<br/>    let results: Vec&lt;i32&gt; = parallel_map(data.clone())<br/>        .into_iter() // an owned iterator over the results<br/>        .flat_map(|thread| thread.join().unwrap()) // join each <br/>         thread<br/>        .collect(); // collect the results into a Vec<br/><br/>    // flatten the original data structure<br/>    let data: Vec&lt;i32&gt; = data.into_iter().flat_map(|e| e)<br/>     .collect();<br/><br/>    // print the results<br/>    println!("{:?} -&gt; {:?}", data, results);<br/>}</pre>
<ol start="4">
<li>With <kbd>cargo run</kbd> we can now see the results:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo run</strong><br/>   Compiling multiple-threads v0.1.0 (Rust-<br/>    Cookbook/Chapter04/multiple-threads)<br/>    Finished dev [unoptimized + debuginfo] target(s) in 0.45s<br/>     Running `target/debug/multiple-threads`<br/>    [1, 2, 3, 4, 4, 5, 6, 7, 7] -&gt; [2, 4, 6, 8, 8, 10, 12, 14, 14]</pre>
<p class="mce-root"><span>Now, let's go behind the scenes to understand the code better.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Admittedly, working with multiple threads in Rust is just the same as if we were working on single threads since there are no convenient methods for joining a list of threads or similar. Instead, we can use the power of Rust's iterators to do that in an expressive way. With these functional constructs, the need for <kbd>for</kbd> loops can be replaced by a chain of functions that lazily process collections, which makes the code easier to handle and more efficient. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>After setting up the project in <em>step 1</em>, we implement a multithreaded function to apply an operation to every chunk. These chunks are simply parts of a vector, and an operation—a simple function that doubles the input variable in this example—can be done with any type of task. <em>Step 3</em> shows how to call the multithreaded <kbd>mapping</kbd> function and how to get results by using the <kbd>JoinHandle</kbd> in a future/promise (<a href="http://dist-prog-book.com/chapter/2/futures.html">http://dist-prog-book.com/chapter/2/futures.html</a>) way. <em>Step 4</em> then simply shows that it works as intended by outputting the doubled chunks as a flat list. </p>
<p>What is also interesting is the number of times we have had to clone data. Since passing data into the threads is only possible by moving the values into each thread's memory space, cloning is often the only way to work around these sharing issues. However, we'll cover a method similar to multiple <kbd>Rc</kbd> in a later recipe (<em>Shared immutable states</em>) in this chapter, so <span>let's move on to the next recipe.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using channels to communicate between threads</h1>
                </header>
            
            <article>
                
<p>Message passing between threads has been an issue in many standard libraries and programming languages since many rely on the user to apply locking. This leads to deadlocks and is somewhat intimidating for newcomers, which is why many developers were excited when Go popularized the concept of channels, something that we can also find in Rust. Rust's channels are great for designing a safe, event-driven application in just a few lines of code without any explicit locking. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's create a simple application that visualizes incoming values on the command line:</p>
<ol>
<li><span>Run</span><span> </span><kbd>cargo new channels</kbd><span> </span><span>to create a new application project and open the directory in Visual Studio Code.</span></li>
<li>First, let's get the basics out of the way. Open <kbd>src/main.rs</kbd> <span>and add the imports and an <kbd>enum</kbd> structure to the file:</span></li>
</ol>
<pre style="padding-left: 60px">use std::sync::mpsc::{Sender, Receiver};<br/>use std::sync::mpsc;<br/>use std::thread;<br/><br/>use rand::prelude::*;<br/>use std::time::Duration;<br/><br/>enum ChartValue {<br/>    Star(usize),<br/>    Pipe(usize)<br/>}</pre>
<ol start="3">
<li>Then, inside the <kbd>main</kbd> function, we create a channel with the <kbd>mpsc::channel()</kbd> function along with two threads that take care of the sending. Afterward, we are going to use two threads to send messages to the main thread with a variable delay. Here's the code:</li>
</ol>
<pre style="padding-left: 60px">fn main() {<br/>    let (tx, rx): (Sender&lt;ChartValue&gt;, Receiver&lt;ChartValue&gt;) = <br/>     mpsc::channel();<br/><br/>    let pipe_sender = tx.clone();<br/><br/>    thread::spawn(move || {<br/>        loop {<br/>            pipe_sender.send(ChartValue::Pipe(random::&lt;usize&gt;() % <br/>             80)).unwrap();<br/>            thread::sleep(Duration::from_millis(random::&lt;u64&gt;() % <br/>             800));<br/>        }<br/>    });<br/><br/>    let star_sender = tx.clone();<br/>    thread::spawn(move || {<br/>        loop {<br/>            star_sender.send(ChartValue::Star(random::&lt;usize&gt;() % <br/>             80)).unwrap();<br/>            thread::sleep(Duration::from_millis(random::&lt;u64&gt;() % <br/>             800));<br/>        }<br/>    });</pre>
<ol start="4">
<li>Both of the threads are sending data to the channel, so what's missing is the channel's receiving end to take care of the input data. The receiver offers two functions, <kbd>recv()</kbd> and <kbd>recv_timeout()</kbd>, both of which block the calling thread until an item is received (or the timeout is reached). We are just going to print the character multiplied by the passed-in value:</li>
</ol>
<pre style="padding-left: 60px">    while let Ok(val) = rx.recv_timeout(Duration::from_secs(3)) {<br/><br/>        println!("{}", match val {<br/>            ChartValue::Pipe(v) =&gt; "|".repeat(v + 1),<br/>            ChartValue::Star(v) =&gt; "*".repeat(v + 1)<br/>        });<br/>    }<br/>}</pre>
<ol start="5">
<li>In order to use <kbd>rand</kbd> when we finally run the program, we still need to add it to <kbd>Cargo.toml</kbd> with the following:</li>
</ol>
<pre style="padding-left: 60px">[dependencies]<br/>rand = "^0.5"</pre>
<ol start="6">
<li>Lastly, let's see how the program runs—it's going to run infinitely. To stop it, press <em>Ctrl</em> + <em>C</em>. Run it with <kbd>cargo run</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo run</strong><br/>   Compiling channels v0.1.0 (Rust-Cookbook/Chapter04/channels)<br/>    Finished dev [unoptimized + debuginfo] target(s) in 1.38s<br/>     Running `target/debug/channels`<br/>||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||<br/>****************************<br/>|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||<br/>|||||||||||||||||||||||||||||||||<br/>*********************************************************<br/>||||||||||||||||||||||||||||<br/>************************************************************<br/>*****************************<br/>||||||||||||||||<br/>***********<br/>||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||<br/>*******************************<br/>|||||||||||||||||||||||||||||||||||||||||<br/>*************************************************************<br/>|||||||||||||||||||||||||||||||||||||||||||||||<br/>*******************************<br/>************************************************************************<br/>*******************<br/>******************************************************<br/>|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||<br/>||||||||||||||||||||||||||||||||<br/>************************************************<br/>*<br/>||||||||||||||||||||||||||||||||||||||||<br/>***********************************************<br/>||||||<br/>*************************<br/>|||||||||||||||||||<br/>|||||||||||||||||||||||||||||||||||<br/>^C⏎ </pre>
<p><span>How does this work? Let's go behind the scenes to understand the code better.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Channels are <strong>multi-producer-single-consumer</strong> data structures, consisting of many senders (with a lightweight clone) but only a single receiver. Under the hood, the channel does not lock but relies on an <kbd>unsafe</kbd> data structure that allows the detection and management of the state of the stream. The channel handles simply sending data across threads well and can be used to create an actor-style framework or a reactive map-reduce style data-processing engine.</p>
<p>This is one example of how Rust does <strong>fearless concurrency</strong>: the data going in is owned by the channel until the receiver retrieves it, which is when a new owner takes over. The channel also acts as a queue and holds elements until they are retrieved. This not only frees the developer from implementing the exchange but also adds concurrency for regular queues for free as well.</p>
<p><span>We create the channel in <em>step 3</em> of this recipe and pass the senders into different threads, which start sending the previously defined (in <em>step 2</em>) <kbd>enum</kbd> types for the receiver to print. This printing is done in <em>step 4</em> by looping over the blocking iterator with a three-second timeout. <em>Step 5</em> then shows how to add the dependency to <kbd>Cargo.toml</kbd>, and in <em>step 6</em> we see the output: multiple full lines with a random number of elements that are either asterisks (<kbd>*</kbd>) or pipes (<kbd>|</kbd>). </span></p>
<p><span>We've successfully covered how to use channels to communicate between threads effortlessly. Now let's move on to the next recipe.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Sharing mutable states</h1>
                </header>
            
            <article>
                
<p>Rust's ownership and borrowing model simplifies immutable data access and transfer considerably—but what about shared states? There are many applications that require mutable access to a shared resource from multiple threads. Let's see how this is done!</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span>In this recipe, we will create a very simple simulation</span><span>:</span></p>
<ol>
<li><span>Run</span><span> </span><kbd>cargo new black-white</kbd><span> </span><span>to create a new application project and open the directory in Visual Studio Code.</span></li>
<li>Open <kbd>src/main.rs</kbd> <span>to add some code. First, we are going to need some imports and an <kbd>enum</kbd> to make our simulation interesting:</span></li>
</ol>
<pre style="padding-left: 60px">use std::sync::{Arc, Mutex};<br/>use std::thread;<br/>use std::time::Duration;<br/><br/>///<br/>/// A simple enum with only two variations: black and white<br/>/// <br/>#[derive(Debug)]<br/>enum Shade {<br/>    Black,<br/>    White,<br/>}</pre>
<ol start="3">
<li>In order to show a shared state between two threads, we obviously need a thread that works on something. This will be a coloring task, where each thread is only adding white to a vector if black was the previous element and vice versa. Thus, each thread is required to read and—depending on the output—write into a shared vector. Let's look at the code that does this:</li>
</ol>
<pre style="padding-left: 60px">fn new_painter_thread(data: Arc&lt;Mutex&lt;Vec&lt;Shade&gt;&gt;&gt;) -&gt; thread::JoinHandle&lt;()&gt; {<br/>    thread::spawn(move || loop {<br/>        {<br/>            // create a scope to release the mutex as quickly as    <br/>            // possible<br/>            let mut d = data.lock().unwrap();<br/>            if d.len() &gt; 0 {<br/>                match d[d.len() - 1] {<br/>                    Shade::Black =&gt; d.push(Shade::White),<br/>                    Shade::White =&gt; d.push(Shade::Black),<br/>                }<br/>            } else {<br/>                d.push(Shade::Black)<br/>            }<br/>            if d.len() &gt; 5 {<br/>                break;<br/>            }<br/>        }<br/>        // slow things down a little<br/>        thread::sleep(Duration::from_secs(1));<br/>    })<br/>}</pre>
<ol start="4">
<li>All that remains at this stage is to create multiple threads and hand them an <kbd>Arc</kbd> instance of the data to work on:</li>
</ol>
<pre style="padding-left: 60px">fn main() {<br/>    let data = Arc::new(Mutex::new(vec![]));<br/>    let threads: Vec&lt;thread::JoinHandle&lt;()&gt;&gt; =<br/>        (0..2)<br/>        .map(|_| new_painter_thread(data.clone()))<br/>        .collect();<br/><br/>    let _: Vec&lt;()&gt; = threads<br/>        .into_iter()<br/>        .map(|t| t.join().unwrap())<br/>        .collect();<br/><br/>    println!("Result: {:?}", data);<br/>}</pre>
<ol start="5">
<li>Let's run the code with <kbd>cargo run</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo run</strong><br/>   Compiling black-white v0.1.0 (Rust-Cookbook/Chapter04/black-<br/>   white)<br/>    Finished dev [unoptimized + debuginfo] target(s) in 0.35s<br/>     Running `target/debug/black-white`<br/>     Result: Mutex { data: [Black, White, Black, White, Black, <br/>    White, Black] }</pre>
<p><span>Now, let's go behind the scenes to understand the code better.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Rust's ownership principle is a double-edged sword: on the one hand, it protects from unintended consequences and enables compile-time memory management; on the other hand, mutable access is significantly more difficult to obtain. While it is more complex to manage, shared mutable access can be great for performance. </p>
<div class="packt_infobox"><kbd>Arc</kbd> stands for <strong>Atomic Reference Counter</strong>. This makes them very similar to regular reference counters (<kbd>Rc</kbd>), with the exception that an <kbd>Arc</kbd> does its job with an <em>atomic increment</em>, which is thread-safe. Therefore, they're the only choice for cross-threaded reference counting. </div>
<p>In Rust, this is done in a way similar to interior <span>mutability (<a href="https://doc.rust-lang.org/book/ch15-05-interior-mutability.html">https://doc.rust-lang.org/book/ch15-05-interior-mutability.html</a>), but using <kbd>Arc</kbd> and <kbd>Mutex</kbd> types (instead of <kbd>Rc</kbd> and <kbd>RefCell</kbd>), where <kbd>Mutex</kbd></span> owns the actual part of the memory it restricts access to (in step 3's snippet, we create the <kbd>Vec</kbd> just like that). As shown in <em>step 2</em>, to obtain a mutable reference to the value, locking the <kbd>Mutex</kbd> instance is strictly required and it will only be returned after the returned data instance is dropped (for example, when the scope ends). Hence, it is important to keep the scope of the <kbd>Mutex</kbd> as small as possible (note the additional <kbd>{ ... }</kbd> in <em>step 2</em>)!</p>
<p>In many use cases, a channel-based approach can achieve the same goal without having to deal with <kbd>Mutex</kbd> and the fear of a deadlock occurring (when several <kbd>Mutex</kbd> locks wait for each other to unlock).</p>
<p><span>We've successfully learned how to use channels to share mutable states. Now let's move on to the next recipe.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Multiprocessing in Rust</h1>
                </header>
            
            <article>
                
<p>Threading is great for in-process concurrency and certainly the preferred method of spreading workloads over multiple cores. Whenever other programs need to be called, or an independent, heavyweight task is required, sub-processes are the way to go. With the recent rise of orchestrator-type applications (Kubernetes, Docker Swarm, Mesos, and many others), managing child processes has become a more important topic as well. In this recipe, we will communicate with and manage child processes. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Follow these steps to create a simple application that searches the filesystem:<span><br/></span></p>
<ol>
<li><span>Create a new project using </span><kbd>cargo new child-processes</kbd><span> and open it in Visual Studio Code.</span></li>
<li><span>On Windows, execute <kbd>cargo run</kbd> (the last step) from a PowerShell window, since it contains all the required binaries.</span></li>
</ol>
<ol start="3">
<li>After importing a few (standard library) dependencies, let's write the basic <kbd>struct</kbd> <span>to hold the result data. Add this on top of the</span> <kbd>main</kbd> <span>function: </span></li>
</ol>
<pre style="padding-left: 60px">use std::io::Write;<br/>use std::process::{Command, Stdio};<br/><br/>#[derive(Debug)]<br/>struct SearchResult {<br/>    query: String,<br/>    results: Vec&lt;String&gt;,<br/>}</pre>
<ol start="4">
<li>The function calling the <kbd>find</kbd> binary (which does the actual searching) translates the results into the <kbd>struct</kbd> from <em>step 1</em>. This is what the function looks like:</li>
</ol>
<pre style="padding-left: 60px">fn search_file(name: String) -&gt; SearchResult {<br/>    let ps_child = Command::new("find")<br/>        .args(&amp;[".", "-iname", &amp;format!("{}", name)])<br/>        .stdout(Stdio::piped())<br/>        .output()<br/>        .expect("Could not spawn process");<br/><br/>    let results = String::from_utf8_lossy(&amp;ps_child.stdout);<br/>    let result_rows: Vec&lt;String&gt; = results<br/>        .split("\n")<br/>        .map(|e| e.to_string())<br/>        .filter(|s| s.len() &gt; 1)<br/>        .collect();<br/><br/>    SearchResult {<br/>        query: name,<br/>        results: result_rows,<br/>    }<br/>}</pre>
<ol start="5">
<li>Great! Now we know how to call an external binary, pass arguments in, and forward any <kbd>stdout</kbd> output to the Rust program. How about writing into the external program's <kbd>stdin</kbd>? We'll add the following function to do that:</li>
</ol>
<pre style="padding-left: 60px">fn process_roundtrip() -&gt; String {<br/>    let mut cat_child = Command::new("cat")<br/>        .stdin(Stdio::piped())<br/>        .stdout(Stdio::piped())<br/>        .spawn()<br/>        .expect("Could not spawn process");<br/><br/>    let stdin = cat_child.stdin.as_mut().expect("Could <br/>     not attach to stdin");<br/><br/>    stdin<br/>        .write_all(b"datadatadata")<br/>        .expect("Could not write to child process");<br/>    String::from_utf8(<br/>        cat_child<br/>            .wait_with_output()<br/>            .expect("Something went wrong")<br/>            .stdout<br/>            .as_slice()<br/>            .iter()<br/>            .cloned()<br/>            .collect(),<br/>    )<br/>    .unwrap()<br/>}</pre>
<ol start="6">
<li>To see it in action, we also need to call the functions in the <kbd>main()</kbd> part of the program. Replace the contents of the default <kbd>main()</kbd> function with the following:</li>
</ol>
<pre style="padding-left: 60px">fn main() {<br/>    println!("Reading from /bin/cat &gt; {:?}", process_roundtrip());<br/>    println!(<br/>        "Using 'find' to search for '*.rs': {:?}",<br/>        search_file("*.rs".to_owned())<br/>    )<br/>}</pre>
<ol start="7">
<li>Now we should see if it works by issuing <kbd>cargo run</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo run</strong><br/>   Compiling child-processes v0.1.0 (Rust-Cookbook/Chapter04/child-<br/>    processes)<br/>    Finished dev [unoptimized + debuginfo] target(s) in 0.59s<br/>    Running `target/debug/child-processes`<br/>    Reading from /bin/cat &gt; "datadatadata"<br/>    Using 'find' to search for '*.rs': SearchResult { query: "<br/>    *.rs", results: ["./src/main.rs"] }</pre>
<p><span>Now, let's go behind the scenes to understand the code better.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">By using Rust's ability to run sub-processes and manipulate their inputs and outputs, it's easy to integrate existing applications into the new program's workflow. In <em>step 1</em>, we do exactly that by using the <kbd>find</kbd> <span>program </span>with parameters and parse the output into our own data structure.</p>
<p>In <em>step 3</em>, we go further and send data into a sub-process and recover the same text (using <kbd>cat</kbd> in an echo-like fashion). You'll notice the parsing of a string in each function, which is required as Windows and Linux/macOS use different byte sizes to encode their characters (<strong>UTF-16</strong> and <strong>UTF-8</strong> respectively). Similarly, the <kbd>b"string"</kbd> transforms the literal into a byte-literal appropriate for the current platform. </p>
<p>The key ingredient for these operations is <strong>piping</strong>, an operation that is available on the command line using a <kbd>|</kbd> (<strong>pipe</strong>) symbol. We encourage you to try out other variants of the <kbd>Stdio</kbd> struct as well and see where they lead!</p>
<p><span>We've successfully learned about multiprocessing in Rust. Now let's move on to the next recipe.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Making sequential code parallel</h1>
                </header>
            
            <article>
                
<p>Creating highly concurrent applications from scratch is relatively simple in many technologies and languages. However, when multiple developers have to build on pre-existing work of some kind (legacy or not), creating these highly concurrent applications gets complicated. Thanks to API differences across languages, best practices, or technical limitations, existing operations on sequences cannot be run in parallel without in-depth analysis. Who would do that if the potential benefit is not significant? With Rust's powerful iterators, can we run operations in parallel without major code changes? Our answer is yes!</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This recipe shows you how to simply make an application run in parallel without massive effort using <kbd>rayon-rs</kbd> in just a few steps:<span><br/></span></p>
<ol>
<li><span>Create a new project using </span><kbd>cargo new use-rayon --lib</kbd><span> and open it in Visual Studio Code.</span></li>
<li>Open <kbd>Cargo.toml</kbd> <span>to add the required dependencies to the project. We are going to build on</span> <kbd>rayon</kbd> <span>and use the benchmarking abilities of</span> <kbd>criterion</kbd><span>:</span></li>
</ol>
<pre style="padding-left: 60px"># replace the default [dependencies] section...<br/>[dependencies]<br/>rayon = "1.0.3"<br/><br/><br/>[dev-dependencies]<br/>criterion = "0.2.11"<br/>rand = "^0.5"<br/><br/><br/>[[bench]]<br/>name = "seq_vs_par"<br/>harness = false</pre>
<ol start="3">
<li>As an example algorithm, we are going to use merge sort, a sophisticated, divide-and-conquer algorithm similar to quicksort (<a href="https://www.geeksforgeeks.org/quick-sort-vs-merge-sort/">https://www.geeksforgeeks.org/quick-sort-vs-merge-sort/</a>). Let's start off with the sequential version by adding the <kbd>merge_sort_seq()</kbd> function to <kbd>src/lib.rs</kbd>:</li>
</ol>
<pre style="padding-left: 60px">///<br/>/// Regular, sequential merge sort implementation<br/>/// <br/>pub fn merge_sort_seq&lt;T: PartialOrd + Clone + Default&gt;(collection: &amp;[T]) -&gt; Vec&lt;T&gt; {<br/>    if collection.len() &gt; 1 {<br/>        let (l, r) = collection.split_at(collection.len() / 2);<br/>        let (sorted_l, sorted_r) = (merge_sort_seq(l), <br/>         merge_sort_seq(r));<br/>        sorted_merge(sorted_l, sorted_r)<br/>    } else {<br/>        collection.to_vec()<br/>    }<br/>}</pre>
<ol start="4">
<li>The high-level view of merge sort is simple: split the collection in half until it's impossible to do it again, then merge the halves back <em>in order</em>. The splitting part is done; what's missing is the merging part. Insert this snippet into <kbd>lib.rs</kbd>:</li>
</ol>
<pre style="padding-left: 60px">///<br/>/// Merges two collections into one. <br/>/// <br/>fn sorted_merge&lt;T: Default + Clone + PartialOrd&gt;(sorted_l: Vec&lt;T&gt;, sorted_r: Vec&lt;T&gt;) -&gt; Vec&lt;T&gt; {<br/>    let mut result: Vec&lt;T&gt; = vec![Default::default(); sorted_l.len() <br/>     + sorted_r.len()];<br/><br/>    let (mut i, mut j) = (0, 0);<br/>    let mut k = 0;<br/>    while i &lt; sorted_l.len() &amp;&amp; j &lt; sorted_r.len() {<br/>        if sorted_l[i] &lt;= sorted_r[j] {<br/>            result[k] = sorted_l[i].clone();<br/>            i += 1;<br/>        } else {<br/>            result[k] = sorted_r[j].clone();<br/>            j += 1;<br/>        }<br/>        k += 1;<br/>    }<br/>    while i &lt; sorted_l.len() {<br/>        result[k] = sorted_l[i].clone();<br/>        k += 1;<br/>        i += 1;<br/>    }<br/><br/>    while j &lt; sorted_r.len() {<br/>        result[k] = sorted_r[j].clone();<br/>        k += 1;<br/>        j += 1;<br/>    }<br/>    result<br/>}</pre>
<p class="mce-root"/>
<ol start="5">
<li>Lastly, we will have to import <kbd>rayon</kbd>, a crate for creating parallel applications with ease, and then add a changed, parallelized version of merge sort:</li>
</ol>
<pre style="padding-left: 60px">use rayon;</pre>
<ol start="6">
<li>Next, we add a modified version of merge sort:</li>
</ol>
<pre style="padding-left: 60px">///<br/>/// Merge sort implementation using parallelism.<br/>/// <br/>pub fn merge_sort_par&lt;T&gt;(collection: &amp;[T]) -&gt; Vec&lt;T&gt;<br/>where<br/>    T: PartialOrd + Clone + Default + Send + Sync,<br/>{<br/>    if collection.len() &gt; 1 {<br/>        let (l, r) = collection.split_at(collection.len() / 2);<br/>        let (sorted_l, sorted_r) = rayon::join(|| merge_sort_par(l), <br/>        || merge_sort_par(r));<br/>        sorted_merge(sorted_l, sorted_r)<br/>    } else {<br/>        collection.to_vec()<br/>    }<br/>}</pre>
<ol start="7">
<li>Great—but can you spot the change? To make sure both variants deliver the same results, let's add a few tests:</li>
</ol>
<pre style="padding-left: 60px">#[cfg(test)]<br/>mod tests {<br/>    use super::*;<br/><br/>    #[test]<br/>    fn test_merge_sort_seq() {<br/>        assert_eq!(merge_sort_seq(&amp;vec![9, 8, 7, 6]), vec![6, 7, 8, <br/>         9]);<br/>        assert_eq!(merge_sort_seq(&amp;vec![6, 8, 7, 9]), vec![6, 7, 8, <br/>         9]);<br/>        assert_eq!(merge_sort_seq(&amp;vec![2, 1, 1, 1, 1]), vec![1, 1, <br/>         1, 1, 2]);<br/>    }<br/><br/>    #[test]<br/>    fn test_merge_sort_par() {<br/>        assert_eq!(merge_sort_par(&amp;vec![9, 8, 7, 6]), vec![6, 7, 8, <br/>         9]);<br/>        assert_eq!(merge_sort_par(&amp;vec![6, 8, 7, 9]), vec![6, 7, 8, <br/>         9]);<br/>        assert_eq!(merge_sort_par(&amp;vec![2, 1, 1, 1, 1]), vec![1, 1, <br/>         1, 1, 2]);<br/>    }<br/>}</pre>
<ol start="8">
<li>Run <kbd>cargo test</kbd> and you should see successful tests:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo test</strong><br/>   Compiling use-rayon v0.1.0 (Rust-Cookbook/Chapter04/use-rayon)<br/>    Finished dev [unoptimized + debuginfo] target(s) in 0.67s<br/>     Running target/debug/deps/use_rayon-1fb58536866a2b92<br/><br/>running 2 tests<br/>test tests::test_merge_sort_seq ... ok<br/>test tests::test_merge_sort_par ... ok<br/><br/>test result: ok. 2 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out<br/><br/>   Doc-tests use-rayon<br/><br/>running 0 tests<br/><br/>test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out</pre>
<ol start="9">
<li>However, we are really interested in the benchmarks—will it be faster? For that, create a <kbd>benches</kbd> folder containing a <kbd>seq_vs_par.rs</kbd> file. Open the file and add the following code:</li>
</ol>
<pre style="padding-left: 60px">#[macro_use]<br/>extern crate criterion;<br/>use criterion::black_box;<br/>use criterion::Criterion;<br/>use rand::prelude::*;<br/>use std::cell::RefCell;<br/>use use_rayon::{merge_sort_par, merge_sort_seq};<br/><br/>fn random_number_vec(size: usize) -&gt; Vec&lt;i64&gt; {<br/>    let mut v: Vec&lt;i64&gt; = (0..size as i64).collect();<br/>    let mut rng = thread_rng();<br/>    rng.shuffle(&amp;mut v);<br/>    v<br/>}<br/><br/>thread_local!(static ITEMS: RefCell&lt;Vec&lt;i64&gt;&gt; = RefCell::new(random_number_vec(100_000)));<br/><br/>fn bench_seq(c: &amp;mut Criterion) {<br/>    c.bench_function("10k merge sort (sequential)", |b| {<br/>        ITEMS.with(|item| b.iter(||         <br/>        black_box(merge_sort_seq(&amp;item.borrow()))));<br/>    });<br/>}<br/><br/>fn bench_par(c: &amp;mut Criterion) {<br/>    c.bench_function("10k merge sort (parallel)", |b| {<br/>        ITEMS.with(|item| b.iter(|| <br/>        black_box(merge_sort_par(&amp;item.borrow()))));<br/>    });<br/>}<br/>criterion_group!(benches, bench_seq, bench_par);<br/><br/>criterion_main!(benches);</pre>
<ol start="10">
<li>When we run <kbd>cargo bench</kbd>, we are getting actual numbers to compare parallel versus sequential implementations (the change refers to previous runs of the same benchmark):</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo bench</strong><br/>   Compiling use-rayon v0.1.0 (Rust-Cookbook/Chapter04/use-rayon)<br/>    Finished release [optimized] target(s) in 1.84s<br/>     Running target/release/deps/use_rayon-eb085695289744ef<br/><br/>running 2 tests<br/>test tests::test_merge_sort_par ... ignored<br/>test tests::test_merge_sort_seq ... ignored<br/><br/>test result: ok. 0 passed; 0 failed; 2 ignored; 0 measured; 0 filtered out<br/><br/>Running target/release/deps/seq_vs_par-6383ba0d412acb2b<br/>Gnuplot not found, disabling plotting<br/>10k merge sort (sequential) <br/>                        time: [13.815 ms 13.860 ms 13.906 ms]<br/>                        change: [-6.7401% -5.1611% -3.6593%] (p = <br/>                         0.00 &lt; 0.05)<br/>                        Performance has improved.<br/>Found 5 outliers among 100 measurements (5.00%)<br/>  3 (3.00%) high mild<br/>  2 (2.00%) high severe<br/><br/>10k merge sort (parallel) <br/>                        time: [10.037 ms 10.067 ms 10.096 ms]<br/>                        change: [-15.322% -13.276% -11.510%] (p = <br/>                        0.00 &lt; 0.05)<br/>                        Performance has improved.<br/>Found 6 outliers among 100 measurements (6.00%)<br/>  1 (1.00%) low severe<br/>  1 (1.00%) high mild<br/>  4 (4.00%) high severe<br/><br/>Gnuplot not found, disabling plotting</pre>
<p><span>Now, let's check what all of this means and pull back the curtains on the code.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><kbd>rayon-rs</kbd> (<a href="https://github.com/rayon-rs/rayon">https://github.com/rayon-rs/rayon</a>) is a popular data-parallelism crate that only requires a few modifications to introduce automatic concurrency into the code. In our example, we are using the <kbd>rayon::join</kbd> operation to create a parallel version of the popular merge sort algorithm. </p>
<p>In <em>step 1</em>, we are adding dependencies for benchmarks (<kbd><span>[dev-dependencies]</span></kbd>) and to actually build the library (<kbd>[dependencies]</kbd>). But in <em>step 2</em> and <em>step 3</em>, we are implementing a regular merge sort variation. Once we add the <kbd>rayon</kbd> dependency in <em>step 4</em>, we can add <kbd>rayon::join</kbd> in <em>step 5</em> to run each branch (to sorting of the left and right parts) in its own closure (<kbd>|/*no params*/| {/* do work */}</kbd>, or <kbd>|/*no params*/| /*do work*/</kbd> for short) in parallel <em>if possible</em>. The docs on <kbd>join</kbd> can be found at <a href="https://docs.rs/rayon/1.2.0/rayon/fn.join.html">https://docs.rs/rayon/1.2.0/rayon/fn.join.html</a>, go into the details about when it speeds things up.</p>
<p>In <em>step 8</em>, we are creating a benchmark test as required by the criterion. The library compiles a file outside the <kbd>src/</kbd> directory to run within the benchmark harness and output numbers (as shown in <em>step 9</em>)—and in these numbers, we can see a slight but consistent improvement in performance just by adding one line of code. Within the benchmark file, we are sorting a copy of the same random vector (<kbd>thread_local!()</kbd> is somewhat akin to <kbd>static</kbd>) of 100,000 random numbers. </p>
<p><span>We've successfully learned how to make sequential code parallel. Now let's move on to the next recipe.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Concurrent data processing in vectors</h1>
                </header>
            
            <article>
                
<p>Rust's <kbd>Vec</kbd> is a great data structure that is used not only for holding data but also as a management tool of sorts. In an earlier recipe (<em>Managing multiple threads</em>) in this chapter, w<span>e saw that</span> when we captured the handles of multiple threads in <kbd>Vec</kbd> and then used the <kbd>map()</kbd> function to join them. <span>This time, we are going to focus on concurrently processing regular</span> <kbd>Vec</kbd> <span>instances without additional overhead. In the previous recipe, we saw the power of <kbd>rayon-rs</kbd> and now we are going to use it to parallelize data processing. </span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's use <kbd>rayon-rs</kbd> some more in the following steps:<span><br/></span></p>
<ol>
<li><span>Create a new project using </span><kbd>cargo new concurrent-processing --lib</kbd><span> and open it in Visual Studio Code.</span></li>
<li>2 First, we have to add <kbd>rayon</kbd> <span>as a dependency by adding a few lines to <kbd>Cargo.toml</kbd>. Additionally, the <kbd>rand</kbd> crate and criterion for benchmarking will be useful later on, so let's add those as well and configure them appropriately:</span></li>
</ol>
<pre style="padding-left: 60px">[dependencies]<br/>rayon = "1.0.3"<br/><br/>[dev-dependencies]<br/>criterion = "0.2.11"<br/>rand = "^0.5"<br/><br/>[[bench]]<br/>name = "seq_vs_par"<br/>harness = false</pre>
<ol start="3">
<li>Since we are going to add a significant statistical error measure, that is, the sum of squared errors, o<span>pen </span><kbd>src/lib.rs</kbd>. In its sequential incarnation, we simply iterate over the predictions and their original value to find out the difference, then square it, and sum up the results. Let's add that to the file:</li>
</ol>
<pre style="padding-left: 60px">pub fn ssqe_sequential(y: &amp;[f32], y_predicted: &amp;[f32]) -&gt; Option&lt;f32&gt; {<br/>    if y.len() == y_predicted.len() {<br/>        let y_iter = y.iter();<br/>        let y_pred_iter = y_predicted.iter();<br/><br/>        Some(<br/>            y_iter<br/>                .zip(y_pred_iter)<br/>                .map(|(y, y_pred)| (y - y_pred).powi(2))<br/>                .sum()<br/>        ) <br/>    } else {<br/>        None<br/>    }<br/>}</pre>
<ol start="4">
<li>That seems easily parallelizable, and <kbd>rayon</kbd> offers us just the tools for it. Let's create almost the same code using concurrency:</li>
</ol>
<pre style="padding-left: 60px">use rayon::prelude::*;<br/><br/>pub fn ssqe(y: &amp;[f32], y_predicted: &amp;[f32]) -&gt; Option&lt;f32&gt; {<br/>    if y.len() == y_predicted.len() {<br/>        let y_iter = y.<strong>par_iter</strong>();<br/>        let y_pred_iter = y_predicted.<strong>par_iter</strong>();<br/><br/>        Some(<br/>            y_iter<br/>                .zip(y_pred_iter)<br/>                .map(|(y, y_pred)| (y - y_pred).powi(2))<br/>                .reduce(|| 0.0, |a, b| a + b),<br/>        ) // or sum()<br/>    } else {<br/>        None<br/>    }<br/>}</pre>
<ol start="5">
<li>While the differences to the sequential code are very subtle, the changes have a substantial impact on execution speed! Before we proceed, we should add some tests to see the results of actually calling the functions. Let's start with the parallel version first:</li>
</ol>
<pre style="padding-left: 60px">#[cfg(test)]<br/>mod tests {<br/>    use super::*;<br/><br/>    #[test]<br/>    fn test_sum_of_sq_errors() {<br/>        assert_eq!(<br/>            ssqe(&amp;[1.0, 1.0, 1.0, 1.0], &amp;[2.0, 2.0, 2.0, 2.0]),<br/>            Some(4.0)<br/>        );<br/>        assert_eq!(<br/>            ssqe(&amp;[-1.0, -1.0, -1.0, -1.0], &amp;[-2.0, -2.0, -2.0, <br/>             -2.0]),<br/>            Some(4.0)<br/>        );<br/>        assert_eq!(<br/>            ssqe(&amp;[-1.0, -1.0, -1.0, -1.0], &amp;[2.0, 2.0, 2.0, 2.0]),<br/>            Some(36.0)<br/>        );<br/>        assert_eq!(<br/>            ssqe(&amp;[1.0, 1.0, 1.0, 1.0], &amp;[2.0, 2.0, 2.0, 2.0]),<br/>            Some(4.0)<br/>        );<br/>        assert_eq!(<br/>            ssqe(&amp;[1.0, 1.0, 1.0, 1.0], &amp;[2.0, 2.0, 2.0, 2.0]),<br/>            Some(4.0)<br/>        );<br/>    }</pre>
<ol start="6">
<li class="mce-root">The sequential code should have the same results, so let's duplicate the test for the sequential version of the code:</li>
</ol>
<pre style="padding-left: 60px">    #[test]<br/>    fn test_sum_of_sq_errors_seq() {<br/>        assert_eq!(<br/>            ssqe_sequential(&amp;[1.0, 1.0, 1.0, 1.0], &amp;[2.0, 2.0, 2.0, <br/>             2.0]),<br/>            Some(4.0)<br/>        );<br/>        assert_eq!(<br/>            ssqe_sequential(&amp;[-1.0, -1.0, -1.0, -1.0], &amp;[-2.0,<br/>             -2.0, -2.0, -2.0]),<br/>            Some(4.0)<br/>        );<br/>        assert_eq!(<br/>            ssqe_sequential(&amp;[-1.0, -1.0, -1.0, -1.0], &amp;[2.0, 2.0, <br/>             2.0, 2.0]),<br/>            Some(36.0)<br/>        );<br/>        assert_eq!(<br/>            ssqe_sequential(&amp;[1.0, 1.0, 1.0, 1.0], &amp;[2.0, 2.0, 2.0, <br/>             2.0]),<br/>            Some(4.0)<br/>        );<br/>        assert_eq!(<br/>            ssqe_sequential(&amp;[1.0, 1.0, 1.0, 1.0], &amp;[2.0, 2.0, 2.0, <br/>             2.0]),<br/>            Some(4.0)<br/>        );<br/>    }<br/>}</pre>
<ol start="7">
<li>In order to check that everything works as expected, run <kbd>cargo test</kbd> in between:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo test</strong><br/>   Compiling concurrent-processing v0.1.0 (Rust-<br/>    Cookbook/Chapter04/concurrent-processing)<br/>    Finished dev [unoptimized + debuginfo] target(s) in 0.84s<br/>     Running target/debug/deps/concurrent_processing-<br/>      250eef41459fd2af<br/><br/>running 2 tests<br/>test tests::test_sum_of_sq_errors_seq ... ok<br/>test tests::test_sum_of_sq_errors ... ok<br/><br/>test result: ok. 2 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out<br/><br/>   Doc-tests concurrent-processing<br/><br/>running 0 tests<br/><br/>test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out</pre>
<ol start="8">
<li>As an additional feat of <kbd>rayon</kbd>, let's also add some more functions to <kbd>src/lib.rs</kbd>. This time, they are related to counting alphanumeric characters in <kbd>str</kbd>:</li>
</ol>
<pre style="padding-left: 60px">pub fn seq_count_alpha_nums(corpus: &amp;str) -&gt; usize {<br/>    corpus.chars().filter(|c| c.is_alphanumeric()).count()<br/>}<br/><br/><br/>pub fn par_count_alpha_nums(corpus: &amp;str) -&gt; usize {<br/>    corpus.par_chars().filter(|c| c.is_alphanumeric()).count()<br/>}</pre>
<ol start="9">
<li>Now let's see which performs better and let's add a benchmark. To do that, create a <kbd>benches/</kbd> <span>directory </span>next to <kbd>src/</kbd> with a <kbd>seq_vs_par.rs</kbd> file. Add the following benchmark and helper functions to see what the speedups are. Let's start with a few helpers that define the basic data the benchmark is processing:</li>
</ol>
<pre style="padding-left: 60px">#[macro_use]<br/>extern crate criterion;<br/>use concurrent_processing::{ssqe, ssqe_sequential, seq_count_alpha_nums, par_count_alpha_nums};<br/>use criterion::{black_box, Criterion};<br/>use std::cell::RefCell;<br/>use rand::prelude::*;<br/><br/>const SEQ_LEN: usize = 1_000_000;<br/>thread_local!(static ITEMS: RefCell&lt;(Vec&lt;f32&gt;, Vec&lt;f32&gt;)&gt; = {<br/>    let y_values: (Vec&lt;f32&gt;, Vec&lt;f32&gt;) = (0..SEQ_LEN).map(|_| <br/>     (random::&lt;f32&gt;(), random::&lt;f32&gt;()) )<br/>    .unzip();<br/>    RefCell::new(y_values)<br/>});<br/><br/><br/>const MAX_CHARS: usize = 100_000;<br/>thread_local!(static CHARS: RefCell&lt;String&gt; = {<br/>    let items: String = (0..MAX_CHARS).map(|_| random::&lt;char&gt;<br/>     ()).collect();<br/>    RefCell::new(items)<br/>});</pre>
<ol start="10">
<li>Next, we are going to create the benchmarks themselves:</li>
</ol>
<pre style="padding-left: 60px">fn bench_count_seq(c: &amp;mut Criterion) {<br/>    c.bench_function("Counting in sequence", |b| {<br/>        CHARS.with(|item| b.iter(|| <br/>         black_box(seq_count_alpha_nums(&amp;item.borrow()))))<br/>    });<br/>}<br/><br/>fn bench_count_par(c: &amp;mut Criterion) {<br/>    c.bench_function("Counting in parallel", |b| {<br/>        CHARS.with(|item| b.iter(|| <br/>         black_box(par_count_alpha_nums(&amp;item.borrow()))))<br/>    });<br/>}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="11">
<li>Let's create another benchmark:</li>
</ol>
<pre style="padding-left: 60px">fn bench_seq(c: &amp;mut Criterion) {<br/>    c.bench_function("Sequential vector operation", |b| {<br/>        ITEMS.with(|y_values| {<br/>            let y_borrowed = y_values.borrow();<br/>            b.iter(|| black_box(ssqe_sequential(&amp;y_borrowed.0, <br/>             &amp;y_borrowed.1)))<br/>        })<br/>    });<br/>}<br/><br/>fn bench_par(c: &amp;mut Criterion) {<br/>    c.bench_function("Parallel vector operation", |b| {<br/>        ITEMS.with(|y_values| {<br/>            let y_borrowed = y_values.borrow();<br/>            b.iter(|| black_box(ssqe(&amp;y_borrowed.0, <br/>            &amp;y_borrowed.1)))<br/>        })<br/>    });<br/>}<br/><br/>criterion_group!(benches, bench_seq, bench_par,bench_count_par, bench_count_seq);<br/><br/>criterion_main!(benches);</pre>
<ol start="12">
<li>With that available, run <kbd>cargo bench</kbd> and (after a while) check the outputs to see the improvements and timings (the changed part refers to the changes from the previous run of the same benchmark): </li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo bench</strong><br/>   Compiling concurrent-processing v0.1.0 (Rust-<br/>    Cookbook/Chapter04/concurrent-processing)<br/>    Finished release [optimized] target(s) in 2.37s<br/>     Running target/release/deps/concurrent_processing-<br/>      eedf0fd3b1e51fe0<br/><br/>running 2 tests<br/>test tests::test_sum_of_sq_errors ... ignored<br/>test tests::test_sum_of_sq_errors_seq ... ignored<br/><br/>test result: ok. 0 passed; 0 failed; 2 ignored; 0 measured; 0 filtered out<br/><br/>Running target/release/deps/seq_vs_par-ddd71082d4bd9dd6<br/>Gnuplot not found, disabling plotting<br/>Sequential vector operation <br/>                        time: [1.0631 ms 1.0681 ms 1.0756 ms]<br/>                        change: [-4.8191% -3.4333% -2.3243%] (p = <br/>                        0.00 &lt; 0.05)<br/>                        Performance has improved.<br/>Found 4 outliers among 100 measurements (4.00%)<br/>  2 (2.00%) high mild<br/>  2 (2.00%) high severe<br/><br/>Parallel vector operation <br/>                        time: [408.93 us 417.14 us 425.82 us]<br/>                        change: [-9.5623% -6.0044% -2.2126%] (p = <br/>                        0.00 &lt; 0.05)<br/>                        Performance has improved.<br/>Found 15 outliers among 100 measurements (15.00%)<br/>  2 (2.00%) low mild<br/>  7 (7.00%) high mild<br/>  6 (6.00%) high severe<br/><br/>Counting in parallel time: [552.01 us 564.97 us 580.51 us] <br/>                        change: [+2.3072% +6.9101% +11.580%] (p = <br/>                        0.00 &lt; 0.05)<br/>                        Performance has regressed.<br/>Found 4 outliers among 100 measurements (4.00%)<br/>  3 (3.00%) high mild<br/>  1 (1.00%) high severe<br/><br/>Counting in sequence time: [992.84 us 1.0137 ms 1.0396 ms] <br/>                        change: [+9.3014% +12.494% +15.338%] (p = <br/>                        0.00 &lt; 0.05)<br/>                        Performance has regressed.<br/>Found 4 outliers among 100 measurements (4.00%)<br/>  4 (4.00%) high mild<br/><br/>Gnuplot not found, disabling plotting</pre>
<p><span>Now, let's go behind the scenes to understand the code better.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Again, <kbd>rayon-rs</kbd>—a fantastic library—has made <span>roughly</span><span> </span>a 50% improvement in the benchmark <span>performance </span>(parallel versus sequential) by changing <strong>a single line of code</strong>.<strong> </strong>This is significant for many applications but in particular for machine learning, where the loss function of an algorithm is required to run hundreds or thousands of times during a training cycle. Cutting this time in half would immediately have a large impact on productivity. </p>
<p>In the first steps after setting everything up (<em>step 3</em>, <em>step 4</em>, and <em>step 5</em>), we are creating a sequential and parallel implementation of the sum of squared errors (<a href="https://hlab.stanford.edu/brian/error_sum_of_squares.html">https://hlab.stanford.edu/brian/error_sum_of_squares.html</a>) with the only difference being <kbd>par_iter()</kbd> versus the <kbd>iter()</kbd> call including some tests. Then we add some—more common—counting functions to our benchmark suite, which we'll create and call in <em>step 7</em> and <em>step 8</em>. Again, the sequential and parallel algorithms work on exactly the same dataset every time to avoid any unfortunate incidents.</p>
<p><span>We've successfully learned how to process data concurrently in vectors. Now let's move on to the next recipe.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Shared immutable states</h1>
                </header>
            
            <article>
                
<p>Sometimes, when a program operates on multiple threads, the current version of settings and many more are available to the threads as a single point of truth. Sharing a state between threads is straightforward in Rust—as long as the variable is immutable and the types are marked as safe to share. In order to mark types as thread-safe, it's important that the implementation makes sure that accessing the information can be done without any kind of inconsistency occurring.</p>
<p>Rust uses two marker traits—<kbd>Send</kbd> and <kbd>Sync</kbd>—to manage these options. Let's see how.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In just a few steps, we'll explore immutable states:</p>
<ol>
<li><span>Run</span><span> </span><kbd>cargo new immutable-states</kbd><span> </span><span>to create a new application project and open the directory in Visual Studio Code.</span></li>
<li>First, we'll add the imports and a <kbd>noop</kbd> <span>function to call to our</span> <kbd>src/main.rs</kbd> <span>file:</span></li>
</ol>
<pre style="padding-left: 60px">use std::thread;<br/>use std::rc::Rc;<br/>use std::sync::Arc;<br/>use std::sync::mpsc::channel;<br/><br/><br/>fn noop&lt;T&gt;(_: T) {}</pre>
<ol start="3">
<li>Let's explore how different types can be shared across threads. The <kbd>mpsc::channel</kbd> type provides a great out-of-the-box example of a shared state. Let's start off with a baseline that works as expected:</li>
</ol>
<pre style="padding-left: 60px">fn main() {<br/>    let (sender, receiver) = channel::&lt;usize&gt;();<br/><br/>    thread::spawn(move || {<br/>        let thread_local_read_only_clone = sender.clone();<br/>        noop(thread_local_read_only_clone);<br/>    });<br/>}</pre>
<ol start="4">
<li>To see it working, execute <kbd>cargo build</kbd>. Any errors with respect to illegal state sharing will be found by the compiler:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo build</strong><br/>   Compiling immutable-states v0.1.0 (Rust-Cookbook/Chapter04<br/>   /immutable-states)<br/>warning: unused import: `std::rc::Rc`<br/> --&gt; src/main.rs:2:5<br/>  |<br/>2 | use std::rc::Rc;<br/>  | ^^^^^^^^^^^<br/>  |<br/>  = note: #[warn(unused_imports)] on by default<br/><br/>warning: unused import: `std::sync::Arc`<br/> --&gt; src/main.rs:3:5<br/>  |<br/>3 | use std::sync::Arc;<br/>  | ^^^^^^^^^^^^^^<br/><br/>warning: unused variable: `receiver`<br/>  --&gt; src/main.rs:10:18<br/>   |<br/>10 | let (sender, receiver) = channel::&lt;usize&gt;();<br/>   | ^^^^^^^^ help: consider prefixing with an underscore: `_receiver`<br/>   |<br/>   = note: #[warn(unused_variables)] on by default<br/><br/>    Finished dev [unoptimized + debuginfo] target(s) in 0.58s</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>Now we'll try the same thing with the receiver. Will it work? Add this to the <kbd>main</kbd> function:</li>
</ol>
<pre>    let c = Arc::new(receiver);<br/>    thread::spawn(move || {<br/>        noop(c.clone());<br/>    });</pre>
<ol start="6">
<li>Run <kbd>cargo build</kbd> to get a more extensive message:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo build</strong><br/>   Compiling immutable-states v0.1.0 (Rust-Cookbook/Chapter04<br/>    /immutable-states)<br/>warning: unused import: `std::rc::Rc`<br/> --&gt; src/main.rs:2:5<br/>  |<br/>2 | use std::rc::Rc;<br/>  | ^^^^^^^^^^^<br/>  |<br/>  = note: #[warn(unused_imports)] on by default<br/><br/>error[E0277]: `std::sync::mpsc::Receiver&lt;usize&gt;` cannot be shared between threads safely<br/>  --&gt; src/main.rs:26:5<br/>   |<br/>26 | thread::spawn(move || {<br/>   | ^^^^^^^^^^^^^ `std::sync::mpsc::Receiver&lt;usize&gt;` cannot be shared between threads safely<br/>   |<br/>   = help: the trait `std::marker::Sync` is not implemented for `std::sync::mpsc::Receiver&lt;usize&gt;`<br/>   = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Arc&lt;std::sync::mpsc::Receiver&lt;usize&gt;&gt;`<br/>   = note: required because it appears within the type `[closure@src/main.rs:26:19: 28:6 c:std::sync::Arc&lt;std::sync::mpsc::Receiver&lt;usize&gt;&gt;]`<br/>   = note: required by `std::thread::spawn`<br/><br/>error: aborting due to previous error<br/><br/>For more information about this error, try `rustc --explain E0277`.<br/>error: Could not compile `immutable-states`.<br/><br/>To learn more, run the command again with --verbose.</pre>
<ol start="7">
<li>Since the receiver is only made for a single thread to fetch data out of the channel, it's to be expected that this cannot be avoided using <kbd>Arc</kbd>. Similarly, it's impossible to simply wrap <kbd>Rc</kbd> into <kbd>Arc</kbd> to make it available across threads. Add the following to see the error:</li>
</ol>
<pre>   let b = Arc::new(Rc::new(vec![]));<br/>    thread::spawn(move || {<br/>        let thread_local_read_only_clone = b.clone();<br/>        noop(thread_local_read_only_clone);<br/>    });</pre>
<ol start="8">
<li><kbd>cargo build</kbd> reveals the consequences again—an error about how the type is unable to be sent across threads:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo build</strong><br/>   Compiling immutable-states v0.1.0 (Rust-Cookbook/Chapter04<br/>   /immutable-states)<br/>error[E0277]: `std::rc::Rc&lt;std::vec::Vec&lt;_&gt;&gt;` cannot be sent between threads safely<br/>  --&gt; src/main.rs:19:5<br/>   |<br/>19 | thread::spawn(move || {<br/>   | ^^^^^^^^^^^^^ `std::rc::Rc&lt;std::vec::Vec&lt;_&gt;&gt;` cannot be sent between threads safely<br/>   |<br/>   = help: the trait `std::marker::Send` is not implemented for `std::rc::Rc&lt;std::vec::Vec&lt;_&gt;&gt;`<br/>   = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Arc&lt;std::rc::Rc&lt;std::vec::Vec&lt;_&gt;&gt;&gt;`<br/>   = note: required because it appears within the type `[closure@src/main.rs:19:19: 22:6 b:std::sync::Arc&lt;std::rc::Rc&lt;std::vec::Vec&lt;_&gt;&gt;&gt;]`<br/>   = note: required by `std::thread::spawn`<br/><br/>error[E0277]: `std::rc::Rc&lt;std::vec::Vec&lt;_&gt;&gt;` cannot be shared between threads safely<br/>  --&gt; src/main.rs:19:5<br/>   |<br/>19 | thread::spawn(move || {<br/>   | ^^^^^^^^^^^^^ `std::rc::Rc&lt;std::vec::Vec&lt;_&gt;&gt;` cannot be shared between threads safely<br/>   |<br/>   = help: the trait `std::marker::Sync` is not implemented for `std::rc::Rc&lt;std::vec::Vec&lt;_&gt;&gt;`<br/>   = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Arc&lt;std::rc::Rc&lt;std::vec::Vec&lt;_&gt;&gt;&gt;`<br/>   = note: required because it appears within the type `[closure@src/main.rs:19:19: 22:6 b:std::sync::Arc&lt;std::rc::Rc&lt;std::vec::Vec&lt;_&gt;&gt;&gt;]`<br/>   = note: required by `std::thread::spawn`<br/><br/>error: aborting due to 2 previous errors<br/><br/>For more information about this error, try `rustc --explain E0277`.<br/>error: Could not compile `immutable-states`.<br/><br/>To learn more, run the command again with --verbose. </pre>
<p><span>Now, let's go behind the scenes to understand the code better.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Since this recipe actually failed to build and pointed to an error message in the last step, what happened? We learned about <kbd>Send</kbd> and <kbd>Sync</kbd>. These marker traits and the types of errors will cross your path in the most surprising and critical situations. Since they work seamlessly when they are present, we had to create a failing example to show you what magic they do and how. </p>
<p>In Rust, marker traits (<a href="https://doc.rust-lang.org/std/marker/index.html">https://doc.rust-lang.org/std/marker/index.html</a>) signal something to the compiler. In the case of concurrency, it's the ability to be shared across threads. The <kbd>Sync</kbd> (shared access from multiple threads) and <kbd>Send</kbd> (ownership can transfer safely from one thread to another) traits are implemented for almost all default data structures, but if <kbd>unsafe</kbd> code is required, then the marker traits have to be added manually—which is also <kbd>unsafe</kbd>. </p>
<p>Hence, most of the data structures will be able to inherit <kbd>Send</kbd> and <kbd>Sync</kbd> from their properties, which is what happens in <em>step 2</em> and <em>step 3</em>. Mostly, you'll wrap your instance in <kbd>Arc</kbd> as well for easier handling. However, multiple instances of <kbd>Arc</kbd> require their contained types to implement <kbd>Send</kbd> and <kbd>Sync</kbd>. In <em>step 4</em> and <em>step 6</em>, we try to get the available types into <kbd>Arc</kbd>—without implementing either <kbd>Sync</kbd> or <kbd>Send</kbd>. <em>Step 5</em> and <em>step 7</em> show the compiler's error messages for either try.  If you want to know more and see how to add the <kbd>marker</kbd> trait (<a href="https://doc.rust-lang.org/std/marker/index.html">https://doc.rust-lang.org/std/marker/index.html</a>) to custom types, check out the documentation at <a href="https://doc.rust-lang.org/nomicon/send-and-sync.html">https://doc.rust-lang.org/nomicon/send-and-sync.html.</a> </p>
<p><span>Now that we know more about <kbd>Send</kbd> and <kbd>Sync</kbd>, sharing states in concurrent programs is less of a mystery. Let's move on to the next recipe.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Handling asynchronous messages with actors</h1>
                </header>
            
            <article>
                
<p>Scalable architectures and asynchronous programming have led to a rise of actors and actor-based designs (<a href="https://mattferderer.com/what-is-the-actor-model-and-when-should-you-use-it">https://mattferderer.com/what-is-the-actor-model-and-when-should-you-use-it</a>), facilitated by frameworks such as Akka (<a href="https://akka.io/">https://akka.io/</a>). Regardless of Rust's powerful concurrency features, actors in Rust are still tricky to get right and they lack the documentation that many other libraries have. In this recipe, we are going to explore the basics of <kbd>actix</kbd>, Rust's actor framework, which was created after the popular Akka.  </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Implement an actor-based sensor data reader in just a few steps:</p>
<ol>
<li><span>Create a new binary application using </span><kbd>cargo new actors</kbd><span> and open the directory in Visual Studio Code.</span></li>
<li>Include the required dependencies in the <kbd>Cargo.toml</kbd> <span>configuration file:</span></li>
</ol>
<pre style="padding-left: 60px">[package]<br/>name = "actors"<br/>version = "0.1.0"<br/>authors = ["Claus Matzinger &lt;claus.matzinger+kb@gmail.com&gt;"]<br/>edition = "2018"<br/><br/>[dependencies]<br/>actix = "^0.8"<br/>rand = "0.5"</pre>
<ol start="3">
<li>Open <kbd>src/main.rs</kbd> to add the code before the <kbd>main</kbd> function. Let's start with the imports:</li>
</ol>
<pre style="padding-left: 60px">use actix::prelude::*;<br/>use std::thread;<br/>use std::time::Duration;<br/>use rand::prelude::*;</pre>
<p class="mce-root"/>
<ol start="4">
<li>In order to create an actor system, we'll have to think about the application's structure. An actor can be thought of as a message receiver with a postbox where messages are piled up until they are processed. For simplicity, let's mock up some sensor data mock as messages, each consisting of a <kbd>u64</kbd> timestamp and a <kbd>f32</kbd> value:</li>
</ol>
<pre style="padding-left: 60px">///<br/>/// A mock sensor function<br/>/// <br/>fn read_sensordata() -&gt; f32 {<br/>     random::&lt;f32&gt;() * 10.0<br/>}<br/><br/>#[derive(Debug, Message)]<br/>struct Sensordata(pub u64, pub f32);</pre>
<ol start="5">
<li>In a typical system, we would use an I/O loop to read from the sensor(s) in scheduled intervals. Since <kbd>actix</kbd> (<a href="https://github.com/actix/actix/">https://github.com/actix/actix/</a>) builds on Tokio (<a href="https://tokio.rs/">https://tokio.rs/</a>), that can be explored outside this recipe. To simulate the fast reading and slow processing steps, <span>we'll implement it as a <kbd>for</kbd> loop</span>:</li>
</ol>
<pre style="padding-left: 60px">fn main() -&gt; std::io::Result&lt;()&gt; {<br/>    System::run(|| {<br/>        println!("&gt;&gt; Press Ctrl-C to stop the program");<br/>        // start multi threaded actor host (arbiter) with 2 threads<br/>        let sender = SyncArbiter::start(N_THREADS, || <br/>        DBWriter);<br/>      <br/>        // send messages to the actor <br/>        for n in 0..10_000 {<br/>            let my_timestamp = n as u64;<br/>            let data = read_sensordata();<br/>            sender.do_send(Sensordata(my_timestamp, data));<br/>        }<br/>    })<br/>}</pre>
<ol start="6">
<li>Let's take care of implementing the most important part: the actor's message handling. <kbd>actix</kbd> requires you to implement the <kbd>Handler&lt;T&gt;</kbd> trait. Add the following implementation just before the <kbd>main</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">struct DBWriter;<br/><br/>impl Actor for DBWriter {<br/>    type Context = SyncContext&lt;Self&gt;;<br/>}<br/><br/>impl Handler&lt;Sensordata&gt; for DBWriter {<br/>    type Result = ();<br/><br/>    fn handle(&amp;mut self, msg: Sensordata, _: &amp;mut Self::Context) -&gt; <br/>     Self::Result {<br/><br/>        // send stuff somewhere and handle the results<br/>        println!(" {:?}", msg);<br/>        thread::sleep(Duration::from_millis(300));<br/>    }<br/>}</pre>
<ol start="7">
<li>Use <kbd>cargo run</kbd> to run the program and see how it generates artificial sensor data (press <em>Ctrl + C</em> if you don't want to wait for it to finish):</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo run</strong><br/>   Compiling actors v0.1.0 (Rust-Cookbook/Chapter04/actors)<br/>    Finished dev [unoptimized + debuginfo] target(s) in 2.05s<br/>     Running `target/debug/actors`<br/>&gt;&gt; Press Ctrl-C to stop the program<br/>  Sensordata(0, 2.2577233)<br/>  Sensordata(1, 4.039347)<br/>  Sensordata(2, 8.981095)<br/>  Sensordata(3, 1.1506838)<br/>  Sensordata(4, 7.5091066)<br/>  Sensordata(5, 2.5614727)<br/>  Sensordata(6, 3.6907816)<br/>  Sensordata(7, 7.907603)<br/>  ^C⏎    </pre>
<p><span>Now, let's go behind the scenes to understand the code better.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The actor model solves the shortcomings of passing data around threads using an object-oriented approach. By utilizing an implicit queue for messages to and from actors, it can prevent expensive locking and corrupt states. There is extensive content on the topic, for example, in Akka's documentation at <a href="https://doc.akka.io/docs/akka/current/guide/actors-intro.html">https://doc.akka.io/docs/akka/current/guide/actors-intro.html</a>. </p>
<p>After preparing the project in the first two steps, <em>step 3</em> shows the implementation of the <kbd>Message</kbd> trait using a macro (<kbd>[#derive()]</kbd>). With that available, we proceed to set up the main <em>system</em>—the main loop that runs the actor scheduling and message passing behind the scenes. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><kbd>actix</kbd> uses <kbd>Arbiters</kbd> to run different actors and tasks. A regular Arbiter is basically a single-threaded event loop, helpful for working in a non-concurrent setting. <kbd>SyncArbiter</kbd>, on the other hand, is a multithreaded version that allows the use of actors across threads. In our case, we used three threads. </p>
<p>In <em>step 5</em>, we see the required minimum implementation of a handler. Using <kbd>SyncArbiter</kbd> does not allow sending messages back via the return value, which is why the result is an empty tuple for now. The handler is also specific to the message type and the handle function simulates a long-running action by issuing <kbd>thread::sleep</kbd>—this only works because it's the only actor running in that particular thread. </p>
<p>We have only scraped the surface of what <kbd>actix</kbd> can do (leaving out the all-powerful Tokio tasks and streams). Check out their book (<a href="https://actix.rs/book/actix/">https://actix.rs/book/actix/</a>) on the topic and the examples in their GitHub repositories<a href="https://tokio.rs">.</a><a href="https://tokio.rs"/></p>
<p><span>We've successfully learned how to handle asynchronous messages with actors. Now let's move on to the next recipe.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Asynchronous programming with futures</h1>
                </header>
            
            <article>
                
<p>Using futures is a common technique in JavaScript, TypeScript, C#, and similar technologies—made popular by the addition of the <kbd>async</kbd>/<kbd>await</kbd> keywords in their syntax. In a nutshell, futures (or promises) is a function's guarantee that, at some point, the handle will be resolved and the actual value will be returned. However, there is no explicit time when this is going to happen—but you can schedule entire chains of promises that are resolved after each other. How does this work in Rust? Let's find out in this recipe.</p>
<div class="packt_infobox">At the time of writing, <kbd>async</kbd>/<kbd>await</kbd> were under heavy development. Depending on when you are reading this book, the examples may have stopped working. In this case, we ask you to open an issue in the accompanying repository so we can fix the issues. For updates, check the Rust <kbd>async</kbd> working group's repository at <a href="https://github.com/rustasync/team">https://github.com/rustasync/team</a>.</div>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In a few steps, we'll be able to use <kbd>async</kbd> and <kbd>await</kbd> in Rust for seamless concurrency:</p>
<ol>
<li><span>Create a new binary application using</span><span> </span><kbd>cargo new async-await</kbd><span> </span><span>and open the directory in Visual Studio Code.</span></li>
<li>As usual, when we are integrating a library, we'll have to add the dependencies to <kbd>Cargo.toml</kbd><span>:</span></li>
</ol>
<pre style="padding-left: 60px">[package]<br/>name = "async-await"<br/>version = "0.1.0"<br/>authors = ["Claus Matzinger &lt;claus.matzinger+kb@gmail.com&gt;"]<br/>edition = "2018"<br/><br/>[dependencies]<br/>runtime = "0.3.0-alpha.6"<br/>surf = "1.0"</pre>
<ol start="3">
<li>In <kbd>src/main.rs</kbd>, we have to import the dependencies. Add the following lines at the top of the file:</li>
</ol>
<pre style="padding-left: 60px">use surf::Exception;<br/>use surf::http::StatusCode;</pre>
<ol start="4">
<li>The classic example is waiting for a web request to finish. This is notoriously difficult to judge since the web resources and/or the network in between is owned by someone else and might be down. <kbd>surf</kbd> (<a href="https://github.com/rustasync/surf">https://github.com/rustasync/surf</a>) is <kbd>async</kbd> by default and therefore requires using the <kbd>.await</kbd> syntax heavily. Let's declare an <kbd>async</kbd> function to do the fetching:</li>
</ol>
<pre style="padding-left: 60px">async fn response_code(url: &amp;str) -&gt; Result&lt;StatusCode, Exception&gt; {<br/>    let res = surf::get(url).await?;<br/>    Ok(res.status())<br/>}</pre>
<ol start="5">
<li>Now we need an <kbd>async main</kbd> function in order to call the <kbd>response_code() async</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">#[runtime::main]<br/>async fn main() -&gt; Result&lt;(), Exception&gt; {<br/>    let url = "https://www.rust-lang.org";<br/>    let status = response_code(url).await?;<br/>    println!("{} responded with HTTP {}", url, status);<br/>    Ok(())<br/>}</pre>
<ol start="6">
<li>Let's see if the code works by running <kbd>cargo run</kbd> (a <kbd>200 OK</kbd> is expected):</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cargo +nightly run</strong><br/> Compiling async-await v0.1.0 (Rust-Cookbook/Chapter04/async-await)<br/>    Finished dev [unoptimized + debuginfo] target(s) in 1.81s<br/>     Running `target/debug/async-await`<br/>     https://www.rust-lang.org responded with HTTP 200 OK</pre>
<p><kbd>async</kbd> and <kbd>await</kbd> have been worked on for a long time in the Rust community. Let's see how this recipe works.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Futures (often called promises) are typically fully integrated into the language and come with a built-in runtime. In Rust, the team chose a more ambitious approach and left the runtime open for the community to implement (for now). Right now the two projects Tokio and Romio (<a href="https://github.com/withoutboats/romio">https://github.com/withoutboats/romio</a>) and <kbd>juliex</kbd> (<a href="https://github.com/withoutboats/juliex">https://github.com/withoutboats/juliex</a>) have the most sophisticated support for these futures. With the recent addition of <kbd>async</kbd>/<kbd>await</kbd> in the Rust syntax in the 2018 edition, it's only a matter of time until the various implementations mature. </p>
<p>After setting up the dependencies in <em>step 1</em>, <em>step 2</em> shows that we don't have to enable the <kbd>async</kbd> and <kbd>await</kbd> macros/syntax to use them in the code—this was a requirement for a long time. Then, we import the required crates. Coincidentally, a new async web library—called <kbd>surf</kbd>—was built by the Rust async working group while we were busy with this book. Since this crate was built fully asynchronous, we preferred it over more established crates such as <kbd>hyper</kbd> (<a href="https://hyper.rs">https://hyper.rs</a>).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In <em>step 3</em>, we declare an <kbd>async</kbd> function, which automatically returns a <kbd>Future</kbd> (<a href="https://doc.rust-lang.org/std/future/trait.Future.html">https://doc.rust-lang.org/std/future/trait.Future.html</a>) type and can only be called from within another <kbd>async</kbd> scope. <em>Step 4</em> shows the creation of such a scope with the <kbd>async</kbd> main function. Does it end there? No—the <kbd>#[runtime::main]</kbd> attribute gives it away: a runtime is seamlessly started and assigned to execute anything async.  </p>
<p><span>While the <kbd>runtime</kbd> crate (<a href="https://docs.rs/runtime/0.3.0-alpha.7/runtime/">https://docs.rs/runtime/0.3.0-alpha.7/runtime/</a>) is agnostic of the actual implementation, the default is a native runtime based on <kbd>romio</kbd> and <kbd>juliex</kbd> (check your <kbd>Cargo.lock</kbd> file), but you can also enable the much more feature-laden <a href="https://tokio.rs">tokio</a> runtime to enable streams, timers, and so on to use on top of async.</span></p>
<p>Inside the <kbd>async</kbd> functions, we can make use of the <kbd>await</kbd> keyword attached to a <kbd>Future</kbd> implementor (<a href="https://doc.rust-lang.org/std/future/trait.Future.html">https://doc.rust-lang.org/std/future/trait.Future.html</a>), such as the <kbd>surf</kbd> request (<a href="https://github.com/rustasync/surf/blob/master/src/request.rs#L563">https://github.com/rustasync/surf/blob/master/src/request.rs#L563</a>), where the runtime calls <kbd>poll()</kbd> until a result is available. This can also result in an error, which means that we have to handle errors as well, which is generally done with the <kbd>?</kbd> operator. <kbd>surf</kbd> also provides a generic <kbd>Exception</kbd> type (<a href="https://docs.rs/surf/1.0.2/surf/type.Exception.html">https://docs.rs/surf/1.0.2/surf/type.Exception.html</a>) alias to handle anything that might happen.</p>
<p>While there are some things that could still change in Rust's fast-moving ecosystem, using <kbd>async</kbd>/<kbd>await</kbd> is finally coming together without requiring highly unstable crates. Having that available is a significant boost to Rust's usefulness. Now, let's move on to another chapter.</p>


            </article>

            
        </section>
    </div></body></html>
<html><head></head><body>
		<div><h1 id="_idParaDest-199" class="chapter-number"><a id="_idTextAnchor200"/>10</h1>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor201"/>Deploying Our Application on AWS</h1>
			<p>In a lot of tutorials and educational materials, deployment is rarely covered. This is because there are a lot of moving parts, and the process can be fairly brittle. It may be more convenient to refer to other resources when mentioning deployment.</p>
			<p>In this chapter, we will cover enough to automate deployment on a server on <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) and then build and connect to a database from there. It must be stressed that deployment and cloud computing are big topics—there are whole books written on them.</p>
			<p>In this chapter, we will get to a point where we can deploy and run our application for others to use. Learning how to deploy applications on a server is the final step. This is where you will turn the application that you have been developing into a practical reality that can be used by others all over the world.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Setting up our build environment</li>
				<li>Managing our software with Docker</li>
				<li>Deploying our application on AWS</li>
			</ul>
			<p>By the end of this chapter, you will be able to wrap your code up in Docker images and deploy it on a server instance on AWS so that it can be accessed by other users. You will also be able to configure infrastructure using <strong class="bold">Terraform</strong>, which is a tool used to define cloud computing infrastructure such as servers and databases as code. Once you have finished this chapter, you will have a few build scripts that create a build and deployment server using Terraform, pass the data from that Terraform build into config files, <strong class="bold">SSH</strong> into these servers, and run a series of commands resulting in database migrations and spinning up the Docker containers on our server.</p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor202"/>Technical requirements</h1>
			<p>In this chapter, we’ll build on the code built in <a href="B18722_09.xhtml#_idTextAnchor182"><em class="italic">Chapter 9</em></a>, <em class="italic">Testing Our Application Endpoints and Components</em>. This can be found at the following URL: <a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter09/building_test_pipeline">https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter09/building_test_pipeline</a>.</p>
			<p>The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter10">https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter10</a>.</p>
			<p>This chapter also has the following requirements:</p>
			<ul>
				<li>We will be using Terraform to automate the building of our servers. Because of this, we will need to install Terraform using the following URL: <a href="https://learn.hashicorp.com/tutorials/terraform/install-cli">https://learn.hashicorp.com/tutorials/terraform/install-cli</a>.</li>
				<li>When we are using Terraform, it will be making calls to the AWS infrastructure. We will need AWS authentication, which will be done using the AWS client. We will be using the AWS client in this chapter, which can be installed using the following URL: <a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html">https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</a>.</li>
				<li>You will also need a Docker Hub account so that we can package and deploy our application. This can be found at <a href="https://hub.docker.com/">https://hub.docker.com/</a>.</li>
				<li>Since you will be deploying the application on a server, you will need to sign up for an AWS account. This can be done at the following URL: <a href="https://aws.amazon.com/">https://aws.amazon.com/</a>.</li>
			</ul>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor203"/>Setting up our build environment</h1>
			<p>So far, we have been running our application with the <code>cargo run</code> command. This has been working <a id="_idIndexMarker929"/>well, but you might have noticed that our application is not very fast. In fact, it is relatively slow when we try to log in to the application. This seems to be counterintuitive as we are learning Rust to develop faster applications.</p>
			<p>So far, it does not look very fast. This is because we are not running an optimized version of our application. We can do this by adding the <code>--release</code> tag. As a result, we run our optimized application using the following command:</p>
			<pre class="console">
cargo run --release config.yml</pre>
			<p>Here, we notice that the compilation takes a lot longer. Running this every time we alter the code, and during a development process, is not ideal; hence, we have been building and running in debug mode using the <code>cargo run</code> command. However, now that our optimized application is running, we can see that the login process is a lot faster. While we can run the server locally, our aim is to deploy our application on a server. To run our application on <a id="_idIndexMarker930"/>a server, we are going to have to build our application in a Docker image. To ensure that our Docker image build runs smoothly, we are going to use an online computing unit on AWS. Before we run our build process, we need to carry out the following steps:</p>
			<ol>
				<li>Set up an AWS <strong class="bold">Secure Shell</strong> (<strong class="bold">SSH</strong>) key for <strong class="bold">Elastic Compute </strong><strong class="bold">Cloud</strong> (<strong class="bold">EC2</strong>)</li>
				<li>Set up the AWS client for our local computer</li>
				<li>Write a Terraform script that builds our build server</li>
				<li>Write a Python script that manages the build</li>
				<li>Write a Bash script that orchestrates the build on the server</li>
			</ol>
			<p>Once we have done the aforementioned steps, we will be able to run an automated pipeline that creates a build EC2 server on AWS and then build our Rust application. First, let us get started by creating an SSH key for our server.</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor204"/>Setting up an AWS SSH key for an AWS EC2 instance</h2>
			<p>If we want to <a id="_idIndexMarker931"/>run commands <a id="_idIndexMarker932"/>on a server, we are going to have to connect to <a id="_idIndexMarker933"/>the server using the SSH protocol over the <strong class="bold">Hypertext Transfer Protocol</strong> (<strong class="bold">HTTP</strong>). However, we cannot have anyone accessing our server as it will not be secure. To stop anyone from connecting to our server and running any commands they want, we will only allow users to connect to our server if they have an SSH key. To create our key, we need to log in to the AWS console and navigate to our EC2 dashboard, which can be accessed via the search box, as seen in the following screenshot:</p>
			<div><div><img src="img/Figure_10.1_B18722.jpg" alt="Figure 10.1 – Navigating to the EC2 dashboard with the search box"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Navigating to the EC2 dashboard with the search box</p>
			<p class="callout-heading">Note</p>
			<p class="callout">It must be noted that AWS might not look like the screenshots in this chapter as AWS keeps changing the UI. However, the fundamental concepts will be the same.</p>
			<p>Once we have <a id="_idIndexMarker934"/>navigated to <a id="_idIndexMarker935"/>the EC2 dashboard, we can navigate to <strong class="bold">Key Pairs</strong> in the <strong class="bold">Network &amp; Security</strong> section of the panel on the left side of the view, as seen in the following screenshot:</p>
			<div><div><img src="img/Figure_10.2_B18722.jpg" alt="Figure 10.2 – Navigating to Key Pairs"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Navigating to Key Pairs</p>
			<p>Once we have navigated to the <strong class="bold">Key Pairs</strong> section, there will be a list of key pairs that you already own. If you have built EC2 instances before, you might already see some listed. If you have never built an EC2 instance before, then you will not have anything listed. On the top <a id="_idIndexMarker936"/>right of the <a id="_idIndexMarker937"/>screen, you can create a key by clicking on the <strong class="bold">Create key pair</strong> button shown in the following screenshot:</p>
			<div><div><img src="img/Figure_10.3_B18722.jpg" alt="Figure 10.3 – Button to allow the creation of a key pair"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Button to allow the creation of a key pair</p>
			<p>Once we have clicked on this button, we will see the following form:</p>
			<div><div><img src="img/Figure_10.4_B18722.jpg" alt="Figure 10.4 – Create key pair form"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Create key pair form</p>
			<p>In the preceding screenshot, we can see that we have named our <code>remotebuild</code> key; we also state that our <a id="_idIndexMarker938"/>key has <a id="_idIndexMarker939"/>the <code>.pem</code> format and is of the <code>.ssh</code> directory. Inside the <code>.ssh</code> directory, we can create a <code>keys</code> directory with the following command:</p>
			<pre class="console">
mkdir "$HOME"/.ssh/keys/</pre>
			<p>The <code>"$HOME"</code> environment variable is always available in the Bash shell, and it denotes the home directory for the user. This means that other users who log in to the computer under a different username cannot access the SSH keys we downloaded if we store them in the directory that we have just created. We can now navigate to where our key has been downloaded and copy it to our <code>keys</code> directory with the following command:</p>
			<pre class="console">
cp ./remotebuild.pem "$HOME"/.ssh/keys/</pre>
			<p>We then must change the permissions for the key so that only the owner of the file can read the file with the 600 code for us to use the key for SSH purposes with the following command:</p>
			<pre class="console">
chmod 600 "$HOME"/.ssh/keys/remotebuild.pem</pre>
			<p>We now <a id="_idIndexMarker940"/>have an SSH key <a id="_idIndexMarker941"/>stored in our <code>.ssh/keys</code> directory where we can access this key with the correct permissions in order to access the servers that we create. Now that we have this, we need programmatic access to our AWS services by setting up our AWS client.</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor205"/>Setting up our AWS client</h2>
			<p>We will be using Terraform <a id="_idIndexMarker942"/>to automate our server build. In order to do this, Terraform will make calls to the AWS infrastructure. This means that we must have the AWS client installed on our local machine. Before we configure our client, we need to get some programmatic user keys. If we do not obtain programmatic access, our code will not be authorized to build infrastructure on our AWS account. The first step of obtaining the user keys is navigating to the <strong class="bold">IAM</strong> section via the search box, as seen in the following screenshot:</p>
			<div><div><img src="img/Figure_10.5_B18722.jpg" alt="Figure 10.5 – Navigating to the IAM section"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Navigating to the IAM section</p>
			<p>Once we have navigated to the <strong class="bold">IAM</strong> section, we will get the following layout:</p>
			<div><div><img src="img/Figure_10.6_B18722.jpg" alt="Figure 10.6 – View of the IAM section"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – View of the IAM section</p>
			<p>We can see in <a id="_idIndexMarker943"/>the preceding screenshot that my user has <strong class="bold">multi-factor authentication</strong> (<strong class="bold">MFA</strong>), and I keep my access key rotated. If you are new to AWS, this checklist might not be satisfactory, and it is suggested that you follow the security recommendations <a id="_idIndexMarker944"/>that AWS gives you. We now must access the <strong class="bold">Users</strong> option on the left-hand side of the view and create a new user by clicking the <strong class="bold">Add users</strong> button in the top-right corner of the screen, as seen in the following screenshot:</p>
			<div><div><img src="img/Figure_10.7_B18722.jpg" alt="Figure 10.7 – Creating users"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Creating users</p>
			<p>We can then create a user. It does not matter what name you call the user, but you must ensure that they have programmatic access by checking the <strong class="bold">Access key - Programmatic access</strong> option, as seen in the following screenshot:</p>
			<div><div><img src="img/Figure_10.8_B18722.jpg" alt="Figure 10.8 – First stage of creating a user"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – First stage of creating a user</p>
			<p>Once we have <a id="_idIndexMarker945"/>highlighted the programmatic access and defined the username, we can move on to the permissions, as seen in the following screenshot:</p>
			<div><div><img src="img/Figure_10.9_B18722.jpg" alt="Figure 10.9 – Defining permissions"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – Defining permissions</p>
			<p>We can see that we have given the user <code>AdministratorAccess</code> permissions, which will enable us to create and destroy servers and databases. The rest of the steps in the user creation process are trivial, and you can get through these by just clicking <strong class="bold">Next</strong>. Once the user is created, you will be exposed to an access key and a secret access key. It is important to note these down in a secure location such as a password manager because you will never be able to see your secret access key again on the AWS site, and we will need them when configuring our AWS client on our local computer. Now that we have the user keys handy, we can configure our AWS client with the following command:</p>
			<pre class="console">
aws configure</pre>
			<p>The AWS client will then prompt you to enter the user keys as and when they are required. Once this <a id="_idIndexMarker946"/>is done, your AWS client is configured, and we can use AWS features programmatically on our local computer. Now, we are ready to start creating servers using Terraform in the next section.</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor206"/>Setting up our Terraform build</h2>
			<p>When it comes to building <a id="_idIndexMarker947"/>infrastructure on AWS, we can simply point and click in the EC2 dashboard. However, this is not desirable. If you are anything like me, when I point and click a series of configuration settings, I forget what I have done unless I document it, and let’s be honest, documenting what you’ve clicked is not something you are going to look forward to. Even if you are a better person than me and you document it, when you change the configuration, there is a chance that you will not go back to update the documentation of that change. Pointing and clicking is also time-consuming. If we wanted to create some infrastructure and then destroy it a week later, and then recreate it a month after that, we would be reluctant to touch it if we had to point and click, and our server bills would be higher. This is where <strong class="bold">infrastructure as code</strong> (<strong class="bold">IaC</strong>) comes in. We will have to do some pointing and <a id="_idIndexMarker948"/>clicking, as we did in the previous sections. We cannot do any programmatic access without pointing and clicking to set up programmatic access.</p>
			<p>Now that we have programmatic access, we can build out our <code>build</code> directory, which should be next to our <code>web_app</code> and <code>front_end</code> directories. In the <code>build</code> directory, we can define our infrastructure for the build server in the <code>build/main.tf</code> file. It must be noted that the <code>.tf</code> extension is the standard extension for Terraform files. First, we define which version of Terraform is being used with the following code:</p>
			<pre class="source-code">
terraform {
  required_version = "&gt;= 1.1.3"
}</pre>
			<p>Now that we have defined the Terraform version, we can declare that we are using the AWS module. The Terraform registry has modules for a range of platforms, including Google Cloud <a id="_idIndexMarker949"/>and Microsoft Azure. Anyone can build modules and abstract infrastructure to be downloaded on the Terraform registry. Our AWS module usage declaration takes the following form:</p>
			<pre class="source-code">
provider "aws" {
    version = "&gt;= 2.28.1"
    region = "eu-west-2"
}</pre>
			<p>You might want to pick a different region if another region suits you. A list of the available <a id="_idIndexMarker950"/>regions on AWS for EC2 can be found via the following link: <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html</a>.</p>
			<p>I’m merely using <code>"eu-west-2"</code> for demonstration purposes. We can now build our EC2 instance with the following code:</p>
			<pre class="source-code">
resource "aws_instance" "build_server" {
    ami = "ami-0fdbd8587b1cf431e"
    instance_type = "t2.medium"
    key_name = "remotebuild"
    user_data = file("server_build.sh")
    tags = {
      Name = "to-do build server"
    }
}</pre>
			<p><code>resource</code> declares that we are defining a resource to be built, and <code>aws_instance</code> states that we are using the EC2 instance template in the AWS module. A list of the available AWS Terraform modules and <a id="_idIndexMarker951"/>their documentation can be found via the following link: <a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs">https://registry.terraform.io/providers/hashicorp/aws/latest/docs</a>.</p>
			<p><code>build_server</code> is what we are calling it. We can refer to <code>build_server</code> anywhere else in the Terraform script, and Terraform will work out the order in which resources need to be built to make sure all references are accounted for. We can see that we have referenced the <code>"remotebuild"</code> key that we defined in the previous section. We can create multiple EC2 instances that can be accessed by one key if we want. We also declare the <a id="_idIndexMarker952"/>name so that when we look at our EC2 instances, we know what the server is for. We must also note that <code>user_data</code> is the Bash script that will be run on the new EC2 server once it has been built. The <code>ami</code> section is a reference to the type of operating system and version being used. Do not directly copy my <strong class="bold">Amazon Machine Image</strong> (<strong class="bold">AMI</strong>) ID in the example unless you are using the same region, as AMI IDs can vary depending on the region. If you want to find out the AMI ID, go to your EC2 dashboard and click on <strong class="bold">Launch an instance</strong>, which will result in the following window:</p>
			<div><div><img src="img/Figure_10.10_B18722.jpg" alt="Figure 10.10 – Launching an instance"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – Launching an instance</p>
			<p>Here, we can see <a id="_idIndexMarker953"/>that we are picking <strong class="bold">Amazon Linux</strong>. You must select this; otherwise, your build scripts will not work. If we zoom in, we can see that the AMI ID is visible, as seen here:</p>
			<div><div><img src="img/Figure_10.11_B18722.jpg" alt="Figure 10.11 – Accessing AMI ID for the server"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11 – Accessing AMI ID for the server</p>
			<p>This will be the AMI ID for the Amazon Linux operating system in the region that you want to launch. You can also see nothing is stopping you from using other operating systems in other Terraform projects. If you have built EC2 instances in the past, you will know that the IP <a id="_idIndexMarker954"/>address is random unless we attach an elastic IP to the EC2 instance. We will have to produce an output of the IP of our EC2 instance so that we can connect to it. Our output is defined with the following code:</p>
			<pre class="source-code">
output "ec2_global_ips" {
  value = ["${aws_instance.build_server.*.public_ip}"]
}</pre>
			<p>Here, we can see that we reference our build server using <code>aws_instance.build_server</code>. Further reading on Terraform outputs is provided in the <em class="italic">Further reading</em> section. At this point, our Terraform build is nearly done. We must remember that we need to build the <code>server_build.sh</code> script that is going to be run on the EC2 instance once the EC2 instance has been built. In our <code>/build/server_build.sh</code> file, we can install the basic requirements needed for our server with the following code:</p>
			<pre class="source-code">
#!/bin/bash
sudo yum update -y
sudo yum install git -y
sudo yum install cmake -y
sudo yum install tree -y
sudo yum install vim -y
sudo yum install tmux -y
sudo yum install make automake gcc gcc-c++ kernel-devel -y</pre>
			<p>With the preceding packages, we will be able to navigate around the server looking at file trees with <code>tree</code>; we will also be able to perform <code>git</code> operations, open files, and edit them using <code>vim</code>, and have multiple panels open through one terminal if we need to with <code>tmux</code>. The other packages enable us to compile our Rust code. We must also note that we have appended each install with a <code>-y</code> tag. This is to tell the computer to bypass input prompts and put in default answers. This means that we can run this script in the background without any problems. We now must install the PostgreSQL drivers with the following code:</p>
			<pre class="source-code">
sudo amazon-linux-extras install postgresql10 vim epel -y
sudo yum install -y postgresql-server postgresql-devel -y</pre>
			<p>We nearly have <a id="_idIndexMarker955"/>everything that we need. In the next section, we will be using Docker to build and package our applications. This can be done with the following code:</p>
			<pre class="source-code">
sudo amazon-linux-extras install docker
sudo service docker start
sudo usermod -a -G docker ec2-user</pre>
			<p>Here, we can see that we install Docker, start the Docker service, and then register our user with the Docker service so that we do not have to use <code>sudo</code> with every Docker command. Seeing as we have installed Docker, we might as well install <code>docker-compose</code> for completeness. This can be done at the end of our script with the following code:</p>
			<pre class="source-code">
sudo curl -L "https://github.com/docker/compose/releases
/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)"
-o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose</pre>
			<p>The <code>curl</code> command downloads <code>docker-compose</code>. We then make the <code>docker-compose</code> executable with the <code>chmod</code> command. The build script is almost finished. However, running all the commands that we have defined in the build script will take a while. There is a chance that we can SSH into our server before the build script has finished. Therefore, we should write a <code>FINISHED</code> string to a file to inform other processes that the server has all the packages that have been installed. We can write our flag with the following code:</p>
			<pre class="source-code">
echo "FINISHED" &gt; home/ec2-user/output.txt</pre>
			<p>We have now built <a id="_idIndexMarker956"/>out the infrastructure that is going to build our applications for deployment. In the next section, we are going to build scripts that orchestrate the building of our application using the build server that we have configured.</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor207"/>Writing our Python application build script</h2>
			<p>We could <a id="_idIndexMarker957"/>technically try to fit our application-building script in the same scripts that we wrote in the previous section. However, it is desired to keep our scripts separate. For instance, if we were going to use a <code>build/run_build.py</code> script, we start by importing everything that we need with the following code:</p>
			<pre class="source-code">
from subprocess import Popen
from pathlib import Path
import json
import time
DIRECTORY_PATH = Path(__file__).resolve().parent</pre>
			<p>We now have the absolute path for the <code>build</code> directory, we can run Bash commands through the <code>Popen</code> class, and we can load JSON. Now that we have everything imported, we can run our Terraform commands with the following code:</p>
			<pre class="source-code">
init_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp; terraform init",
                     shell=True)
init_process.wait()
apply_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp; terraform apply",
                      shell=True)
apply_process.wait()</pre>
			<p>In each of the <a id="_idIndexMarker959"/>commands, we go to the directory path and then perform the Terraform command. We then wait for the command to finish before moving on to the next command. While Python is a slow, easy, and not strongly typed language, here it adds a lot of power. We can run multiple Bash commands at the same time and then wait for them later if needed. We can also pull data out of processes and manipulate this data and feed it into another command easily. The two Terraform commands that we carry out in the preceding snippet are <code>init</code> and <code>apply</code>. <code>init</code> sets up the Terraform state to record what is going on and downloads the modules that we need, which in this case is AWS. The <code>apply</code> command runs the Terraform build, which will build our EC2 server.</p>
			<p>Once our EC2 instance is built, we can get the output of Terraform and write it to a JSON file, and then load the data from that JSON file with the following code:</p>
			<pre class="source-code">
produce_output = Popen(f"cd {DIRECTORY_PATH} &amp;&amp; terraform
                        output -json &gt; {DIRECTORY_PATH}/
                        output.json", shell=True)
produce_output.wait()
with open(f"{DIRECTORY_PATH}/output.json", "r") as file:
    data = json.loads(file.read())
server_ip = data["ec2_global_ips"]["value"][0][0]</pre>
			<p>Now that we have the server IP, we can SSH into this server and get it to do our builds. However, there could be some concurrency issues. There is a small-time window where the Terraform build finishes, but the server is not ready yet to accept connections. Therefore, we just need the script to wait for a small period before continuing with the following code:</p>
			<pre class="source-code">
print("waiting for server to be built")
time.sleep(5)
print("attempting to enter server")</pre>
			<p>Once this is <a id="_idIndexMarker960"/>done, we need to pass our server IP into another Bash script that manages the build and then destroy the server afterward with the following code:</p>
			<pre class="source-code">
build_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp;
                      sh ./run_build.sh {server_ip}",
                      shell=True)
build_process.wait()
destroy_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp;
                        terraform destroy", shell=True)
destroy_process.wait()</pre>
			<p>We have now built the orchestration of a build and the Terraform script that defines the infrastructure for the build. We now must build the final build script that will run the build commands on the server.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor208"/>Writing our Bash deployment script</h2>
			<p>Our Bash deployment script must take in the IP address of the build server, SSH into the server, and run <a id="_idIndexMarker961"/>a series of commands on the server. It will also have to copy our code onto the server to be built. We can see from the preceding code that we can build our build Bash script in the <code>/build/run_build.sh</code> file. First, we start with the standard boilerplate code:</p>
			<pre class="source-code">
#!/usr/bin/env bash
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH</pre>
			<p>With this boilerplate code, we have stated that this is a Bash script and that the rest of the Bash code will run in the <code>build</code> directory. We then upload the code from the Rust application with the following code:</p>
			<pre class="source-code">
rm -rf ../web_app/target/
scp -i "~/.ssh/keys/remotebuild.pem" -r
../web_app ec2-user@$1:/home/ec2-user/web_app</pre>
			<p>Here, we can see that we remove the <code>target</code> directory. As we remember, the <code>target</code> directory is built when we build our Rust application; we do not need to upload build files <a id="_idIndexMarker962"/>from the local build. We then copy our Rust code using the <code>scp</code> command. We access the first argument passed into the script, which is <code>$1</code>. Remember that we pass in the IP address, so <code>$1</code> is the IP address. We then SSH into our server and run commands on this server with the following code:</p>
			<pre class="source-code">
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  until [ -f ./output.txt ]
  do
      sleep 2
  done
  echo "File found"
  curl https://sh.rustup.rs -sSf | bash -s -- -y
  source ~/.cargo/env
  cd web_app
  cargo build --release
EOF</pre>
			<p>Here, we see that we loop sleeping for 2 seconds on each iteration until the <code>output.txt</code> file is present. Once the <code>output.txt</code> file is present, we know that the build script from the Terraform is complete and we can start our build. We signal this by echoing <code>"File found"</code> to the console. We then install Rust, load our <code>cargo </code>commands <a id="_idIndexMarker963"/>into our shell with the <code>source</code> command, move into the <code>web_app</code> directory, and build our Rust application.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We can get rid of the Python dependency if needed by using the <code>jq</code> command in our <code>run_build.sh</code> script with the following code insertion:</p>
			<pre class="source-code">
. . .
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH
terraform init
terraform apply
terraform output -json &gt; ./output.json
IP_ADDRESS=$(jq --raw-output '.ec2_global_ips.value[0][0]' output.json)
echo $IP_ADDRESS
echo "waiting for server to be built"
sleep 5
echo "attempting to enter server"
rm -rf ../web_app/target/
scp -i "~/.ssh/keys/remotebuild.pem" -r
../web_app ec2-user@$IP_ADDRESS:/home/ec2-user/web_app
. . .</pre>
			<p class="callout">It must be noted that the references to the <code>$1</code> variable are replaced with <code>$IP_ADDRESS</code>.</p>
			<p class="callout">To run our pipeline without the Python dependency, we merely need to run the following command:</p>
			<pre class="source-code">
Sh run_build.sh</pre>
			<p class="callout">However, we will be relying on our Python script later on in the chapter.</p>
			<p>Now that our <a id="_idIndexMarker964"/>build pipeline is built, we can run it with the following command:</p>
			<pre class="console">
python3 run_build.py</pre>
			<p class="callout-heading">WARNING</p>
			<p class="callout">Terraform can sometimes be temperamental; if it fails, you may need to run it again. Sometimes, I’ve had to <a id="_idIndexMarker965"/>perform a Terraform run up to three times before it fully works. Every action is stored by Terraform, so do not worry—running the Python script again will not result in duplicate servers.</p>
			<p>When running this command, you will be prompted to write <code>yes</code> at three different points of the process. The first time is to approve the building of the server with Terraform, the second time is to add the IP address of the server to the list of known hosts to approve the SSH connection, and the last time is to approve the destruction of the server. It usually makes sense to show the printouts in this book; however, the printout here is long and would probably take up multiple pages. Also, the printouts are obvious. Terraform openly states what is being built, the copying and building are also verbose, and the destruction of the server with Terraform is also printed out. It will be very clear what is going on when you run this build pipeline. You might also notice that there is a range of Terraform files that have been created. These files keep track of the state of our resources that have been built on the AWS platform. If you delete these state files, you have no way of knowing what is built, and duplicates will be spun up. It will also prevent Terraform from cleaning up. At the place where I work, at the time of writing this, we use Terraform to build massive data models for calculating the risk of financial loss over geographical locations. The data being processed can go over terabytes <a id="_idIndexMarker966"/>per chunk. We use Terraform to spin up a range of powerful computers, run data through it (which can take days), and then shut it down when it is finished. Multiple people need to monitor this process, so our Terraform state is housed in a <code>state.tf</code> file:</p>
			<pre class="source-code">
terraform {
    backend "s3" {
    bucket = "some_bucket"
    key    = "some/ptaht/terraform.tfstate"
    region = "eu-west-1"
  }
}</pre>
			<p>It must be noted that your account needs to have access to the bucket defined.</p>
			<p>Now, it is time to build our frontend application. Looking and what we have just done, all we need to do is add to the <code>/build/run_build.sh</code> file the steps to upload our frontend code to the build server and build the frontend application. At this point, you should be able to code this yourself. Right now, it would be a good use of your time to stop reading and attempt to build it. If you have attempted it yourself, it should look like the code shown here:</p>
			<pre class="source-code">
rm -rf ../front_end/node_modules/
scp -i "~/.ssh/keys/remotebuild.pem" -r
../front_end ec2-user@$1:/home/ec2-user/front_end
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  curl -o- https://raw.githubusercontent.com/nvm-sh
  /nvm/v0.37.2/install.sh | bash
  . ~/.nvm/nvm.sh
  nvm install --lts
  cd front_end
  npm install
EOF</pre>
			<p>Here, we remove the node modules, copy the code to the server, install node, and then run the <code>install</code> command <a id="_idIndexMarker968"/>for our application. Now that our build pipeline is fully working, we can move on to wrapping our software in Docker so that we can package the software in Docker and deploy it.</p>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor209"/>Managing our software with Docker</h1>
			<p>So far, we have been using Docker to manage our PostgreSQL and Redis databases. When it comes <a id="_idIndexMarker969"/>to running our frontend and Rust server, we have <a id="_idIndexMarker970"/>merely been running it directly on our local computer. However, when it comes to running our applications on remote servers, it is simpler and easier to distribute. Before we get on to deploying our Docker images on servers, we need to build and run them locally, which starts with writing our Docker image file.</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor210"/>Writing Docker image files</h2>
			<p>Before we proceed, it must be noted that the approach carried out here is the simplest, least optimized way <a id="_idIndexMarker971"/>to build a Rust server Docker image, because we are juggling a lot of new concepts. We cover an optimized way of building Rust server Docker images in <a href="B18722_13.xhtml#_idTextAnchor264"><em class="italic">Chapter 13</em></a>, <em class="italic">Best Practices for a Clean Web App Repository</em>. When it comes to building a Docker image, we need a Dockerfile. This is where we define the steps needed to build our image. In our <code>web_app/Dockerfile</code> file, we basically borrow a base image and then run our commands on top of this image for our application to work. We can define the base image and the requirements to run our database interactions with the following code:</p>
			<pre class="source-code">
FROM rust:1.61
RUN apt-get update -yqq &amp;&amp; apt-get install -yqq cmake g++
RUN cargo install diesel_cli --no-default-features --features postgres</pre>
			<p>Here in our Docker build, we are starting with the official <code>rust</code> image. We then update <code>apt</code> so that we can download all the available packages. We then install <code>g++</code> and the <code>diesel</code> client so that our database operations will work. We then copy the code and config files from our Rust application and define our work directory with the following code:</p>
			<pre class="source-code">
COPY . .
WORKDIR .</pre>
			<p>Now, we have everything to build our Rust application with the following code:</p>
			<pre class="source-code">
RUN cargo clean
RUN cargo build --release</pre>
			<p>Now our build is done, we move the static binary from the <code>target</code> directory into our home directory, remove excessive code such as the <code>target</code> and <code>src</code> directories, and allow the static binary file to be executable with the following code:</p>
			<pre class="source-code">
RUN cp ./target/release/web_app ./web_app
RUN rm -rf ./target
RUN rm -rf ./src
RUN chmod +x ./web_app</pre>
			<p>Now everything is done, we can expose the port at which the web server will be running to make the server <a id="_idIndexMarker972"/>exposed by the container and execute the command that gets run when the Docker image is spun up with the following code:</p>
			<pre class="source-code">
EXPOSE 8000
CMD ["./web_app", "config.yml"]</pre>
			<p>Now that our Docker image file is written, we can move on to building Docker images.</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor211"/>Building Docker images</h2>
			<p>Before we can <a id="_idIndexMarker973"/>run this, however, we need to remove our build script. There is only one thing left our Docker image build needs. When copying over the code into the Docker image, we know that the <code>target</code> directory has a lot of code and files that we do not need in our image. We can avoid copying over the <code>target</code> directory by having the following code in the <code>.</code><code>dockerignore</code> file:</p>
			<pre class="source-code">
target</pre>
			<p>If we try to compile our application with the build script, Docker will just throw itself into an infinite file loop and then time out. This means that our <code>ALLOWED_VERSION</code> variable in our <code>main.rs</code> file takes the following form:</p>
			<pre class="source-code">
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    const ALLOWED_VERSION: &amp;'static str = "v1";
    . . .</pre>
			<p>And we then must comment out our <code>build</code> dependency in our <code>Cargo.toml</code> file with the following code and remove the <code>build.rs</code> file entirely:</p>
			<pre class="source-code">
[package]
name = "web_app"
version = "0.1.0"
edition = "2021"
# build = "build.rs"</pre>
			<p>We are now ready to build our image; we navigate to where the Dockerfile is and run the following command:</p>
			<pre class="console">
docker build . -t rust_app</pre>
			<p>This command executes the build defined by the Dockerfile in the current directory. The image is tagged <code>rust_app</code>. We can list our images with the following command:</p>
			<pre class="console">
docker image ls</pre>
			<p>This will give us the following printout:</p>
			<pre class="console">
REPOSITORY   TAG       IMAGE ID       CREATED        SIZE
rust_app     latest    c2e465545e30   2 hours ago    2.69GB
. . .</pre>
			<p>We can then test <a id="_idIndexMarker974"/>to see if our application is properly built; we just run the following command:</p>
			<pre class="console">
docker run rust_app</pre>
			<p>This directly runs our Rust app. Our application should crash instantly with the following error:</p>
			<pre class="console">
thread 'main' panicked at 'called `Result::unwrap()`
on an `Err` value: Connection refused (os error 111)',
src/counter.rs:24:47 note: run with `RUST_BACKTRACE=1`
environment variable to display a backtrace</pre>
			<p>We can see that the error is not down to our build, but a connection issue with Redis from our counter file. This is reassuring, and it will work when we run our Rust application with our two databases.</p>
			<p>There is an approach in Docker where you can do multi-layered builds. This is where we start off with the <code>rust</code> base image, build our application, and then move our build into another Docker image with no dependencies. The result is that our server image is usually merely 100 MB as opposed to multiple GB. However, our application has a lot of dependencies, and this <a id="_idIndexMarker975"/>multi-layered build approach will result in multiple driver errors. We explore building tiny images in <a href="B18722_13.xhtml#_idTextAnchor264"><em class="italic">Chapter 13</em></a>, <em class="italic">Best Practices for a Clean Web </em><em class="italic">App Repository</em>.</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor212"/>Building an EC2 build server using Terraform</h2>
			<p>We have now built <a id="_idIndexMarker976"/>our Rust Docker image locally. We <a id="_idIndexMarker977"/>can now build it on our build server. Before we do this, we are going to have to increase the size of the hard drive on our builder server; otherwise, the image will refuse to build due to lack of space. This can be done in our <code>/build/main.tf</code> file by adding a root block device, as seen in the following code snippet:</p>
			<pre class="source-code">
resource "aws_instance" "build_server" {
    ami = "ami-0fdbd8587b1cf431e"
    instance_type = "t2.medium"
    key_name = "remotebuild"
    user_data = file("server_build.sh")
    tags = {
      Name = "to-do build server"
    }
    # root disk
    root_block_device {
      volume_size           = "150"
      volume_type           = "gp2"
      delete_on_termination = true
    }
}</pre>
			<p><code>gp2</code> is the version of SSD that AWS supports we are using. <code>150</code> is the number of GB that we are connecting <a id="_idIndexMarker978"/>to the server. This will be enough <a id="_idIndexMarker979"/>memory to build our Docker images, leaving us only to build the pipeline that constructs our Docker images.</p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor213"/>Orchestrating builds with Bash</h2>
			<p>At this point, we are <a id="_idIndexMarker980"/>also going to optimize our build Bash script <a id="_idIndexMarker981"/>in our <code>/build/run_build.sh</code> file. First, we do not remove the <code>target</code> directory; instead, we are selective with what we upload onto our server with the following code:</p>
			<pre class="source-code">
#!/usr/bin/env bash
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 "mkdir web_app"
scp -i "~/.ssh/keys/remotebuild.pem" -r
../web_app/src ec2-user@$1:/home/ec2-user/web_app/src
scp -i "~/.ssh/keys/remotebuild.pem"
../web_app/Cargo.toml ec2-user@$1:/home/ec2-user/web_app/Cargo.toml
scp -i "~/.ssh/keys/remotebuild.pem"
../web_app/config.yml ec2-user@$1:/home/ec2-user/web_app/config.yml
scp -i "~/.ssh/keys/remotebuild.pem"
../web_app/Dockerfile ec2-user@$1:/home/ec2-user/web_app/Dockerfile</pre>
			<p>Here, we can see that we make the <code>web_app</code> directory, and then upload the files and directories <a id="_idIndexMarker982"/>that we need to build our Rust Docker image. We then <a id="_idIndexMarker983"/>need to connect to the server to install Rust with the following code:</p>
			<pre class="source-code">
echo "installing Rust"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  curl https://sh.rustup.rs -sSf | bash -s -- -y
  until [ -f ./output.txt ]
  do
      sleep 2
  done
  echo "File found"
EOF
echo "Rust has been installed"</pre>
			<p>Here, we can see that we install Rust before we block the script until the server is ready with everything installed. This means that we are running the installation of Rust at the same time the rest of the server is being built, saving time. We then exit the connection to our build server. Finally, we connect to the server again and build our Docker image with the following code:</p>
			<pre class="source-code">
echo "building Rust Docker image"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  cd web_app
  docker build . -t rust_app
EOF
echo "Docker image built"</pre>
			<p>We have then built the pipeline that builds our Rust Docker image on our build server. This seems like a lot of steps, and you would be right. We can build our image locally with a different <a id="_idIndexMarker984"/>target operating system and chip architecture. Exploring it <a id="_idIndexMarker985"/>here would disjoint the flow of what we are trying to achieve, but further information on compiling with different targets will be provided in the <em class="italic">Further </em><em class="italic">reading</em> section.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor214"/>Writing a Docker image file for the React frontend</h2>
			<p>For now, we are <a id="_idIndexMarker986"/>going to package our frontend <a id="_idIndexMarker987"/>application in Docker. In our <code>front_end/Dockerfile</code> file, we inherit the node base image, copy the code, and define the working directory with the following code:</p>
			<pre class="source-code">
FROM node:17.0.0
WORKDIR .
COPY . ./</pre>
			<p>We then install a <code>serve</code> package to serve the web app build files, the modules needed to build the application, and build the React application with the following code:</p>
			<pre class="source-code">
RUN npm install -g serve
RUN npm install
RUN npm run react-build</pre>
			<p>We then expose the port and server to our application with the following code:</p>
			<pre class="source-code">
EXPOSE 4000
CMD ["serve", "-s", "build", "-l", "4000"]</pre>
			<p>We can then build our Docker image with the following command:</p>
			<pre class="console">
docker build . -t front_end</pre>
			<p>We can then run the recently built image with the following command:</p>
			<pre class="console">
docker run -p 80:4000 front_end</pre>
			<p>This routes the container’s external port (<code>80</code>) to the locally exposed port <code>4000</code>. When our image is running, we get the following printout:</p>
			<pre class="console">
INFO: Accepting connections at http://localhost:4000</pre>
			<p>This shows <a id="_idIndexMarker988"/>that our image is running in <a id="_idIndexMarker989"/>a container. We will be able to access our frontend container by merely accessing our localhost, which is port <code>80</code>, as seen in the following screenshot:</p>
			<p class="IMG---Figure"> </p>
			<div><div><img src="img/Figure_10.12_B18722.jpg" alt="Figure 10.12 – Accessing our frontend container using localhost"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12 – Accessing our frontend container using localhost</p>
			<p>We will not be able to do anything with it, however, because our Rust server is not running. We can now lift the steps that we have carried out to build our frontend into our <code>/build/run_build.sh</code> script to have our build pipeline construct our frontend image as well. This is a good opportunity for you to try to add the step yourself. We will have to install node and then carry out the frontend build steps on the build server.</p>
			<p>If you have had an attempt at incorporating our React build in our pipeline, it should look like the following code:</p>
			<pre class="source-code">
echo "copying React app"
rm -rf ../front_end/node_modules/
scp -i "~/.ssh/keys/remotebuild.pem" -r
../front_end ec2-user@$1:/home/ec2-user/front_end
echo "React app copied"
echo "installing node on build server"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm
  /v0.37.2/install.sh | bash
  . ~/.nvm/nvm.sh
  nvm install --lts
EOF
echo "node installed"
echo "building front-end on server"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  cd front_end
  docker build . -t front_end
EOF
echo "front-end Docker image has been built"</pre>
			<p>We can be <a id="_idIndexMarker990"/>more optimal in our <a id="_idIndexMarker991"/>implementation; however, the preceding code is the simplest application. First, we copy over the code we need to the build server. We then connect to our build server to install node. After installing node, we connect to the server again to move into the React application directory and build our Docker image.</p>
			<p>Our build pipeline is now working. Just think of what we have achieved here—we have built a pipeline that constructs a build server; we then copied our code onto the build server, constructed Docker images, and then destroyed the server after the build was done. Even <a id="_idIndexMarker992"/>though this pipeline is not <a id="_idIndexMarker993"/>perfect, we have explored some powerful tools that will enable you to automate tasks and lift a lot of the code that we have covered in this subsection in other CI pipeline tools. However, right now, we are just building the Docker images and then destroying them with the server. In the next section, we will deploy our images on <strong class="bold">Docker Hub</strong>.</p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor215"/>Deploying images onto Docker Hub</h2>
			<p>Before we <a id="_idIndexMarker994"/>can pull our images from Docker Hub, we will have <a id="_idIndexMarker995"/>to push our images to Docker Hub. Before we can push our images to Docker Hub, we will have to create a Docker Hub repo. Registering our image on Docker Hub is straightforward. After logging in, we click on the <strong class="bold">Create Repository</strong> button in the top-right corner, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_10.13_B18722.jpg" alt="Figure 10.13 – Creating a new repository on Docker Hub"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.13 – Creating a new repository on Docker Hub</p>
			<p>Once we have clicked on this, we define the repository with the following configuration:</p>
			<div><div><img src="img/Figure_10.14_B18722.jpg" alt="Figure 10.14 – Defining a new Docker repository"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.14 – Defining a new Docker repository</p>
			<p>We can see that there is an option for connecting our repository with GitHub by clicking on the <em class="italic">GitHub</em> button seen in the preceding screenshot. The <strong class="bold">Connected</strong> GitHub status in the preceding screenshot simply means that my GitHub is connected to my Docker Hub account. This means that every time a successful pull request gets completed, the image is rebuilt <a id="_idIndexMarker996"/>with the code, and then it is sent to the repository. This <a id="_idIndexMarker997"/>can be helpful if you are building a fully automated pipeline. However, for this book, we will not connect our GitHub repository. We will push it onto our build server. You will also need to create a Docker Hub repository for our frontend if you are deploying the React application.</p>
			<p>Now that we have defined our Docker Hub repositories, we need to add a Docker login in our build pipeline. This means that we must pass our Docker Hub password and username into our Python build script. Our Python script can then pass the Docker Hub credentials into the build Bash script. This build Bash script will then log in to Docker on the build server so that we can push our images to our Docker Hub. In our <code>/build/run_build.py</code> file, we define the arguments passed into the Python script with the following code:</p>
			<pre class="source-code">
. . .
import argparse
. . .
parser = argparse.ArgumentParser(
                    description='Run the build'
                  )
parser.add_argument('--u', action='store',
                    help='docker username',
                    type=str, required=True)
parser.add_argument('--p', action='store',
                    help='docker password',
                    type=str, required=True)
args = parser.parse_args()</pre>
			<p>We can see that we have set <code>required</code> to <code>True</code>, which means that the Python script will not run unless both parameters are supplied. If we supply a <code>-h</code> parameter in the Python script, the <a id="_idIndexMarker998"/>parameters that we have defined in the preceding <a id="_idIndexMarker999"/>code will be printed out with help information. Now that we have ingested the Docker credentials, we can then pass them into our build Bash script in the <code>/build/run_build.py</code> file with the following adaptation to our code:</p>
			<pre class="source-code">
. . .
print("attempting to enter server")
build_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp; sh ./run_build.sh
    {server_ip} {args.u} {args.p}", shell=True)
build_process.wait()
destroy_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp; terraform destroy",
    shell=True)
. . .</pre>
			<p>Here, we can see that the Docker username is accessed using the <code>args.u</code> attribute and the Docker <a id="_idIndexMarker1000"/>password is accessed using the <code>args.p</code> attribute. Now <a id="_idIndexMarker1001"/>that we are passing Docker credentials into our build Bash script, we need to use these credentials to push our images. In our <code>/build/run_build.sh</code> file, we should log in after Docker is installed on our build server with the following code:</p>
			<pre class="source-code">
. . .
echo "Rust has been installed"
echo "logging in to Docker"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  echo $3 | docker login --username $2 --password-stdin
EOF
echo "logged in to Docker"
echo "building Rust Docker image"
. . .</pre>
			<p>In the preceding code, we use <code>–password-stdin</code> to pipe our password into the Docker login. <code>stdin</code> ensures that the password is not stored in the logs, making it a little bit more secure. We can then build, tag, and then push our Rust application with the update in the following code:</p>
			<pre class="source-code">
echo "building Rust Docker image"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  cd web_app
  docker build . -t rust_app
  docker tag rust_app:latest maxwellflitton/to_do_actix:latest
  docker push maxwellflitton/to_do_actix:latest
EOF
echo "Docker image built"</pre>
			<p>Here, we build the Rust image as we always do. We then tag the Rust app image as the latest release and then push it to the Docker repository. We also must push our frontend <a id="_idIndexMarker1002"/>application to Docker Hub. At this point, this is a good <a id="_idIndexMarker1003"/>chance for you to write the code that pushes the frontend image. If you did attempt to push the frontend image, it should look like the following code snippet:</p>
			<pre class="source-code">
echo "building front-end on server"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  cd front_end
  docker build . -t front_end
  docker tag front_end:latest maxwellflitton/to_do_react:latest
  docker push maxwellflitton/to_do_react:latest
EOF
echo "front-end Docker image has been built"</pre>
			<p>We have now coded all that we need to build both of our images and push them to our Docker repositories. We can run our Python build script with the following command:</p>
			<pre class="console">
python3 run_build.py --u some_username --p some_password</pre>
			<p>Again, the printout is lengthy, but we can check our Docker Hub repository to see if our image was pushed. As we can see in the following screenshot, the Docker Hub repository states when the image has been pushed:</p>
			<div><div><img src="img/Figure_10.15_B18722.jpg" alt="Figure 10.15 – View of pushed Docker repository"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.15 – View of pushed Docker repository</p>
			<p>We now have our images pushed onto our Docker Hub repositories! This means that we can pull them <a id="_idIndexMarker1004"/>onto any computer that we need, just as we did when <a id="_idIndexMarker1005"/>pulling the PostgreSQL image in the <code>docker-compose</code> file. We should now pull our images onto a server so that other people can access and use our application. In the next section, we will deploy our application for external use.</p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor216"/>Deploying our application on AWS</h1>
			<p>Even though <a id="_idIndexMarker1006"/>we have packaged our Rust application in <a id="_idIndexMarker1007"/>Docker, we have not run our Rust application in a Docker container. Before we run our Rust application on a server on AWS, we should run our Rust application locally. This will help us understand how a simple deployment works without having to build servers.</p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor217"/>Running our application locally</h2>
			<p>When it comes <a id="_idIndexMarker1008"/>to running our application locally, we will be using <code>docker-compose</code> with the following layout:</p>
			<div><div><img src="img/Figure_10.16_B18722.jpg" alt="Figure 10.16 – Structure for local deployment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.16 – Structure for local deployment</p>
			<p>Here, we can see that the NGINX container takes in traffic from outside of the <code>docker-compose</code> network and directs the traffic to the appropriate container. Now that we understand our structure, we can define our <code>docker-compose</code> file. First, we need to make a directory called <code>deployment</code> next to our <code>build</code>, <code>front_end</code>, and <code>web_app</code> directories. Our general layout for our <code>docker-compose.yml</code> file in our <code>deployment</code> directory takes the following form:</p>
			<pre class="source-code">
services:
  nginx:
    . . .
  postgres_production:
    . . .
  redis_production:
      . . .
  rust_app:
    . . .
  front_end:
    . . .</pre>
			<p>We can start <a id="_idIndexMarker1009"/>with our NGINX service. Now that we know the outside port is <code>80</code>, it makes sense that our NGINX container listens to the outside port of <code>80</code> with the following code:</p>
			<pre class="source-code">
  nginx:
    container_name: 'nginx-rust'
    image: "nginx:latest"
    ports:
      - "80:80"
    links:
      - rust_app
      - front_end
    volumes:
      - ./nginx_config.conf:/etc/nginx/nginx.conf</pre>
			<p>We can see that we get the latest NGINX image. We also have links to the <code>front_end</code> and <code>rust_app</code> containers because we will be passing HTTP requests to these containers. It also must be noted that we have a volume. This is where we share a volume outside of the container with a directory inside the container. So, this volume definition means that our <code>deploy/nginx_config.conf</code> file can be accessed in the NGINX container in the <code>etc/nginx/nginx.conf</code> directory. With this volume, we can configure NGINX routing rules in our <code>deploy/nginx_config.conf</code> file.</p>
			<p>First, we set the number of worker processes to <code>auto</code>. We can manually define the number of worker processes if needed. <code>auto</code> detects the number of CPU cores available and sets the number to that with the following code:</p>
			<pre class="source-code">
worker_processes  auto;
error_log  /var/log/nginx/error.log warn;</pre>
			<p>The <code>error_log</code> directive defines the logging to a particular file. We not only define the file, but we also state the minimum level needed to write to the log file, which is <code>warning</code>. By default, the <a id="_idIndexMarker1010"/>logging level needed to write to the file is <code>error</code>. We can now move on to defining contexts in our <code>deploy/nginx_config.conf</code> file. In the <code>events</code> context, we define the maximum number of connections that a worker can entertain at a time. This is achieved with the following code:</p>
			<pre class="source-code">
events {
    worker_connections  512;
}</pre>
			<p>The number of workers that we have defined is the default number that NGINX sets. Now that this is done, we can move on to our <code>http</code> context. Here, we define the <code>server</code> context. Inside this, we instruct the server to listen to port <code>80</code>, which is the port that listens to outside traffic, with the following code:</p>
			<pre class="source-code">
http {
    server {
        listen 80;
        location /v1 {
            proxy_pass http://rust_app:8000/v1;
        }
        location / {
            proxy_pass http://front_end:4000/;
        }
    }
}</pre>
			<p>Here, we can see that if the URL has <code>/v1/</code> at the start of the endpoint, we then pass it through the Rust server. It must be noted that we pass <code>v1</code> forward to the Rust server. If we did not pass <code>v1</code> forward to the Rust server, <code>v1</code> will be missing from the URL when it hits the Rust server. If the URL does not contain <code>v1</code> in the URL, then we forward it to our <code>front_end</code> container. Our NGINX container is now ready to manage traffic in our <code>docker-compose</code> system. Before we move on to our frontend and backend services, we need to define the <a id="_idIndexMarker1011"/>Redis and PostgreSQL databases. There is nothing new here, so at this point, you can try to define them yourself. If you have, then your code should look like this:</p>
			<pre class="source-code">
  postgres_production:
    container_name: 'to-do-postgres-production'
    image: 'postgres:latest'
    restart: always
    ports:
      - '5433:5432'
    environment:
      - 'POSTGRES_USER=username'
      - 'POSTGRES_DB=to_do'
      - 'POSTGRES_PASSWORD=password'
    expose:
      - 5433
  redis_production:
      container_name: 'to-do-redis'
      image: 'redis:5.0.5'
      ports:
        - '6379:6379'
      volumes:
    	  - ./data/redis:/tmp</pre>
			<p>The preceding database definitions are the same as what we have used before in local development. With these databases, we can define our Rust application with the following code:</p>
			<pre class="source-code">
  rust_app:
    container_name: rust_app
    build: "../web_app"
    restart: always
    ports:
      - "8000:8000"
    links:
      - postgres_production
      - redis_production
    expose:
      - 8000
    volumes:
      - ./rust_config.yml:/config.yml</pre>
			<p>Here, we can see that we are not defining an image. Instead of declaring the image, we point to a build. The <code>build</code> tag is where we point to a <code>deploy/rust_config.yml</code> file takes the following form:</p>
			<pre class="source-code">
DB_URL: postgres://username:password@postgres_production/to_do
SECRET_KEY: secret
EXPIRE_MINUTES: 120
REDIS_URL: redis://redis_production/</pre>
			<p>Here, we can see that we reference the name of the service defined in the <code>docker-compose</code> system instead of the URL. We also must change the address for our Rust application in our <code>web_app/src/main.rs</code> file to zeros, as seen here:</p>
			<pre class="source-code">
})
.bind("0.0.0.0:8000")?
.run()</pre>
			<p>We then must remove our config file in our Docker build in our <code>web_app/Dockerfile</code> file with the following line of code:</p>
			<pre class="source-code">
RUN rm config.yml</pre>
			<p>If we do not do <a id="_idIndexMarker1013"/>this, then our Rust application will not connect with the NGINX container. Now everything is defined for our Rust server, we can move on to defining the frontend application in our <code>docker-compose.yml</code> file with the following code:</p>
			<pre class="source-code">
  front_end:
    container_name: front_end
    image: "maxwellflitton/to_do_react:latest"
    restart: always
    ports:
      - "4000:4000"
    expose:
      - 4000</pre>
			<p>Here, we see that we reference our image in Docker Hub and expose the ports. Now that our local system is defined, we can run our system and interact with it by running the following command:</p>
			<pre class="console">
docker-compose up</pre>
			<p>Everything <a id="_idIndexMarker1014"/>will build and run automatically. Before we can interact with our system, we need to run our <code>diesel</code> migrations in our Rust application build with the following command:</p>
			<pre class="console">
diesel migration run</pre>
			<p>We then need to create a user with the following <code>curl</code> command:</p>
			<pre class="console">
curl --location --request POST 'http://localhost/v1/user/create' \
--header 'Content-Type: application/json' \
--data-raw '{
    "name": "maxwell",
    "email": "maxwellflitton@gmail.com",
    "password": "test"
}'</pre>
			<p>We now have everything in place to interact with our application. We can see that localhost with no reference to ports works with the following screenshot:</p>
			<p class="IMG---Figure"> </p>
			<div><div><img src="img/Figure_10.17_B18722.jpg" alt="Figure 10.17 – Accessing our docker-compose system through the browser"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.17 – Accessing our docker-compose system through the browser</p>
			<p>If your NGINX is <a id="_idIndexMarker1015"/>working, you should be able to log in and interact with the to-do application as before. We are now able to deploy our system on AWS so that other people can access and use our to-do application in the next section.</p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor218"/>Running our application on AWS</h2>
			<p>We can <a id="_idIndexMarker1016"/>deploy our application on AWS by carrying <a id="_idIndexMarker1017"/>out the following steps:</p>
			<ol>
				<li value="1">Building a server</li>
				<li>Running our <code>docker-compose</code> system on that server</li>
				<li>Running database migrations on that server</li>
				<li>Creating a user</li>
			</ol>
			<p>Once we have carried out those steps, we will be able to access the application on the remote server. However, before we do this, we are going to have to alter the React application. Right now, our React application makes API calls to the localhost via <code>127.0.0.1</code>. When we are using a remote server, this will not work as we will have to make calls to the server to which we have deployed our application. To do this, we can extract where our API calls are made in the React application and update the root of the URL for the API call with the following code:</p>
			<pre class="source-code">
axios.get(window.location.href + "/v1/item/get",</pre>
			<p>What is happening here is that <code>window.location.href</code> returns the current location, which will be the IP of the server our application is deployed on, or localhost if we are developing it locally on our computer. The following files have API calls that need to be updated:</p>
			<ul>
				<li><code>src/components/LoginForm.js</code></li>
				<li><code>src/components/CreateToDoitem.js</code></li>
				<li><code>src/components/ToDoitem.js</code></li>
				<li><code>src/App.js</code></li>
			</ul>
			<p>Once we have updated these files, we will be able to run another build in the <code>build</code> directory by running the following command:</p>
			<pre class="console">
python3 run_build.py --u some_username --p some_password</pre>
			<p>Once our build has <a id="_idIndexMarker1018"/>been done, both our images will be <a id="_idIndexMarker1019"/>updated. We can now move to our <code>deployment</code> directory and flesh it out with the following files:</p>
			<ul>
				<li><code>main.tf</code>: This should be the same as the <code>main.tf</code> file in the <code>build</code> directory, except for the server having a different tag</li>
				<li><code>run_build.py</code>: This should be the same as the <code>run_build.py</code> file in the <code>build</code> directory, except for the <em class="italic">destroy server</em> process at the end of the <code>run_build.py</code> script</li>
				<li><code>server_build.sh</code>: This should be the same as the <code>server_build.sh</code> script in the <code>build</code> directory as we want our server to have the same environment as when our images were built</li>
				<li><code>deployment-compose.yml</code>: This should be the same as the <code>docker-compose.yml</code> file in the <code>deployment</code> directory, except that <code>rust_app</code> service has an image tag instead of a build tag and the image tag should have the image of <code>maxwellflitton/to_do_actix:latest</code></li>
				<li><code>.env</code>: This should be the same as the <code>.env</code> file in the <code>web_app</code> directory, and we will need it to perform database migrations</li>
			</ul>
			<p>We are now <a id="_idIndexMarker1020"/>ready to code our <code>run_build.sh</code> file that <a id="_idIndexMarker1021"/>will enable us to deploy our application, run migrations, and create a user. First, we start off with some standard boilerplate code to ensure that we are in the right directory, as follows:</p>
			<pre class="source-code">
#!/usr/bin/env bash
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH</pre>
			<p>We then copy the files needed to spin up our <code>docker-compose</code> system and perform database migrations with the following code:</p>
			<pre class="source-code">
scp -i "~/.ssh/keys/remotebuild.pem"
./deployment-compose.yml ec2-user@$1:/home/ec2-user/docker-compose.yml
scp -i "~/.ssh/keys/remotebuild.pem"
./rust_config.yml ec2-user@$1:/home/ec2-user/rust_config.yml
scp -i "~/.ssh/keys/remotebuild.pem"
./.env ec2-user@$1:/home/ec2-user/.env
scp -i "~/.ssh/keys/remotebuild.pem"
./nginx_config.conf ec2-user@$1:/home/ec2-user/nginx_config.conf
scp -i "~/.ssh/keys/remotebuild.pem" -r
../web_app/migrations ec2-user@$1:/home/ec2-user/migrations</pre>
			<p>None of this should be a surprise as we needed all the preceding files to run our <code>docker-compose</code> system. We then install Rust and wait for the server build to be done with the following code:</p>
			<pre class="source-code">
echo "installing Rust"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  curl https://sh.rustup.rs -sSf | bash -s -- -y
  until [ -f ./output.txt ]
  do
      sleep 2
  done
  echo "File found"
EOF
echo "Rust has been installed"</pre>
			<p>Again, this is <a id="_idIndexMarker1022"/>nothing new that we have not seen. We then <a id="_idIndexMarker1023"/>install the <code>diesel</code> client with the following code:</p>
			<pre class="source-code">
echo "installing diesel"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  cargo install diesel_cli --no-default-features --features postgres
EOF
echo "diesel has been installed"</pre>
			<p>We then log in to Docker, spin up our <code>docker-compose</code> system, run our migrations, and then make a user with the following code:</p>
			<pre class="source-code">
echo "building system"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  echo $3 | docker login --username $2 --password-stdin
  docker-compose up -d
  sleep 2
  diesel migration run
  curl --location --request POST 'http://localhost/v1/user/create' \
  --header 'Content-Type: application/json' \
  --data-raw '{
      "name": "maxwell",
      "email": "maxwellflitton@gmail.com",
      "password": "test"
  }'
EOF
echo "system has been built"</pre>
			<p>With this, our <a id="_idIndexMarker1024"/>system is deployed! We will be able to access <a id="_idIndexMarker1025"/>our application on our server by putting the IP of the server that is in the <code>output.json</code> file into the browser. We will be able to log in and use our to-do application just like when we were running our system on our local computer, as seen on in the following screenshot:</p>
			<div><div><img src="img/Figure_10.18_B18722.jpg" alt="Figure 10.18 – Our application on our deployment server"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.18 – Our application on our deployment server</p>
			<p>As we can <a id="_idIndexMarker1026"/>see, the connection is not secure and our browser <a id="_idIndexMarker1027"/>is giving us a warning because we are not implementing the HTTPS protocol. This is because our connection is not encrypted. We will cover how to encrypt our connection in the next chapter.</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor219"/>Writing our application build script</h2>
			<p>Right now, our application <a id="_idIndexMarker1028"/>is running a database locally on the EC2 instance. This has a few problems. Firstly, it means that the EC2 is stateful. If we tear down the instance, we will lose all our data.</p>
			<p>Secondly, if we wipe the containers on the instance, we could also lose all our data. Data vulnerability is not the only issue here. Let’s say that our traffic drastically increases, and we need more computing instances to manage it. This can be done by using NGINX as a load balancer between two instances, as shown in the following diagram:</p>
			<div><div><img src="img/Figure_10.19_B18722.jpg" alt="Figure 10.19 – Doubling our EC2 instances for our system"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.19 – Doubling our EC2 instances for our system</p>
			<p>As you can see, the problem here is accessing random data. If user one creates an item, and this request hits the instance on the left, it is stored in the database on the left. However, user <a id="_idIndexMarker1029"/>one can then make a <code>GET</code> request, which hits the instance on the right side. The second request will not be able to access the item that was created in the first request. The user would be accessing random states depending on which instance the request hit.</p>
			<p>This can be solved by deleting the database from our <code>docker-compose</code> file and creating a database outside it, as shown in this diagram:</p>
			<div><div><img src="img/Figure_10.20_B18722.jpg" alt="Figure 10.20 – Our new improved system"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.20 – Our new improved system</p>
			<p>Now, we have a single point of truth for our data, and our EC2 instances are stateless, meaning we have the freedom to create and delete instances as and when we need to.</p>
			<p>When it comes to adding an AWS database to our deployment, we are going to have to build our <a id="_idIndexMarker1030"/>database in Terraform, and then pass the information about the constructed database to our <code>deployment/.env</code> file for database migrations and our <code>deployment/rust_config.yml</code> file for our Rust server to access. First, we must add the database definition to our <code>deployment/main.tf</code> file with the following code:</p>
			<pre class="source-code">
resource "aws_db_instance" "main_db" {
  instance_class         = "db.t3.micro"
  allocated_storage      = 5
  engine                 = "postgres"
  username               = var.db_username
  password               = var.db_password
  db_name                = "to_do"
  publicly_accessible    = true
  skip_final_snapshot    = true
  tags = {
      Name = "to-do production database"
    }
}</pre>
			<p>The fields defined in the preceding code are straightforward apart from the <code>allocated_storage</code> field, which is the number of GB allocated to the database. We can also see that <a id="_idIndexMarker1031"/>we use variables with the <code>var</code> variable. This means that we must pass a password and username into our Terraform build when running. We need to define our input variables in the <code>deployment/variables.tf</code> file with the following code:</p>
			<pre class="source-code">
variable "db_password" {
    description = "The password for the database"
    default = "password"
}
variable "db_username" {
    description = "The username for the database"
    default = "username"
}</pre>
			<p>These variables have defaults, so we do not need to pass in a variable. However, if we want to pass in a variable, we can do this with the following layout:</p>
			<pre class="console">
-var"db_password=some_password" -var"db_username=some_username"</pre>
			<p>We now must pass these parameters into the needed files and the Terraform build. This is where Python starts to shine. We will be reading and writing YAML files, so we will have to install the YAML Python package with the following command:</p>
			<pre class="console">
pip install pyyaml</pre>
			<p>We then import this package in our <code>deployment/run_build.py</code> file at the top of the script with the following code:</p>
			<pre class="source-code">
import yaml</pre>
			<p>We then load database parameters from a JSON file called <code>database.json</code> and create our <code>vars</code> command string with the following code:</p>
			<pre class="source-code">
with open("./database.json") as json_file:
    db_data = json.load(json_file)
params = f' -var="db_password={db_data["password"]}"
            -var="db_username={db_data["user"]}"'</pre>
			<p>You can come up <a id="_idIndexMarker1032"/>with any parameters you want for your <code>deployment/database.json</code> file; I have recently been playing with GitHub Copilot, which is an AI pair programmer that auto-fills code, and this gave me the following parameters:</p>
			<pre class="source-code">
{
    "user": "Santiago",
    "password": "1234567890",
    "host": "localhost",
    "port": "5432",
    "database": "test"
}</pre>
			<p>I do not know who <code>Santiago</code> is, but the Copilot AI clearly thinks that <code>Santiago</code> is the right user, so I am going to use it. Going back to our <code>deployment/run_build.py</code> file, we must pass our parameters to the Terraform <code>apply</code> command by updating the following code:</p>
			<pre class="source-code">
apply_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp; terraform apply" + params, shell=True)</pre>
			<p>After the processes that run the Terraform build have finished, we then store the output of that build <a id="_idIndexMarker1033"/>in a JSON file. We then create our own database URL and write this URL to a text file with the following code:</p>
			<pre class="source-code">
. . .
produce_output.wait()
with open(f"{DIRECTORY_PATH}/output.json", "r") as file:
    data = json.loads(file.read())
database_url = f"postgresql://{db_data['user']}:{db_data['password']}
    @{data['db_endpoint']['value'][0]}/to_do"
with open("./database.txt", "w") as text_file:
    text_file.write("DATABASE_URL=" + database_url)</pre>
			<p>The only thing we need to do now is update our Rust application config data with the following code:</p>
			<pre class="source-code">
with open("./rust_config.yml") as yaml_file:
    config = yaml.load(yaml_file, Loader=yaml.FullLoader)
config["DB_URL"] = database_url
with open("./rust_config.yml", "w") as yaml_file:
    yaml.dump(config, yaml_file, default_flow_style=False)</pre>
			<p>There is only one change left in our pipeline, and this is in our <code>deployment/run_build.sh</code> file. Instead of copying our local <code>.env</code> file to our deployment server, we copy our <code>deployment/database.txt</code> file with the following code:</p>
			<pre class="source-code">
scp -i "~/.ssh/keys/remotebuild.pem"
./database.txt ec2-user@$1:/home/ec2-user/.env</pre>
			<p>Running our deployment again will deploy our server and connect it to the AWS database that we <a id="_idIndexMarker1034"/>have created. Again, these build scripts can be brittle. Sometimes, a connection can be refused when copying one of the files to the deployment server, which can result in the breaking of the entire pipeline. Because we have coded all the steps ourselves and understand each step, if there is a break, it will not take us much to manually sort out the break or try to run the Python build script again.</p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor220"/>Summary</h1>
			<p>We have finally come to the end of our journey. We have created our own Docker image, packaging our Rust application. We then ran this on our local computer with the protection of an NGINX container. We then deployed it onto a Docker Hub account, enabling us to use it to deploy onto an AWS server that we set up.</p>
			<p>It must be noted that we have gone through the lengthy steps of configuring containers and accessing our server via SSH. This has enabled us to apply this process to other platforms as our general approach was not AWS-centric. We merely used AWS to set up the server. However, if we set up a server on another provider, we would still be able to install Docker on the server, deploy our image onto it, and run it with NGINX and a connection to a database.</p>
			<p>There are a few more things we can do as a developer’s work is never done. However, we have covered and achieved the core basics of building a Rust web application from scratch and deploying it in an automated fashion.</p>
			<p>Considering this, there is little holding back developers from building web applications in Rust. Frontend frameworks can be added to improve the frontend functionality, and extra modules can be added to our application to increase its functionality and API endpoints. We now have a solid base to build a range of applications and read further on topics to enable us to develop our skills and knowledge of web development in Rust.</p>
			<p>We are at an exciting time with Rust and web development, and hopefully, after getting to this point, you feel empowered to push Rust forward in the field of web development. In the next chapter, we will be encrypting our web traffic to our application using HTTPS.</p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor221"/>Further reading</h1>
			<ul>
				<li>Rust compiling to different targets documentation: <a href="https://doc.rust-lang.org/rustc/targets/index.html">https://doc.rust-lang.org/rustc/targets/index.html</a></li>
				<li>GitHub Actions documentation: <a href="https://github.com/features/actions">https://github.com/features/actions</a></li>
				<li>Travis CI documentation: <a href="https://docs.travis-ci.com/user/for-beginners/">https://docs.travis-ci.com/user/for-beginners/</a></li>
				<li>CircleCI documentation: <a href="https://circleci.com/docs/">https://circleci.com/docs/</a></li>
				<li>Jenkins documentation: <a href="https://www.jenkins.io/doc/">https://www.jenkins.io/doc/</a></li>
				<li>Terraform output documentation: <a href="https://developer.hashicorp.com/terraform/language/values/outputs">https://developer.hashicorp.com/terraform/language/values/outputs</a></li>
				<li><em class="italic">AWS Certified Developer - Associate Guide</em>, <em class="italic">Second Edition</em>, <em class="italic">V. Tankariya</em> and <em class="italic">B. Parmar</em> (2019), <em class="italic">Packt Publishing</em>, <a href="B18722_05.xhtml#_idTextAnchor091"><em class="italic">Chapter 5</em></a>, <em class="italic">Getting Started with Elastic Compute Cloud (EC2)</em>, page 165</li>
				<li><em class="italic">AWS Certified Developer - Associate Guide</em>, <em class="italic">Second Edition</em>, <em class="italic">V. Tankariya</em> and <em class="italic">B. Parmar</em> (2019), <em class="italic">Packt Publishing</em>, <a href="B18722_10.xhtml#_idTextAnchor200"><em class="italic">Chapter 10</em></a>, <em class="italic">AWS Relational Database Service (RDS)</em>, page 333</li>
				<li><em class="italic">AWS Certified Developer - Associate Guide</em>, <em class="italic">Second Edition</em>, <em class="italic">V. Tankariya</em> and <em class="italic">B. Parmar</em> (2019), <em class="italic">Packt Publishing</em>, <em class="italic">Chapter 21</em>, <em class="italic">Getting Started with AWS CodeDeploy</em>, page 657</li>
				<li><em class="italic">Mastering Kubernetes</em>, <em class="italic">G. Sayfan</em> (2020), <em class="italic">Packt Publishing</em></li>
				<li><em class="italic">Getting Started with Terraform</em>, <em class="italic">K. Shirinkin</em> (2017), <em class="italic">Packt Publishing</em></li>
				<li><em class="italic">Nginx HTTP Server</em>, <em class="italic">Fourth Edition</em>, <em class="italic">M. Fjordvald</em> and <em class="italic">C. Nedelcu</em> (2018), <em class="italic">Packt Publishing</em></li>
			</ul>
		</div>
	</body></html>
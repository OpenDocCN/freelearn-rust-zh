<html><head></head><body>
		<div id="_idContainer127">
			<h1 id="_idParaDest-199" class="chapter-number"><a id="_idTextAnchor200"/>10</h1>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor201"/>Deploying Our Application on AWS</h1>
			<p>In a lot of tutorials and educational materials, deployment is rarely covered. This is because there are a lot of moving parts, and the process can be fairly brittle. It may be more convenient to refer to other resources when <span class="No-Break">mentioning deployment.</span></p>
			<p>In this chapter, we will cover enough to automate deployment on a server on <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) and then build and connect to a database from there. It must be stressed that deployment and cloud computing are big topics—there are whole books written <span class="No-Break">on them.</span></p>
			<p>In this chapter, we will get to a point where we can deploy and run our application for others to use. Learning how to deploy applications on a server is the final step. This is where you will turn the application that you have been developing into a practical reality that can be used by others all over <span class="No-Break">the world.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Setting up our <span class="No-Break">build environment</span></li>
				<li>Managing our software <span class="No-Break">with Docker</span></li>
				<li>Deploying our application <span class="No-Break">on AWS</span></li>
			</ul>
			<p>By the end of this chapter, you will be able to wrap your code up in Docker images and deploy it on a server instance on AWS so that it can be accessed by other users. You will also be able to configure infrastructure using <strong class="bold">Terraform</strong>, which is a tool used to define cloud computing infrastructure such as servers and databases as code. Once you have finished this chapter, you will have a few build scripts that create a build and deployment server using Terraform, pass the data from that Terraform build into config files, <strong class="bold">SSH</strong> into these servers, and run a series of commands resulting in database migrations and spinning up the Docker containers on <span class="No-Break">our server.</span></p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor202"/>Technical requirements</h1>
			<p>In this chapter, we’ll build on the code built in <a href="B18722_09.xhtml#_idTextAnchor182"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Testing Our Application Endpoints and Components</em>. This can be found at the following <span class="No-Break">URL: </span><a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter09/building_test_pipeline"><span class="No-Break">https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter09/building_test_pipeline</span></a><span class="No-Break">.</span></p>
			<p>The code for this chapter can be found <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter10"><span class="No-Break">https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter10</span></a><span class="No-Break">.</span></p>
			<p>This chapter also has the <span class="No-Break">following requirements:</span></p>
			<ul>
				<li>We will be using Terraform to automate the building of our servers. Because of this, we will need to install Terraform using the following <span class="No-Break">URL: </span><a href="https://learn.hashicorp.com/tutorials/terraform/install-cli"><span class="No-Break">https://learn.hashicorp.com/tutorials/terraform/install-cli</span></a><span class="No-Break">.</span></li>
				<li>When we are using Terraform, it will be making calls to the AWS infrastructure. We will need AWS authentication, which will be done using the AWS client. We will be using the AWS client in this chapter, which can be installed using the following <span class="No-Break">URL: </span><a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html"><span class="No-Break">https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</span></a><span class="No-Break">.</span></li>
				<li>You will also need a Docker Hub account so that we can package and deploy our application. This can be found <span class="No-Break">at </span><a href="https://hub.docker.com/"><span class="No-Break">https://hub.docker.com/</span></a><span class="No-Break">.</span></li>
				<li>Since you will be deploying the application on a server, you will need to sign up for an AWS account. This can be done at the following <span class="No-Break">URL: </span><a href="https://aws.amazon.com/"><span class="No-Break">https://aws.amazon.com/</span></a><span class="No-Break">.</span></li>
			</ul>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor203"/>Setting up our build environment</h1>
			<p>So far, we have been running our application with the <strong class="source-inline">cargo run</strong> command. This has been working <a id="_idIndexMarker929"/>well, but you might have noticed that our application is not very fast. In fact, it is relatively slow when we try to log in to the application. This seems to be counterintuitive as we are learning Rust to develop <span class="No-Break">faster applications.</span></p>
			<p>So far, it does not look very fast. This is because we are not running an optimized version of our application. We can do this by adding the <strong class="source-inline">--release</strong> tag. As a result, we run our optimized application using the <span class="No-Break">following command:</span></p>
			<pre class="console">
cargo run --release config.yml</pre>
			<p>Here, we notice that the compilation takes a lot longer. Running this every time we alter the code, and during a development process, is not ideal; hence, we have been building and running in debug mode using the <strong class="source-inline">cargo run</strong> command. However, now that our optimized application is running, we can see that the login process is a lot faster. While we can run the server locally, our aim is to deploy our application on a server. To run our application on <a id="_idIndexMarker930"/>a server, we are going to have to build our application in a Docker image. To ensure that our Docker image build runs smoothly, we are going to use an online computing unit on AWS. Before we run our build process, we need to carry out the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Set up an AWS <strong class="bold">Secure Shell</strong> (<strong class="bold">SSH</strong>) key for <strong class="bold">Elastic Compute </strong><span class="No-Break"><strong class="bold">Cloud</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">EC2</strong></span><span class="No-Break">)</span></li>
				<li>Set up the AWS client for our <span class="No-Break">local computer</span></li>
				<li>Write a Terraform script that builds our <span class="No-Break">build server</span></li>
				<li>Write a Python script that manages <span class="No-Break">the build</span></li>
				<li>Write a Bash script that orchestrates the build on <span class="No-Break">the server</span></li>
			</ol>
			<p>Once we have done the aforementioned steps, we will be able to run an automated pipeline that creates a build EC2 server on AWS and then build our Rust application. First, let us get started by creating an SSH key for <span class="No-Break">our server.</span></p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor204"/>Setting up an AWS SSH key for an AWS EC2 instance</h2>
			<p>If we want to <a id="_idIndexMarker931"/>run commands <a id="_idIndexMarker932"/>on a server, we are going to have to connect to <a id="_idIndexMarker933"/>the server using the SSH protocol over the <strong class="bold">Hypertext Transfer Protocol</strong> (<strong class="bold">HTTP</strong>). However, we cannot have anyone accessing our server as it will not be secure. To stop anyone from connecting to our server and running any commands they want, we will only allow users to connect to our server if they have an SSH key. To create our key, we need to log in to the AWS console and navigate to our EC2 dashboard, which can be accessed via the search box, as seen in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/Figure_10.1_B18722.jpg" alt="Figure 10.1 – Navigating to the EC2 dashboard with the search box"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Navigating to the EC2 dashboard with the search box</p>
			<p class="callout-heading">Note</p>
			<p class="callout">It must be noted that AWS might not look like the screenshots in this chapter as AWS keeps changing the UI. However, the fundamental concepts will be <span class="No-Break">the same.</span></p>
			<p>Once we have <a id="_idIndexMarker934"/>navigated to <a id="_idIndexMarker935"/>the EC2 dashboard, we can navigate to <strong class="bold">Key Pairs</strong> in the <strong class="bold">Network &amp; Security</strong> section of the panel on the left side of the view, as seen in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/Figure_10.2_B18722.jpg" alt="Figure 10.2 – Navigating to Key Pairs"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Navigating to Key Pairs</p>
			<p>Once we have navigated to the <strong class="bold">Key Pairs</strong> section, there will be a list of key pairs that you already own. If you have built EC2 instances before, you might already see some listed. If you have never built an EC2 instance before, then you will not have anything listed. On the top <a id="_idIndexMarker936"/>right of the <a id="_idIndexMarker937"/>screen, you can create a key by clicking on the <strong class="bold">Create key pair</strong> button shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/Figure_10.3_B18722.jpg" alt="Figure 10.3 – Button to allow the creation of a key pair"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Button to allow the creation of a key pair</p>
			<p>Once we have clicked on this button, we will see the <span class="No-Break">following form:</span></p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/Figure_10.4_B18722.jpg" alt="Figure 10.4 – Create key pair form"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Create key pair form</p>
			<p>In the preceding screenshot, we can see that we have named our <strong class="source-inline">remotebuild</strong> key; we also state that our <a id="_idIndexMarker938"/>key has <a id="_idIndexMarker939"/>the <strong class="source-inline">.pem</strong> format and is of the <strong class="bold">RSA</strong> type. Once we click on the <strong class="bold">Create key pair</strong> button, you will download the key. We now must store our key. This can be done by storing it in our home directory in the <strong class="source-inline">.ssh</strong> directory. Inside the <strong class="source-inline">.ssh</strong> directory, we can create a <strong class="source-inline">keys</strong> directory with the <span class="No-Break">following command:</span></p>
			<pre class="console">
mkdir "$HOME"/.ssh/keys/</pre>
			<p>The <strong class="source-inline">"$HOME"</strong> environment variable is always available in the Bash shell, and it denotes the home directory for the user. This means that other users who log in to the computer under a different username cannot access the SSH keys we downloaded if we store them in the directory that we have just created. We can now navigate to where our key has been downloaded and copy it to our <strong class="source-inline">keys</strong> directory with the <span class="No-Break">following command:</span></p>
			<pre class="console">
cp ./remotebuild.pem "$HOME"/.ssh/keys/</pre>
			<p>We then must change the permissions for the key so that only the owner of the file can read the file with the 600 code for us to use the key for SSH purposes with the <span class="No-Break">following command:</span></p>
			<pre class="console">
chmod 600 "$HOME"/.ssh/keys/remotebuild.pem</pre>
			<p>We now <a id="_idIndexMarker940"/>have an SSH key <a id="_idIndexMarker941"/>stored in our <strong class="source-inline">.ssh/keys</strong> directory where we can access this key with the correct permissions in order to access the servers that we create. Now that we have this, we need programmatic access to our AWS services by setting up our <span class="No-Break">AWS client.</span></p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor205"/>Setting up our AWS client</h2>
			<p>We will be using Terraform <a id="_idIndexMarker942"/>to automate our server build. In order to do this, Terraform will make calls to the AWS infrastructure. This means that we must have the AWS client installed on our local machine. Before we configure our client, we need to get some programmatic user keys. If we do not obtain programmatic access, our code will not be authorized to build infrastructure on our AWS account. The first step of obtaining the user keys is navigating to the <strong class="bold">IAM</strong> section via the search box, as seen in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/Figure_10.5_B18722.jpg" alt="Figure 10.5 – Navigating to the IAM section"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Navigating to the IAM section</p>
			<p>Once we have navigated to the <strong class="bold">IAM</strong> section, we will get the <span class="No-Break">following layout:</span></p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/Figure_10.6_B18722.jpg" alt="Figure 10.6 – View of the IAM section"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – View of the IAM section</p>
			<p>We can see in <a id="_idIndexMarker943"/>the preceding screenshot that my user has <strong class="bold">multi-factor authentication</strong> (<strong class="bold">MFA</strong>), and I keep my access key rotated. If you are new to AWS, this checklist might not be satisfactory, and it is suggested that you follow the security recommendations <a id="_idIndexMarker944"/>that AWS gives you. We now must access the <strong class="bold">Users</strong> option on the left-hand side of the view and create a new user by clicking the <strong class="bold">Add users</strong> button in the top-right corner of the screen, as seen in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/Figure_10.7_B18722.jpg" alt="Figure 10.7 – Creating users"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Creating users</p>
			<p>We can then create a user. It does not matter what name you call the user, but you must ensure that they have programmatic access by checking the <strong class="bold">Access key - Programmatic access</strong> option, as seen in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/Figure_10.8_B18722.jpg" alt="Figure 10.8 – First stage of creating a user"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – First stage of creating a user</p>
			<p>Once we have <a id="_idIndexMarker945"/>highlighted the programmatic access and defined the username, we can move on to the permissions, as seen in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/Figure_10.9_B18722.jpg" alt="Figure 10.9 – Defining permissions"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – Defining permissions</p>
			<p>We can see that we have given the user <strong class="source-inline">AdministratorAccess</strong> permissions, which will enable us to create and destroy servers and databases. The rest of the steps in the user creation process are trivial, and you can get through these by just clicking <strong class="bold">Next</strong>. Once the user is created, you will be exposed to an access key and a secret access key. It is important to note these down in a secure location such as a password manager because you will never be able to see your secret access key again on the AWS site, and we will need them when configuring our AWS client on our local computer. Now that we have the user keys handy, we can configure our AWS client with the <span class="No-Break">following command:</span></p>
			<pre class="console">
aws configure</pre>
			<p>The AWS client will then prompt you to enter the user keys as and when they are required. Once this <a id="_idIndexMarker946"/>is done, your AWS client is configured, and we can use AWS features programmatically on our local computer. Now, we are ready to start creating servers using Terraform in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor206"/>Setting up our Terraform build</h2>
			<p>When it comes to building <a id="_idIndexMarker947"/>infrastructure on AWS, we can simply point and click in the EC2 dashboard. However, this is not desirable. If you are anything like me, when I point and click a series of configuration settings, I forget what I have done unless I document it, and let’s be honest, documenting what you’ve clicked is not something you are going to look forward to. Even if you are a better person than me and you document it, when you change the configuration, there is a chance that you will not go back to update the documentation of that change. Pointing and clicking is also time-consuming. If we wanted to create some infrastructure and then destroy it a week later, and then recreate it a month after that, we would be reluctant to touch it if we had to point and click, and our server bills would be higher. This is where <strong class="bold">infrastructure as code</strong> (<strong class="bold">IaC</strong>) comes in. We will have to do some pointing and <a id="_idIndexMarker948"/>clicking, as we did in the previous sections. We cannot do any programmatic access without pointing and clicking to set up <span class="No-Break">programmatic access.</span></p>
			<p>Now that we have programmatic access, we can build out our <strong class="source-inline">build</strong> directory, which should be next to our <strong class="source-inline">web_app</strong> and <strong class="source-inline">front_end</strong> directories. In the <strong class="source-inline">build</strong> directory, we can define our infrastructure for the build server in the <strong class="source-inline">build/main.tf</strong> file. It must be noted that the <strong class="source-inline">.tf</strong> extension is the standard extension for Terraform files. First, we define which version of Terraform is being used with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
terraform {
  required_version = "&gt;= 1.1.3"
}</pre>
			<p>Now that we have defined the Terraform version, we can declare that we are using the AWS module. The Terraform registry has modules for a range of platforms, including Google Cloud <a id="_idIndexMarker949"/>and Microsoft Azure. Anyone can build modules and abstract infrastructure to be downloaded on the Terraform registry. Our AWS module usage declaration takes the <span class="No-Break">following form:</span></p>
			<pre class="source-code">
provider "aws" {
    version = "&gt;= 2.28.1"
    region = "eu-west-2"
}</pre>
			<p>You might want to pick a different region if another region suits you. A list of the available <a id="_idIndexMarker950"/>regions on AWS for EC2 can be found via the following <span class="No-Break">link: </span><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html"><span class="No-Break">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html</span></a><span class="No-Break">.</span></p>
			<p>I’m merely using <strong class="source-inline">"eu-west-2"</strong> for demonstration purposes. We can now build our EC2 instance with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
resource "aws_instance" "build_server" {
    ami = "ami-0fdbd8587b1cf431e"
    instance_type = "t2.medium"
    key_name = "remotebuild"
    user_data = file("server_build.sh")
    tags = {
      Name = "to-do build server"
    }
}</pre>
			<p><strong class="source-inline">resource</strong> declares that we are defining a resource to be built, and <strong class="source-inline">aws_instance</strong> states that we are using the EC2 instance template in the AWS module. A list of the available AWS Terraform modules and <a id="_idIndexMarker951"/>their documentation can be found via the following <span class="No-Break">link: </span><a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs"><span class="No-Break">https://registry.terraform.io/providers/hashicorp/aws/latest/docs</span></a><span class="No-Break">.</span></p>
			<p><strong class="source-inline">build_server</strong> is what we are calling it. We can refer to <strong class="source-inline">build_server</strong> anywhere else in the Terraform script, and Terraform will work out the order in which resources need to be built to make sure all references are accounted for. We can see that we have referenced the <strong class="source-inline">"remotebuild"</strong> key that we defined in the previous section. We can create multiple EC2 instances that can be accessed by one key if we want. We also declare the <a id="_idIndexMarker952"/>name so that when we look at our EC2 instances, we know what the server is for. We must also note that <strong class="source-inline">user_data</strong> is the Bash script that will be run on the new EC2 server once it has been built. The <strong class="source-inline">ami</strong> section is a reference to the type of operating system and version being used. Do not directly copy my <strong class="bold">Amazon Machine Image</strong> (<strong class="bold">AMI</strong>) ID in the example unless you are using the same region, as AMI IDs can vary depending on the region. If you want to find out the AMI ID, go to your EC2 dashboard and click on <strong class="bold">Launch an instance</strong>, which will result in the <span class="No-Break">following window:</span></p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/Figure_10.10_B18722.jpg" alt="Figure 10.10 – Launching an instance"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – Launching an instance</p>
			<p>Here, we can see <a id="_idIndexMarker953"/>that we are picking <strong class="bold">Amazon Linux</strong>. You must select this; otherwise, your build scripts will not work. If we zoom in, we can see that the AMI ID is visible, as <span class="No-Break">seen here:</span></p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/Figure_10.11_B18722.jpg" alt="Figure 10.11 – Accessing AMI ID for the server"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11 – Accessing AMI ID for the server</p>
			<p>This will be the AMI ID for the Amazon Linux operating system in the region that you want to launch. You can also see nothing is stopping you from using other operating systems in other Terraform projects. If you have built EC2 instances in the past, you will know that the IP <a id="_idIndexMarker954"/>address is random unless we attach an elastic IP to the EC2 instance. We will have to produce an output of the IP of our EC2 instance so that we can connect to it. Our output is defined with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
output "ec2_global_ips" {
  value = ["${aws_instance.build_server.*.public_ip}"]
}</pre>
			<p>Here, we can see that we reference our build server using <strong class="source-inline">aws_instance.build_server</strong>. Further reading on Terraform outputs is provided in the <em class="italic">Further reading</em> section. At this point, our Terraform build is nearly done. We must remember that we need to build the <strong class="source-inline">server_build.sh</strong> script that is going to be run on the EC2 instance once the EC2 instance has been built. In our <strong class="source-inline">/build/server_build.sh</strong> file, we can install the basic requirements needed for our server with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
#!/bin/bash
sudo yum update -y
sudo yum install git -y
sudo yum install cmake -y
sudo yum install tree -y
sudo yum install vim -y
sudo yum install tmux -y
sudo yum install make automake gcc gcc-c++ kernel-devel -y</pre>
			<p>With the preceding packages, we will be able to navigate around the server looking at file trees with <strong class="source-inline">tree</strong>; we will also be able to perform <strong class="source-inline">git</strong> operations, open files, and edit them using <strong class="source-inline">vim</strong>, and have multiple panels open through one terminal if we need to with <strong class="source-inline">tmux</strong>. The other packages enable us to compile our Rust code. We must also note that we have appended each install with a <strong class="source-inline">-y</strong> tag. This is to tell the computer to bypass input prompts and put in default answers. This means that we can run this script in the background without any problems. We now must install the PostgreSQL drivers with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
sudo amazon-linux-extras install postgresql10 vim epel -y
sudo yum install -y postgresql-server postgresql-devel -y</pre>
			<p>We nearly have <a id="_idIndexMarker955"/>everything that we need. In the next section, we will be using Docker to build and package our applications. This can be done with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
sudo amazon-linux-extras install docker
sudo service docker start
sudo usermod -a -G docker ec2-user</pre>
			<p>Here, we can see that we install Docker, start the Docker service, and then register our user with the Docker service so that we do not have to use <strong class="source-inline">sudo</strong> with every Docker command. Seeing as we have installed Docker, we might as well install <strong class="source-inline">docker-compose</strong> for completeness. This can be done at the end of our script with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
sudo curl -L "https://github.com/docker/compose/releases
/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)"
-o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose</pre>
			<p>The <strong class="source-inline">curl</strong> command downloads <strong class="source-inline">docker-compose</strong>. We then make the <strong class="source-inline">docker-compose</strong> executable with the <strong class="source-inline">chmod</strong> command. The build script is almost finished. However, running all the commands that we have defined in the build script will take a while. There is a chance that we can SSH into our server before the build script has finished. Therefore, we should write a <strong class="source-inline">FINISHED</strong> string to a file to inform other processes that the server has all the packages that have been installed. We can write our flag with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
echo "FINISHED" &gt; home/ec2-user/output.txt</pre>
			<p>We have now built <a id="_idIndexMarker956"/>out the infrastructure that is going to build our applications for deployment. In the next section, we are going to build scripts that orchestrate the building of our application using the build server that we <span class="No-Break">have configured.</span></p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor207"/>Writing our Python application build script</h2>
			<p>We could <a id="_idIndexMarker957"/>technically try to fit our application-building script in the same scripts that we wrote in the previous section. However, it is desired to keep our scripts separate. For instance, if we were going to use a <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>) solution such as GitHub Actions, Travis <a id="_idIndexMarker958"/>CI, or CircleCI, we would not need the Terraform build process because we could build on their systems automatically. In this section, we will build a script that orchestrates the entire build process in Python because it is an easy scripting language that can handle JSON data that we get from the Terraform output. In our <strong class="source-inline">build/run_build.py</strong> script, we start by importing everything that we need with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
from subprocess import Popen
from pathlib import Path
import json
import time
DIRECTORY_PATH = Path(__file__).resolve().parent</pre>
			<p>We now have the absolute path for the <strong class="source-inline">build</strong> directory, we can run Bash commands through the <strong class="source-inline">Popen</strong> class, and we can load JSON. Now that we have everything imported, we can run our Terraform commands with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
init_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp; terraform init",
                     shell=True)
init_process.wait()
apply_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp; terraform apply",
                      shell=True)
apply_process.wait()</pre>
			<p>In each of the <a id="_idIndexMarker959"/>commands, we go to the directory path and then perform the Terraform command. We then wait for the command to finish before moving on to the next command. While Python is a slow, easy, and not strongly typed language, here it adds a lot of power. We can run multiple Bash commands at the same time and then wait for them later if needed. We can also pull data out of processes and manipulate this data and feed it into another command easily. The two Terraform commands that we carry out in the preceding snippet are <strong class="source-inline">init</strong> and <strong class="source-inline">apply</strong>. <strong class="source-inline">init</strong> sets up the Terraform state to record what is going on and downloads the modules that we need, which in this case is AWS. The <strong class="source-inline">apply</strong> command runs the Terraform build, which will build our <span class="No-Break">EC2 server.</span></p>
			<p>Once our EC2 instance is built, we can get the output of Terraform and write it to a JSON file, and then load the data from that JSON file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
produce_output = Popen(f"cd {DIRECTORY_PATH} &amp;&amp; terraform
                        output -json &gt; {DIRECTORY_PATH}/
                        output.json", shell=True)
produce_output.wait()
with open(f"{DIRECTORY_PATH}/output.json", "r") as file:
    data = json.loads(file.read())
server_ip = data["ec2_global_ips"]["value"][0][0]</pre>
			<p>Now that we have the server IP, we can SSH into this server and get it to do our builds. However, there could be some concurrency issues. There is a small-time window where the Terraform build finishes, but the server is not ready yet to accept connections. Therefore, we just need the script to wait for a small period before continuing with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
print("waiting for server to be built")
time.sleep(5)
print("attempting to enter server")</pre>
			<p>Once this is <a id="_idIndexMarker960"/>done, we need to pass our server IP into another Bash script that manages the build and then destroy the server afterward with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
build_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp;
                      sh ./run_build.sh {server_ip}",
                      shell=True)
build_process.wait()
destroy_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp;
                        terraform destroy", shell=True)
destroy_process.wait()</pre>
			<p>We have now built the orchestration of a build and the Terraform script that defines the infrastructure for the build. We now must build the final build script that will run the build commands on <span class="No-Break">the server.</span></p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor208"/>Writing our Bash deployment script</h2>
			<p>Our Bash deployment script must take in the IP address of the build server, SSH into the server, and run <a id="_idIndexMarker961"/>a series of commands on the server. It will also have to copy our code onto the server to be built. We can see from the preceding code that we can build our build Bash script in the <strong class="source-inline">/build/run_build.sh</strong> file. First, we start with the standard <span class="No-Break">boilerplate code:</span></p>
			<pre class="source-code">
#!/usr/bin/env bash
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH</pre>
			<p>With this boilerplate code, we have stated that this is a Bash script and that the rest of the Bash code will run in the <strong class="source-inline">build</strong> directory. We then upload the code from the Rust application with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
rm -rf ../web_app/target/
scp -i "~/.ssh/keys/remotebuild.pem" -r
../web_app ec2-user@$1:/home/ec2-user/web_app</pre>
			<p>Here, we can see that we remove the <strong class="source-inline">target</strong> directory. As we remember, the <strong class="source-inline">target</strong> directory is built when we build our Rust application; we do not need to upload build files <a id="_idIndexMarker962"/>from the local build. We then copy our Rust code using the <strong class="source-inline">scp</strong> command. We access the first argument passed into the script, which is <strong class="source-inline">$1</strong>. Remember that we pass in the IP address, so <strong class="source-inline">$1</strong> is the IP address. We then SSH into our server and run commands on this server with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  until [ -f ./output.txt ]
  do
      sleep 2
  done
  echo "File found"
  curl https://sh.rustup.rs -sSf | bash -s -- -y
  source ~/.cargo/env
  cd web_app
  cargo build --release
EOF</pre>
			<p>Here, we see that we loop sleeping for 2 seconds on each iteration until the <strong class="source-inline">output.txt</strong> file is present. Once the <strong class="source-inline">output.txt</strong> file is present, we know that the build script from the Terraform is complete and we can start our build. We signal this by echoing <strong class="source-inline">"File found"</strong> to the console. We then install Rust, load our <strong class="source-inline">cargo </strong>commands <a id="_idIndexMarker963"/>into our shell with the <strong class="source-inline">source</strong> command, move into the <strong class="source-inline">web_app</strong> directory, and build our <span class="No-Break">Rust application.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">We can get rid of the Python dependency if needed by using the <strong class="source-inline">jq</strong> command in our <strong class="source-inline">run_build.sh</strong> script with the following <span class="No-Break">code insertion:</span></p>
			<pre class="source-code">
. . .
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH
terraform init
terraform apply
terraform output -json &gt; ./output.json
IP_ADDRESS=$(jq --raw-output '.ec2_global_ips.value[0][0]' output.json)
echo $IP_ADDRESS
echo "waiting for server to be built"
sleep 5
echo "attempting to enter server"
rm -rf ../web_app/target/
scp -i "~/.ssh/keys/remotebuild.pem" -r
../web_app ec2-user@$IP_ADDRESS:/home/ec2-user/web_app
. . .</pre>
			<p class="callout">It must be noted that the references to the <strong class="source-inline">$1</strong> variable are replaced <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">$IP_ADDRESS</strong></span><span class="No-Break">.</span></p>
			<p class="callout">To run our pipeline without the Python dependency, we merely need to run the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
Sh run_build.sh</pre>
			<p class="callout">However, we will be relying on our Python script later on in <span class="No-Break">the chapter.</span></p>
			<p>Now that our <a id="_idIndexMarker964"/>build pipeline is built, we can run it with the <span class="No-Break">following command:</span></p>
			<pre class="console">
python3 run_build.py</pre>
			<p class="callout-heading">WARNING</p>
			<p class="callout">Terraform can sometimes be temperamental; if it fails, you may need to run it again. Sometimes, I’ve had to <a id="_idIndexMarker965"/>perform a Terraform run up to three times before it fully works. Every action is stored by Terraform, so do not worry—running the Python script again will not result in <span class="No-Break">duplicate servers.</span></p>
			<p>When running this command, you will be prompted to write <strong class="source-inline">yes</strong> at three different points of the process. The first time is to approve the building of the server with Terraform, the second time is to add the IP address of the server to the list of known hosts to approve the SSH connection, and the last time is to approve the destruction of the server. It usually makes sense to show the printouts in this book; however, the printout here is long and would probably take up multiple pages. Also, the printouts are obvious. Terraform openly states what is being built, the copying and building are also verbose, and the destruction of the server with Terraform is also printed out. It will be very clear what is going on when you run this build pipeline. You might also notice that there is a range of Terraform files that have been created. These files keep track of the state of our resources that have been built on the AWS platform. If you delete these state files, you have no way of knowing what is built, and duplicates will be spun up. It will also prevent Terraform from cleaning up. At the place where I work, at the time of writing this, we use Terraform to build massive data models for calculating the risk of financial loss over geographical locations. The data being processed can go over terabytes <a id="_idIndexMarker966"/>per chunk. We use Terraform to spin up a range of powerful computers, run data through it (which can take days), and then shut it down when it is finished. Multiple people need to monitor this process, so our Terraform state is housed in a <strong class="bold">Simple Storage Service</strong> (<strong class="bold">S3</strong>) bucket so that anyone on the <a id="_idIndexMarker967"/>project can shut it down if needed. An example of this configuration takes the following form in the <span class="No-Break"><strong class="source-inline">state.tf</strong></span><span class="No-Break"> file:</span></p>
			<pre class="source-code">
terraform {
    backend "s3" {
    bucket = "some_bucket"
    key    = "some/ptaht/terraform.tfstate"
    region = "eu-west-1"
  }
}</pre>
			<p>It must be noted that your account needs to have access to the <span class="No-Break">bucket defined.</span></p>
			<p>Now, it is time to build our frontend application. Looking and what we have just done, all we need to do is add to the <strong class="source-inline">/build/run_build.sh</strong> file the steps to upload our frontend code to the build server and build the frontend application. At this point, you should be able to code this yourself. Right now, it would be a good use of your time to stop reading and attempt to build it. If you have attempted it yourself, it should look like the code <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
rm -rf ../front_end/node_modules/
scp -i "~/.ssh/keys/remotebuild.pem" -r
../front_end ec2-user@$1:/home/ec2-user/front_end
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  curl -o- https://raw.githubusercontent.com/nvm-sh
  /nvm/v0.37.2/install.sh | bash
  . ~/.nvm/nvm.sh
  nvm install --lts
  cd front_end
  npm install
EOF</pre>
			<p>Here, we remove the node modules, copy the code to the server, install node, and then run the <strong class="source-inline">install</strong> command <a id="_idIndexMarker968"/>for our application. Now that our build pipeline is fully working, we can move on to wrapping our software in Docker so that we can package the software in Docker and <span class="No-Break">deploy it.</span></p>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor209"/>Managing our software with Docker</h1>
			<p>So far, we have been using Docker to manage our PostgreSQL and Redis databases. When it comes <a id="_idIndexMarker969"/>to running our frontend and Rust server, we have <a id="_idIndexMarker970"/>merely been running it directly on our local computer. However, when it comes to running our applications on remote servers, it is simpler and easier to distribute. Before we get on to deploying our Docker images on servers, we need to build and run them locally, which starts with writing our Docker <span class="No-Break">image file.</span></p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor210"/>Writing Docker image files</h2>
			<p>Before we proceed, it must be noted that the approach carried out here is the simplest, least optimized way <a id="_idIndexMarker971"/>to build a Rust server Docker image, because we are juggling a lot of new concepts. We cover an optimized way of building Rust server Docker images in <a href="B18722_13.xhtml#_idTextAnchor264"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, <em class="italic">Best Practices for a Clean Web App Repository</em>. When it comes to building a Docker image, we need a Dockerfile. This is where we define the steps needed to build our image. In our <strong class="source-inline">web_app/Dockerfile</strong> file, we basically borrow a base image and then run our commands on top of this image for our application to work. We can define the base image and the requirements to run our database interactions with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
FROM rust:1.61
RUN apt-get update -yqq &amp;&amp; apt-get install -yqq cmake g++
RUN cargo install diesel_cli --no-default-features --features postgres</pre>
			<p>Here in our Docker build, we are starting with the official <strong class="source-inline">rust</strong> image. We then update <strong class="source-inline">apt</strong> so that we can download all the available packages. We then install <strong class="source-inline">g++</strong> and the <strong class="source-inline">diesel</strong> client so that our database operations will work. We then copy the code and config files from our Rust application and define our work directory with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
COPY . .
WORKDIR .</pre>
			<p>Now, we have everything to build our Rust application with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
RUN cargo clean
RUN cargo build --release</pre>
			<p>Now our build is done, we move the static binary from the <strong class="source-inline">target</strong> directory into our home directory, remove excessive code such as the <strong class="source-inline">target</strong> and <strong class="source-inline">src</strong> directories, and allow the static binary file to be executable with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
RUN cp ./target/release/web_app ./web_app
RUN rm -rf ./target
RUN rm -rf ./src
RUN chmod +x ./web_app</pre>
			<p>Now everything is done, we can expose the port at which the web server will be running to make the server <a id="_idIndexMarker972"/>exposed by the container and execute the command that gets run when the Docker image is spun up with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
EXPOSE 8000
CMD ["./web_app", "config.yml"]</pre>
			<p>Now that our Docker image file is written, we can move on to building <span class="No-Break">Docker images.</span></p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor211"/>Building Docker images</h2>
			<p>Before we can <a id="_idIndexMarker973"/>run this, however, we need to remove our build script. There is only one thing left our Docker image build needs. When copying over the code into the Docker image, we know that the <strong class="source-inline">target</strong> directory has a lot of code and files that we do not need in our image. We can avoid copying over the <strong class="source-inline">target</strong> directory by having the following code in the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">dockerignore</strong></span><span class="No-Break"> file:</span></p>
			<pre class="source-code">
target</pre>
			<p>If we try to compile our application with the build script, Docker will just throw itself into an infinite file loop and then time out. This means that our <strong class="source-inline">ALLOWED_VERSION</strong> variable in our <strong class="source-inline">main.rs</strong> file takes the <span class="No-Break">following form:</span></p>
			<pre class="source-code">
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    const ALLOWED_VERSION: &amp;'static str = "v1";
    . . .</pre>
			<p>And we then must comment out our <strong class="source-inline">build</strong> dependency in our <strong class="source-inline">Cargo.toml</strong> file with the following code and remove the <strong class="source-inline">build.rs</strong> <span class="No-Break">file entirely:</span></p>
			<pre class="source-code">
[package]
name = "web_app"
version = "0.1.0"
edition = "2021"
# build = "build.rs"</pre>
			<p>We are now ready to build our image; we navigate to where the Dockerfile is and run the <span class="No-Break">following command:</span></p>
			<pre class="console">
docker build . -t rust_app</pre>
			<p>This command executes the build defined by the Dockerfile in the current directory. The image is tagged <strong class="source-inline">rust_app</strong>. We can list our images with the <span class="No-Break">following command:</span></p>
			<pre class="console">
docker image ls</pre>
			<p>This will give us the <span class="No-Break">following printout:</span></p>
			<pre class="console">
REPOSITORY   TAG       IMAGE ID       CREATED        SIZE
rust_app     latest    c2e465545e30   2 hours ago    2.69GB
. . .</pre>
			<p>We can then test <a id="_idIndexMarker974"/>to see if our application is properly built; we just run the <span class="No-Break">following command:</span></p>
			<pre class="console">
docker run rust_app</pre>
			<p>This directly runs our Rust app. Our application should crash instantly with the <span class="No-Break">following error:</span></p>
			<pre class="console">
thread 'main' panicked at 'called `Result::unwrap()`
on an `Err` value: Connection refused (os error 111)',
src/counter.rs:24:47 note: run with `RUST_BACKTRACE=1`
environment variable to display a backtrace</pre>
			<p>We can see that the error is not down to our build, but a connection issue with Redis from our counter file. This is reassuring, and it will work when we run our Rust application with our <span class="No-Break">two databases.</span></p>
			<p>There is an approach in Docker where you can do multi-layered builds. This is where we start off with the <strong class="source-inline">rust</strong> base image, build our application, and then move our build into another Docker image with no dependencies. The result is that our server image is usually merely 100 MB as opposed to multiple GB. However, our application has a lot of dependencies, and this <a id="_idIndexMarker975"/>multi-layered build approach will result in multiple driver errors. We explore building tiny images in <a href="B18722_13.xhtml#_idTextAnchor264"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, <em class="italic">Best Practices for a Clean Web </em><span class="No-Break"><em class="italic">App Repository</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor212"/>Building an EC2 build server using Terraform</h2>
			<p>We have now built <a id="_idIndexMarker976"/>our Rust Docker image locally. We <a id="_idIndexMarker977"/>can now build it on our build server. Before we do this, we are going to have to increase the size of the hard drive on our builder server; otherwise, the image will refuse to build due to lack of space. This can be done in our <strong class="source-inline">/build/main.tf</strong> file by adding a root block device, as seen in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
resource "aws_instance" "build_server" {
    ami = "ami-0fdbd8587b1cf431e"
    instance_type = "t2.medium"
    key_name = "remotebuild"
    user_data = file("server_build.sh")
    tags = {
      Name = "to-do build server"
    }
    # root disk
    root_block_device {
      volume_size           = "150"
      volume_type           = "gp2"
      delete_on_termination = true
    }
}</pre>
			<p><strong class="source-inline">gp2</strong> is the version of SSD that AWS supports we are using. <strong class="source-inline">150</strong> is the number of GB that we are connecting <a id="_idIndexMarker978"/>to the server. This will be enough <a id="_idIndexMarker979"/>memory to build our Docker images, leaving us only to build the pipeline that constructs our <span class="No-Break">Docker images.</span></p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor213"/>Orchestrating builds with Bash</h2>
			<p>At this point, we are <a id="_idIndexMarker980"/>also going to optimize our build Bash script <a id="_idIndexMarker981"/>in our <strong class="source-inline">/build/run_build.sh</strong> file. First, we do not remove the <strong class="source-inline">target</strong> directory; instead, we are selective with what we upload onto our server with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
#!/usr/bin/env bash
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 "mkdir web_app"
scp -i "~/.ssh/keys/remotebuild.pem" -r
../web_app/src ec2-user@$1:/home/ec2-user/web_app/src
scp -i "~/.ssh/keys/remotebuild.pem"
../web_app/Cargo.toml ec2-user@$1:/home/ec2-user/web_app/Cargo.toml
scp -i "~/.ssh/keys/remotebuild.pem"
../web_app/config.yml ec2-user@$1:/home/ec2-user/web_app/config.yml
scp -i "~/.ssh/keys/remotebuild.pem"
../web_app/Dockerfile ec2-user@$1:/home/ec2-user/web_app/Dockerfile</pre>
			<p>Here, we can see that we make the <strong class="source-inline">web_app</strong> directory, and then upload the files and directories <a id="_idIndexMarker982"/>that we need to build our Rust Docker image. We then <a id="_idIndexMarker983"/>need to connect to the server to install Rust with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
echo "installing Rust"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  curl https://sh.rustup.rs -sSf | bash -s -- -y
  until [ -f ./output.txt ]
  do
      sleep 2
  done
  echo "File found"
EOF
echo "Rust has been installed"</pre>
			<p>Here, we can see that we install Rust before we block the script until the server is ready with everything installed. This means that we are running the installation of Rust at the same time the rest of the server is being built, saving time. We then exit the connection to our build server. Finally, we connect to the server again and build our Docker image with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
echo "building Rust Docker image"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  cd web_app
  docker build . -t rust_app
EOF
echo "Docker image built"</pre>
			<p>We have then built the pipeline that builds our Rust Docker image on our build server. This seems like a lot of steps, and you would be right. We can build our image locally with a different <a id="_idIndexMarker984"/>target operating system and chip architecture. Exploring it <a id="_idIndexMarker985"/>here would disjoint the flow of what we are trying to achieve, but further information on compiling with different targets will be provided in the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section.</span></p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor214"/>Writing a Docker image file for the React frontend</h2>
			<p>For now, we are <a id="_idIndexMarker986"/>going to package our frontend <a id="_idIndexMarker987"/>application in Docker. In our <strong class="source-inline">front_end/Dockerfile</strong> file, we inherit the node base image, copy the code, and define the working directory with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
FROM node:17.0.0
WORKDIR .
COPY . ./</pre>
			<p>We then install a <strong class="source-inline">serve</strong> package to serve the web app build files, the modules needed to build the application, and build the React application with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
RUN npm install -g serve
RUN npm install
RUN npm run react-build</pre>
			<p>We then expose the port and server to our application with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
EXPOSE 4000
CMD ["serve", "-s", "build", "-l", "4000"]</pre>
			<p>We can then build our Docker image with the <span class="No-Break">following command:</span></p>
			<pre class="console">
docker build . -t front_end</pre>
			<p>We can then run the recently built image with the <span class="No-Break">following command:</span></p>
			<pre class="console">
docker run -p 80:4000 front_end</pre>
			<p>This routes the container’s external port (<strong class="source-inline">80</strong>) to the locally exposed port <strong class="source-inline">4000</strong>. When our image is running, we get the <span class="No-Break">following printout:</span></p>
			<pre class="console">
INFO: Accepting connections at http://localhost:4000</pre>
			<p>This shows <a id="_idIndexMarker988"/>that our image is running in <a id="_idIndexMarker989"/>a container. We will be able to access our frontend container by merely accessing our localhost, which is port <strong class="source-inline">80</strong>, as seen in the <span class="No-Break">following screenshot:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/Figure_10.12_B18722.jpg" alt="Figure 10.12 – Accessing our frontend container using localhost"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12 – Accessing our frontend container using localhost</p>
			<p>We will not be able to do anything with it, however, because our Rust server is not running. We can now lift the steps that we have carried out to build our frontend into our <strong class="source-inline">/build/run_build.sh</strong> script to have our build pipeline construct our frontend image as well. This is a good opportunity for you to try to add the step yourself. We will have to install node and then carry out the frontend build steps on the <span class="No-Break">build server.</span></p>
			<p>If you have had an attempt at incorporating our React build in our pipeline, it should look like the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
echo "copying React app"
rm -rf ../front_end/node_modules/
scp -i "~/.ssh/keys/remotebuild.pem" -r
../front_end ec2-user@$1:/home/ec2-user/front_end
echo "React app copied"
echo "installing node on build server"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm
  /v0.37.2/install.sh | bash
  . ~/.nvm/nvm.sh
  nvm install --lts
EOF
echo "node installed"
echo "building front-end on server"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  cd front_end
  docker build . -t front_end
EOF
echo "front-end Docker image has been built"</pre>
			<p>We can be <a id="_idIndexMarker990"/>more optimal in our <a id="_idIndexMarker991"/>implementation; however, the preceding code is the simplest application. First, we copy over the code we need to the build server. We then connect to our build server to install node. After installing node, we connect to the server again to move into the React application directory and build our <span class="No-Break">Docker image.</span></p>
			<p>Our build pipeline is now working. Just think of what we have achieved here—we have built a pipeline that constructs a build server; we then copied our code onto the build server, constructed Docker images, and then destroyed the server after the build was done. Even <a id="_idIndexMarker992"/>though this pipeline is not <a id="_idIndexMarker993"/>perfect, we have explored some powerful tools that will enable you to automate tasks and lift a lot of the code that we have covered in this subsection in other CI pipeline tools. However, right now, we are just building the Docker images and then destroying them with the server. In the next section, we will deploy our images on <span class="No-Break"><strong class="bold">Docker Hub</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor215"/>Deploying images onto Docker Hub</h2>
			<p>Before we <a id="_idIndexMarker994"/>can pull our images from Docker Hub, we will have <a id="_idIndexMarker995"/>to push our images to Docker Hub. Before we can push our images to Docker Hub, we will have to create a Docker Hub repo. Registering our image on Docker Hub is straightforward. After logging in, we click on the <strong class="bold">Create Repository</strong> button in the top-right corner, as shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/Figure_10.13_B18722.jpg" alt="Figure 10.13 – Creating a new repository on Docker Hub"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.13 – Creating a new repository on Docker Hub</p>
			<p>Once we have clicked on this, we define the repository with the <span class="No-Break">following configuration:</span></p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/Figure_10.14_B18722.jpg" alt="Figure 10.14 – Defining a new Docker repository"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.14 – Defining a new Docker repository</p>
			<p>We can see that there is an option for connecting our repository with GitHub by clicking on the <em class="italic">GitHub</em> button seen in the preceding screenshot. The <strong class="bold">Connected</strong> GitHub status in the preceding screenshot simply means that my GitHub is connected to my Docker Hub account. This means that every time a successful pull request gets completed, the image is rebuilt <a id="_idIndexMarker996"/>with the code, and then it is sent to the repository. This <a id="_idIndexMarker997"/>can be helpful if you are building a fully automated pipeline. However, for this book, we will not connect our GitHub repository. We will push it onto our build server. You will also need to create a Docker Hub repository for our frontend if you are deploying the <span class="No-Break">React application.</span></p>
			<p>Now that we have defined our Docker Hub repositories, we need to add a Docker login in our build pipeline. This means that we must pass our Docker Hub password and username into our Python build script. Our Python script can then pass the Docker Hub credentials into the build Bash script. This build Bash script will then log in to Docker on the build server so that we can push our images to our Docker Hub. In our <strong class="source-inline">/build/run_build.py</strong> file, we define the arguments passed into the Python script with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
. . .
import argparse
. . .
parser = argparse.ArgumentParser(
                    description='Run the build'
                  )
parser.add_argument('--u', action='store',
                    help='docker username',
                    type=str, required=True)
parser.add_argument('--p', action='store',
                    help='docker password',
                    type=str, required=True)
args = parser.parse_args()</pre>
			<p>We can see that we have set <strong class="source-inline">required</strong> to <strong class="source-inline">True</strong>, which means that the Python script will not run unless both parameters are supplied. If we supply a <strong class="source-inline">-h</strong> parameter in the Python script, the <a id="_idIndexMarker998"/>parameters that we have defined in the preceding <a id="_idIndexMarker999"/>code will be printed out with help information. Now that we have ingested the Docker credentials, we can then pass them into our build Bash script in the <strong class="source-inline">/build/run_build.py</strong> file with the following adaptation to <span class="No-Break">our code:</span></p>
			<pre class="source-code">
. . .
print("attempting to enter server")
build_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp; sh ./run_build.sh
    {server_ip} {args.u} {args.p}", shell=True)
build_process.wait()
destroy_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp; terraform destroy",
    shell=True)
. . .</pre>
			<p>Here, we can see that the Docker username is accessed using the <strong class="source-inline">args.u</strong> attribute and the Docker <a id="_idIndexMarker1000"/>password is accessed using the <strong class="source-inline">args.p</strong> attribute. Now <a id="_idIndexMarker1001"/>that we are passing Docker credentials into our build Bash script, we need to use these credentials to push our images. In our <strong class="source-inline">/build/run_build.sh</strong> file, we should log in after Docker is installed on our build server with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
. . .
echo "Rust has been installed"
echo "logging in to Docker"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  echo $3 | docker login --username $2 --password-stdin
EOF
echo "logged in to Docker"
echo "building Rust Docker image"
. . .</pre>
			<p>In the preceding code, we use <strong class="source-inline">–password-stdin</strong> to pipe our password into the Docker login. <strong class="source-inline">stdin</strong> ensures that the password is not stored in the logs, making it a little bit more secure. We can then build, tag, and then push our Rust application with the update in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
echo "building Rust Docker image"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  cd web_app
  docker build . -t rust_app
  docker tag rust_app:latest maxwellflitton/to_do_actix:latest
  docker push maxwellflitton/to_do_actix:latest
EOF
echo "Docker image built"</pre>
			<p>Here, we build the Rust image as we always do. We then tag the Rust app image as the latest release and then push it to the Docker repository. We also must push our frontend <a id="_idIndexMarker1002"/>application to Docker Hub. At this point, this is a good <a id="_idIndexMarker1003"/>chance for you to write the code that pushes the frontend image. If you did attempt to push the frontend image, it should look like the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
echo "building front-end on server"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  cd front_end
  docker build . -t front_end
  docker tag front_end:latest maxwellflitton/to_do_react:latest
  docker push maxwellflitton/to_do_react:latest
EOF
echo "front-end Docker image has been built"</pre>
			<p>We have now coded all that we need to build both of our images and push them to our Docker repositories. We can run our Python build script with the <span class="No-Break">following command:</span></p>
			<pre class="console">
python3 run_build.py --u some_username --p some_password</pre>
			<p>Again, the printout is lengthy, but we can check our Docker Hub repository to see if our image was pushed. As we can see in the following screenshot, the Docker Hub repository states when the image has <span class="No-Break">been pushed:</span></p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/Figure_10.15_B18722.jpg" alt="Figure 10.15 – View of pushed Docker repository"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.15 – View of pushed Docker repository</p>
			<p>We now have our images pushed onto our Docker Hub repositories! This means that we can pull them <a id="_idIndexMarker1004"/>onto any computer that we need, just as we did when <a id="_idIndexMarker1005"/>pulling the PostgreSQL image in the <strong class="source-inline">docker-compose</strong> file. We should now pull our images onto a server so that other people can access and use our application. In the next section, we will deploy our application for <span class="No-Break">external use.</span></p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor216"/>Deploying our application on AWS</h1>
			<p>Even though <a id="_idIndexMarker1006"/>we have packaged our Rust application in <a id="_idIndexMarker1007"/>Docker, we have not run our Rust application in a Docker container. Before we run our Rust application on a server on AWS, we should run our Rust application locally. This will help us understand how a simple deployment works without having to <span class="No-Break">build servers.</span></p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor217"/>Running our application locally</h2>
			<p>When it comes <a id="_idIndexMarker1008"/>to running our application locally, we will be using <strong class="source-inline">docker-compose</strong> with the <span class="No-Break">following layout:</span></p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/Figure_10.16_B18722.jpg" alt="Figure 10.16 – Structure for local deployment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.16 – Structure for local deployment</p>
			<p>Here, we can see that the NGINX container takes in traffic from outside of the <strong class="source-inline">docker-compose</strong> network and directs the traffic to the appropriate container. Now that we understand our structure, we can define our <strong class="source-inline">docker-compose</strong> file. First, we need to make a directory called <strong class="source-inline">deployment</strong> next to our <strong class="source-inline">build</strong>, <strong class="source-inline">front_end</strong>, and <strong class="source-inline">web_app</strong> directories. Our general layout for our <strong class="source-inline">docker-compose.yml</strong> file in our <strong class="source-inline">deployment</strong> directory takes the <span class="No-Break">following form:</span></p>
			<pre class="source-code">
services:
  nginx:
    . . .
  postgres_production:
    . . .
  redis_production:
      . . .
  rust_app:
    . . .
  front_end:
    . . .</pre>
			<p>We can start <a id="_idIndexMarker1009"/>with our NGINX service. Now that we know the outside port is <strong class="source-inline">80</strong>, it makes sense that our NGINX container listens to the outside port of <strong class="source-inline">80</strong> with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
  nginx:
    container_name: 'nginx-rust'
    image: "nginx:latest"
    ports:
      - "80:80"
    links:
      - rust_app
      - front_end
    volumes:
      - ./nginx_config.conf:/etc/nginx/nginx.conf</pre>
			<p>We can see that we get the latest NGINX image. We also have links to the <strong class="source-inline">front_end</strong> and <strong class="source-inline">rust_app</strong> containers because we will be passing HTTP requests to these containers. It also must be noted that we have a volume. This is where we share a volume outside of the container with a directory inside the container. So, this volume definition means that our <strong class="source-inline">deploy/nginx_config.conf</strong> file can be accessed in the NGINX container in the <strong class="source-inline">etc/nginx/nginx.conf</strong> directory. With this volume, we can configure NGINX routing rules in our <span class="No-Break"><strong class="source-inline">deploy/nginx_config.conf</strong></span><span class="No-Break"> file.</span></p>
			<p>First, we set the number of worker processes to <strong class="source-inline">auto</strong>. We can manually define the number of worker processes if needed. <strong class="source-inline">auto</strong> detects the number of CPU cores available and sets the number to that with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
worker_processes  auto;
error_log  /var/log/nginx/error.log warn;</pre>
			<p>The <strong class="source-inline">error_log</strong> directive defines the logging to a particular file. We not only define the file, but we also state the minimum level needed to write to the log file, which is <strong class="source-inline">warning</strong>. By default, the <a id="_idIndexMarker1010"/>logging level needed to write to the file is <strong class="source-inline">error</strong>. We can now move on to defining contexts in our <strong class="source-inline">deploy/nginx_config.conf</strong> file. In the <strong class="source-inline">events</strong> context, we define the maximum number of connections that a worker can entertain at a time. This is achieved with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
events {
    worker_connections  512;
}</pre>
			<p>The number of workers that we have defined is the default number that NGINX sets. Now that this is done, we can move on to our <strong class="source-inline">http</strong> context. Here, we define the <strong class="source-inline">server</strong> context. Inside this, we instruct the server to listen to port <strong class="source-inline">80</strong>, which is the port that listens to outside traffic, with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
http {
    server {
        listen 80;
        location /v1 {
            proxy_pass http://rust_app:8000/v1;
        }
        location / {
            proxy_pass http://front_end:4000/;
        }
    }
}</pre>
			<p>Here, we can see that if the URL has <strong class="source-inline">/v1/</strong> at the start of the endpoint, we then pass it through the Rust server. It must be noted that we pass <strong class="source-inline">v1</strong> forward to the Rust server. If we did not pass <strong class="source-inline">v1</strong> forward to the Rust server, <strong class="source-inline">v1</strong> will be missing from the URL when it hits the Rust server. If the URL does not contain <strong class="source-inline">v1</strong> in the URL, then we forward it to our <strong class="source-inline">front_end</strong> container. Our NGINX container is now ready to manage traffic in our <strong class="source-inline">docker-compose</strong> system. Before we move on to our frontend and backend services, we need to define the <a id="_idIndexMarker1011"/>Redis and PostgreSQL databases. There is nothing new here, so at this point, you can try to define them yourself. If you have, then your code should look <span class="No-Break">like this:</span></p>
			<pre class="source-code">
  postgres_production:
    container_name: 'to-do-postgres-production'
    image: 'postgres:latest'
    restart: always
    ports:
      - '5433:5432'
    environment:
      - 'POSTGRES_USER=username'
      - 'POSTGRES_DB=to_do'
      - 'POSTGRES_PASSWORD=password'
    expose:
      - 5433
  redis_production:
      container_name: 'to-do-redis'
      image: 'redis:5.0.5'
      ports:
        - '6379:6379'
      volumes:
    	  - ./data/redis:/tmp</pre>
			<p>The preceding database definitions are the same as what we have used before in local development. With these databases, we can define our Rust application with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
  rust_app:
    container_name: rust_app
    build: "../web_app"
    restart: always
    ports:
      - "8000:8000"
    links:
      - postgres_production
      - redis_production
    expose:
      - 8000
    volumes:
      - ./rust_config.yml:/config.yml</pre>
			<p>Here, we can see that we are not defining an image. Instead of declaring the image, we point to a build. The <strong class="source-inline">build</strong> tag is where we point to a <strong class="bold">Dockerfile</strong> that is going to be built for the Rust application server. We do this because our laptop might have a different chip <a id="_idIndexMarker1012"/>architecture from the build server. If we directly pull our image from Docker Hub, it might not run. Also, we are developing our application and running it locally. If we make changes in the Rust application, it will be directly run in our local system without us having to push and pull from Docker Hub every time we make a change to the Rust code and want to run it in a local system. We also define the links. This is because calling localhost inside a container will be looking for services inside the container. Instead, we must reference the link when calling the service. We can also see that we have our own config file in the volumes that we update. With the defined links, our <strong class="source-inline">deploy/rust_config.yml</strong> file takes the <span class="No-Break">following form:</span></p>
			<pre class="source-code">
DB_URL: postgres://username:password@postgres_production/to_do
SECRET_KEY: secret
EXPIRE_MINUTES: 120
REDIS_URL: redis://redis_production/</pre>
			<p>Here, we can see that we reference the name of the service defined in the <strong class="source-inline">docker-compose</strong> system instead of the URL. We also must change the address for our Rust application in our <strong class="source-inline">web_app/src/main.rs</strong> file to zeros, as <span class="No-Break">seen here:</span></p>
			<pre class="source-code">
})
.bind("0.0.0.0:8000")?
.run()</pre>
			<p>We then must remove our config file in our Docker build in our <strong class="source-inline">web_app/Dockerfile</strong> file with the following line <span class="No-Break">of code:</span></p>
			<pre class="source-code">
RUN rm config.yml</pre>
			<p>If we do not do <a id="_idIndexMarker1013"/>this, then our Rust application will not connect with the NGINX container. Now everything is defined for our Rust server, we can move on to defining the frontend application in our <strong class="source-inline">docker-compose.yml</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
  front_end:
    container_name: front_end
    image: "maxwellflitton/to_do_react:latest"
    restart: always
    ports:
      - "4000:4000"
    expose:
      - 4000</pre>
			<p>Here, we see that we reference our image in Docker Hub and expose the ports. Now that our local system is defined, we can run our system and interact with it by running the <span class="No-Break">following command:</span></p>
			<pre class="console">
docker-compose up</pre>
			<p>Everything <a id="_idIndexMarker1014"/>will build and run automatically. Before we can interact with our system, we need to run our <strong class="source-inline">diesel</strong> migrations in our Rust application build with the <span class="No-Break">following command:</span></p>
			<pre class="console">
diesel migration run</pre>
			<p>We then need to create a user with the following <span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break"> command:</span></p>
			<pre class="console">
curl --location --request POST 'http://localhost/v1/user/create' \
--header 'Content-Type: application/json' \
--data-raw '{
    "name": "maxwell",
    "email": "maxwellflitton@gmail.com",
    "password": "test"
}'</pre>
			<p>We now have everything in place to interact with our application. We can see that localhost with no reference to ports works with the <span class="No-Break">following screenshot:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/Figure_10.17_B18722.jpg" alt="Figure 10.17 – Accessing our docker-compose system through the browser"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.17 – Accessing our docker-compose system through the browser</p>
			<p>If your NGINX is <a id="_idIndexMarker1015"/>working, you should be able to log in and interact with the to-do application as before. We are now able to deploy our system on AWS so that other people can access and use our to-do application in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor218"/>Running our application on AWS</h2>
			<p>We can <a id="_idIndexMarker1016"/>deploy our application on AWS by carrying <a id="_idIndexMarker1017"/>out the <span class="No-Break">following steps:</span></p>
			<ol>
				<li value="1">Building <span class="No-Break">a server</span></li>
				<li>Running our <strong class="source-inline">docker-compose</strong> system on <span class="No-Break">that server</span></li>
				<li>Running database migrations on <span class="No-Break">that server</span></li>
				<li>Creating <span class="No-Break">a user</span></li>
			</ol>
			<p>Once we have carried out those steps, we will be able to access the application on the remote server. However, before we do this, we are going to have to alter the React application. Right now, our React application makes API calls to the localhost via <strong class="source-inline">127.0.0.1</strong>. When we are using a remote server, this will not work as we will have to make calls to the server to which we have deployed our application. To do this, we can extract where our API calls are made in the React application and update the root of the URL for the API call with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
axios.get(window.location.href + "/v1/item/get",</pre>
			<p>What is happening here is that <strong class="source-inline">window.location.href</strong> returns the current location, which will be the IP of the server our application is deployed on, or localhost if we are developing it locally on our computer. The following files have API calls that need to <span class="No-Break">be updated:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">src/components/LoginForm.js</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">src/components/CreateToDoitem.js</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">src/components/ToDoitem.js</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">src/App.js</strong></span></li>
			</ul>
			<p>Once we have updated these files, we will be able to run another build in the <strong class="source-inline">build</strong> directory by running the <span class="No-Break">following command:</span></p>
			<pre class="console">
python3 run_build.py --u some_username --p some_password</pre>
			<p>Once our build has <a id="_idIndexMarker1018"/>been done, both our images will be <a id="_idIndexMarker1019"/>updated. We can now move to our <strong class="source-inline">deployment</strong> directory and flesh it out with the <span class="No-Break">following files:</span></p>
			<ul>
				<li><strong class="source-inline">main.tf</strong>: This should be the same as the <strong class="source-inline">main.tf</strong> file in the <strong class="source-inline">build</strong> directory, except for the server having a <span class="No-Break">different tag</span></li>
				<li><strong class="source-inline">run_build.py</strong>: This should be the same as the <strong class="source-inline">run_build.py</strong> file in the <strong class="source-inline">build</strong> directory, except for the <em class="italic">destroy server</em> process at the end of the <span class="No-Break"><strong class="source-inline">run_build.py</strong></span><span class="No-Break"> script</span></li>
				<li><strong class="source-inline">server_build.sh</strong>: This should be the same as the <strong class="source-inline">server_build.sh</strong> script in the <strong class="source-inline">build</strong> directory as we want our server to have the same environment as when our images <span class="No-Break">were built</span></li>
				<li><strong class="source-inline">deployment-compose.yml</strong>: This should be the same as the <strong class="source-inline">docker-compose.yml</strong> file in the <strong class="source-inline">deployment</strong> directory, except that <strong class="source-inline">rust_app</strong> service has an image tag instead of a build tag and the image tag should have the image <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">maxwellflitton/to_do_actix:latest</strong></span></li>
				<li><strong class="source-inline">.env</strong>: This should be the same as the <strong class="source-inline">.env</strong> file in the <strong class="source-inline">web_app</strong> directory, and we will need it to perform <span class="No-Break">database migrations</span></li>
			</ul>
			<p>We are now <a id="_idIndexMarker1020"/>ready to code our <strong class="source-inline">run_build.sh</strong> file that <a id="_idIndexMarker1021"/>will enable us to deploy our application, run migrations, and create a user. First, we start off with some standard boilerplate code to ensure that we are in the right directory, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
#!/usr/bin/env bash
SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"
cd $SCRIPTPATH</pre>
			<p>We then copy the files needed to spin up our <strong class="source-inline">docker-compose</strong> system and perform database migrations with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
scp -i "~/.ssh/keys/remotebuild.pem"
./deployment-compose.yml ec2-user@$1:/home/ec2-user/docker-compose.yml
scp -i "~/.ssh/keys/remotebuild.pem"
./rust_config.yml ec2-user@$1:/home/ec2-user/rust_config.yml
scp -i "~/.ssh/keys/remotebuild.pem"
./.env ec2-user@$1:/home/ec2-user/.env
scp -i "~/.ssh/keys/remotebuild.pem"
./nginx_config.conf ec2-user@$1:/home/ec2-user/nginx_config.conf
scp -i "~/.ssh/keys/remotebuild.pem" -r
../web_app/migrations ec2-user@$1:/home/ec2-user/migrations</pre>
			<p>None of this should be a surprise as we needed all the preceding files to run our <strong class="source-inline">docker-compose</strong> system. We then install Rust and wait for the server build to be done with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
echo "installing Rust"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  curl https://sh.rustup.rs -sSf | bash -s -- -y
  until [ -f ./output.txt ]
  do
      sleep 2
  done
  echo "File found"
EOF
echo "Rust has been installed"</pre>
			<p>Again, this is <a id="_idIndexMarker1022"/>nothing new that we have not seen. We then <a id="_idIndexMarker1023"/>install the <strong class="source-inline">diesel</strong> client with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
echo "installing diesel"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  cargo install diesel_cli --no-default-features --features postgres
EOF
echo "diesel has been installed"</pre>
			<p>We then log in to Docker, spin up our <strong class="source-inline">docker-compose</strong> system, run our migrations, and then make a user with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
echo "building system"
ssh -i "~/.ssh/keys/remotebuild.pem" -t ec2-user@$1 &lt;&lt; EOF
  echo $3 | docker login --username $2 --password-stdin
  docker-compose up -d
  sleep 2
  diesel migration run
  curl --location --request POST 'http://localhost/v1/user/create' \
  --header 'Content-Type: application/json' \
  --data-raw '{
      "name": "maxwell",
      "email": "maxwellflitton@gmail.com",
      "password": "test"
  }'
EOF
echo "system has been built"</pre>
			<p>With this, our <a id="_idIndexMarker1024"/>system is deployed! We will be able to access <a id="_idIndexMarker1025"/>our application on our server by putting the IP of the server that is in the <strong class="source-inline">output.json</strong> file into the browser. We will be able to log in and use our to-do application just like when we were running our system on our local computer, as seen on in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/Figure_10.18_B18722.jpg" alt="Figure 10.18 – Our application on our deployment server"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.18 – Our application on our deployment server</p>
			<p>As we can <a id="_idIndexMarker1026"/>see, the connection is not secure and our browser <a id="_idIndexMarker1027"/>is giving us a warning because we are not implementing the HTTPS protocol. This is because our connection is not encrypted. We will cover how to encrypt our connection in the <span class="No-Break">next chapter.</span></p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor219"/>Writing our application build script</h2>
			<p>Right now, our application <a id="_idIndexMarker1028"/>is running a database locally on the EC2 instance. This has a few problems. Firstly, it means that the EC2 is stateful. If we tear down the instance, we will lose all <span class="No-Break">our data.</span></p>
			<p>Secondly, if we wipe the containers on the instance, we could also lose all our data. Data vulnerability is not the only issue here. Let’s say that our traffic drastically increases, and we need more computing instances to manage it. This can be done by using NGINX as a load balancer between two instances, as shown in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/Figure_10.19_B18722.jpg" alt="Figure 10.19 – Doubling our EC2 instances for our system"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.19 – Doubling our EC2 instances for our system</p>
			<p>As you can see, the problem here is accessing random data. If user one creates an item, and this request hits the instance on the left, it is stored in the database on the left. However, user <a id="_idIndexMarker1029"/>one can then make a <strong class="source-inline">GET</strong> request, which hits the instance on the right side. The second request will not be able to access the item that was created in the first request. The user would be accessing random states depending on which instance the <span class="No-Break">request hit.</span></p>
			<p>This can be solved by deleting the database from our <strong class="source-inline">docker-compose</strong> file and creating a database outside it, as shown in <span class="No-Break">this diagram:</span></p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/Figure_10.20_B18722.jpg" alt="Figure 10.20 – Our new improved system"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.20 – Our new improved system</p>
			<p>Now, we have a single point of truth for our data, and our EC2 instances are stateless, meaning we have the freedom to create and delete instances as and when we <span class="No-Break">need to.</span></p>
			<p>When it comes to adding an AWS database to our deployment, we are going to have to build our <a id="_idIndexMarker1030"/>database in Terraform, and then pass the information about the constructed database to our <strong class="source-inline">deployment/.env</strong> file for database migrations and our <strong class="source-inline">deployment/rust_config.yml</strong> file for our Rust server to access. First, we must add the database definition to our <strong class="source-inline">deployment/main.tf</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
resource "aws_db_instance" "main_db" {
  instance_class         = "db.t3.micro"
  allocated_storage      = 5
  engine                 = "postgres"
  username               = var.db_username
  password               = var.db_password
  db_name                = "to_do"
  publicly_accessible    = true
  skip_final_snapshot    = true
  tags = {
      Name = "to-do production database"
    }
}</pre>
			<p>The fields defined in the preceding code are straightforward apart from the <strong class="source-inline">allocated_storage</strong> field, which is the number of GB allocated to the database. We can also see that <a id="_idIndexMarker1031"/>we use variables with the <strong class="source-inline">var</strong> variable. This means that we must pass a password and username into our Terraform build when running. We need to define our input variables in the <strong class="source-inline">deployment/variables.tf</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
variable "db_password" {
    description = "The password for the database"
    default = "password"
}
variable "db_username" {
    description = "The username for the database"
    default = "username"
}</pre>
			<p>These variables have defaults, so we do not need to pass in a variable. However, if we want to pass in a variable, we can do this with the <span class="No-Break">following layout:</span></p>
			<pre class="console">
-var"db_password=some_password" -var"db_username=some_username"</pre>
			<p>We now must pass these parameters into the needed files and the Terraform build. This is where Python starts to shine. We will be reading and writing YAML files, so we will have to install the YAML Python package with the <span class="No-Break">following command:</span></p>
			<pre class="console">
pip install pyyaml</pre>
			<p>We then import this package in our <strong class="source-inline">deployment/run_build.py</strong> file at the top of the script with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
import yaml</pre>
			<p>We then load database parameters from a JSON file called <strong class="source-inline">database.json</strong> and create our <strong class="source-inline">vars</strong> command string with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
with open("./database.json") as json_file:
    db_data = json.load(json_file)
params = f' -var="db_password={db_data["password"]}"
            -var="db_username={db_data["user"]}"'</pre>
			<p>You can come up <a id="_idIndexMarker1032"/>with any parameters you want for your <strong class="source-inline">deployment/database.json</strong> file; I have recently been playing with GitHub Copilot, which is an AI pair programmer that auto-fills code, and this gave me the <span class="No-Break">following parameters:</span></p>
			<pre class="source-code">
{
    "user": "Santiago",
    "password": "1234567890",
    "host": "localhost",
    "port": "5432",
    "database": "test"
}</pre>
			<p>I do not know who <strong class="source-inline">Santiago</strong> is, but the Copilot AI clearly thinks that <strong class="source-inline">Santiago</strong> is the right user, so I am going to use it. Going back to our <strong class="source-inline">deployment/run_build.py</strong> file, we must pass our parameters to the Terraform <strong class="source-inline">apply</strong> command by updating the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
apply_process = Popen(f"cd {DIRECTORY_PATH} &amp;&amp; terraform apply" + params, shell=True)</pre>
			<p>After the processes that run the Terraform build have finished, we then store the output of that build <a id="_idIndexMarker1033"/>in a JSON file. We then create our own database URL and write this URL to a text file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
. . .
produce_output.wait()
with open(f"{DIRECTORY_PATH}/output.json", "r") as file:
    data = json.loads(file.read())
database_url = f"postgresql://{db_data['user']}:{db_data['password']}
    @{data['db_endpoint']['value'][0]}/to_do"
with open("./database.txt", "w") as text_file:
    text_file.write("DATABASE_URL=" + database_url)</pre>
			<p>The only thing we need to do now is update our Rust application config data with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
with open("./rust_config.yml") as yaml_file:
    config = yaml.load(yaml_file, Loader=yaml.FullLoader)
config["DB_URL"] = database_url
with open("./rust_config.yml", "w") as yaml_file:
    yaml.dump(config, yaml_file, default_flow_style=False)</pre>
			<p>There is only one change left in our pipeline, and this is in our <strong class="source-inline">deployment/run_build.sh</strong> file. Instead of copying our local <strong class="source-inline">.env</strong> file to our deployment server, we copy our <strong class="source-inline">deployment/database.txt</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
scp -i "~/.ssh/keys/remotebuild.pem"
./database.txt ec2-user@$1:/home/ec2-user/.env</pre>
			<p>Running our deployment again will deploy our server and connect it to the AWS database that we <a id="_idIndexMarker1034"/>have created. Again, these build scripts can be brittle. Sometimes, a connection can be refused when copying one of the files to the deployment server, which can result in the breaking of the entire pipeline. Because we have coded all the steps ourselves and understand each step, if there is a break, it will not take us much to manually sort out the break or try to run the Python build <span class="No-Break">script again.</span></p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor220"/>Summary</h1>
			<p>We have finally come to the end of our journey. We have created our own Docker image, packaging our Rust application. We then ran this on our local computer with the protection of an NGINX container. We then deployed it onto a Docker Hub account, enabling us to use it to deploy onto an AWS server that we <span class="No-Break">set up.</span></p>
			<p>It must be noted that we have gone through the lengthy steps of configuring containers and accessing our server via SSH. This has enabled us to apply this process to other platforms as our general approach was not AWS-centric. We merely used AWS to set up the server. However, if we set up a server on another provider, we would still be able to install Docker on the server, deploy our image onto it, and run it with NGINX and a connection to <span class="No-Break">a database.</span></p>
			<p>There are a few more things we can do as a developer’s work is never done. However, we have covered and achieved the core basics of building a Rust web application from scratch and deploying it in an <span class="No-Break">automated fashion.</span></p>
			<p>Considering this, there is little holding back developers from building web applications in Rust. Frontend frameworks can be added to improve the frontend functionality, and extra modules can be added to our application to increase its functionality and API endpoints. We now have a solid base to build a range of applications and read further on topics to enable us to develop our skills and knowledge of web development <span class="No-Break">in Rust.</span></p>
			<p>We are at an exciting time with Rust and web development, and hopefully, after getting to this point, you feel empowered to push Rust forward in the field of web development. In the next chapter, we will be encrypting our web traffic to our application <span class="No-Break">using HTTPS.</span></p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor221"/>Further reading</h1>
			<ul>
				<li>Rust compiling to different targets <span class="No-Break">documentation:</span><span class="No-Break"> </span><a href="https://doc.rust-lang.org/rustc/targets/index.html"><span class="No-Break">https://doc.rust-lang.org/rustc/targets/index.html</span></a></li>
				<li>GitHub Actions <span class="No-Break">documentation: </span><a href="https://github.com/features/actions"><span class="No-Break">https://github.com/features/actions</span></a></li>
				<li>Travis CI <span class="No-Break">documentation: </span><a href="https://docs.travis-ci.com/user/for-beginners/"><span class="No-Break">https://docs.travis-ci.com/user/for-beginners/</span></a></li>
				<li>CircleCI <span class="No-Break">documentation: </span><a href="https://circleci.com/docs/"><span class="No-Break">https://circleci.com/docs/</span></a></li>
				<li>Jenkins <span class="No-Break">documentation: </span><a href="https://www.jenkins.io/doc/"><span class="No-Break">https://www.jenkins.io/doc/</span></a></li>
				<li>Terraform output <span class="No-Break">documentation:</span><span class="No-Break"><span class="hidden"> </span></span><a href="https://developer.hashicorp.com/terraform/language/values/outputs"><span class="No-Break">https://developer.hashicorp.com/terraform/language/values/outputs</span></a></li>
				<li><em class="italic">AWS Certified Developer - Associate Guide</em>, <em class="italic">Second Edition</em>, <em class="italic">V. Tankariya</em> and <em class="italic">B. Parmar</em> (2019), <em class="italic">Packt Publishing</em>, <a href="B18722_05.xhtml#_idTextAnchor091"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Getting Started with Elastic Compute Cloud (EC2)</em>, <span class="No-Break">page 165</span></li>
				<li><em class="italic">AWS Certified Developer - Associate Guide</em>, <em class="italic">Second Edition</em>, <em class="italic">V. Tankariya</em> and <em class="italic">B. Parmar</em> (2019), <em class="italic">Packt Publishing</em>, <a href="B18722_10.xhtml#_idTextAnchor200"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">AWS Relational Database Service (RDS)</em>, <span class="No-Break">page 333</span></li>
				<li><em class="italic">AWS Certified Developer - Associate Guide</em>, <em class="italic">Second Edition</em>, <em class="italic">V. Tankariya</em> and <em class="italic">B. Parmar</em> (2019), <em class="italic">Packt Publishing</em>, <span class="No-Break"><em class="italic">Chapter 21</em></span>, <em class="italic">Getting Started with AWS CodeDeploy</em>, <span class="No-Break">page 657</span></li>
				<li><em class="italic">Mastering Kubernetes</em>, <em class="italic">G. Sayfan</em> (2020), <span class="No-Break"><em class="italic">Packt Publishing</em></span></li>
				<li><em class="italic">Getting Started with Terraform</em>, <em class="italic">K. Shirinkin</em> (2017), <span class="No-Break"><em class="italic">Packt Publishing</em></span></li>
				<li><em class="italic">Nginx HTTP Server</em>, <em class="italic">Fourth Edition</em>, <em class="italic">M. Fjordvald</em> and <em class="italic">C. Nedelcu</em> (2018), <span class="No-Break"><em class="italic">Packt Publishing</em></span></li>
			</ul>
		</div>
	</body></html>
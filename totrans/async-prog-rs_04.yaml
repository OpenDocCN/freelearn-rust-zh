- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Create Your Own Event Queue
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建您自己的事件队列
- en: In this chapter, we’ll create a simple version of an event queue using epoll.
    We’ll take inspiration from `mio` has the added benefit of making it easier to
    dive into their code base if you wish to explore how a real production-ready library
    works.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 epoll 创建事件队列的简单版本。我们将从 `mio` 中汲取灵感，这也有助于如果你想要探索一个真正的生产就绪库是如何工作的，更容易地深入研究它们的代码库。
- en: 'By the end of this chapter, you should be able to understand the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该能够理解以下内容：
- en: The difference between blocking and non-blocking I/O
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阻塞和非阻塞 I/O 之间的区别
- en: How to use epoll to make your own event queue
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 epoll 创建自己的事件队列
- en: The source code of cross-platform event queue libraries such as mio
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨平台事件队列库（如 mio）的源代码
- en: Why we need an abstraction layer on top of epoll, kqueue, and IOCP if we want
    a program or library to work across different platforms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们想让程序或库在不同的平台上工作，为什么需要在 epoll、kqueue 和 IOCP 之上添加抽象层
- en: 'We’ve divided the chapter into the following sections:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将本章分为以下几部分：
- en: Design and introduction to epoll
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: epoll 的设计和介绍
- en: The `ffi` module
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ffi` 模块'
- en: The `Poll` module
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Poll` 模块'
- en: The `main` program
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main` 程序'
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter focuses on epoll, which is specific to Linux. Unfortunately, epoll
    is not part of the **Portable Operating System Interface** (**POSIX**) standard,
    so this example will require you to run Linux and won’t work with macOS, BSD,
    or Windows operating systems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍 epoll，它是 Linux 特有的。不幸的是，epoll 不是 **可移植操作系统接口** (**POSIX**) 标准的一部分，因此这个示例将需要你在
    Linux 上运行，不会与 macOS、BSD 或 Windows 操作系统兼容。
- en: If you’re on a machine running Linux, you’re already set and can run the examples
    without any further steps.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一台运行 Linux 的机器上，你已经设置好了，可以运行示例而无需进一步操作。
- en: If you’re on Windows, my recommendation is to set up **WSL** ([https://learn.microsoft.com/en-us/windows/wsl/install](https://learn.microsoft.com/en-us/windows/wsl/install)),
    if you haven’t already, and install Rust on the Linux operating system running
    on WSL.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是 Windows，我的建议是如果你还没有设置，请设置 **WSL** ([https://learn.microsoft.com/en-us/windows/wsl/install](https://learn.microsoft.com/en-us/windows/wsl/install))
    并在 WSL 上运行的 Linux 操作系统中安装 Rust。
- en: If you’re using Mac, you can create a **virtual machine** (**VM**) running Linux,
    for example, by using the **QEMU**-based **UTM** application ([https://mac.getutm.app/](https://mac.getutm.app/))
    or any other solution for managing VMs on a Mac.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是 Mac，你可以通过使用基于 QEMU 的 **UTM** 应用程序 ([https://mac.getutm.app/](https://mac.getutm.app/))
    或其他任何在 Mac 上管理虚拟机 (VM) 的解决方案来创建一个运行 Linux 的 **虚拟机** (VM)，例如。
- en: A last option is to rent a Linux server (there are even some providers with
    a free layer), install Rust, and either use an editor such as Vim or Emacs in
    the console or develop on the remote machine using VS Code through SSH ([https://code.visualstudio.com/docs/remote/ssh](https://code.visualstudio.com/docs/remote/ssh)).
    I personally have good experience with Linode’s offering ([https://www.linode.com/](https://www.linode.com/)),
    but there are many, many other options out there.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个选择是租用一个 Linux 服务器（甚至有些提供商提供免费层），安装 Rust，然后在控制台中使用 Vim 或 Emacs 等编辑器，或者在远程机器上通过
    SSH 使用 VS Code 进行开发 ([https://code.visualstudio.com/docs/remote/ssh](https://code.visualstudio.com/docs/remote/ssh))。我个人对
    Linode 的服务有很好的体验 ([https://www.linode.com/](https://www.linode.com/))，但市面上有很多其他选择。
- en: It’s theoretically possible to run the examples on the Rust playground, but
    since we need a delay server, we would have to use a remote delay server service
    that accepts plain HTTP requests (not HTTPS) and modify the code so that the modules
    are all in one file instead. It’s possible in a clinch but not really recommended.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，可以在 Rust playground 上运行示例，但由于我们需要延迟服务器，我们可能需要使用接受纯 HTTP 请求（不是 HTTPS）的远程延迟服务器服务，并修改代码，使所有模块都在一个文件中。这在紧急情况下是可能的，但并不推荐。
- en: The delay server
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟服务器
- en: This example relies on calls made to a server that delays the response for a
    configurable duration. In the repository, there is a project named `delayserver`
    in the root folder.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例依赖于对延迟响应可配置持续时间的服务器的调用。在存储库中，根目录下有一个名为 `delayserver` 的项目。
- en: You can set up the server by simply entering the folder in a separate console
    window and writing `cargo run`. Just leave the server running in a separate, open
    terminal window as we’ll use it in our example.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在单独的控制台窗口中进入文件夹并编写 `cargo run` 来设置服务器。只需将服务器在单独的、打开的终端窗口中运行即可，因为我们将在示例中使用它。
- en: The `delayserver` program is cross-platform, so it works without any modification
    on all platforms that Rust supports. If you’re running WSL on Windows, I recommend
    running the `delayserver` program in WSL as well. Depending on your setup, you
    might get away with running the server in a Windows console and still be able
    to reach it when running the example in WSL. Just be aware that it might not work
    out of the box.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`delayserver` 程序是跨平台的，因此它可以在 Rust 支持的所有平台上无需任何修改即可运行。如果你在 Windows 上运行 WSL，我建议你也在
    WSL 中运行 `delayserver` 程序。根据你的配置，你可能可以在 Windows 控制台中运行服务器，同时在 WSL 中运行示例时仍然能够访问它。只是要注意，它可能不会直接工作。'
- en: The server will listen to port `8080` by default and the examples there assume
    this is the port used. You can change the listening port in the `delayserver`
    code before you start the server, but just remember to make the same corrections
    in the example code.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器默认将监听端口 `8080`，那里的示例假设这是使用的端口。你可以在启动服务器之前在 `delayserver` 代码中更改监听端口，但请记住在示例代码中也进行相同的修正。
- en: The actual code for `delayserver` is less than 30 lines, so going through the
    code should only take a few minutes if you want to see what the server does.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`delayserver` 的实际代码不到 30 行，所以如果你想看看服务器做了什么，浏览代码只需几分钟。'
- en: Design and introduction to epoll
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计和 epoll 介绍
- en: Okay, so this chapter will be centered around one main example you can find
    in the repository under `ch04/a-epoll`. We’ll start by taking a look at how we
    design our example.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以本章将围绕一个主要示例展开，你可以在 `ch04/a-epoll` 仓库中找到这个示例。我们将首先看看我们是如何设计我们的示例的。
- en: As I mentioned at the start of this chapter, we’ll take our inspiration from
    `mio`. This has one big upside and one downside. The upside is that we get a gentle
    introduction to how `mio` is designed, making it much easier to dive into that
    code base if you want to learn more than what we cover in this example. The downside
    is that we introduce an overly thick abstraction layer over epoll, including some
    design decisions that are very specific to `mio`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在本章开头提到的，我们将从 `mio` 中汲取灵感。这有一个很大的优点和一个缺点。优点是，我们得到了一个关于 `mio` 如何设计的温和介绍，这使得如果你想要学习比我们在这个示例中涵盖的更多内容，更容易深入到那个代码库。缺点是我们对
    epoll 引入了一个过于厚重的抽象层，包括一些非常具体于 `mio` 的设计决策。
- en: I think the upsides outweigh the downsides for the simple reason that if you
    ever want to implement a production-quality event loop, you’ll probably want to
    look into the implementations that are already out there, and the same goes for
    if you want to dig deeper into the building blocks of asynchronous programming
    in Rust. In Rust, `mio` is one of the important libraries underpinning much of
    the async ecosystem, so gaining a little familiarity with it is an added bonus.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为优点大于缺点，简单的理由是，如果你想要实现一个生产级别的事件循环，你可能会想查看已经存在的实现，同样，如果你想要深入挖掘 Rust 中异步编程的构建块，也是如此。在
    Rust 中，`mio` 是支撑大部分异步生态系统的重要库之一，因此对它有所了解是一个额外的加分项。
- en: It’s important to note that `mio` is a cross-platform library that creates an
    abstraction over epoll, kqueue, and IOCP (through Wepoll, as we described in [*Chapter
    3*](B20892_03.xhtml#_idTextAnchor063)). Not only that, `mio` supports iOS and
    Android, and in the future, it will likely support other platforms as well. So,
    leaving the door open to unify an API over so many different systems is bound
    to also come with some compromises if you compare it to what you can achieve if
    you only plan to support one platform.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，`mio` 是一个跨平台库，它对 epoll、kqueue 和 IOCP（通过 Wepoll，正如我们在 [*第 3 章*](B20892_03.xhtml#_idTextAnchor063)
    中描述的）进行了抽象。不仅如此，`mio` 支持 iOS 和 Android，未来它可能还会支持其他平台。因此，如果只计划支持一个平台，那么在这么多不同的系统上统一
    API 的可能性必然也会带来一些妥协。
- en: mio
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: mio
- en: mio describes itself as a “*fast, low-level I/O library for Rust focusing on
    non-blocking APIs and event notification for building performance I/O apps with
    as little overhead as possible over the* *OS abstractions*.”
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`mio` 自称是一个“*针对 Rust 的快速、低级 I/O 库，专注于非阻塞 API 和事件通知，以尽可能少的开销在* *操作系统抽象之上* *构建性能
    I/O 应用。””'
- en: mio drives the event queue in Tokio, which is one of the most popular and widely
    used asynchronous runtimes in Rust. This means that mio is driving I/O for popular
    frameworks such as Actix Web ([https://actix.rs/](https://actix.rs/)), Warp ([https://github.com/seanmonstar/warp](https://github.com/seanmonstar/warp)),
    and Rocket ([https://rocket.rs/](https://rocket.rs/)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: mio驱动Tokio中的事件队列，Tokio是Rust中最受欢迎和广泛使用的异步运行时之一。这意味着mio正在驱动像Actix Web([https://actix.rs/](https://actix.rs/))、Warp([https://github.com/seanmonstar/warp](https://github.com/seanmonstar/warp))和Rocket([https://rocket.rs/](https://rocket.rs/))这样的流行框架的I/O。
- en: The version of mio we’ll use as design inspiration in this example is version
    **0.8.8**. The API has changed in the past and may change in the future, but the
    parts of the API we cover here have been stable since 2019, so it’s a good bet
    that there will not be significant changes to it in the near future.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将使用mio的**0.8.8**版本作为设计灵感的来源。API在过去已经改变，并且未来可能会改变，但我们在本处覆盖的API部分自2019年以来一直保持稳定，因此可以合理地预测，在不久的将来它不会发生重大变化。
- en: As is the case with all cross-platform abstractions, it’s often necessary to
    go the route of choosing the least common denominator. Some choices will limit
    flexibility and efficiency on one or more platforms in the pursuit of having a
    unified API that works with all of them. We’ll discuss some of those choices in
    this chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所有跨平台抽象一样，通常有必要走选择最小公倍数的路线。一些选择可能会限制一个或多个平台上的灵活性和效率，以追求一个与所有这些平台都兼容的统一API。我们将在本章中讨论一些这些选择。
- en: Before we go further, let’s create a blank project and give it a name. We’ll
    refer to it as `a-epoll` going forward, but you will of course need to replace
    that with the name you choose.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们创建一个空白项目并给它起一个名字。我们将从现在开始称它为`a-epoll`，但当然你需要用你选择的名称来替换它。
- en: Enter the folder and type the `cargo` `init` command.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 进入文件夹并输入`cargo init`命令。
- en: 'In this example, we’ll divide the project into a few modules, and we’ll split
    the code up into the following files:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将项目分成几个模块，并将代码拆分到以下文件中：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Their descriptions are as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的描述如下：
- en: '`ffi.rs`: This module will contain the code related to the syscalls we need
    to communicate with the host operating system'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ffi.rs`：此模块将包含与我们需要与宿主操作系统通信的syscalls相关的代码'
- en: '`main.rs`: This is the example program itself'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main.rs`：这是示例程序本身'
- en: '`poll.rs`: This module contains the main abstraction, which is a thin layer
    over epoll'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`poll.rs`：此模块包含主要抽象，它是在epoll之上的一个薄层'
- en: Next, create the four files, mentioned in the preceding list, in the `src` folder.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在`src`文件夹中创建前面提到的四个文件。
- en: 'In `main.rs`, we need to declare the modules as well:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main.rs`中，我们还需要声明模块：
- en: a-epoll/src/main.rs
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: a-epoll/src/main.rs
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have our project set up, we can start by going through how we’ll
    design the API we’ll use. The main abstraction is in `poll.rs`, so go ahead and
    open that file.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了项目，我们可以开始讨论我们将如何设计我们将使用的API。主要抽象在`poll.rs`文件中，所以请打开该文件。
- en: 'Let’s start by stubbing out the structures and functions we need. It’s easier
    to discuss them when we have them in front of us:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先构建出我们需要的结构和函数。当我们把它们放在面前时，讨论它们会更简单：
- en: a-epoll/src/poll.rs
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: a-epoll/src/poll.rs
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We’ve replaced all the implementations with `todo!()` for now. This macro will
    let us compile the program even though we’ve yet to implement the function body.
    If our execution ever reaches `todo!()`, it will panic.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前用`todo!()`替换了所有的实现。这个宏允许我们在尚未实现函数体的情况下编译程序。如果我们的执行达到`todo!()`，它将引发panic。
- en: The first thing you’ll notice is that we’ll pull the `ffi` module in scope in
    addition to some types from the standard library.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先会注意到，我们将除了标准库中的一些类型外，还将`ffi`模块引入作用域。
- en: We’ll also use the `std::io::Result` type as our own `Result` type. It’s convenient
    since most errors will stem from one of our calls into the operating system, and
    an operating system error can be mapped to an `io::Error` type.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用`std::io::Result`类型作为我们自己的`Result`类型。这很方便，因为大多数错误都源于我们对操作系统的调用之一，操作系统错误可以映射到`io::Error`类型。
- en: There are two main abstractions over epoll. One is a structure called `Poll`
    and the other is called `Registry`. The name and functionality of these functions
    are the same as they are in `mio`. Naming abstractions such as these is surprisingly
    difficult, and both constructs could very well have had a different name, but
    let’s lean on the fact that someone else has spent time on this before us and
    decided to go with these in our example.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `epoll` 上有两个主要的抽象。一个是名为 `Poll` 的结构，另一个是名为 `Registry`。这些函数的名称和功能与 `mio` 中相同。命名这样的抽象出人意料地困难，这两个构造完全可以用不同的名称，但让我们依靠这样一个事实：在我们之前，有人已经花时间研究过这个问题，并决定在我们的示例中使用这些名称。
- en: '`Poll` is a struct that represents the event queue itself. It has a few methods:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`Poll` 是一个表示事件队列本身的 `struct`。它有几个方法：'
- en: '`new`: Creates a new event queue'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new`: 创建一个新的事件队列'
- en: '`registry`: Returns a reference to the registry that we can use to register
    interest to be notified about new events'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`registry`: 返回一个引用，我们可以用它来注册对新事件的兴趣'
- en: '`poll`: Blocks the thread it’s called on until an event is ready or it times
    out, whichever occurs first'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`poll`: 阻塞调用该方法的线程，直到有事件准备好或超时，以先发生者为准'
- en: '`Registry` is the other half of the equation. While `Poll` represents the event
    queue, `Registry` is a handle that allows us to register interest in new events.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`Registry` 是等式的另一半。虽然 `Poll` 代表事件队列，但 `Registry` 是一个句柄，它允许我们注册对新事件的兴趣。'
- en: '`Registry` will only have one method: `register`. Again, we mimic the API `mio`
    uses ([https://docs.rs/mio/0.8.8/mio/struct.Registry.html](https://docs.rs/mio/0.8.8/mio/struct.Registry.html)),
    and instead of accepting a predefined list of methods for registering different
    interests, we accept an `interests` argument, which will indicate what kind of
    events we want our event queue to keep track of.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`Registry` 只有一个方法：`register`。再次强调，我们模仿 `mio` 使用的 API ([https://docs.rs/mio/0.8.8/mio/struct.Registry.html](https://docs.rs/mio/0.8.8/mio/struct.Registry.html))，并且不是接受一个预定义的方法列表来注册不同的兴趣，而是接受一个
    `interests` 参数，它将指示我们希望我们的事件队列跟踪哪种类型的事件。'
- en: One more thing to note is that we won’t use a generic type for all sources.
    We’ll only implement this for `TcpStream`, even though there are many things we
    could potentially track with an event queue.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点需要注意的是，我们不会为所有源使用泛型类型。我们只会为 `TcpStream` 实现，尽管我们可以用事件队列跟踪许多潜在的东西。
- en: This is especially true when we want to make this cross-platform since, depending
    on the platforms you want to support, there are many types of event sources we
    might want to track.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这尤其适用于我们想要实现跨平台时，因为根据你想要支持的平台，可能有许多类型的事件源我们想要跟踪。
- en: mio solves this by having `Registry::register` accept an object implementing
    the `Source` trait that `mio` defines. As long as you implement this trait for
    the source, you can use the event queue to track events on it.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`mio` 通过让 `Registry::register` 接受一个实现了 `mio` 定义的 `Source` 特性的对象来解决此问题。只要为源实现这个特性，你就可以使用事件队列来跟踪它的事件。'
- en: 'In the following pseudo-code, you’ll get an idea of how we plan to use this
    API:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的伪代码中，你会了解我们计划如何使用这个 API：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You might wonder why we need the `Registry` struct at all.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么我们真的需要 `Registry` 结构。
- en: To answer that question, we need to remember that `mio` abstracts over epoll,
    kqueue, and IOCP. It does this by making `Registry` wrap around a `Selector` object.
    The `Selector` object is conditionally compiled so that every platform has its
    own `Selector` implementation corresponding to the relevant syscalls to make IOCP,
    kqueue, and epoll do the same thing.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答这个问题，我们需要记住 `mio` 抽象了 `epoll`、`kqueue` 和 `IOCP`。它是通过使 `Registry` 包围一个 `Selector`
    对象来做到这一点的。`Selector` 对象是条件编译的，以便每个平台都有其自己的 `Selector` 实现，对应于执行 IOCP、`kqueue` 和
    `epoll` 的相关系统调用。
- en: '`Registry` implements one important method we won’t implement in our example,
    called `try_clone`. The reason we won’t implement this is that we don’t need it
    to understand how an event loop like this works and we want to keep the example
    simple and easy to understand. However, this method is important for understanding
    why the responsibility of registering events and the queue itself is divided.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`Registry` 实现了一个我们不会在示例中实现的重要方法，称为 `try_clone`。我们不实现这个方法的原因是我们不需要它来理解这种事件循环是如何工作的，我们希望保持示例简单易懂。然而，这个方法对于理解为什么注册事件和队列本身的责任是分开的是很重要的。'
- en: Important note
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: By moving the concern of registering interests to a separate struct like this,
    users can call `Registry::try_clone` to get an owned `Registry` instance. This
    instance can be passed to, or shared through `Arc<Registry>` with, other threads,
    allowing multiple threads to register interest to the same `Poll` instance even
    when `Poll` is blocking another thread while waiting for new events to happen
    in `Poll::poll`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将注册兴趣的关注点移动到这样一个单独的结构中，用户可以通过调用`Registry::try_clone`来获取一个拥有的`Registry`实例。这个实例可以被传递给其他线程，或者通过`Arc<Registry>`与其他线程共享，使得多个线程可以在`Poll`阻塞另一个线程等待`Poll::poll`中发生新事件时注册对同一个`Poll`实例的兴趣。
- en: '`Poll::poll` requires exclusive access since it takes a `&mut self`, so when
    we’re waiting for events in `Poll::poll`, there is no way to register interest
    from a different thread at the same time if we rely on using `Poll` to register
    interest, since that will be prevented by Rust’s type system.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`Poll::poll`需要独占访问，因为它接受一个`&mut self`，所以当我们等待`Poll::poll`中的事件时，如果我们依赖于使用`Poll`来注册兴趣，那么我们就无法从不同的线程同时注册兴趣，因为这会被Rust的类型系统所阻止。'
- en: It also makes it effectively impossible to have multiple threads waiting for
    events by calling `Poll::poll` on the same instance in any meaningful way since
    it would require synchronization that essentially would make each call sequential
    anyway.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这本质上会使得每个调用都变成顺序的，因此通过在同一个实例上调用`Poll::poll`来使多个线程等待事件变得在有意义的方式上实际上是不可能的。
- en: The design lets users interact with the queue from potentially many threads
    by registering interest, while one thread makes the blocking call and handles
    the notifications from the operating system.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计允许用户通过注册兴趣从潜在的许多线程与队列交互，而一个线程进行阻塞调用并处理来自操作系统的通知。
- en: Note
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The fact that `mio` doesn’t enable you to have multiple threads that are blocked
    on the same call to `Poll::poll` isn’t a limitation due to epoll, kqueue, or IOCP.
    They all allow for the scenario that many threads will call `Poll::poll` on the
    same instance and get notifications on events in the queue. epoll even allows
    specific flags to dictate whether the operating system should wake up only one
    or all threads that wait for notification (specifically the `EPOLLEXCLUSIVE` flag).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`mio`不能让你有多个线程在同一时刻阻塞在`Poll::poll`的调用上，这并不是由于epoll、kqueue或IOCP的限制。它们都允许许多线程在同一个实例上调用`Poll::poll`并接收队列中的事件通知。epoll甚至允许特定的标志来决定操作系统是否只唤醒一个或所有等待通知的线程（特别是`EPOLLEXCLUSIVE`标志）。'
- en: The problem is partly about how the different platforms decide which threads
    to wake when there are many of them waiting for events on the same queue, and
    partly about the fact that there doesn’t seem to be a huge interest in that functionality.
    For example, epoll will, by default, wake all threads that block on `Poll`, while
    Windows, by default, will only wake up one thread. You can modify this behavior
    to some extent, and there have been ideas on implementing a `try_clone` method
    on `Poll` as well in the future. For now, the design is like we outlined, and
    we will stick to that in our example as well.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 问题部分在于不同的平台在许多线程都在同一个队列上等待事件时如何决定唤醒哪些线程，部分在于似乎对这种功能没有很大的兴趣。例如，epoll默认会唤醒所有在`Poll`上阻塞的线程，而Windows默认只会唤醒一个线程。你可以在一定程度上修改这种行为，并且已经有人提出了在`Poll`上实现`try_clone`方法的想法。目前，设计就像我们概述的那样，我们将在示例中也坚持这一点。
- en: This brings us to another topic we should cover before we start implementing
    our example.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了另一个我们在开始实现示例之前应该讨论的话题。
- en: Is all I/O blocking?
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所有的I/O都是阻塞的吗？
- en: Finally, a question that’s easy to answer. The answer is a big, resounding…
    maybe. The thing is that not all I/O operations will block in the sense that the
    operating system will park the calling thread and it will be more efficient to
    switch to another task. The reason for this is that the operating system is smart
    and will cache a lot of information in memory. If information is in the cache,
    a syscall requesting that information would simply return immediately with the
    data, so forcing a context switch or any rescheduling of the current task might
    be less efficient than just handling the data synchronously. The problem is that
    there is no way to know for sure whether I/O is blocking and it depends on what
    you’re doing.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个容易回答的问题。答案是响亮的大……也许吧。问题是并非所有的I/O操作都会阻塞，即操作系统会挂起调用线程，切换到另一个任务会更有效率。原因是操作系统很智能，会在内存中缓存大量信息。如果信息在缓存中，请求该信息的系统调用将立即返回数据，因此强制上下文切换或重新调度当前任务可能不如同步处理数据更有效率。问题是无法确定I/O是否阻塞，这取决于你正在做什么。
- en: Let me give you two examples.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我给你举两个例子。
- en: DNS lookup
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DNS查找
- en: When creating a TCP connection, one of the first things that happens is that
    you need to convert a typical address such as [www.google.com](https://www.google.com)
    to an IP address such as `216.58.207.228`. The operating system maintains a mapping
    of local addresses and addresses it’s previously looked up in a cache and will
    be able to resolve them almost immediately. However, the first time you look up
    an unknown address, it might have to make a call to a DNS server, which takes
    a lot of time, and the OS will park the calling thread while waiting for the response
    if it’s not handled in a non-blocking manner.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建TCP连接时，首先发生的一件事是你需要将典型的地址，如[www.google.com](https://www.google.com)，转换为IP地址，如`216.58.207.228`。操作系统维护一个本地地址和之前在缓存中查找的地址的映射，并且几乎可以立即解析它们。然而，第一次查找未知地址时，它可能需要调用DNS服务器，这需要很长时间，如果未以非阻塞方式处理，操作系统将挂起调用线程等待响应。
- en: File I/O
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文件输入/输出
- en: Files on the local filesystem are another area where the operating system performs
    quite a bit of caching. Smaller files that are frequently read are often cached
    in memory, so requesting that file might not block at all. If you have a web server
    that serves static files, there is most likely a rather limited set of small files
    you’ll be serving. The chances are that these are cached in memory. However, there
    is no way to know for sure – if an operating system is running low on memory,
    it might have to map memory pages to the hard drive, which makes what would normally
    be a very fast memory lookup excruciatingly slow. The same is true if there is
    a huge number of small files that are accessed randomly, or if you serve very
    large files since the operating system will only cache a limited amount of information.
    You’ll also encounter this kind of unpredictability if you have many unrelated
    processes running on the same operating system as it might not cache the information
    that’s important to you.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本地文件系统上的文件是另一个操作系统执行大量缓存操作的区域。经常被读取的小文件通常会被缓存在内存中，因此请求该文件可能根本不会阻塞。如果你有一个提供静态文件的Web服务器，你很可能只提供一小部分小文件。这些文件很可能被缓存在内存中。然而，无法确定这一点——如果操作系统内存不足，它可能不得不将内存页面映射到硬盘上，这使得通常非常快的内存查找变得极其缓慢。同样，如果随机访问大量小文件，或者如果你提供非常大的文件（因为操作系统只会缓存有限的信息），情况也是如此。如果你在同一操作系统上运行许多无关的过程，你也可能会遇到这种不可预测性，因为它可能不会缓存对你重要的信息。
- en: A popular way of handling these cases is to forget about non-blocking I/O, and
    actually make a blocking call instead. You don’t want to do these calls in the
    same thread that runs a `Poll` instance (since every small delay will block all
    tasks), but you would probably relegate that task to a **thread pool**. In the
    thread pool, you have a limited number of threads that are tasked with making
    regular blocking calls for things such as DNS lookups or file I/O.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这些情况的一个流行方法是忘记非阻塞I/O，实际上进行阻塞调用。你不想在运行`Poll`实例的同一个线程中执行这些调用（因为任何小的延迟都会阻塞所有任务），但你可能会将这个任务委派给**线程池**。在线程池中，你有一有限数量的线程，它们负责为诸如DNS查找或文件I/O之类的操作进行常规的阻塞调用。
- en: An example of a runtime that does exactly this is `libuv` ([http://docs.libuv.org/en/v1.x/threadpool.html#threadpool](http://docs.libuv.org/en/v1.x/threadpool.html#threadpool)).
    `libuv` is the asynchronous I/O library that Node.js is built upon.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一个运行时示例，它正好做这件事的是 `libuv` ([http://docs.libuv.org/en/v1.x/threadpool.html#threadpool](http://docs.libuv.org/en/v1.x/threadpool.html#threadpool))。`libuv`
    是 Node.js 构建在之上的异步 I/O 库。
- en: While its scope is larger than `mio` (which only cares about non-blocking I/O),
    `libuv` is to `Node` in JavaScript what `mio` is to Tokio in Rust.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `libuv` 的范围比 `mio` 更广（`mio` 只关心非阻塞 I/O），但 `libuv` 在 JavaScript 中的地位相当于 `mio`
    在 Rust 中的地位。
- en: Note
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The reason for doing file I/O in a thread pool is that there have historically
    been poor cross-platform APIs for non-blocking file I/O. While it’s true that
    many runtimes choose to relegate this task to a thread pool making blocking calls
    to the OS, it might not be true in the future as the OS APIs evolve over time.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在线程池中执行文件 I/O 的原因在于，历史上非阻塞文件 I/O 的跨平台 API 一直很差。虽然许多运行时选择将这个任务委托给线程池，并通过操作系统进行阻塞调用，但这可能不会在未来成为事实，因为操作系统
    API 随着时间的推移而演变。
- en: Creating a thread pool to handle these cases is outside the scope of this example
    (even `mio` considers this outside its scope, just to be clear). We’ll focus on
    showing how epoll works and mention these topics in the text, even though we won’t
    actually implement a solution for them in this example.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个线程池来处理这些情况超出了这个示例的范围（即使是 `mio` 也认为这超出了它的范围，为了明确起见）。我们将专注于展示 epoll 的工作原理，并在文本中提及这些主题，尽管我们不会在这个示例中实际实现解决方案。
- en: Now that we’ve covered a lot of basic information about epoll, mio, and the
    design of our example, it’s time to write some code and see for ourselves how
    this all works in practice.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了关于 epoll、mio 以及我们示例设计的很多基本信息，是时候编写一些代码，亲自看看这一切在实际中是如何工作的了。
- en: The ffi module
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`ffi` 模块'
- en: Let’s start with the modules that don’t depend on any others and work our way
    from there. The `ffi` module contains mappings to the syscalls and data structures
    we need to communicate with the operating system. We’ll also explain how epoll
    works in detail once we have presented the syscalls.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从那些不依赖于其他模块的模块开始，逐步深入。`ffi` 模块包含了我们与操作系统通信所需的系统调用和数据结构的映射。一旦我们介绍了系统调用，我们也会详细解释
    epoll 的工作原理。
- en: 'It’s only a few lines of code, so I’ll place the first part here so it’s easier
    to keep track of where we are in the file since there’s quite a bit to explain.
    Open the `ffi.rs` file and write the following lines of code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一些代码行，所以我会在这里放置第一部分，这样更容易跟踪文件中的位置，因为有很多东西需要解释。打开 `ffi.rs` 文件，并写下以下代码行：
- en: ch04/a-epoll/src/ffi.rs
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ch04/a-epoll/src/ffi.rs
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first thing you’ll notice is that we declare a few constants called `EPOLL_CTL_ADD`,
    `EPOLLIN`, and `EPOLLET`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先会注意到，我们声明了一些名为 `EPOLL_CTL_ADD`、`EPOLLIN` 和 `EPOLLET` 的常量。
- en: 'I’ll get back to explaining what these constants are in a moment. Let’s first
    take a look at the syscalls we need to make. Fortunately, we’ve already covered
    syscalls in detail, so you already know the basics of `ffi` and why we link to
    C in the preceding code:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我稍后会解释这些常量的含义。首先，让我们看看我们需要执行的系统调用。幸运的是，我们已经详细介绍了系统调用，所以你已经知道了 `ffi` 的基础知识以及为什么在前面的代码中我们要链接到
    C：
- en: '`epoll_create` is the syscall we make to create an epoll queue. You can find
    the documentation for it at [https://man7.org/linux/man-pages/man2/epoll_create.2.html](https://man7.org/linux/man-pages/man2/epoll_create.2.html).
    This method accepts one argument called `size`, but `size` is there only for historical
    reasons. The argument will be ignored but must have a value larger than *0*.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epoll_create` 是我们用来创建 epoll 队列的系统调用。你可以在这里找到它的文档 [https://man7.org/linux/man-pages/man2/epoll_create.2.html](https://man7.org/linux/man-pages/man2/epoll_create.2.html)。这个方法接受一个名为
    `size` 的参数，但 `size` 只是为了历史原因而存在的。这个参数将被忽略，但必须有一个大于 *0* 的值。'
- en: '`close` is the syscall we need to close the file descriptor we get when we
    create our `epoll` instance, so we release our resources properly. You can read
    the documentation for the syscall at [https://man7.org/linux/man-pages/man2/close.2.html](https://man7.org/linux/man-pages/man2/close.2.html).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`close` 是我们需要在创建 `epoll` 实例时关闭文件描述符的系统调用，这样我们就可以正确地释放资源。你可以在 [https://man7.org/linux/man-pages/man2/close.2.html](https://man7.org/linux/man-pages/man2/close.2.html)
    阅读这个系统调用的文档。'
- en: '`epoll_ctl` is the control interface we use to perform operations on our epoll
    instance. This is the call we use to register interest in events on a source.
    It supports three main operations: *add*, *modify*, or *delete*. The first argument,
    `epfd`, is the epoll file descriptor we want to perform operations on. The second
    argument, `op`, is the argument where we specify whether we want to perform an
    *add*, *modify*, or *delete* operation'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epoll_ctl`是我们用来对epoll实例执行操作的控件接口。这是我们用来在源上注册对事件兴趣的调用。它支持三种主要操作：*添加*、*修改*或*删除*。第一个参数`epfd`是我们想要执行操作的epoll文件描述符。第二个参数`op`是我们指定是否想要执行一个*添加*、*修改*或*删除*操作的参数。'
- en: 'In our case, we’re only interested in adding interest for events, so we’ll
    only pass in `EPOLL_CTL_ADD`, which is the value to indicate that we want to perform
    an *add* operation. `epoll_event` is a little more complicated, so we’ll discuss
    it in more detail. It does two important things for us: first, the `events` field
    indicates what kind of events we want to be notified of and it can also modify
    the behavior of *how* and *when* we get notified. Second, the `data` field passes
    on a piece of data to the kernel that it will return to us when an event occurs.
    The latter is important since we need this data to identify exactly what event
    occurred since that’s the only information we’ll receive in return that can identify
    what source we got the notification for. You can find the documentation for this
    syscall here: [https://man7.org/linux/man-pages/man2/epoll_ctl.2.html](https://man7.org/linux/man-pages/man2/epoll_ctl.2.html).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们只对为事件添加兴趣感兴趣，因此我们只会传递`EPOLL_CTL_ADD`，这是表示我们想要执行一个*添加*操作的值。`epoll_event`稍微复杂一些，所以我们将在更多细节中讨论它。它为我们做了两件重要的事情：首先，`events`字段指示我们想要被通知的事件类型，并且它还可以修改我们*如何*和*何时*被通知的行为。其次，`data`字段将一块数据传递给内核，当发生事件时内核会将其返回给我们。后者很重要，因为我们需要这些数据来精确识别发生了什么事件，因为这是我们唯一会收到的可以识别我们收到通知来源的信息。您可以在以下链接中找到此系统调用的文档：[https://man7.org/linux/man-pages/man2/epoll_ctl.2.html](https://man7.org/linux/man-pages/man2/epoll_ctl.2.html)。
- en: '`epoll_wait` is the call that will block the current thread and wait until
    one of two things happens: we receive a notification that an event has occurred
    or it times out. `epfd` is the epoll file descriptor identifying the queue we
    made with `epoll_create`. `events` is an array of the same `Event` structure we
    used in `epoll_ctl`. The difference is that the `events` field now gives us information
    about what event *did* occur, and importantly the `data` field contains the same
    data that we passed in when we registered interest'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epoll_wait`是会阻塞当前线程并等待以下两种情况之一的调用：我们收到一个事件已发生的通知，或者它超时了。`epfd`是标识我们使用`epoll_create`创建的队列的epoll文件描述符。`events`是我们用于`epoll_ctl`的相同`Event`结构体的数组。区别在于，`events`字段现在提供了关于发生了什么事件的信息，并且重要的是，`data`字段包含我们在注册兴趣时传递的相同数据。'
- en: For example, the `data` field lets us identify which file descriptor has data
    that’s ready to be read. The `maxevents` arguments tell the kernel how many events
    we have reserved space for in our array. Lastly, the `timeout` argument tells
    the kernel how long we will wait for events before it will wake us up again so
    we don’t potentially block forever. You can read the documentation for `epoll_wait`
    at [https://man7.org/linux/man-pages/man2/epoll_wait.2.html](https://man7.org/linux/man-pages/man2/epoll_wait.2.html).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，`data`字段让我们能够识别哪个文件描述符有准备读取的数据。`maxevents`参数告诉内核我们在我们的数组中为多少事件预留了空间。最后，`timeout`参数告诉内核在它再次唤醒我们之前我们将等待事件多长时间，这样我们就不可能永远阻塞。您可以在以下链接中阅读`epoll_wait`的文档：[https://man7.org/linux/man-pages/man2/epoll_wait.2.html](https://man7.org/linux/man-pages/man2/epoll_wait.2.html)。
- en: 'The last part of the code in this file is the `Event` struct:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件中的代码最后一部分是`Event`结构体：
- en: ch04/a-epoll/src/ffi.rs
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ch04/a-epoll/src/ffi.rs
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This structure is used to communicate to the operating system in `epoll_ctl`,
    and the operating system uses the same structure to communicate with us in `epoll_wait`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结构体用于在`epoll_ctl`中与操作系统通信，操作系统使用相同的结构体在`epoll_wait`中与我们通信。
- en: Events are defined as a `u32`, but it’s more than just a number. This field
    is what we call a **bitmask**. I’ll take the time to explain bitmasks in a later
    section since it’s common in most syscalls and not something everyone has encountered
    before. In simple terms, it’s a way to use the bit representation as a set of
    yes/no flags to indicate whether an option has been chosen or not.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 事件被定义为`u32`，但它不仅仅是数字。这个字段就是我们所说的**位掩码**。我会在稍后的部分花时间解释位掩码，因为它在大多数系统调用中都很常见，并不是每个人都遇到过。简单来说，它是一种使用位表示作为一组是/否标志的方法，以指示是否选择了某个选项。
- en: 'The different options are described in the link I provided for the `epoll_ctl`
    syscall. I won’t explain all of them in detail here, but just cover the ones we’ll
    use:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的选项在我提供的`epoll_ctl`系统调用链接中有描述。在这里，我不会详细解释所有选项，但只介绍我们将要使用的：
- en: '`EPOLLIN` represents a bitflag indicating we’re interested in read operations
    on the file handle'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EPOLLIN`代表一个位标志，表示我们对文件句柄上的读取操作感兴趣。'
- en: '`EPOLLET` represents a bitflag indicating that we’re interested in getting
    events notified with epoll set to an edge-triggered mode'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EPOLLET`代表一个位标志，表示我们感兴趣的是通过将epoll设置为边沿触发模式来通知事件。'
- en: We’ll get back to explaining bitflags, bitmasks, and what edge-triggered mode
    really means in a moment, but let’s just finish with the code first.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会回到解释bitflags、bitmasks以及边沿触发模式真正意味着什么，但让我们先完成代码部分。
- en: The last field on the `Event` struct is `epoll_data`. This field is defined
    as a union in the documentation. A union is much like an enum, but in contrast
    to Rust’s enums, it doesn’t carry any information on what type it is, so it’s
    up to us to make sure we know what type of data it holds.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`Event`结构体的最后一个字段是`epoll_data`。这个字段在文档中被定义为联合体。联合体很像枚举，但与Rust的枚举不同，它不携带任何关于它是什么类型的信息，因此我们得确保我们知道它持有的数据类型。'
- en: We use this field to simply hold a `usize` so we can pass in an integer identifying
    each event when we register interest using `epoll_ctl`. It would be perfectly
    fine to pass in a pointer instead – just as long as we make sure that the pointer
    is still valid when it’s returned to us in `epoll_wait`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这个字段简单地保存一个`usize`，这样我们就可以在注册兴趣时使用`epoll_ctl`传递一个标识每个事件的整数。传递一个指针也是完全可以接受的——只要我们确保在`epoll_wait`返回时指针仍然是有效的。
- en: We can think of this field as a token, which is exactly what `mio` does, and
    to keep the API as similar as possible, we copy `mio` and provide a `token` method
    on the struct to get this value.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以把这个字段看作是一个令牌，这正是`mio`所做的事情，为了使API尽可能相似，我们复制了`mio`，并在结构体上提供了一个`token`方法来获取这个值。
- en: 'What does #[repr(packed)] do?'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`#[repr(packed)]`的作用是什么？'
- en: The `#[repr(packed)]` annotation is new to us. Usually, a struct will have padding
    either between fields or at the end of the struct. This happens even when we’ve
    specified `#[repr(C)]`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`#[repr(packed)]`这个注解对我们来说是新的。通常，一个结构体会在字段之间或结构体末尾有填充。即使我们指定了`#[repr(C)]`，这种情况也会发生。'
- en: The reason has to do with efficient access to the data stored in the struct
    by not having to make multiple fetches to get the data stored in a struct field.
    In the case of the `Event` struct, the usual padding would be adding 4 bytes of
    padding at the end of the `events` field. When the operating system expects a
    packed struct for `Event`, and we give it a padded one, it will write parts of
    `event_data` to the padding between the fields. When you try to read `event_data`
    later on, you’ll end up only reading the last part of `event_data`, which happened
    to overlap and get the wrong data
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 原因在于，通过不需要多次获取结构体字段中存储的数据，我们可以更有效地访问结构体中存储的数据。在`Event`结构体的例子中，通常的填充会在`events`字段末尾添加4个字节的填充。当操作系统期望`Event`结构体是打包的，而我们提供了一个填充的结构体时，它会在字段之间的填充中写入`event_data`的部分。当你稍后尝试读取`event_data`时，你将只读取`event_data`的最后部分，这恰好与填充重叠并获取了错误的数据。
- en: '![](img/B20892_04_0.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20892_04_0.jpg)'
- en: The fact that the operating systemexpects a packed `Event` struct isn’t obvious
    by reading the manpages for Linux, so you have to read the appropriate C header
    files to know for sure. You could of course simply rely on the `libc` crate ([https://github.com/rust-lang/libc](https://github.com/rust-lang/libc)),
    which we would do too if we weren’t here to learn things like this for ourselves.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统期望`Event`结构体是打包的，这一点通过阅读Linux的man手册页并不明显，因此你必须阅读适当的C头文件才能确定。当然，你可以简单地依赖`libc`包([https://github.com/rust-lang/libc](https://github.com/rust-lang/libc))，如果我们不是在这里自己学习这类知识，我们也会这样做。
- en: So, now that we’ve finished walking through the code, there are a few topics
    that we promised to get back to.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经走过了代码，有几个主题我们承诺要回过头来讨论。
- en: Bitflags and bitmasks
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位标志和位掩码
- en: You’ll encounter this all the time when making syscalls (in fact, the concept
    of bitmasks is pretty common in low-level programming). A bitmask is a way to
    treat each bit as a switch, or a flag, to indicate that an option is either enabled
    or disabled.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当你进行系统调用时（实际上，位掩码的概念在底层编程中相当常见），你经常会遇到这种情况。位掩码是一种将每个位视为开关或标志的方法，以指示一个选项是启用还是禁用。
- en: An integer, such as `i32`, can be expressed as 32 bits. `EPOLLIN` has the hex
    value of `0x1` (which is simply 1 in decimal). Represented in bits, this would
    look like `00000000000000000000000000000001`.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数，例如`i32`，可以表示为32位。`EPOLLIN`的十六进制值为`0x1`（这在十进制中就是1）。以二进制表示，这将看起来像`00000000000000000000000000000001`。
- en: '`EPOLLET`, on the other hand, has a value of `1 << 31`. This simply means the
    bit representation of the decimal number 1, shifted 31 bits to the left. The decimal
    number 1 is incidentally the same as `EPOLLIN`, so by looking at that representation
    and shifting the bits 31 times to the left, we get a number with the bit representation
    of `10000000000000000000000000000000`.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`EPOLLET`的值为`1 << 31`。这仅仅意味着十进制数1的位表示向左移动了31位。巧合的是，十进制数1与`EPOLLIN`相同，所以通过查看这个表示并将位向左移动31次，我们得到一个位表示为`10000000000000000000000000000000`的数字。
- en: The way we use bitflags is that we use the OR operator, `|`, and by OR’ing the
    values together, we get a bitmask with each flag we OR’ed set to 1\. In our example,
    the bitmask would look like `10000000000000000000000000000001`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用位标志的方式是使用OR运算符`|`，通过将值组合在一起，我们得到一个掩码，其中每个OR过的标志都设置为1。在我们的例子中，掩码将看起来像`10000000000000000000000000000001`。
- en: The receiver of the bitmask (in this case, the operating system) can then do
    an opposite operation, check which flags are set, and act accordingly.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码的接收者（在这种情况下，操作系统）可以执行相反的操作，检查哪些标志被设置，并相应地采取行动。
- en: 'We can create a very simple example in code to show how this works in practice
    (you can simply run this in the Rust playground or create a new empty project
    for throwaway experiments such as this):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个简单的代码示例来展示这在实践中是如何工作的（你可以在Rust playground中运行这个示例，或者为这种类型的实验创建一个新的空项目）：
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This code will output the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将输出以下内容：
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The next topic we will introduce in this chapter is the concept of edge-triggered
    events, which probably need some explanation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍下一个主题，即边沿触发事件的概念，这可能需要一些解释。
- en: Level-triggered versus edge-triggered events
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 电平触发与边沿触发事件
- en: In a perfect world, we wouldn’t need to discuss this, but when working with
    epoll, it’s almost impossible to avoid having to know about the difference. It’s
    not obvious by reading the documentation, especially not if you haven’t had previous
    experience with these terms before. The interesting part of this is that it allows
    us to create a parallel between how events are handled in epoll and how events
    are handled at the hardware level.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个完美的世界里，我们不需要讨论这个问题，但当我们与epoll一起工作时，几乎不可避免地需要了解这些差异。通过阅读文档，这一点并不明显，尤其是如果你之前没有接触过这些术语。有趣的是，这允许我们创建一个并行，即epoll中事件的处理方式与硬件级别事件的处理方式。
- en: epoll can notify events in a `events` bitmask on the `Event` struct, we set
    the `EPOLLET` flag to get notified in edge-triggered mode (the default if you
    specify nothing is level-triggered).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: epoll可以在`Event`结构体的`events`掩码上通知事件，我们设置`EPOLLET`标志以在边沿触发模式下接收通知（如果没有指定，默认为电平触发）。
- en: This way of modeling event notification and event handling has a lot of similarities
    to how computers handle interrupts.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对事件通知和事件处理的方式进行建模，与计算机处理中断的方式有很多相似之处。
- en: Level-triggered means that the answer to the question “Has an event happened”
    is true as long as the electrical signal on an interrupt line is reported as high.
    If we translate this to our example, *a read event has occurred as long as there
    is data in the buffer associated with the* *file handle.*
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 电平触发意味着只要中断线上报告的电信号为高，问题的答案“事件是否发生”就是真的。如果我们把这一点应用到我们的例子中，*只要与文件句柄关联的缓冲区中有数据，读取事件就已经发生。*
- en: When handling interrupts, you would clear the interrupt by servicing whatever
    hardware caused it, or you could mask the interrupt, which simply disables interrupts
    on that line until it’s explicitly unmasked later on.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理中断时，你会通过服务引起中断的任何硬件来清除中断，或者你可以屏蔽中断，这只是在稍后显式取消屏蔽之前禁用该线路上的中断。
- en: In our example, we clear the *interrupt* by draining all the data in the buffer
    by reading it. When the buffer is drained, the answer to our question changes
    to *false*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们通过读取缓冲区中的所有数据来清除 *中断*。当缓冲区被清空时，我们问题的答案变为 *false*。
- en: When using epoll in its default mode, which is level-triggered, we can encounter
    a case where we get multiple notifications on the same event since we haven’t
    had time to drain the buffer yet (remember, as long as there is data in the buffer,
    epoll will notify you over and over again). This is especially apparent when we
    have one thread that reports events and then delegates the task of handling the
    event (reading from the stream) to other worker threads since epoll will happily
    report that an event is ready even though we’re in the process of handling it.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用默认模式的 epoll 时，它是电平触发的，我们可能会遇到在相同事件上收到多个通知的情况，因为我们还没有时间清空缓冲区（记住，只要缓冲区中有数据，epoll
    就会不断地通知你）。这在我们有一个线程报告事件然后将处理事件（从流中读取）的任务委托给其他工作线程时尤为明显，因为 epoll 会愉快地报告事件已准备好，即使我们正在处理它。
- en: To remedy this, epoll has a flag named `EPOLLONESHOT`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，epoll 有一个名为 `EPOLLONESHOT` 的标志。
- en: '`EPOLLONESHOT` tells epoll that once we receive an event on this file descriptor,
    it should disable the file descriptor in the interest list. It won’t remove it,
    but we won’t get any more notifications on that file descriptor unless we explicitly
    reactivate it by calling `epoll_ctl` with the `EPOLL_CTL_MOD` argument and a new
    bitmask.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`EPOLLONESHOT` 告诉 epoll，一旦我们在这个文件描述符上收到一个事件，它应该在该兴趣列表中禁用文件描述符。它不会移除它，但除非我们通过调用带有
    `EPOLL_CTL_MOD` 参数和新的掩码的新 `epoll_ctl` 来显式重新激活它，否则我们不会在该文件描述符上收到任何更多通知。'
- en: 'If we didn’t add this flag, the following could happen: if *thread 1* is the
    thread where we call `epoll_wait`, then once it receives a notification about
    a read event, it starts a task in *thread 2* to read from that file descriptor,
    and then calls `epoll_wait` again to get notifications on new events. In this
    case, the call to `epoll_wait` would return again and tell us that data is ready
    on the same file descriptor since we haven’t had the time to drain the buffer
    on that file descriptor yet. We know that the task is taken care of by `thread
    2`, but we still get a notification. Without additional synchronization and logic,
    we could end up giving the task of reading from the same file descriptor to *thread
    3*, which could cause problems that are quite hard to debug.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有添加这个标志，可能会发生以下情况：如果 *线程 1* 是我们调用 `epoll_wait` 的线程，那么一旦它收到关于读取事件的通告，它就会在
    *线程 2* 中启动一个任务来读取该文件描述符，然后再次调用 `epoll_wait` 来获取新事件的通告。在这种情况下，`epoll_wait` 的调用将再次返回并告诉我们数据已在该文件描述符上准备好，因为我们还没有时间清空该文件描述符上的缓冲区。我们知道任务已经被
    `thread 2` 处理，但我们仍然收到通知。如果没有额外的同步和逻辑，我们可能会将读取同一文件描述符的任务分配给 *线程 3*，这可能会引起难以调试的问题。
- en: Using `EPOLLONESHOT` solves this problem since *thread 2* will have to reactivate
    the file descriptor in the event queue once it’s done handling its task, thereby
    telling our epoll queue that it’s finished with it and that we are interested
    in getting notifications on that file descriptor again.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `EPOLLONESHOT` 解决了这个问题，因为 *线程 2* 在完成其任务后必须重新激活事件队列中的文件描述符，从而通知我们的 epoll 队列它已经处理完毕，并且我们再次对该文件描述符上的通知感兴趣。
- en: To go back to our original analogy of hardware interrupts, `EPOLLONESHOT` could
    be thought of as masking an interrupt. You haven’t actually cleared the source
    of the event notification yet, but you don’t want further notifications until
    you’ve done that and explicitly unmask it. In epoll, the `EPOLLONESHOT` flag will
    disable notifications on the file descriptor until you explicitly enable it by
    calling `epoll_ctl` with the `op` argument set to `EPOLL_CTL_MOD`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要回到我们最初的硬件中断的类比，`EPOLLONESHOT` 可以被看作是屏蔽中断。你实际上还没有清除事件通知的来源，但你不想在完成并显式取消屏蔽之前收到更多通知。在
    epoll 中，`EPOLLONESHOT` 标志将禁用文件描述符上的通知，直到你通过调用 `epoll_ctl` 并将 `op` 参数设置为 `EPOLL_CTL_MOD`
    来显式启用它。
- en: 'Edge-triggered means that the answer to the question “Has an event happened”
    is true only if the electrical signal has *changed* from low to high. If we translate
    this to our example: a read event has occurred when the buffer has changed from
    *having no data* to *having data*. As long as there is data in the buffer, no
    new events will be reported. You still handle the event by draining all the data
    from the socket, but you won’t get a new notification until the buffer is fully
    drained and then filled with new data.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 边沿触发意味着对于“是否发生了事件”这个问题的答案只有在电信号从低到高*改变*时才是真的。如果我们把这个翻译到我们的例子中：当缓冲区从*没有数据*变为*有数据*时，就会发生读取事件。只要缓冲区中有数据，就不会报告新的事件。你仍然通过从套接字中清除所有数据来处理事件，但直到缓冲区完全清除并重新填充新数据，你都不会收到新的通知。
- en: Edge-triggered mode also comes with some pitfalls. The biggest one is that if
    you don’t drain the buffer properly, you will never receive a notification on
    that file handle again.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 边沿触发模式也有一些陷阱。最大的一个问题是，如果你没有正确清除缓冲区，你将永远不会在那个文件句柄上再次收到通知。
- en: '![Figure 4.1 – Edge-triggered versus level-triggered events](img/B20892_04_1.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 边沿触发与电平触发事件](img/B20892_04_1.jpg)'
- en: Figure 4.1 – Edge-triggered versus level-triggered events
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 边沿触发与电平触发事件
- en: mio doesn’t, at the time of writing, support `EPOLLONESHOT` and uses epoll in
    an edge-triggered mode, which we will do as well in our example.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: mio在写作时，不支持`EPOLLONESHOT`，并使用边沿触发模式的epoll，我们将在我们的示例中也这样做。
- en: What about waiting on epoll_wait in multiple threads?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 那么在多个线程中等待`epoll_wait`呢？
- en: As long as we only have one `Poll` instance, we avoid the problems and subtleties
    of having multiple threads calling `epoll_wait` on the same epoll instance. Using
    level-triggered events will wake up all threads that are waiting in the `epoll_wait`
    call, causing all of them to try to handle the event (this is often referred to
    as the problem of the thundering heard). epoll has another flag you can set, called
    `EPOLLEXCLUSIVE`, that solves this issue. Events that are set to be edge-triggered
    will only wake up one of the threads blocking in `epoll_wait` by default and avoid
    this issue.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 只要我们只有一个`Poll`实例，我们就可以避免在同一个epoll实例上多个线程调用`epoll_wait`时出现的问题和微妙之处。使用电平触发事件会唤醒所有在`epoll_wait`调用中等待的线程，导致它们都试图处理事件（这通常被称为雷鸣般的响声问题）。epoll还有一个你可以设置的标志，称为`EPOLLEXCLUSIVE`，可以解决这个问题。默认情况下，设置为边沿触发的事件只会唤醒在`epoll_wait`中阻塞的一个线程，从而避免这个问题。
- en: Since we only use one `Poll` instance from a single thread, this will not be
    an issue for us.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只从一个线程中使用一个`Poll`实例，这对我们来说不会是问题。
- en: I know and understand that this sounds very complex. The general concept of
    event queues is rather simple, but the details can get a bit complex. That said,
    epoll is one of the most complex APIs in my experience since the API has clearly
    been evolving over time to adapt the original design to suit modern requirements,
    and there is really no easy way to actually use and understand it correctly without
    covering at least the topics we covered here.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道并理解这听起来非常复杂。事件队列的一般概念相当简单，但细节可能会变得有点复杂。话虽如此，epoll是我所经历的最复杂的API之一，因为API显然随着时间的推移而不断演变，以适应原始设计以适应现代需求，而且实际上没有简单的方法可以正确使用和理解它，除非至少覆盖我们在这里讨论的这些主题。
- en: One word of comfort here is that both kqueue and IOCP have APIs that are easier
    to understand. There is also the fact that Unix has a new asynchronous I/O interface
    called `io_uring` that will be more and more and more common in the future.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，有一个安慰的话是，kqueue和IOCP都有更容易理解的API。还有这样一个事实，Unix有一个新的异步I/O接口叫做`io_uring`，它将在未来变得越来越普遍。
- en: Now that we’ve covered the hard part of this chapter and gotten a high-level
    overview of how epoll works, it’s time to implement our mio-inspired API in `poll.rs`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经覆盖了本章的难点，并对epoll的工作原理有了高级概述，是时候在`poll.rs`中实现我们受mio启发的API了。
- en: The Poll module
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Poll模块
- en: If you haven’t written or copied the code we presented in the *Design and introduction
    to epoll* section, it’s time to do it now. We’ll implement all the functions where
    we just had `todo!()` earlier.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有编写或复制我们在*epoll的设计和介绍*部分中展示的代码，现在是时候做了。我们将实现所有之前只是有`todo!()`的地方。
- en: 'We start by implementing the methods on our `Poll` struct. First up is opening
    the `impl Poll` block and implementing the `new` function:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从实现我们的`Poll`结构体上的方法开始。首先，是打开`impl Poll`块并实现`new`函数：
- en: ch04/a-epoll/src/poll.rs
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ch04/a-epoll/src/poll.rs
- en: '[PRE8]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Given the thorough introduction to epoll in the *The ffi module* section, this
    should be pretty straightforward. We call `ffi::epoll_create` with an argument
    of 1 (remember, the argument is ignored but must have a non-zero value). If we
    get any errors, we ask the operating system to report the last error for our process
    and return that. If the call succeeds, we return a new `Poll` instance that simply
    wraps around our registry that holds the epoll file descriptor.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在*The ffi module*部分对epoll的详细介绍之后，这应该相当直接。我们用参数1调用`ffi::epoll_create`（记住，参数被忽略，但必须是非零值）。如果出现任何错误，我们要求操作系统为我们进程报告最后一个错误并返回它。如果调用成功，我们返回一个新的`Poll`实例，它简单地封装了我们持有的epoll文件描述符的注册表。
- en: 'Next up is our registry method, which simply hands out a reference to the inner
    `Registry` struct:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们的注册方法，它只是简单地返回内部`Registry`结构的引用：
- en: ch04/a-epoll/src/poll.rs
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ch04/a-epoll/src/poll.rs
- en: '[PRE9]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The last method on `Poll` is the most interesting one. It’s the `poll` function,
    which will park the current thread and tell the operating system to wake it up
    when an event has happened on a source we’re tracking, or the timeout has elapsed,
    whichever comes first. We also close the `impl Poll` block here:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`Poll`上的最后一个方法是`poll`函数，它将挂起当前线程，并告诉操作系统在我们跟踪的源上发生事件或超时（哪个先发生）时唤醒它。我们还在这里关闭了`impl
    Poll`块：'
- en: ch04/a-epoll/src/poll.rs
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ch04/a-epoll/src/poll.rs
- en: '[PRE10]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The first thing we do is to get the raw file descriptor for the event queue
    and store it in the `fd` variable.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先做的事情是获取事件队列的原始文件描述符，并将其存储在`fd`变量中。
- en: Next is our `timeout`. If it’s `Some`, we unwrap that value, and if it’s `None`,
    we set it to `–1`, which is the value that tells the operating system that we
    want to block until an event occurs even though that might never happen.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是`timeout`。如果它是`Some`，我们就展开那个值；如果是`None`，我们就将其设置为`-1`，这个值告诉操作系统我们想要阻塞，直到发生事件，即使这种情况可能永远不会发生。
- en: At the top of the file, we defined `Events` as a type alias for `Vec<ffi::Event>`,
    so the next thing we do is to get the capacity of that `Vec`. It’s important that
    we don’t rely on `Vec::len` since that reports how many items we have in the `Vec`.
    `Vec::capacity` reports the space we’ve allocated and that’s what we’re after.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件顶部，我们将`Events`定义为`Vec<ffi::Event>`的类型别名，所以接下来我们要做的是获取那个`Vec`的容量。我们依赖`Vec::capacity`而不是`Vec::len`是很重要的，因为`Vec::len`报告`Vec`中的项目数量，而`Vec::capacity`报告我们分配的空间，这是我们想要的。
- en: Next up is the call to `ffi::epoll_wait`. This call will return successfully
    if it has a value of 0 or larger, telling us how many events have occurred.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是调用`ffi::epoll_wait`。如果返回值为0或更大，表示成功，这告诉我们发生了多少事件。
- en: Note
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We would get a value of 0 if a timeout elapses before an event has happened.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在事件发生之前超时，我们会得到一个值为0。
- en: The last thing we do is to make an unsafe call to `events.set_len(res as usize)`.
    This function is unsafe since we could potentially set the length so that we would
    access memory that’s not been initialized yet in safe Rust. We know from the guarantee
    the operating system gives us that the number of events it returns is pointing
    to valid data in our `Vec`, so this is safe in our case.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后要做的事情是调用`events.set_len(res as usize)`，这是一个不安全的调用。因为这个函数可能会设置一个长度，导致我们在安全的Rust代码中访问尚未初始化的内存。我们知道操作系统给我们的保证是它返回的事件数量指向我们`Vec`中的有效数据，所以在这种情况下这是安全的。
- en: 'Next up is our `Registry` struct. We will only implement one method, called
    `register`, and lastly, we’ll implement the `Drop` trait for it, closing the epoll
    instance:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们的`Registry`结构。我们将只实现一个名为`register`的方法，最后，我们将为它实现`Drop`特质，关闭epoll实例：
- en: ch04/a-epoll/src/poll.rs
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ch04/a-epoll/src/poll.rs
- en: '[PRE11]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The register function takes a `&TcpStream` as a source, a token of type `usize`,
    and a bitmask named `interests`, which is of type `i32`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 注册函数接受一个`&TcpStream`作为源，一个类型为`usize`的令牌，以及一个名为`interests`的掩码，其类型为`i32`。
- en: Note
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This is where mio does things differently. The source argument is specific to
    each platform. Instead of having the implementation of register on `Registry`,
    it’s handled in a platform-specific way in the source argument it receives.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是mio以不同的方式做事的地方。源参数对每个平台都是特定的。在注册函数在`Registry`上实现的地方，它以接收到的源参数的特定平台方式处理。
- en: The first thing we do is to create an `ffi::Event` object. The `events` field
    is simply set to the bitmask we received and named `interests`, and `epoll_data`
    is set to the value we passed in the `token` argument.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个`ffi::Event`对象。`events`字段简单地设置为接收到的掩码，并命名为`interests`，而`epoll_data`被设置为在`token`参数中传递的值。
- en: The operation we want to perform on the epoll queue is adding interest in events
    on a new file descriptor. Therefore, we set the `op` argument to the `ffi::EPOLL_CTL_ADD`
    constant value.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要在epoll队列上执行的操作是在新的文件描述符上添加对事件的兴趣。因此，我们将`op`参数设置为`ffi::EPOLL_CTL_ADD`常量值。
- en: Next up is the call to `ffi::epoll_ctl`. We pass in the file descriptor to the
    epoll instance first, then we pass in the `op` argument to indicate what kind
    of operation we want to perform. The last two arguments are the file descriptor
    we want the queue to track and the `Event` object we created to indicate what
    kind of events we’re interested in getting notifications for.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是调用`ffi::epoll_ctl`。我们首先传入epoll实例的文件描述符，然后传入`op`参数来指示我们想要执行的操作。最后两个参数是我们想要队列跟踪的文件描述符和我们创建的`Event`对象，以指示我们感兴趣的事件类型。
- en: The last part of the function body is simply the error handling, which should
    be familiar by now.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 函数体的最后一部分仅仅是错误处理，现在应该已经很熟悉了。
- en: 'The last part of `poll.rs` is the `Drop` implementation for `Registry`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`poll.rs`的最后一部分是`Registry`的`Drop`实现：'
- en: ch04/a-epoll/src/poll.rs
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ch04/a-epoll/src/poll.rs
- en: '[PRE12]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `Drop` implementation simply calls `ffi::close` on the epoll file descriptor.
    Adding a panic to `drop` is rarely a good idea since `drop` can be called within
    a panic already, which will cause the process to simply abort. mio logs errors
    if they occur in its Drop implementation but doesn’t handle them in any other
    way. For our simple example, we’ll just print the error so we can see if anything
    goes wrong since we don’t implement any kind of logging here.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`Drop`实现简单地调用epoll文件描述符上的`ffi::close`。在`drop`中添加panic很少是一个好主意，因为`drop`可以在panic中调用，这会导致进程简单地中止。如果mio的Drop实现中发生错误，它将记录错误，但不会以任何其他方式处理它们。对于我们的简单示例，我们只是打印错误，这样我们就可以看到是否有什么出错，因为我们没有实现任何类型的日志记录。'
- en: The last part is the code for running our example, and that leads us to `main.rs`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分是运行我们的示例的代码，这把我们带到了`main.rs`。
- en: The main program
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主程序
- en: Let’s see how it all works in practice. Make sure that `delayserver` is up and
    running, because we’ll need it for these examples to work.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这一切在实际中是如何工作的。确保`delayserver`正在运行，因为我们需要它来使这些例子正常工作。
- en: The goal is to send a set of requests to `delayserver` with varying delays and
    then use epoll to wait for the responses. Therefore, we’ll only use epoll to track
    `read` events in this example. The program doesn’t do much more than that for
    now.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是向`delayserver`发送一组具有不同延迟的请求，然后使用epoll等待响应。因此，在这个例子中，我们只会使用epoll来跟踪`read`事件。目前程序所做的并不多。
- en: 'The first thing we do is to make sure our `main.rs` file is set up correctly:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先确保我们的`main.rs`文件设置正确：
- en: ch04/a-epoll/src/main.rs
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ch04/a-epoll/src/main.rs
- en: '[PRE13]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We import a few types from our own crate and from the standard library, which
    we’ll need going forward, as well as declaring our two modules.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从自己的crate和标准库中导入了一些类型，这些类型是我们接下来需要用到的，以及声明我们的两个模块。
- en: We’ll be working directly with `TcpStreams` in this example, and that means
    that we’ll have to format the HTTP requests we make to our `delayserver` ourselves.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将直接与`TcpStreams`一起工作，这意味着我们必须自己格式化发送给`delayserver`的HTTP请求。
- en: 'The server will accept `GET` requests, so we create a small helper function
    to format a valid HTTP `GET` request for us:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器将接受`GET`请求，因此我们创建了一个小的辅助函数来为我们格式化一个有效的HTTP `GET`请求：
- en: ch04/a-epoll/src/main.rs
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ch04/a-epoll/src/main.rs
- en: '[PRE14]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code simply takes a path as an input argument and formats a valid
    `GET` request with it. The *path* is the part of the URL after the scheme and
    host. In our case, the path would be everything in bold in the following URL:
    `http://localhost:8080`**/2000/hello-world**.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码只是接受一个路径作为输入参数，并使用它格式化一个有效的`GET`请求。*路径*是URL方案和主机之后的部分。在我们的例子中，路径将是以下URL中加粗的部分：`http://localhost:8080`**/2000/hello-world**。
- en: 'Next up is our `main` function. It’s divided into two parts:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们的`main`函数。它分为两部分：
- en: Setup and sending requests
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置和发送请求
- en: Wait and handle incoming events
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等待和处理传入的事件
- en: 'The first part of the `main` function looks like this:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`main`函数的第一部分看起来是这样的：'
- en: '[PRE15]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first thing we do is to create a new `Poll` instance. We also specify what
    number of events we want to create and handle in our example.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先做的是创建一个新的`Poll`实例。我们还在我们的例子中指定了我们想要创建和处理的事件数量。
- en: The next step is creating a variable to hold a collection of `Vec<TcpStream>`
    objects.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个变量来存储`Vec<TcpStream>`对象集合。
- en: We also store the address to our local `delayserver` in a variable called `addr`.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将本地`delayserver`的地址存储在一个名为`addr`的变量中。
- en: The next part is where we create a set of requests that we issue to our `delayserver`,
    which will eventually respond to us. For each request, we expect a read event
    to happen sometime later on in the `TcpStream` we sent the request on.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分是我们创建一系列发送给我们的`delayserver`的请求，它最终会回应我们。对于每个请求，我们期望在发送请求的`TcpStream`上稍后发生一个读取事件。
- en: The first thing we do in the loop is set the delay time in milliseconds. Setting
    the delay to `(n_events - i) * 1000` simply sets the first request we make to
    have the longest timeout, so we should expect the responses to arrive in the reverse
    order from which they were sent.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环中我们首先设置延迟时间（以毫秒为单位）。将延迟设置为`(n_events - i) * 1000`只是将我们发出的第一个请求的超时时间设置得最长，因此我们应该期望响应以发送的相反顺序到达。
- en: Note
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For simplicity, we use the index the event will have in the `streams` collection
    as its ID. This ID will be the same as the `i` variable in our loop. For example,
    in the first loop, `i` will be `0`; it will also be the first stream to be pushed
    to our `streams` collection, so the index will be `0` as well. We therefore use
    `0` as the identification for this stream/event throughout since retrieving the
    `TcpStream` associated with this event will be as simple as indexing to that location
    in the `streams` collection.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们使用事件将在`streams`集合中的索引作为其ID。这个ID将与我们的循环中的`i`变量相同。例如，在第一个循环中，`i`将是`0`；它也将是第一个推送到我们的`streams`集合的流，因此索引也将是`0`。因此，我们使用`0`作为此流/事件的标识，因为检索与该事件相关的`TcpStream`将像在`streams`集合中索引那样简单。
- en: The next line, `format!("/{delay}/request-{i}")`, formats the *path* for our
    `GET` request. We set the timeout as described previously, and we also set a message
    where we store the identifier for this event, `i`, so we can track this event
    on the server side as well.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行，`format!("/{delay}/request-{i}")`格式化我们的`GET`请求的*路径*。我们设置了之前描述的超时时间，并且我们还设置了一个消息，其中存储了此事件的标识符`i`，这样我们就可以在服务器端跟踪此事件。
- en: Next up is creating a `TcpStream`. You’ve probably noticed that the `TcpStream`
    in Rust doesn’t accept `&str` but an argument that implements the `ToSocketAddrs`
    trait. This trait is implemented for `&str` already, so that’s why we can simply
    write it like we do in this example.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是创建一个`TcpStream`。你可能已经注意到，Rust中的`TcpStream`不接受`&str`，而是接受一个实现了`ToSocketAddrs`特质的参数。这个特质已经为`&str`实现了，这就是为什么我们可以像在这个例子中那样简单地写出来。
- en: Before `Tcpstream::connect` actually opens a socket, it will try to parse the
    address we pass in as an IP address. If it fails, it will parse it as a domain
    address and a port number, and then ask the operating system to do a DNS lookup
    for that address, which it then can use to actually connect to our server. So,
    you see, there is potentially quite a bit going on when we do a simple connection.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Tcpstream::connect`实际打开套接字之前，它将尝试解析我们传入的地址作为IP地址。如果失败，它将解析为域名地址和端口号，然后请求操作系统对该地址进行DNS查找，然后可以使用它来实际连接到我们的服务器。所以，你看，当我们进行简单的连接时，实际上可能有很多事情在进行。
- en: You probably remember that we discussed some of the nuances of the DNS lookup
    earlier and the fact that such a call could either be very fast since the operating
    system already has the information stored in memory or block while waiting for
    a response from the DNS server. This is a potential downside if you use `TcpStream`
    from the standard library if you want full control over the entire process.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得我们之前讨论了一些DNS查找的细微差别以及这样一个调用可能由于操作系统已经将信息存储在内存中而非常快，或者由于等待DNS服务器的响应而阻塞。如果你使用标准库中的`TcpStream`并希望完全控制整个过程，这是一个潜在的缺点。
- en: TcpStream in Rust and Nagle’s algorithm
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Rust中的TcpStream和Nagle算法
- en: Here is a little fact for you (I originally intended to call it a “fun fact,”
    but realized that’s stretching the concept of “fun” just a little too far!). In
    Rust’s `TcpStream`, and, more importantly, most APIs that aim to mimic the standard
    library’s `TcpStream` such as mio or Tokio, the stream is created with the `TCP_NODELAY`
    flag set to `false`. In practice, this means that Nagle’s algorithm is used, which
    can cause some issues with latency outliers and possibly reduced throughput on
    some workloads.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个小事实告诉你（我最初打算称它为“有趣的事实”，但意识到这有点过分夸大“有趣”的概念了！）！在Rust的`TcpStream`中，以及更重要的是，大多数旨在模仿标准库的`TcpStream`的API，如mio或Tokio，流是以`TCP_NODELAY`标志设置为`false`创建的。在实践中，这意味着使用了Nagle算法，这可能会在某些延迟异常和可能的工作负载中减少吞吐量。
- en: Nagle’s algorithm is an algorithm that aims to reduce network congestion by
    pooling small network packages together. If you look at non-blocking I/O implementations
    in other languages, many, if not most, disable this algorithm by default. This
    is not the case in most Rust implementations and is worth being aware of. You
    can disable it by simply calling `TcpStream::set_nodelay(true)`. If you try to
    create your own async library or rely on Tokio/mio, and observe lower throughput
    than expected or latency problems, it’s worth checking whether this flag is set
    to `true` or not.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Nagle算法是一种旨在通过合并小网络数据包来减少网络拥塞的算法。如果你查看其他语言中的非阻塞I/O实现，许多（如果不是大多数）默认禁用此算法。在大多数Rust实现中并非如此，这一点值得注意。你可以通过简单地调用`TcpStream::set_nodelay(true)`来禁用它。如果你尝试创建自己的异步库或依赖于Tokio/mio，并观察到低于预期的吞吐量或延迟问题，那么检查此标志是否设置为`true`是值得的。
- en: To continue with the code, the next step is setting `TcpStream` to non-blocking
    by calling `Tcp``Stream::set_nonblocking(true)`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 要继续代码，下一步是将`TcpStream`设置为非阻塞，通过调用`TcpStream::set_nonblocking(true)`。
- en: After that, we write our request to the server before we register interest in
    read events by setting the `EPOLLIN` flag bit in the `interests` bitmask.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在注册对读事件感兴趣之前将请求写入服务器，通过在`interests`掩码中设置`EPOLLIN`标志位。
- en: For each iteration, we push the stream to the end of our `streams` collection.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次迭代，我们将流推送到`streams`集合的末尾。
- en: The next part of the `main` function is handling incoming events.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`main`函数的下一部分是处理传入的事件。'
- en: 'Let’s take a look at the last part of our `main` function:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`main`函数的最后部分：
- en: '[PRE16]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The first thing we do is create a variable called `handled_events` to track
    how many events we have handled.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个名为`handled_events`的变量来跟踪我们处理了多少事件。
- en: Next is our event loop. We loop as long as the handled events are less than
    the number of events we expect. Once all events are handled, we exit the loop.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们的事件循环。只要处理的事件少于我们期望的事件数，我们就继续循环。一旦所有事件都得到处理，我们就退出循环。
- en: Inside the loop, we create a `Vec<Event>` with the capacity to store 10 events.
    It’s important that we create this using `Vec::with_capacity` since the operating
    system will assume that we pass it memory that we’ve allocated. We could choose
    any number of events here and it would work just fine, but setting too low a number
    would limit how many events the operating system could notify us about on each
    wakeup.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环内部，我们创建一个容量为10个事件的`Vec<Event>`。我们使用`Vec::with_capacity`来创建这一点很重要，因为操作系统会假设我们传递给它的是我们已分配的内存。我们在这里可以选择任何数量的事件，它都会正常工作，但设置得太低会限制操作系统在每次唤醒时通知我们的事件数量。
- en: Next is our blocking call to `Poll::poll`. As you know, this will actually tell
    the operating system to park our thread and wake us up when an event has occurred.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们的阻塞调用`Poll::poll`。正如你所知，这实际上会告诉操作系统暂停我们的线程，并在发生事件时唤醒我们。
- en: If we’re woken up, but there are no events in the list, it’s either a timeout
    or a spurious event (which could happen, so we need a way to check whether a timeout
    has actually elapsed if that’s important to us). If that’s the case, we simply
    call `Poll::poll` once more.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们被唤醒，但列表中没有事件，那么可能是超时或虚假事件（这可能会发生，因此我们需要一种方法来检查是否确实已经超时，如果这对我们很重要）。如果是这种情况，我们只需再次调用一次`Poll::poll`。
- en: If there are events to be handled, we pass these on to the `handle_events` function
    together with a mutable reference to our `streams` collection.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有待处理的事件，我们将这些事件连同对`streams`集合的可变引用一起传递给`handle_events`函数。
- en: The last part of `main` is simply to write `FINISHED` to the console to let
    us know we exited `main` at that point.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`main`函数的最后部分只是将`FINISHED`写入控制台，让我们知道在那个点退出了`main`。'
- en: The last bit of code in this chapter is the `handle_events` function. This function
    takes two arguments, a slice of `Event` structs and a mutable slice of `TcpStream`
    objects.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the code before we explain it:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The first thing we do is to create a variable, `handled_events`, to track how
    many events we consider handled on each wakeup. The next step is looping through
    the events we received.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: In the loop, we retrieve the *token* that identifies which `TcpStream` we received
    an event for. As we explained earlier in this example, this *token* is the same
    as the index for that particular stream in the `streams` collection, so we can
    simply use it to index into our `streams` collection and retrieve the right `TcpStream`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Before we start reading data, we create a buffer with a size of 4,096 bytes
    (you can, of course, allocate a larger or smaller buffer for this if you want
    to).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: We create a loop since we might need to call `read` multiple times to be sure
    that we’ve actually drained the buffer. *Remember how important it is to fully
    drain the buffer when using epoll in* *edge-triggered mode*.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'We match on the result of calling `TcpStream::read` since we want to take different
    actions based on the result:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: If we get `Ok(n)` and the value is 0, we’ve drained the buffer; we consider
    the event as handled and break out of the loop.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we get `Ok(n)` with a value larger than 0, we read the data to a `String`
    and print it out with some formatting. We do not break out of the loop yet since
    we have to call `read` until 0 is returned (or an error) to be sure that we’ve
    drained the buffers fully.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we get `Err` and the error is of the `io::ErrorKind::WouldBlock` type, we
    simply break out of the loop. We don’t consider the event handled yet since `WouldBlock`
    indicates that the data transfer is not complete, but there is no data ready right
    now.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we get any other error, we simply return that error and consider it a failure.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: There is one more error condition you’d normally want to cover, and that is
    `io::ErrorKind::Interrupted`. Reading from a stream could be interrupted by a
    signal from the operating system. This should be expected and probably not considered
    a failure. The way to handle this is the same as what we do when we get an error
    of the `WouldBlock` type.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: If the `read` operation is successful, we return the number of events handled.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Be careful with using TcpStream::read_to_end
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: You should be careful with using `TcpStream::read_to_end` or any other function
    that fully drains the buffer for you when using non-blocking buffers. If you get
    an error of the `io::WouldBlock` type, it will report that as an error even though
    you had several successful reads before you got that error. You have no way of
    knowing how much data you read successfully other than observing any changes to
    the `&mut Vec` you passed in.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we run our program, we should get the following output:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As you see, the responses are sent in reverse order. You can easily confirm
    this by looking at the output on the terminal on running the `delayserver` instance.
    The output should look like this:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The ordering might be different sometimes as the server receives them almost
    simultaneously, and can choose to handle them in a slightly different order.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we track events on the stream with ID `4`:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: In `send_requests`, we assigned the ID `4` to the last stream we created.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Socket 4 sends a request to `delayserver`, setting a delay of 1,000 ms and a
    message of `request-4` so we can identify it on the server side.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We register socket 4 with the event queue, making sure to set the `epoll_data`
    field to `4` so we can identify on what stream the event occurred.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`delayserver` receives that request and delays the response for 1,000 ms before
    it sends an `HTTP/1.1 200 OK` response back, together with the message we originally
    sent.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`epoll_wait` wakes up, notifying us that an event is ready. In the `epoll_data`
    field of the `Event` struct, we get back the same data that we passed in when
    registering the event. This tells us that it was an event on stream 4 that occurred.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then read data from stream 4 and print it out.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this example, we’ve kept things at a very low level even though we used the
    standard library to handle the intricacies of establishing a connection. Even
    though you’ve actually made a raw HTTP request to your own local server, you’ve
    set up an epoll instance to track events on a `TcpStream` and you’ve used epoll
    and syscalls to handle incoming events.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: That’s no small feat – congratulations!
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Before we leave this example, I wanted to point out how few changes we need
    to make to have our example use mio as the event loop instead of the one we created.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: In the repository under `ch04/b-epoll-mio`, you’ll see an example where we do
    the exact same thing using mio instead. It only requires importing a few types
    from mio instead of our own modules and making *only five minor changes to* *our
    code*!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Not only have you replicated what mio does, but you pretty much know how to
    use mio to create an event loop as well!
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of epoll, kqueue, and IOCP is pretty simple at a high level, but
    the devil is in the details. It’s just not that easy to understand and get it
    working correctly. Even programmers who work on these things will often specialize
    in one platform (epoll/kqueue or Windows). It’s rare that one person will know
    all the intricacies of all platforms, and you could probably write a whole book
    about this subject alone.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'If we summarize what you’ve learned and got firsthand experience with in this
    chapter, the list is quite impressive:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: You learned a lot about how mio is designed, enabling you to go to that repository
    and know what to look for and how to get started on that code base much easier
    than before reading this chapter
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You learned a lot about making syscalls on Linux
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You created an epoll instance, registered events with it, and handled those
    events
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你创建了一个epoll实例，向其注册了事件，并处理了这些事件。
- en: You learned quite a bit about how epoll is designed and its API
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你对epoll的设计及其API有了相当多的了解。
- en: You learned about edge-triggering and level-triggering, which are extremely
    low-level, but useful, concepts to have an understanding of outside the context
    of epoll as well
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你了解了边缘触发和电平触发，这些是即使在epoll的上下文之外也是非常有用且非常低级的概念。
- en: You made a raw HTTP request
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你发起了一个原始的HTTP请求。
- en: You saw how non-blocking sockets behave and how error codes reported by the
    operating system can be a way of communicating certain conditions that you’re
    expected to handle
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你看到了非阻塞套接字的行为以及操作系统报告的错误代码可以是一种传达你预期要处理某些条件的方式。
- en: You learned that not all I/O is equally “blocking” by looking at DNS resolution
    and file I/O
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过观察DNS解析和文件I/O，你了解到并非所有的I/O都是同等“阻塞”的。
- en: That’s pretty good for a single chapter, I think!
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这对于一个章节来说已经相当不错了！
- en: If you dive deeper into the topics we covered here, you’ll soon realize that
    there are gotchas and rabbit holes everywhere – especially if you expand this
    example to abstract over epoll, kqueue, and IOCP. You’ll probably end up reading
    Linus Torvald’s emails on how edge-triggered mode was supposed to work on pipes
    before you know it.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你进一步深入研究我们在这里讨论的主题，你很快就会意识到到处都是陷阱和死胡同——特别是如果你将这个示例扩展到抽象epoll、kqueue和IOCP。你可能很快就会开始阅读林纳斯·托瓦兹关于边缘触发模式应该在管道上如何工作的电子邮件。
- en: At least you now have a good foundation for further exploration. You can expand
    on our simple example and create a proper event loop that handles connecting,
    writing, timeouts, and scheduling; you can dive deeper into kqueue and IOCP by
    looking at how `mio` solves that problem; or you can be happy that you don’t have
    to directly deal with it again and appreciate the effort that went into libraries
    such as `mio`, `polling`, and `libuv`.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 至少你现在有了进一步探索的良好基础。你可以在我们的简单示例基础上进行扩展，创建一个适当的事件循环来处理连接、写入、超时和调度；你可以通过查看`mio`如何解决这个问题来深入了解kqueue和IOCP；或者你可以高兴地发现你不必再次直接处理它，并欣赏像`mio`、`polling`和`libuv`这样的库所付出的努力。
- en: By this point, we’ve gained a lot of knowledge about the basic building blocks
    of asynchronous programming, so it’s time to start exploring how different programming
    languages create abstractions over asynchronous operations and use these building
    blocks to give us as programmers efficient, expressive, and productive ways to
    write our asynchronous programs.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经对异步编程的基本构建块有了很多了解，因此是时候开始探索不同的编程语言是如何在异步操作上创建抽象，并使用这些构建块为我们程序员提供高效、表达性和生产性的异步程序编写方式了。
- en: First off is one of my favorite examples, where we’ll look into how fibers (or
    green threads) work by implementing them ourselves.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是一个我最喜欢的例子，我们将通过自己实现它们来探究纤程（或绿色线程）是如何工作的。
- en: You’ve earned a break now. Yeah, go on, the next chapter can wait. Get a cup
    of tea or coffee and reset so you can start the next chapter with a fresh mind.
    I promise it will be both fun and interesting.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该休息一下了。是的，继续吧，下一章可以稍后再说。泡一杯茶或咖啡，让自己放松一下，以便以清醒的头脑开始下一章。我保证这将既有趣又引人入胜。

- en: Fearless Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency and parallelism are important parts of modern-day programming and
    Rust is perfectly equipped to deal with these challenges. The borrowing and ownership
    model is great for preventing data races (**anomalies**, as they are called in
    the database world) since variables are immutable by default and if mutability
    is required, there cannot be any other reference to the data. This makes any type
    of concurrency safe and less complex in Rust (compared to many other languages).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover several ways of employing concurrency to solve
    problems and will even look at futures, which are—at the time of writing—not part
    of the language yet. If you are reading this in the future (no pun intended),
    this may be part of the core language already and you can check out the *Asynchronous
    programming with futures* recipe for historical reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Moving data into new threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing multiple threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Message passing between threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared mutable states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making sequential code parallel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrent data processing in vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared immutable states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actors and asynchronous messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Async programming with futures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving data into new threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rust threads operate just like in any other language—in scopes. Any other scope
    (such as closures) can easily borrow the variables from the parent scope since
    it's easy to determine if and when variables are dropped. However, when spawning
    a thread, its lifetime, compared to its parent's lifetime, is impossible to know
    and therefore the reference can become invalid at any time.
  prefs: []
  type: TYPE_NORMAL
- en: To tackle this problem, the threaded scope can take ownership of its variables—the
    memory is **moved** into the thread's scope. Let's see how this is done!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow these steps to see how to move memory between threads:'
  prefs: []
  type: TYPE_NORMAL
- en: Use `cargo new simple-threads` to create a new application project and open
    the directory in Visual Studio Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edit `src/main.rs` and spawn a simple thread that does not move data into its
    scope. Since it''s the simplest form of a thread, let''s print something to the
    command line and wait:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s call the new function from within `fn main()`. Replace the `hello
    world` snippet with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run the code to see if it works:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s get some outside data into a thread. Add another function to `src/main.rs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To demonstrate what''s happening under the hood, we have left out the `move`
    keyword for now. Expand the `main` function with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Does it work? Let''s try `cargo run`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'There we have it: to get any kind of data into a threaded scope, we need to
    transfer ownership by moving the value into the scope using the `move` keyword.
    Let''s follow the compiler''s instructions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try again with `cargo run`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's go behind the scenes to understand the code better.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Threads in Rust behave a lot like regular functions: they can take ownership
    and operate on the same syntax as closures (`|| {}` is an empty/`noop` function
    without parameters). Therefore, we have to treat them like we treat functions
    and think of them in terms of ownership and borrowing, or more specifically: lifetimes.
    Passing a reference (the default behavior) into this thread function makes it
    impossible for the compiler to keep track of the validity of the reference, which
    is a problem for code safety. Rust solves this by introducing the `move` keyword.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the `move` keyword changes the default behavior of borrowing to moving
    the ownership of every variable into the scope. Hence, unless these values implement
    the `Copy` trait (like `i32`), or have a longer lifetime than the thread when
    borrowing (like the `'static` lifetime for `str` literals), they become unavailable
    to the thread's parent scope.
  prefs: []
  type: TYPE_NORMAL
- en: Giving back ownership also works just like in a function—via the `return` statement.
    The thread that waits for the other (using `join()`) can then retrieve the return
    value by unwrapping the `join()` result.
  prefs: []
  type: TYPE_NORMAL
- en: Threads in Rust are native threads for each operating system and have their
    own local state and execution stack. When they panic, only the thread stops, not
    the entire program.
  prefs: []
  type: TYPE_NORMAL
- en: We've successfully gone through moving data into new threads. Now let's move
    on to the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Managing multiple threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Single threads are great, but in reality, many use cases demand a wealth of
    threads to execute on a large-scale data set in parallel. This has been popularized
    by the map/reduce pattern, published several years ago, and is still a great way
    to process something distinct such as multiple files, rows in a database result,
    and many more in parallel. Whatever the source, as long as the processing is not
    inter-dependent, it can be chunked and **mapped**—both of which Rust can make
    easy and free of data-race conditions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll add some more threads to do map-style data processing.
    Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run `cargo new multiple-threads` to create a new application project and open
    the directory in Visual Studio Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In `src/main.rs`, add the following function on top of `main()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this function, we spawn a thread for each chunk that has been passed in.
    This thread only doubles the number and therefore the function returns `Vec<i32>`
    for each chunk containing the results of this transformation. Now we need to create
    input data and call the function. Let''s extend `main` to do that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'With `cargo run` we can now see the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's go behind the scenes to understand the code better.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Admittedly, working with multiple threads in Rust is just the same as if we
    were working on single threads since there are no convenient methods for joining
    a list of threads or similar. Instead, we can use the power of Rust's iterators
    to do that in an expressive way. With these functional constructs, the need for
    `for` loops can be replaced by a chain of functions that lazily process collections,
    which makes the code easier to handle and more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: After setting up the project in *step 1*, we implement a multithreaded function
    to apply an operation to every chunk. These chunks are simply parts of a vector,
    and an operation—a simple function that doubles the input variable in this example—can
    be done with any type of task. *Step 3* shows how to call the multithreaded `mapping`
    function and how to get results by using the `JoinHandle` in a future/promise
    ([http://dist-prog-book.com/chapter/2/futures.html](http://dist-prog-book.com/chapter/2/futures.html))
    way. *Step 4* then simply shows that it works as intended by outputting the doubled
    chunks as a flat list.
  prefs: []
  type: TYPE_NORMAL
- en: What is also interesting is the number of times we have had to clone data. Since
    passing data into the threads is only possible by moving the values into each
    thread's memory space, cloning is often the only way to work around these sharing
    issues. However, we'll cover a method similar to multiple `Rc` in a later recipe
    (*Shared immutable states*) in this chapter, so let's move on to the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Using channels to communicate between threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Message passing between threads has been an issue in many standard libraries
    and programming languages since many rely on the user to apply locking. This leads
    to deadlocks and is somewhat intimidating for newcomers, which is why many developers
    were excited when Go popularized the concept of channels, something that we can
    also find in Rust. Rust's channels are great for designing a safe, event-driven
    application in just a few lines of code without any explicit locking.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create a simple application that visualizes incoming values on the command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: Run `cargo new channels` to create a new application project and open the directory
    in Visual Studio Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, let''s get the basics out of the way. Open `src/main.rs` and add the
    imports and an `enum` structure to the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, inside the `main` function, we create a channel with the `mpsc::channel()`
    function along with two threads that take care of the sending. Afterward, we are
    going to use two threads to send messages to the main thread with a variable delay.
    Here''s the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Both of the threads are sending data to the channel, so what''s missing is
    the channel''s receiving end to take care of the input data. The receiver offers
    two functions, `recv()` and `recv_timeout()`, both of which block the calling
    thread until an item is received (or the timeout is reached). We are just going
    to print the character multiplied by the passed-in value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to use `rand` when we finally run the program, we still need to add
    it to `Cargo.toml` with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, let''s see how the program runs—it''s going to run infinitely. To stop
    it, press *Ctrl* + *C*. Run it with `cargo run`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How does this work? Let's go behind the scenes to understand the code better.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Channels are **multi-producer-single-consumer** data structures, consisting
    of many senders (with a lightweight clone) but only a single receiver. Under the
    hood, the channel does not lock but relies on an `unsafe` data structure that
    allows the detection and management of the state of the stream. The channel handles
    simply sending data across threads well and can be used to create an actor-style
    framework or a reactive map-reduce style data-processing engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is one example of how Rust does **fearless concurrency**: the data going
    in is owned by the channel until the receiver retrieves it, which is when a new
    owner takes over. The channel also acts as a queue and holds elements until they
    are retrieved. This not only frees the developer from implementing the exchange
    but also adds concurrency for regular queues for free as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create the channel in *step 3* of this recipe and pass the senders into
    different threads, which start sending the previously defined (in *step 2*) `enum`
    types for the receiver to print. This printing is done in *step 4* by looping
    over the blocking iterator with a three-second timeout. *Step 5* then shows how
    to add the dependency to `Cargo.toml`, and in *step 6* we see the output: multiple
    full lines with a random number of elements that are either asterisks (`*`) or
    pipes (`|`).'
  prefs: []
  type: TYPE_NORMAL
- en: We've successfully covered how to use channels to communicate between threads
    effortlessly. Now let's move on to the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing mutable states
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rust's ownership and borrowing model simplifies immutable data access and transfer
    considerably—but what about shared states? There are many applications that require
    mutable access to a shared resource from multiple threads. Let's see how this
    is done!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will create a very simple simulation:'
  prefs: []
  type: TYPE_NORMAL
- en: Run `cargo new black-white` to create a new application project and open the
    directory in Visual Studio Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open `src/main.rs` to add some code. First, we are going to need some imports
    and an `enum` to make our simulation interesting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to show a shared state between two threads, we obviously need a thread
    that works on something. This will be a coloring task, where each thread is only
    adding white to a vector if black was the previous element and vice versa. Thus,
    each thread is required to read and—depending on the output—write into a shared
    vector. Let''s look at the code that does this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'All that remains at this stage is to create multiple threads and hand them
    an `Arc` instance of the data to work on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run the code with `cargo run`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's go behind the scenes to understand the code better.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rust''s ownership principle is a double-edged sword: on the one hand, it protects
    from unintended consequences and enables compile-time memory management; on the
    other hand, mutable access is significantly more difficult to obtain. While it
    is more complex to manage, shared mutable access can be great for performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Arc` stands for **Atomic Reference Counter**. This makes them very similar
    to regular reference counters (`Rc`), with the exception that an `Arc` does its
    job with an *atomic increment*, which is thread-safe. Therefore, they''re the
    only choice for cross-threaded reference counting.'
  prefs: []
  type: TYPE_NORMAL
- en: In Rust, this is done in a way similar to interior mutability ([https://doc.rust-lang.org/book/ch15-05-interior-mutability.html](https://doc.rust-lang.org/book/ch15-05-interior-mutability.html)),
    but using `Arc` and `Mutex` types (instead of `Rc` and `RefCell`), where `Mutex` owns
    the actual part of the memory it restricts access to (in step 3's snippet, we
    create the `Vec` just like that). As shown in *step 2*, to obtain a mutable reference
    to the value, locking the `Mutex` instance is strictly required and it will only
    be returned after the returned data instance is dropped (for example, when the
    scope ends). Hence, it is important to keep the scope of the `Mutex` as small
    as possible (note the additional `{ ... }` in *step 2*)!
  prefs: []
  type: TYPE_NORMAL
- en: In many use cases, a channel-based approach can achieve the same goal without
    having to deal with `Mutex` and the fear of a deadlock occurring (when several
    `Mutex` locks wait for each other to unlock).
  prefs: []
  type: TYPE_NORMAL
- en: We've successfully learned how to use channels to share mutable states. Now
    let's move on to the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing in Rust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threading is great for in-process concurrency and certainly the preferred method
    of spreading workloads over multiple cores. Whenever other programs need to be
    called, or an independent, heavyweight task is required, sub-processes are the
    way to go. With the recent rise of orchestrator-type applications (Kubernetes,
    Docker Swarm, Mesos, and many others), managing child processes has become a more
    important topic as well. In this recipe, we will communicate with and manage child
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow these steps to create a simple application that searches the filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new project using `cargo new child-processes` and open it in Visual
    Studio Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On Windows, execute `cargo run` (the last step) from a PowerShell window, since
    it contains all the required binaries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After importing a few (standard library) dependencies, let''s write the basic
    `struct` to hold the result data. Add this on top of the `main` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The function calling the `find` binary (which does the actual searching) translates
    the results into the `struct` from *step 1*. This is what the function looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! Now we know how to call an external binary, pass arguments in, and forward
    any `stdout` output to the Rust program. How about writing into the external program''s
    `stdin`? We''ll add the following function to do that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To see it in action, we also need to call the functions in the `main()` part
    of the program. Replace the contents of the default `main()` function with the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we should see if it works by issuing `cargo run`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's go behind the scenes to understand the code better.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By using Rust's ability to run sub-processes and manipulate their inputs and
    outputs, it's easy to integrate existing applications into the new program's workflow.
    In *step 1*, we do exactly that by using the `find` program with parameters and
    parse the output into our own data structure.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 3*, we go further and send data into a sub-process and recover the
    same text (using `cat` in an echo-like fashion). You'll notice the parsing of
    a string in each function, which is required as Windows and Linux/macOS use different
    byte sizes to encode their characters (**UTF-16** and **UTF-8** respectively).
    Similarly, the `b"string"` transforms the literal into a byte-literal appropriate
    for the current platform.
  prefs: []
  type: TYPE_NORMAL
- en: The key ingredient for these operations is **piping**, an operation that is
    available on the command line using a `|` (**pipe**) symbol. We encourage you
    to try out other variants of the `Stdio` struct as well and see where they lead!
  prefs: []
  type: TYPE_NORMAL
- en: We've successfully learned about multiprocessing in Rust. Now let's move on
    to the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Making sequential code parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating highly concurrent applications from scratch is relatively simple in
    many technologies and languages. However, when multiple developers have to build
    on pre-existing work of some kind (legacy or not), creating these highly concurrent
    applications gets complicated. Thanks to API differences across languages, best
    practices, or technical limitations, existing operations on sequences cannot be
    run in parallel without in-depth analysis. Who would do that if the potential
    benefit is not significant? With Rust's powerful iterators, can we run operations
    in parallel without major code changes? Our answer is yes!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe shows you how to simply make an application run in parallel without
    massive effort using `rayon-rs` in just a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new project using `cargo new use-rayon --lib` and open it in Visual
    Studio Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open `Cargo.toml` to add the required dependencies to the project. We are going
    to build on `rayon` and use the benchmarking abilities of `criterion`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As an example algorithm, we are going to use merge sort, a sophisticated, divide-and-conquer
    algorithm similar to quicksort ([https://www.geeksforgeeks.org/quick-sort-vs-merge-sort/](https://www.geeksforgeeks.org/quick-sort-vs-merge-sort/)).
    Let''s start off with the sequential version by adding the `merge_sort_seq()` function
    to `src/lib.rs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The high-level view of merge sort is simple: split the collection in half until
    it''s impossible to do it again, then merge the halves back *in order*. The splitting
    part is done; what''s missing is the merging part. Insert this snippet into `lib.rs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we will have to import `rayon`, a crate for creating parallel applications
    with ease, and then add a changed, parallelized version of merge sort:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we add a modified version of merge sort:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Great—but can you spot the change? To make sure both variants deliver the same
    results, let''s add a few tests:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Run `cargo test` and you should see successful tests:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we are really interested in the benchmarks—will it be faster? For
    that, create a `benches` folder containing a `seq_vs_par.rs` file. Open the file
    and add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run `cargo bench`, we are getting actual numbers to compare parallel
    versus sequential implementations (the change refers to previous runs of the same
    benchmark):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's check what all of this means and pull back the curtains on the code.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`rayon-rs` ([https://github.com/rayon-rs/rayon](https://github.com/rayon-rs/rayon))
    is a popular data-parallelism crate that only requires a few modifications to
    introduce automatic concurrency into the code. In our example, we are using the
    `rayon::join` operation to create a parallel version of the popular merge sort
    algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: In *step 1*, we are adding dependencies for benchmarks (`[dev-dependencies]`)
    and to actually build the library (`[dependencies]`). But in *step 2* and *step
    3*, we are implementing a regular merge sort variation. Once we add the `rayon`
    dependency in *step 4*, we can add `rayon::join` in *step 5* to run each branch
    (to sorting of the left and right parts) in its own closure (`|/*no params*/|
    {/* do work */}`, or `|/*no params*/| /*do work*/` for short) in parallel *if
    possible*. The docs on `join` can be found at [https://docs.rs/rayon/1.2.0/rayon/fn.join.html](https://docs.rs/rayon/1.2.0/rayon/fn.join.html),
    go into the details about when it speeds things up.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 8*, we are creating a benchmark test as required by the criterion.
    The library compiles a file outside the `src/` directory to run within the benchmark
    harness and output numbers (as shown in *step 9*)—and in these numbers, we can
    see a slight but consistent improvement in performance just by adding one line
    of code. Within the benchmark file, we are sorting a copy of the same random vector
    (`thread_local!()` is somewhat akin to `static`) of 100,000 random numbers.
  prefs: []
  type: TYPE_NORMAL
- en: We've successfully learned how to make sequential code parallel. Now let's move
    on to the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent data processing in vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rust's `Vec` is a great data structure that is used not only for holding data
    but also as a management tool of sorts. In an earlier recipe (*Managing multiple
    threads*) in this chapter, we saw that when we captured the handles of multiple
    threads in `Vec` and then used the `map()` function to join them. This time, we
    are going to focus on concurrently processing regular `Vec` instances without
    additional overhead. In the previous recipe, we saw the power of `rayon-rs` and
    now we are going to use it to parallelize data processing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s use `rayon-rs` some more in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new project using `cargo new concurrent-processing --lib` and open
    it in Visual Studio Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '2 First, we have to add `rayon` as a dependency by adding a few lines to `Cargo.toml`.
    Additionally, the `rand` crate and criterion for benchmarking will be useful later
    on, so let''s add those as well and configure them appropriately:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are going to add a significant statistical error measure, that is,
    the sum of squared errors, open `src/lib.rs`. In its sequential incarnation, we
    simply iterate over the predictions and their original value to find out the difference,
    then square it, and sum up the results. Let''s add that to the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'That seems easily parallelizable, and `rayon` offers us just the tools for
    it. Let''s create almost the same code using concurrency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'While the differences to the sequential code are very subtle, the changes have
    a substantial impact on execution speed! Before we proceed, we should add some
    tests to see the results of actually calling the functions. Let''s start with
    the parallel version first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The sequential code should have the same results, so let''s duplicate the test
    for the sequential version of the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to check that everything works as expected, run `cargo test` in between:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'As an additional feat of `rayon`, let''s also add some more functions to `src/lib.rs`.
    This time, they are related to counting alphanumeric characters in `str`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see which performs better and let''s add a benchmark. To do that,
    create a `benches/` directory next to `src/` with a `seq_vs_par.rs` file. Add
    the following benchmark and helper functions to see what the speedups are. Let''s
    start with a few helpers that define the basic data the benchmark is processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we are going to create the benchmarks themselves:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create another benchmark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'With that available, run `cargo bench` and (after a while) check the outputs
    to see the improvements and timings (the changed part refers to the changes from
    the previous run of the same benchmark):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's go behind the scenes to understand the code better.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, `rayon-rs`—a fantastic library—has made roughly a 50% improvement in
    the benchmark performance (parallel versus sequential) by changing **a single
    line of code**.This is significant for many applications but in particular for
    machine learning, where the loss function of an algorithm is required to run hundreds
    or thousands of times during a training cycle. Cutting this time in half would
    immediately have a large impact on productivity.
  prefs: []
  type: TYPE_NORMAL
- en: In the first steps after setting everything up (*step 3*, *step 4*, and *step
    5*), we are creating a sequential and parallel implementation of the sum of squared
    errors ([https://hlab.stanford.edu/brian/error_sum_of_squares.html](https://hlab.stanford.edu/brian/error_sum_of_squares.html))
    with the only difference being `par_iter()` versus the `iter()` call including
    some tests. Then we add some—more common—counting functions to our benchmark suite,
    which we'll create and call in *step 7* and *step 8*. Again, the sequential and
    parallel algorithms work on exactly the same dataset every time to avoid any unfortunate incidents.
  prefs: []
  type: TYPE_NORMAL
- en: We've successfully learned how to process data concurrently in vectors. Now
    let's move on to the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Shared immutable states
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, when a program operates on multiple threads, the current version
    of settings and many more are available to the threads as a single point of truth.
    Sharing a state between threads is straightforward in Rust—as long as the variable
    is immutable and the types are marked as safe to share. In order to mark types
    as thread-safe, it's important that the implementation makes sure that accessing
    the information can be done without any kind of inconsistency occurring.
  prefs: []
  type: TYPE_NORMAL
- en: Rust uses two marker traits—`Send` and `Sync`—to manage these options. Let's
    see how.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In just a few steps, we''ll explore immutable states:'
  prefs: []
  type: TYPE_NORMAL
- en: Run `cargo new immutable-states` to create a new application project and open
    the directory in Visual Studio Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we''ll add the imports and a `noop` function to call to our `src/main.rs`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explore how different types can be shared across threads. The `mpsc::channel`
    type provides a great out-of-the-box example of a shared state. Let''s start off
    with a baseline that works as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'To see it working, execute `cargo build`. Any errors with respect to illegal
    state sharing will be found by the compiler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we''ll try the same thing with the receiver. Will it work? Add this to
    the `main` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Run `cargo build` to get a more extensive message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the receiver is only made for a single thread to fetch data out of the
    channel, it''s to be expected that this cannot be avoided using `Arc`. Similarly,
    it''s impossible to simply wrap `Rc` into `Arc` to make it available across threads.
    Add the following to see the error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '`cargo build` reveals the consequences again—an error about how the type is
    unable to be sent across threads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's go behind the scenes to understand the code better.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since this recipe actually failed to build and pointed to an error message in
    the last step, what happened? We learned about `Send` and `Sync`. These marker
    traits and the types of errors will cross your path in the most surprising and
    critical situations. Since they work seamlessly when they are present, we had
    to create a failing example to show you what magic they do and how.
  prefs: []
  type: TYPE_NORMAL
- en: In Rust, marker traits ([https://doc.rust-lang.org/std/marker/index.html](https://doc.rust-lang.org/std/marker/index.html)) signal
    something to the compiler. In the case of concurrency, it's the ability to be
    shared across threads. The `Sync` (shared access from multiple threads) and `Send` (ownership
    can transfer safely from one thread to another) traits are implemented for almost
    all default data structures, but if `unsafe` code is required, then the marker
    traits have to be added manually—which is also `unsafe`.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, most of the data structures will be able to inherit `Send` and `Sync` from
    their properties, which is what happens in *step 2* and *step 3*. Mostly, you'll
    wrap your instance in `Arc` as well for easier handling. However, multiple instances
    of `Arc` require their contained types to implement `Send` and `Sync`. In *step
    4* and *step 6*, we try to get the available types into `Arc`—without implementing
    either `Sync` or `Send`. *Step 5* and *step 7* show the compiler's error messages
    for either try.  If you want to know more and see how to add the `marker` trait
    ([https://doc.rust-lang.org/std/marker/index.html](https://doc.rust-lang.org/std/marker/index.html))
    to custom types, check out the documentation at [https://doc.rust-lang.org/nomicon/send-and-sync.html.](https://doc.rust-lang.org/nomicon/send-and-sync.html)
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know more about `Send` and `Sync`, sharing states in concurrent
    programs is less of a mystery. Let's move on to the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Handling asynchronous messages with actors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scalable architectures and asynchronous programming have led to a rise of actors
    and actor-based designs ([https://mattferderer.com/what-is-the-actor-model-and-when-should-you-use-it](https://mattferderer.com/what-is-the-actor-model-and-when-should-you-use-it)),
    facilitated by frameworks such as Akka ([https://akka.io/](https://akka.io/)).
    Regardless of Rust's powerful concurrency features, actors in Rust are still tricky
    to get right and they lack the documentation that many other libraries have. In
    this recipe, we are going to explore the basics of `actix`, Rust's actor framework,
    which was created after the popular Akka.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Implement an actor-based sensor data reader in just a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new binary application using `cargo new actors` and open the directory
    in Visual Studio Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Include the required dependencies in the `Cargo.toml` configuration file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Open `src/main.rs` to add the code before the `main` function. Let''s start
    with the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to create an actor system, we''ll have to think about the application''s
    structure. An actor can be thought of as a message receiver with a postbox where
    messages are piled up until they are processed. For simplicity, let''s mock up
    some sensor data mock as messages, each consisting of a `u64` timestamp and a
    `f32` value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'In a typical system, we would use an I/O loop to read from the sensor(s) in
    scheduled intervals. Since `actix` ([https://github.com/actix/actix/](https://github.com/actix/actix/))
    builds on Tokio ([https://tokio.rs/](https://tokio.rs/)), that can be explored
    outside this recipe. To simulate the fast reading and slow processing steps, we''ll
    implement it as a `for` loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take care of implementing the most important part: the actor''s message
    handling. `actix` requires you to implement the `Handler<T>` trait. Add the following
    implementation just before the `main` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `cargo run` to run the program and see how it generates artificial sensor
    data (press *Ctrl + C* if you don''t want to wait for it to finish):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's go behind the scenes to understand the code better.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The actor model solves the shortcomings of passing data around threads using
    an object-oriented approach. By utilizing an implicit queue for messages to and
    from actors, it can prevent expensive locking and corrupt states. There is extensive
    content on the topic, for example, in Akka's documentation at [https://doc.akka.io/docs/akka/current/guide/actors-intro.html](https://doc.akka.io/docs/akka/current/guide/actors-intro.html).
  prefs: []
  type: TYPE_NORMAL
- en: After preparing the project in the first two steps, *step 3* shows the implementation
    of the `Message` trait using a macro (`[#derive()]`). With that available, we
    proceed to set up the main *system*—the main loop that runs the actor scheduling
    and message passing behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: '`actix` uses `Arbiters` to run different actors and tasks. A regular Arbiter
    is basically a single-threaded event loop, helpful for working in a non-concurrent
    setting. `SyncArbiter`, on the other hand, is a multithreaded version that allows
    the use of actors across threads. In our case, we used three threads.'
  prefs: []
  type: TYPE_NORMAL
- en: In *step 5*, we see the required minimum implementation of a handler. Using
    `SyncArbiter` does not allow sending messages back via the return value, which
    is why the result is an empty tuple for now. The handler is also specific to the
    message type and the handle function simulates a long-running action by issuing `thread::sleep`—this
    only works because it's the only actor running in that particular thread.
  prefs: []
  type: TYPE_NORMAL
- en: We have only scraped the surface of what `actix` can do (leaving out the all-powerful
    Tokio tasks and streams). Check out their book ([https://actix.rs/book/actix/](https://actix.rs/book/actix/))
    on the topic and the examples in their GitHub repositories[.](https://tokio.rs)
  prefs: []
  type: TYPE_NORMAL
- en: We've successfully learned how to handle asynchronous messages with actors.
    Now let's move on to the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming with futures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using futures is a common technique in JavaScript, TypeScript, C#, and similar
    technologies—made popular by the addition of the `async`/`await` keywords in their
    syntax. In a nutshell, futures (or promises) is a function's guarantee that, at
    some point, the handle will be resolved and the actual value will be returned.
    However, there is no explicit time when this is going to happen—but you can schedule
    entire chains of promises that are resolved after each other. How does this work
    in Rust? Let's find out in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, `async`/`await` were under heavy development. Depending
    on when you are reading this book, the examples may have stopped working. In this
    case, we ask you to open an issue in the accompanying repository so we can fix
    the issues. For updates, check the Rust `async` working group's repository at [https://github.com/rustasync/team](https://github.com/rustasync/team).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a few steps, we''ll be able to use `async` and `await` in Rust for seamless
    concurrency:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new binary application using `cargo new async-await` and open the directory
    in Visual Studio Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As usual, when we are integrating a library, we''ll have to add the dependencies
    to `Cargo.toml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'In `src/main.rs`, we have to import the dependencies. Add the following lines
    at the top of the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The classic example is waiting for a web request to finish. This is notoriously
    difficult to judge since the web resources and/or the network in between is owned
    by someone else and might be down. `surf` ([https://github.com/rustasync/surf](https://github.com/rustasync/surf))
    is `async` by default and therefore requires using the `.await` syntax heavily.
    Let''s declare an `async` function to do the fetching:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need an `async main` function in order to call the `response_code()
    async` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see if the code works by running `cargo run` (a `200 OK` is expected):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '`async` and `await` have been worked on for a long time in the Rust community.
    Let''s see how this recipe works.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Futures (often called promises) are typically fully integrated into the language
    and come with a built-in runtime. In Rust, the team chose a more ambitious approach
    and left the runtime open for the community to implement (for now). Right now
    the two projects Tokio and Romio ([https://github.com/withoutboats/romio](https://github.com/withoutboats/romio))
    and `juliex` ([https://github.com/withoutboats/juliex](https://github.com/withoutboats/juliex))
    have the most sophisticated support for these futures. With the recent addition
    of `async`/`await` in the Rust syntax in the 2018 edition, it's only a matter
    of time until the various implementations mature.
  prefs: []
  type: TYPE_NORMAL
- en: After setting up the dependencies in *step 1*, *step 2* shows that we don't
    have to enable the `async` and `await` macros/syntax to use them in the code—this
    was a requirement for a long time. Then, we import the required crates. Coincidentally,
    a new async web library—called `surf`—was built by the Rust async working group
    while we were busy with this book. Since this crate was built fully asynchronous,
    we preferred it over more established crates such as `hyper` ([https://hyper.rs](https://hyper.rs)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In *step 3*, we declare an `async` function, which automatically returns a
    `Future` ([https://doc.rust-lang.org/std/future/trait.Future.html](https://doc.rust-lang.org/std/future/trait.Future.html))
    type and can only be called from within another `async` scope. *Step 4* shows
    the creation of such a scope with the `async` main function. Does it end there?
    No—the `#[runtime::main]` attribute gives it away: a runtime is seamlessly started
    and assigned to execute anything async.'
  prefs: []
  type: TYPE_NORMAL
- en: While the `runtime` crate ([https://docs.rs/runtime/0.3.0-alpha.7/runtime/](https://docs.rs/runtime/0.3.0-alpha.7/runtime/))
    is agnostic of the actual implementation, the default is a native runtime based
    on `romio` and `juliex` (check your `Cargo.lock` file), but you can also enable
    the much more feature-laden [tokio](https://tokio.rs) runtime to enable streams,
    timers, and so on to use on top of async.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the `async` functions, we can make use of the `await` keyword attached
    to a `Future` implementor ([https://doc.rust-lang.org/std/future/trait.Future.html](https://doc.rust-lang.org/std/future/trait.Future.html)),
    such as the `surf` request ([https://github.com/rustasync/surf/blob/master/src/request.rs#L563](https://github.com/rustasync/surf/blob/master/src/request.rs#L563)),
    where the runtime calls `poll()` until a result is available. This can also result
    in an error, which means that we have to handle errors as well, which is generally
    done with the `?` operator. `surf` also provides a generic `Exception` type ([https://docs.rs/surf/1.0.2/surf/type.Exception.html](https://docs.rs/surf/1.0.2/surf/type.Exception.html))
    alias to handle anything that might happen.
  prefs: []
  type: TYPE_NORMAL
- en: While there are some things that could still change in Rust's fast-moving ecosystem,
    using `async`/`await` is finally coming together without requiring highly unstable
    crates. Having that available is a significant boost to Rust's usefulness. Now,
    let's move on to another chapter.
  prefs: []
  type: TYPE_NORMAL

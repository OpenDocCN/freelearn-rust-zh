<html><head></head><body><div><p>&#13;
			<h1 id="_idParaDest-143"><em class="italic"><a id="_idTextAnchor142"/>Chapter 8</em>: Structuring an End-to-End Python Package in Rust</h1>&#13;
			<p>Now that we have covered enough Rust and <code>pyo3</code> to theoretically build a range of real-world solutions, we must be careful. It would not be good if we decided to reinvent the wheel in Rust and ended up with a slower outcome after coding the solution. Hence, understanding how to solve a problem and testing our implementation is important. In this chapter, we will be building a Python package using Rust that solves a simplified real-world problem and loads data from files to build a catastrophe model. We will structure the package in a manner where we can slot in extra functionality if our model gets more complex. Once we build our model, we will test it to see whether our implementation is worth it in terms of scaling and speed.</p>&#13;
			<p>In this chapter, we will cover the following topics:</p>&#13;
			<ul>&#13;
				<li>Breaking down a catastrophe modeling problem for our package</li>&#13;
				<li>Building an end-to-end solution as a package</li>&#13;
				<li>Utilizing and testing our package</li>&#13;
			</ul>&#13;
			<p>This chapter enables us to take what we have learned throughout the book and solve a real-world problem and handle data files. Testing our solution will also enable us to avoid spending too much time on a solution that will have a slower result, preventing us from potentially missing our shot at implementing Rust in Python systems at our place of work.</p>&#13;
			<h1 id="_idParaDest-144"><a id="_idTextAnchor143"/>Technical requirements</h1>&#13;
			<p>The code and data for this chapter can be found at<a href="https://github.com/PacktPublishing/Speed-up-your-Python-with-Rust/tree/main/chapter_eight"> https://github.com/PacktPublishing/Speed-up-your-Python-with-Rust/tree/main/chapter_eight</a>.</p>&#13;
			<h1 id="_idParaDest-145"><a id="_idTextAnchor144"/>Breaking down a catastrophe modeling problem for our package</h1>&#13;
			<p>The project that we are going to build is a catastrophe model. This is where we calculate the probability of a<a id="_idIndexMarker535"/> catastrophe such as a hurricane, flood, or terror attack happening in a particular geographical location. We could do this using longitude and latitude coordinates. However, if we are going to do this, it is going to take a lot of computational power and time with little benefit. For instance, if we were going to calculate the probability of the flooding at Charing Cross Hospital in London, we could use the coordinates <em class="italic">51.4869° N, 0.2195° W</em>.</p>&#13;
			<p>However, if we use the coordinates <em class="italic">51.4865° N, 0.2190° W,</em> we would still be hitting Charing Cross Hospital, despite us changing the coordinates by <em class="italic">0.0004° N, 0.0005° W.</em> We could change the coordinates even more and we would still be hitting Charing Cross Hospital. Therefore, we would be doing loads of computations to calculate repeatedly the probability of flooding of the same building, which is not efficient. To combat this, we can break down the locations into bins and give them a numerical value, as shown here:</p>&#13;
			<p>&#13;
				<div>&#13;
					<img src="img/Figure_8.01_B17720.jpg" alt="Figure 8.1 – Geographical bins for a catastrophe model of an island&#13;&#10;" width="582" height="582"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 8.1 – Geographical bins for a catastrophe model of an island</p>&#13;
			<p>Here, we can <a id="_idIndexMarker536"/>see that if a line of data in our model referred to bin <code>25</code>, this means that the line of data is referring to land in the middle of our island that we are concerned with. We can make our calculations even more efficient. For instance, we can see that the squares in <em class="italic">Figure 8.1</em> with the coordinates of <code>33</code>, <code>35</code>, <code>47</code>, and <code>49</code> and <code>1</code>, <code>2</code>, <code>8</code>, and <code>9</code> are in the sea. Therefore, the probability of flooding in these squares is zero because it is already water, and there is nothing that we care about in terms of flooding in the sea. Because we are merely mapping our calculations onto these bins, nothing is stopping us from redefining all of the bins inside these squares as one bin.</p>&#13;
			<p>Therefore, we must perform only one operation to calculate the risk of flooding in all our sea bins and that would be zero because the sea is already flooded. In fact, nothing is stopping us from sticking to square classifications for one bin. Bin number 1 could be all the squares that are 100% inside the sea, saving us a lot of time. We can also go the other way. We can make some of our bins more refined. For instance, areas near the coast might have more nuanced gradients of flooding, as a small distance closer to the sea could greatly increase the risk of flooding; therefore, we could break bin number 26 down into smaller bins. To avoid being dragged into the weeds, we will just refer to arbitrary bin numbers in our model data. Catastrophe modeling is its own subject, and we are merely using it to show how to build Rust Python packages that can solve real problems as opposed to trying to build the most accurate catastrophe model. Now that we understand how we map geographical data with probabilities, we can move on to the calculation of those probabilities.</p>&#13;
			<p>Like with<a id="_idIndexMarker537"/> the mapping of geographical data, probability calculations are more complex and nuanced than what we are going to cover in this book. Companies like OASISLMF work with academic departments at universities to model risks of catastrophes and the damage inflicted. However, there is an overarching theme that we must do when calculating these probabilities. We will have to calculate the total probability of damage using the probability of the event happening in the area, and the probability of the event causing damage. To do this, we must multiply these probabilities together. We also must break down the probability of the event happening at a certain intensity. For instance, a category one hurricane is less likely to cause damage to a building compared to a category five hurricane. Therefore, we are going to run these probability calculations for each intensity bin.</p>&#13;
			<p>We cannot go any further in designing our process without looking at the data that we have available. The data is in the form of <code>CSV</code> files and is available in our GitHub repository stated in the <em class="italic">Technical requirements</em> section. The first data file that we can inspect is the <code>footprint.csv</code> file. This file presents the probability of a catastrophe with a certain intensity happening in an area:</p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Table_8.1.jpg" alt="" width="1087" height="190"/>&#13;
				</p>&#13;
			</p>&#13;
			<p>Here, we can see that we have taken in a series of event IDs. We can merge the <code>footprint.csv</code> data with the event IDs we passed in. This enables us to map the event IDs that we passed in with an area, intensity, and probability of it happening. </p>&#13;
			<p>Now that we have merged our geographical data, we can now look at our damage data in the <code>vulnerability.csv</code> file:</p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Table_8.2.jpg" alt="" width="1106" height="175"/>&#13;
				</p>&#13;
			</p>&#13;
			<p>Looking at this, we can merge the damage data of the intensity bin ID, duplicating whatever we need. We then must multiply the probabilities to get the total probability. The flow can<a id="_idIndexMarker538"/> be summed up as follows:</p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Figure_8.02_B17720.jpg" alt="Figure 8.2 – Catastrophe model flow&#13;&#10;" width="738" height="756"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 8.2 – Catastrophe model flow</p>&#13;
			<p>Considering the data and flow, we can see that we now have events that have an intensity bin ID, damage bin ID, probability of the event happening in the area, and the probability of the event causing damage in a certain bin. These can then be passed on to another stage, which is the process of calculating financial losses. We will stop here, but we must remember that real-world applications need to adapt for expansion. For instance, there is interpolation. This is where we use a function to estimate the values across a bin, which is demonstrated here:</p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Figure_8.03_B17720.jpg" alt="" width="634" height="489"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 8.3 – Linear interpolation of a distribution</p>&#13;
			<p>Here, we <a id="_idIndexMarker539"/>can see that if we just use bins, our reading between <code>2</code> and <code>2.9</code> would be the same. We know that the distribution is increasing, so we use a simple linear function, and the value of our reading increases as the reading increases. There are other more complex functions we can use, but this can increase the accuracy of readings if the bins are too wide. While we will not be using interpolation in our example, it is a legitimate step that we might want to slot in later. Considering this, our processes need to be isolated.</p>&#13;
			<p>There is only one other thing that we must consider when designing our package, which is the storage of our model data. Our probabilities will be defined by an academic team that collected and analyzed a range of data sources and specific knowledge. For instance, damage to buildings requires structural engineering knowledge and knowledge of hurricanes. While we might expect our teams to update the models in later releases, we do not want the end user to easily manipulate data. We also do not want to hardcode the <a id="_idIndexMarker540"/>data into our Rust code; therefore, storing <code>CSV</code> files in our package would be useful for this demonstration. Considering this, our package should take the following structure:</p>&#13;
			<pre>├── Cargo.toml</pre>&#13;
			<pre>├── MANIFEST.in</pre>&#13;
			<pre>├── README.md</pre>&#13;
			<pre>├── flitton_oasis_risk_modelling</pre>&#13;
			<pre>│   ├── __init__.py</pre>&#13;
			<pre>│   ├── footprint.csv</pre>&#13;
			<pre>│   └── vulnerability.csv</pre>&#13;
			<pre>├── src</pre>&#13;
			<pre>│   ├── lib.rs</pre>&#13;
			<pre>│   ├── main.rs</pre>&#13;
			<pre>│   ├── footprint</pre>&#13;
			<pre>│   │   ├── mod.rs</pre>&#13;
			<pre>│   │   ├── processes.rs</pre>&#13;
			<pre>│   │   └── structs.rs</pre>&#13;
			<pre>│   └── vulnerabilities</pre>&#13;
			<pre>│       ├── mod.rs</pre>&#13;
			<pre>│       ├── processes.rs</pre>&#13;
			<pre>│       └── structs.rs</pre>&#13;
			<p>The structure should be familiar to you. In the preceding file structure, we can see that our merge processes for the probability of the event happening and the damage are in their own folders. Data structures for the process are housed in the <code>structs.rs</code> file and functions around the process are defined in the <code>processes.rs</code> file. The <code>flitton_oasis_risk_modelling</code> folder will house our compiled Rust code; therefore, our <code>CSV</code> files are also stored there.</p>&#13;
			<p>We state that we are storing our <code>CSV</code> files in the <code>MANIFEST.in</code> file. Our <code>lib.rs</code> file is where our interface between our Rust and Python is defined. Now that we have defined the process for our catastrophe model, we can move on to the next section of building our end-to-end package.</p>&#13;
			<h1 id="_idParaDest-146"><a id="_idTextAnchor145"/>Building an end-to-end solution as a package</h1>&#13;
			<p>In the <a id="_idIndexMarker541"/>previous section, we identified what we needed to do to build our catastrophe model package. We can achieve it with the following steps:</p>&#13;
			<ol>&#13;
				<li>Build a footprint merging process.</li>&#13;
				<li>Build a vulnerability and probability merging process.</li>&#13;
				<li>Build a Python interface in Rust.</li>&#13;
				<li>Build an interface in Python.</li>&#13;
				<li>Build package installation instructions.</li>&#13;
			</ol>&#13;
			<p>Before we build anything, we must define our dependencies in our <code>Cargo.toml</code> file with the following code:</p>&#13;
			<pre>[package]</pre>&#13;
			<pre>name = "flitton_oasis_risk_modelling"</pre>&#13;
			<pre>version = "0.1.0"</pre>&#13;
			<pre>authors = ["Maxwell Flitton &lt;maxwellflitton@gmail.com&gt;"]</pre>&#13;
			<pre>edition = "2018"</pre>&#13;
			<pre>[dependencies]</pre>&#13;
			<pre>csv = "1.1"</pre>&#13;
			<pre>serde = { version = "1", features = ["derive"] }</pre>&#13;
			<pre>[lib]</pre>&#13;
			<pre>name = "flitton_oasis_risk_modelling"</pre>&#13;
			<pre>crate-type=["rlib", "cdylib"]</pre>&#13;
			<pre>[dependencies.pyo3]</pre>&#13;
			<pre>version = "0.13.2"</pre>&#13;
			<pre>features = ["extension-module"]</pre>&#13;
			<p>Here, we <a id="_idIndexMarker542"/>can see that we are using the <code>csv</code> crate to load our data and the <code>serde</code> crate to serialize the data that we had loaded from the <code>CSV</code> file. With this approach, it is important that we start by coding the processes first. This enables us to know what we need when we get to building our interfaces. Considering this, we can start building our footprint merging process.</p>&#13;
			<h2 id="_idParaDest-147"><a id="_idTextAnchor146"/>Building a footprint merging process</h2>&#13;
			<p>Our<a id="_idIndexMarker543"/> footprint merging process is essentially loading our footprint data and merging it with our input IDs. Once this is done, we then return the data to be fed into another process. We initially need to <a id="_idIndexMarker544"/>build our data structs before we build our processes, as our processes will need them. We can build our footprint struct in the <code>src/footprint/structs.rs</code> file with the following code:</p>&#13;
			<pre>use serde::Deserialize;</pre>&#13;
			<pre>#[derive(Debug, Deserialize, Clone)]</pre>&#13;
			<pre>pub struct FootPrint {</pre>&#13;
			<pre>    pub event_id: i32,</pre>&#13;
			<pre>    pub areaperil_id: i32,</pre>&#13;
			<pre>    pub intensity_bin_id: i32,</pre>&#13;
			<pre>    pub probability: f32</pre>&#13;
			<pre>}</pre>&#13;
			<p>Here, we can see that we apply the <code>Deserialize</code> macro to the struct so that when we load <a id="_idIndexMarker545"/>data from the file, it can be directly loaded into our <code>FootPrint</code> struct. We will also want to clone our<a id="_idIndexMarker546"/> struct if similar multiple event IDs are being passed into our package. </p>&#13;
			<p>Now that we have our struct, we can build our merging process in our <code>src/footprint/processes.rs</code> file:</p>&#13;
			<ol>&#13;
				<li value="1">First, we have to define the imports we need with the following code:<pre>use std::error::Error;
use std::fs::File;
use csv;
use super::structs::FootPrint;</pre><p>We must remember that we did not define our struct in the <code>src/footprint/mod.rs</code> file, so this will not run yet, but we will define it in time before running our code. </p></li>&#13;
				<li>We can now build a function that will read a footprint from the file with the following code:<pre>pub fn read_footprint(mut base_path: String) -&gt; \
  Result&lt;Vec&lt;FootPrint&gt;, Box&lt;dyn Error&gt;&gt; {
    base_path.push_str("/footprint.csv");
    let file = File::open(base_path.as_str())?;
    let mut rdr = csv::Reader::from_reader(file);
    let mut buffer = Vec::new();
    
    for result in rdr.deserialize() {
        let record: FootPrint = result?;
        buffer.push(record);
    }
    Ok(buffer)
}</pre><p>Here, we <a id="_idIndexMarker547"/>can see our<a id="_idIndexMarker548"/> function requires the directory where our data file is housed. We then add the filename to the path, open the file, and pass it through the <code>from_reader</code> function. We then define an empty vector and add the data that we deserialize. We now have a vector of <code>FootPrint</code> structs, which we return. </p></li>&#13;
				<li>Now that we have our <code>load data</code> function, we can now build our <code>merge footprints</code> function in the same file with the following code:<pre>pub fn merge_footprint_with_events(event_ids: \
  Vec&lt;i32&gt;, 
      footprints: Vec&lt;FootPrint&gt;) -&gt; Vec&lt;FootPrint&gt; {
    let mut buffer = Vec::new();
    for event_id in event_ids {
        for footprint in &amp;footprints {
            if footprint.event_id == event_id {
                buffer.push(footprint.clone());
            }
        }
    }
    return buffer
}</pre><p>Here, we can see that we take in a vector of event IDs and a vector of <code>FootPrint</code> structs. We then loop through our event IDs. For each event, we then loop <a id="_idIndexMarker549"/>through all the <code>FootPrint</code> structs, adding the struct to our buffer if it matches<a id="_idIndexMarker550"/> the event ID. We then return the buffer meaning that we have merged all that we need. We do not need to code any more processes. To make them useful, we can build an interface in the <code>src/footprint/mod.rs</code> file. </p></li>&#13;
				<li>So, we must import what we need with the following code:<pre>pub mod structs;
pub mod processes;
use structs::FootPrint;
use processes::{merge_footprint_with_events, \
  read_footprint};</pre></li>&#13;
				<li>Now that we have imported all that we need, we can build our interface in the same file with the following code:<pre>pub fn merge_event_ids_with_footprint(event_ids: \
  Vec&lt;i32&gt;, 
       base_path: String) -&gt; Vec&lt;FootPrint&gt; {
    let foot_prints = \
      read_footprint(base_path).unwrap();
    return merge_footprint_with_events(event_ids, \
      foot_prints)
}</pre><p>Here, we<a id="_idIndexMarker551"/> merely accept the <a id="_idIndexMarker552"/>file path and event IDs and pass them through our processes, returning the results.</p></li>&#13;
			</ol>&#13;
			<p>With this, our footprint processes are built, meaning that we can move on to the next step of building the vulnerability merge processes.</p>&#13;
			<h2 id="_idParaDest-148"><a id="_idTextAnchor147"/>Building the vulnerability merge process</h2>&#13;
			<p>Now that<a id="_idIndexMarker553"/> we have merged <a id="_idIndexMarker554"/>our event IDs with our footprint data, we have a working map of the probabilities of certain events happening at certain intensities within a range of geographical locations. We can merge this with the probabilities of damage occurring due to the catastrophe by following these steps: </p>&#13;
			<ol>&#13;
				<li value="1">In this process, we must load the vulnerabilities and then merge them with our existing data. To facilitate this, we will have to build two structs – one for the data that is loaded from the file and another for the result after the merge. Because we are loading the data, we will need to use the <code>serde</code> crate. In our <code>src/vulnerabilities/structs.rs</code> file, we import it with the following code:<pre>use serde::Deserialize;</pre></li>&#13;
				<li>We then build our struct to load the file with the following code:<pre>#[derive(Debug, Deserialize, Clone)]
pub struct Vulnerability {
    pub vulnerability_id: i32,
    pub intensity_bin_id: i32,
    pub damage_bin_id: i32,
    pub probability: f32
}</pre><p>We must note here that the probability of the data we are loading is labeled under the <code>probability</code> field. This is the same with our <code>FootPrint</code> struct. Because of this, we<a id="_idIndexMarker555"/> must rename the <code>probability</code> field to avoid clashes during the merge. We also need to calculate the total probability. </p></li>&#13;
				<li>Considering <a id="_idIndexMarker556"/>this, our result after the merge takes the form of the following code:<pre>#[derive(Debug, Deserialize, Clone)]
pub struct VulnerabilityFootPrint {
    pub vulnerability_id: i32,
    pub intensity_bin_id: i32,
    pub damage_bin_id: i32,
    pub damage_probability: f32,
    pub event_id: i32,
    pub areaperil_id: i32,
    pub footprint_probability: f32,
    pub total_probability: f32
}</pre></li>&#13;
			</ol>&#13;
			<p>With this, our structs are complete and we can build our processes in our <code>src/vulnerabilities/processes.rs</code> file. Here, we are going to have two functions, reading the vulnerabilities, and then merging them with our model:</p>&#13;
			<ol>&#13;
				<li value="1">First, we must import everything that we need with the following code:<pre>use std::error::Error;
use std::fs::File;
use csv;
use crate::footprint::structs::FootPrint;
use super::structs::{Vulnerability, \
  VulnerabilityFootPrint};</pre><p>Here, we can see that we are relying on the <code>FootPrint</code> struct from our <code>footprint</code> module. </p></li>&#13;
				<li>Now that <a id="_idIndexMarker557"/>we have<a id="_idIndexMarker558"/> everything, we can build our first process, which is loading the data with the following code:<pre>pub fn read_vulnerabilities(mut base_path: String) \
  -&gt; Result&lt;Vec&lt;Vulnerability&gt;, Box&lt;dyn Error&gt;&gt; {
    base_path.push_str("/vulnerability.csv");
    let file = File::open(base_path.as_str())?;
    let mut rdr = csv::Reader::from_reader(file);
    let mut buffer = Vec::new();
    
    for result in rdr.deserialize() {
        let record: Vulnerability = result?;
        buffer.push(record);
    }
    Ok(buffer)
}</pre><p>Here, we can see that this is similar code to our loading process in our footprint module. Refactoring this into a generalized function would be a good exercise. </p></li>&#13;
				<li>Now that we have our loading function, we can merge <code>Vec&lt;Vulnerability&gt;</code> with <code>Vec&lt;FootPrint&gt;</code> to get <code>Vec&lt;VulnerabilityFootPrint&gt;</code>. We can define the function with the following code:<pre>pub fn merge_footprint_with_vulnerabilities(
  vulnerabilities: Vec&lt;Vulnerability&gt;, 
  footprints: Vec&lt;FootPrint&gt;) -&gt; \
    Vec&lt;VulnerabilityFootPrint&gt; {
    let mut buffer = Vec::new();
    for vulnerability in &amp;vulnerabilities {
        for footprint in &amp;footprints {
            if footprint.intensity_bin_id == \
              vulnerability
                .intensity_bin_id {
                . . .
            }
        }
    }
    return buffer
}</pre><p>Here, we<a id="_idIndexMarker559"/> can see that we <a id="_idIndexMarker560"/>have a new vector called <code>buffer</code>, which is where the merged data will be stored in the <code>. . .</code> placeholder. We can see that we loop through the footprints for each vulnerability. If <code>intensity_bin_id</code> matches, we execute the code in the <code>. . .</code> placeholder, which is the following code:</p><pre>buffer.push(VulnerabilityFootPrint{
    vulnerability_id: vulnerability.vulnerability_id,
    intensity_bin_id: vulnerability.intensity_bin_id,
    damage_bin_id: vulnerability.damage_bin_id,
    damage_probability: vulnerability.probability,
    event_id: footprint.event_id,
    areaperil_id: footprint.areaperil_id,
    footprint_probability: footprint.probability,
    total_probability: footprint.probability * \
      vulnerability.probability
                });</pre><p>Here, we are merely mapping the correct values to the correct fields of our <code>VulnerabilityFootPrint</code> struct. In the last field, we calculate the total probability by multiplying the other probabilities together.</p></li>&#13;
			</ol>&#13;
			<p>Our <a id="_idIndexMarker561"/>processes are finally done, so we<a id="_idIndexMarker562"/> move on to building our interface for this process in our <code>src/vulnerabilities/mod.rs</code> file:</p>&#13;
			<ol>&#13;
				<li value="1">We first import what we need with the following code:<pre>pub mod structs;
pub mod processes;
use structs::VulnerabilityFootPrint;
use processes::{merge_footprint_with_vulnerabilities \
    ,read_vulnerabilities};
use crate::footprint::structs::FootPrint;</pre><p>With this, we can create a function that takes in a base path for the directory of where our data files are and our footprint data.</p></li>&#13;
				<li>We then pass <a id="_idIndexMarker563"/>them through both of our processes, loading and merging, and then<a id="_idIndexMarker564"/> return our merged data with the following code:<pre>pub fn merge_vulnerabilities_with_footprint( \
  footprint: Vec&lt;FootPrint&gt;, mut base_path: String) \
    -&gt; Vec&lt;VulnerabilityFootPrint&gt; {
    let vulnerabilities = read_vulnerabilities( \
      base_path).unwrap();
    return merge_footprint_with_vulnerabilities( \
      vulnerabilities, footprint)
}</pre></li>&#13;
			</ol>&#13;
			<p>We have now built our two processes for constructing our data model. We can move on to our next step, which is building our Python interface in Rust.</p>&#13;
			<h2 id="_idParaDest-149"><a id="_idTextAnchor148"/>Building a Python interface in Rust</h2>&#13;
			<p>The Python interface<a id="_idIndexMarker565"/> is defined in the <code>src/lib.rs</code> file, where we use the <code>pyo3</code> crate to get our Rust code to <a id="_idIndexMarker566"/>communicate with the Python system. Here are the steps:</p>&#13;
			<ol>&#13;
				<li value="1">First, we must import what we need with the following code:<pre>use pyo3::prelude::*;
use pyo3::wrap_pyfunction;
use pyo3::types::PyDict;
mod footprint;
mod vulnerabilities;
use footprint::merge_event_ids_with_footprint;
use vulnerabilities::merge_vulnerabilities_with_footprint;
use vulnerabilities::structs::VulnerabilityFootPrint; </pre><p>Here, we can see that we import what we need from the <code>pyo3</code> crate. We will be wrapping a <code>get_model</code> function with <code>wrap_pyfunction</code> and returning a list of <code>PyDict</code> structs. We <a id="_idIndexMarker567"/>also define the process modules, structs, and functions that we need to build our model. </p></li>&#13;
				<li>We <a id="_idIndexMarker568"/>can then define our function with the following code:<pre>#[pyfunction]
fn get_model&lt;'a&gt;(event_ids: Vec&lt;i32&gt;, \
    mut base_path: String, py: Python) -&gt; Vec&lt;&amp;PyDict&gt; {
    let footprints = merge_event_ids_with_footprint( \
      event_ids, base_path.clone());
    let model = merge_vulnerabilities_with_footprint \
      (footprints, base_path);
   let mut buffer = Vec::new();
   
   for i in model {
       . . .
   }
   return buffer
}</pre><p>It must be noted that we accept a <code>Python</code> struct into our function. This is automatically filled. If we get the <code>Python</code> struct via the <code>Python</code> struct, we <a id="_idIndexMarker570"/>can return the Python structures that we create in the function using the <code>Python</code> struct that we took in.</p></li>&#13;
				<li>In the <code>. . .</code> placeholder, we create a <code>PyDict</code> struct with all the data for the model row and<a id="_idIndexMarker571"/> push it to our buffer with the following code:<pre>    let placeholder = PyDict::new(py);
    placeholder.set_item("vulnerability_id", \
      i.vulnerability_id);
    placeholder.set_item("intensity_bin_id", \
      i.intensity_bin_id);
    placeholder.set_item("damage_bin_id", \
      i.damage_bin_id);
    placeholder.set_item("damage_probability",\ 
      i.damage_probability);
    placeholder.set_item("event_id", \
      i.event_id);
    placeholder.set_item("areaperil_id",\ 
      i.areaperil_id);
    placeholder.set_item("footprint_probability", \
      i.footprint_probability);
    placeholder.set_item("total_probability", \
      i.total_probability);
       buffer.push(placeholder);</pre><p>Here, we <a id="_idIndexMarker572"/>can see that we can push different types to our <code>PyDict</code> struct and Rust does not care. </p></li>&#13;
				<li>We can <a id="_idIndexMarker573"/>then wrap our function and define our module with the following code:<pre>#[pymodule]
fn flitton_oasis_risk_modelling(_py: Python, \
    m: &amp;PyModule) -&gt; PyResult&lt;()&gt; {
    m.add_wrapped(wrap_pyfunction!(get_model));
    Ok(())
}</pre></li>&#13;
			</ol>&#13;
			<p>Now that all our Rust programming is done, we can move on to building our Python interface in the next step.</p>&#13;
			<h2 id="_idParaDest-150"><a id="_idTextAnchor149"/>Building our interface in Python</h2>&#13;
			<p>When it comes to our Python interface, we will have to build a function in a Python script in the <code>flitton_oasis_rist_modelling/__init__.py</code> file. We also store our<a id="_idIndexMarker574"/> data <code>CSV</code> files in the <code>flitton_oasis_rist_modelling</code> directory. Remember, we do not want our users interfering with the <code>CSV</code> files or having to know where they are. To do this, we will use the <code>os</code> Python<a id="_idIndexMarker575"/> module to find the directory of our module to load our <code>CSV</code> data.</p>&#13;
			<p>To do this, we import what we need in the <code>flitton_oasis_rist_modelling/__init__.py</code> file with the following code:</p>&#13;
			<pre>import os </pre>&#13;
			<pre>from .flitton_oasis_risk_modelling import *</pre>&#13;
			<p>Remember, our Rust code will compile into a binary and be stored in the <code>flitton_oasis_rist_modelling</code> directory, so we can do a relative import for all the wrapped<a id="_idIndexMarker576"/> functions in our Rust code. Now, we can code our <code>construct_model</code> model function with the following code:</p>&#13;
			<pre>def construct_model(event_ids):</pre>&#13;
			<pre>    dir_path = os.path.dirname(os.path.realpath(__file__))</pre>&#13;
			<pre>    return get_model(event_ids, str(dir_path))</pre>&#13;
			<p>Here, we <a id="_idIndexMarker577"/>can see that all the user needs to do is pass in the event IDs. However, if we tried to install this package using <code>pip</code>, we would get errors stating that the <code>CSV</code> files cannot be found; this is because our setup does not include the data files. We can solve this in our next step of building package installation instructions.</p>&#13;
			<h2 id="_idParaDest-151"><a id="_idTextAnchor150"/>Building package installation instructions</h2>&#13;
			<p>To do this, we <a id="_idIndexMarker578"/>must <a id="_idIndexMarker579"/>state that we want to keep all <code>CSV</code> files in our <code>MANIFEST.in</code> file with the following code:</p>&#13;
			<pre>recursive-include flitton_oasis_risk_modelling/*.csv</pre>&#13;
			<p>Now that we have done this, we can move to our <code>setup.py</code> file to define our setup:</p>&#13;
			<ol>&#13;
				<li value="1">First, we must import what we need with the following code:<pre>#!/usr/bin/env python
from setuptools import dist
dist.Distribution().fetch_build_eggs([ \
  'setuptools_rust'])
from setuptools import setup
from setuptools_rust import Binding, RustExtension</pre><p>Here, as we have done before, we fetch the <code>setuptools_rust</code> package; although it is not essential for the running of the package, it is needed for the installation. </p></li>&#13;
				<li>We can<a id="_idIndexMarker580"/> now define our setup parameters with the following code:<pre>setup(
    name="flitton-oasis-risk-modelling",
    version="0.1",
    rust_extensions=[RustExtension(
    ".flitton_oasis_risk_modelling.flitton_oasis \
      _risk_modelling",
       path="Cargo.toml", binding=Binding.PyO3)],
    packages=["flitton_oasis_risk_modelling"],
    include_package_data=True,
    package_data={'': ['*.csv']},
    zip_safe=False,
)</pre><p>Here, we<a id="_idIndexMarker581"/> can see that we do not need any Python third-party packages. We have also defined our Rust extension, set the <code>include_package_data</code> parameter to <code>True</code>, and defined our package data with <code>package_data={'': ['*.csv']}</code>. With this, all <code>CSV</code> files will be kept when installing our package. </p></li>&#13;
				<li>We are nearly finished; all we have to do is define the <code>rustflags</code> environment variables in the <code>.cargo/config</code> file with the following code:<pre>[target.x86_64-apple-darwin]
rustflags = [
    "-C", "link-arg=-undefined",
    "-C", "link-arg=dynamic_lookup",
]
[target.aarch64-apple-darwin]
rustflags = [
    "-C", "link-arg=-undefined",
    "-C", "link-arg=dynamic_lookup",
]</pre><p>With this, we can upload our code and install it in our Python system.</p></li>&#13;
			</ol>&#13;
			<p>We can<a id="_idIndexMarker582"/> now use our Python module. We <a id="_idIndexMarker583"/>can test this in our module with the terminal output, as follows:</p>&#13;
			<pre>&gt;&gt;&gt; from flitton_oasis_risk_modelling import </pre>&#13;
			<pre>  construct_model</pre>&#13;
			<pre>&gt;&gt;&gt; construct_model([1, 2])</pre>&#13;
			<pre>[{'vulnerability_id': 1, 'intensity_bin_id': 1, </pre>&#13;
			<pre>'damage_bin_id': 1, 'damage_probability': 0.44999998807907104, </pre>&#13;
			<pre>'event_id': 1, 'areaperil_id': 10, </pre>&#13;
			<pre>'footprint_probability': 0.4699999988079071, </pre>&#13;
			<pre>'total_probability': 0.21149998903274536}, </pre>&#13;
			<pre>{'vulnerability_id': 1, 'intensity_bin_id': 1, </pre>&#13;
			<pre>'damage_bin_id': 1, 'damage_probability': </pre>&#13;
			<pre>0.44999998807907104, </pre>&#13;
			<pre>'event_id': 2, 'areaperil_id': 20, </pre>&#13;
			<pre>'footprint_probability': 0.30000001192092896, </pre>&#13;
			<pre>'total_probability': 0.13500000536441803}, </pre>&#13;
			<pre>{'vulnerability_id': 1, 'intensity_bin_id': 2, </pre>&#13;
			<pre>'damage_bin_id': 2, 'damage_probability': </pre>&#13;
			<pre>0.6499999761581421, </pre>&#13;
			<pre>'event_id': 1, 'areaperil_id': 10, </pre>&#13;
			<pre>'footprint_probability': 0.5299999713897705, </pre>&#13;
			<pre>'total_probability': 0.34449997544288635}, </pre>&#13;
			<pre>{'vulnerability_id': 1, 'intensity_bin_id': 2, </pre>&#13;
			<pre>'damage_bin_id': 2, </pre>&#13;
			<pre>'damage_probability': 0.6499999761581421, 'event_id': 2, </pre>&#13;
			<pre>'areaperil_id': 20, 'footprint_probability': </pre>&#13;
			<pre>0.699999988079071, </pre>&#13;
			<pre>'total_probability': 0.45499998331069946},</pre>&#13;
			<pre>. . .</pre>&#13;
			<p>There's more <a id="_idIndexMarker584"/>data that is printed out, but if your printout correlates with the preceding output, then <a id="_idIndexMarker585"/>there is a high chance that the rest of your data is accurate. Here, we have built a real-world solution that loads data and does a series of operations and processes to come up with a model. However, it is a basic model that would not be used in real-life catastrophe modeling; we have coded it in isolated modules so that we can slot in more processes when we need to.</p>&#13;
			<p>However, we need to ensure that all our effort was not for nothing. We can do what we did in this package with a few lines of Python code using pandas, which is written in C, so it could be quicker or at the same speed. Considering this, we need to test to ensure that we are not wasting our time by testing our code in the next section.</p>&#13;
			<h1 id="_idParaDest-152"><a id="_idTextAnchor151"/>Utilizing and testing our package</h1>&#13;
			<p>We <a id="_idIndexMarker586"/>have started building out our solution in a Python package coded in Rust. However, we need to justify to our team and ourselves that all this <a id="_idIndexMarker587"/>effort was worth it. We can test to see whether we should continue with our efforts in a single isolated Python script. In this Python script, we can test by following these steps:</p>&#13;
			<ol>&#13;
				<li value="1">Build a Python construct model using pandas.</li>&#13;
				<li>Build random event ID generator functions.</li>&#13;
				<li>Time our Python and Rust implementations with a series of different data sizes.</li>&#13;
			</ol>&#13;
			<p>Once we have<a id="_idIndexMarker588"/> carried out all the aforementioned steps, we will know whether we should progress further with our module.</p>&#13;
			<p>In our<a id="_idIndexMarker589"/> testing script, before we start coding anything, we must import all of what we need with the following code:</p>&#13;
			<pre>import random</pre>&#13;
			<pre>import time</pre>&#13;
			<pre>import matplotlib.pyplot as plt</pre>&#13;
			<pre>import pandas as pd</pre>&#13;
			<pre>from flitton_oasis_risk_modelling import construct_model</pre>&#13;
			<p>Here, we are using the <code>random</code> module to generate random event IDs and the <code>time</code> module to time our implementations. We are using <code>pandas</code> to build our model, <code>matplotlib</code> to plot the outcomes, and our Rust implementation. We can now build our model.</p>&#13;
			<h2 id="_idParaDest-153"><a id="_idTextAnchor152"/>Building a Python construct model using pandas</h2>&#13;
			<p>Now that we <a id="_idIndexMarker590"/>have imported <a id="_idIndexMarker591"/>everything that we need, we can move on to loading data from the CSV files in Python and use it to construct a model in Python using pandas with the following steps:</p>&#13;
			<ol>&#13;
				<li value="1">First, our function must take in event IDs. We also must load our data from our <code>CSV</code> files with the following code:<pre>def python_construct_model(event_ids):
    vulnerabilities = \
      pd.read_csv("./vulnerability.csv")
    foot_print = pd.read_csv("./footprint.csv")
    event_ids = pd.DataFrame(event_ids)</pre></li>&#13;
				<li>Now <a id="_idIndexMarker592"/>that we <a id="_idIndexMarker593"/>have all our data, we can merge our data and rename the <code>probability</code> column to avoid clashing with the following code:<pre>    model = pd.merge(
        event_ids, foot_print, how="inner", \
          on="event_id"
    )
    model.rename(
        columns={"probability": \
          "footprint_probability"},
        inplace=True
    )</pre><p>Here, we can see that we are using less code.</p></li>&#13;
				<li>Now, we can do our final process, which is merging with the vulnerabilities and then calculating the total probability with the following code:<pre>    model = pd.merge(
        model, vulnerabilities,
        how="inner", on="intensity_bin_id"
    )
    model.rename(
        columns={"probability": \
          "vulnerability_probability"},
        inplace=True
    )
    model["total_prob"] = \
      model["footprint_probability"] * \
        model["vulnerability_probability"]
    return model</pre></li>&#13;
			</ol>&#13;
			<p>With this, our <a id="_idIndexMarker594"/>Python model <a id="_idIndexMarker595"/>is now complete. We can now move on to our next step of building our random event ID generator functions.</p>&#13;
			<h2 id="_idParaDest-154"><a id="_idTextAnchor153"/>Building a random event ID generator function</h2>&#13;
			<p>When it <a id="_idIndexMarker596"/>comes to our Rust implementation, we need a list of integers. For our Python model, we need<a id="_idIndexMarker597"/> to pass in a list of dictionaries with an event ID stored in it. We can define these functions with the following code:</p>&#13;
			<pre>def generate_event_ids_for_python(number_of_events):</pre>&#13;
			<pre>    return [{"event_id": random.randint(1, 4)} for _</pre>&#13;
			<pre>            in range(0, number_of_events)]</pre>&#13;
			<pre>def generate_event_ids_for_rust(number_of_events):</pre>&#13;
			<pre>    return [random.randint(1, 4) for _</pre>&#13;
			<pre>            in range(0, number_of_events)]</pre>&#13;
			<p>Now that we have everything we need, we can carry out the final step of testing our implementations.</p>&#13;
			<h2 id="_idParaDest-155"><a id="_idTextAnchor154"/>Timing our Python and Rust implementations with a series of different data sizes</h2>&#13;
			<p>We now have<a id="_idIndexMarker598"/> everything we need to test our Rust and Python<a id="_idIndexMarker599"/> implementation. Running both Python and Rust models with timing can be done by carrying out the following steps:</p>&#13;
			<ol>&#13;
				<li value="1">To test our implementation, we define our entry point and all the data structures for our time graph with the following code:<pre>if __name__ == "__main__":
    x = []
    python_y = []
    rust_y = []</pre></li>&#13;
				<li>For our testing data, we are going to loop through a list of integers from <code>10</code> to <code>3000</code> in steps of <code>10</code> with the following code:<pre>    for i in range(10, 3000, 10):
        x.append(i)</pre><p>Both Python and Rust implementations will be running the same event ID dataset sizes, which is why we only have one <code>x</code> vector. We can now test our Python implementation with the following code:</p><pre>        python_event_ids = \
          generate_event_ids_for_python(
            number_of_events=i
        )
        python_start = time.time()
        python_construct_model(event_ids= \
          python_event_ids)
        python_finish = time.time()
        python_y.append(python_finish - python_start)</pre><p>Here, we generate our ID dataset to the size of the integer of the loop. We then start <a id="_idIndexMarker600"/>our timer, construct our model in Python, finish the timer, and add the time taken to our Python data list.</p></li>&#13;
				<li>We take the same approach with our Rust test with the following code:<pre>        rust_event_ids = generate_event_ids_for_rust(
            number_of_events=i
        )
        rust_start = time.time()
        construct_model(rust_event_ids)
        rust_finish = time.time()
        rust_y.append(rust_finish - rust_start)</pre><p>Our data collection is now complete. </p></li>&#13;
				<li>All we need to do is plot the results when the loop has finished with the following code:<pre>    plt.plot(x, python_y)
    plt.plot(x, rust_y)
    plt.show()</pre></li>&#13;
			</ol>&#13;
			<p>We have now written all the code for testing, which should display a graph like this:</p>&#13;
			<p>&#13;
				<p>&#13;
					<img src="img/Figure_8.04_B17720.jpg" alt="Figure 8.4 – Rust versus Python for the time taken for model generation for the size of data&#13;&#10;" width="1124" height="808"/>&#13;
				</p>&#13;
			</p>&#13;
			<p class="figure-caption">Figure 8.4 – Rust versus Python for the time taken for model generation for the size of data</p>&#13;
			<p>In the preceding figure, we see that initially, our Rust implementation is faster than our Python pandas implementation. However, once we get past the 1,300 mark, our Rust model gets slower than our Python pandas model. This is because our code does not scale well. We are performing loops within loops. In our pandas model, we vectorize our total <a id="_idIndexMarker601"/>probability. pandas is a well-written module where multiple developers have optimized the merge functions.</p>&#13;
			<p>Therefore, although our Rust code will be faster than Python and pandas code, if our implementation is sloppy and does not scale well, we may even be slowing down our program. I have seen poorly implemented C++ be beaten by Python pandas. Understanding this nuance is important when trying to implement Rust in your system. Rust is a new language, and colleagues will be let down if you promise big gains, poorly implement code, and result in slower performance after burning a lot of time coding implementation in Rust.</p>&#13;
			<p>Seeing that this is a book about building Python packages in Rust as opposed to data processing in Rust, this is where we stop. However, Xavier Tao implemented an efficient merge process in Rust, resulting in Rust taking 75% less time and 78% less memory. This is noted in the <em class="italic">Further reading</em> section. There is also a Rust implementation of pandas called <strong class="bold">Polars</strong>, which<a id="_idIndexMarker602"/> also has Python bindings. It is faster than standard pandas, and this documentation is also listed in the <em class="italic">Further reading</em> section.</p>&#13;
			<p>The takeaway <a id="_idIndexMarker603"/>message here is that Rust enables us to build fast memory-efficient solutions, but we must be careful with our implementation and test to see whether what we are doing is sensible. We should be careful, especially if we are trying to build a solution from scratch that has an optimized solution in an existing Python package.</p>&#13;
			<h1 id="_idParaDest-156"><a id="_idTextAnchor155"/>Summary</h1>&#13;
			<p>In this chapter, we went through the basics of building a simple catastrophe model. We then broke down the logic and converted it into steps so that we could build the catastrophe model in Rust. This included taking in paths, loading data from files, including data in our package, and building a Python interface so that our users do not have to know about what is going on under the hood when constructing a model. After all of this, we tested our module and ensured that we kept increasing the data size of the test to see how it scales. We saw that, initially, our Rust solution was faster because Rust is faster than Python and pandas. However, our implementation did not scale well, as we did a loop within a loop for our merge.</p>&#13;
			<p>As the data size increased, our Rust code ended up being slower. In previous chapters, we have shown multiple times that Rust implementations are generally faster. However, this does not counteract the effects of bad code implementation. If you are relying on a Python third-party module to perform a complex process, it probably is not a good idea to rewrite it in Rust for performance gains. If a Rust crate is not available for the same solution, then it is probably best to leave that part of the solution to the Python module.</p>&#13;
			<p>In the next chapter, we will be building a Flask web application to lay the groundwork for applying Rust to a Python web application.</p>&#13;
			<h1 id="_idParaDest-157"><a id="_idTextAnchor156"/>Further reading</h1>&#13;
			<ul>&#13;
				<li>Polars documentation for Rust Crate Polars (2021): <a href="https://docs.rs/polars/0.15.1/polars/frame/struct.DataFrame.html%0D">https://docs.rs/polars/0.15.1/polars/frame/struct.DataFrame.html</a></li>&#13;
				<li><em class="italic">Data Manipulation: Pandas vs Rust</em>, <em class="italic">Xavier Tao</em> (2021): <a href="https://able.bio/haixuanTao/data-manipulation-pandas-vs-rust--1d70e7fc">https://able.bio/haixuanTao/data-manipulation-pandas-vs-rust--1d70e7fc</a></li>&#13;
			</ul>&#13;
		</div>&#13;
	</div></body></html>
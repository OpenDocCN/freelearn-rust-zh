<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Futurism – Near-Term Rust</h1>
                </header>
            
            <article>
                
<p class="mce-root">We've <span>covered a lot of material in this</span> book<span>.</span><br/></p>
<p>If you already knew parallel programming well from another systems programming language, I hope that now you've got a confident handle on the way Rust operates. Its ownership model is strict, and can feel foreign at first. Unsafe Rust, if my own experiences are any kind of general guide, feels like more familiar ground. I hope that you too have come to appreciate the general safety of Rust, as well as its ability to do fiddly things in memory when needed. I find being able to implement unsafe code that is then wrapped up in a safe interface nothing short of incredible.</p>
<p>If you knew parallel programming fairly well from a high-level, garbage-collected programming language, then I hope that this book has served as an introduction to parallel programming at a systems level. As you know, memory safety does not guarantee correct operation, hence this book's continual focus on testing—generative and fuzz, in addition to performance inspection—and careful reasoning about models. Rust, I have argued, is one of the easier systems languages to do parallel programming well in because of the language's focus on ownership management with lifetimes and the strong, static type system. Rust is not immune to bugs—in fact, it's vulnerable to a wide variety—but these are common to all languages on modern hardware, and Rust does manage to solve a swath of memory-related issues.</p>
<p>If you already knew Rust, but didn't know much about parallel programming, I very much hope that this book has managed to convince you of one thing—c<span>oncurrency is not magic. Parallel programming is a wide, wide field of endeavor, one that takes a great deal of study, but it can be understood and mastered. Moreover, it need not be mastered all at once. The shape of this book argues for one path among several. May this book be the first of many on your journey.</span></p>
<p>Whatever your background, thank you for reading through to the end. I know that, at times, the material covered was not easy and it must have taken some real willpower to get through it all. I appreciate it. I wrote this book to be what I would have wanted for myself ten years ago. It was a real pleasure getting the opportunity to write it out for you.</p>
<p><span>By the end of this chapter, we will have:</span></p>
<ul>
<li><span>Discussed the future of SIMD in Rust</span></li>
<li><span>Discussed the future of async IO in Rust</span></li>
<li><span>Discussed the possibility of specialization being stablized and in what fashion</span></li>
<li><span>Discussed more extensive testing methods for Rust</span></li>
<li><span>Introduced various avenues to get involved with the Rust community</span></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>This chapter has no software requirements.</p>
<p>You can find the source code for this book's projects at <a href="https://github.com/PacktPublishing/Hands-On-Concurrency-with-Rust">https://github.com/PacktPublishing/Hands-On-Concurrency-with-Rust</a>. Chapter 10 has its source code under <kbd>Chapter10/</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Near-term improvements</h1>
                </header>
            
            <article>
                
<p>The focus of 2018's Rust development has been stabilization. Since the 1.0 days in 2015, many important crates were nightly-only, whether because<span> of modifications to the compiler or because of the fear of standardizing too quickly on awkward APIs. This fear was reasonable. At the time, Rust was a new language and it changed</span> <em>very</em> <span>drastically in the lead-up to 1.0. A new language takes time to resolve into its natural style. By the end of 2017, the community had come to a general feeling that a stabilization cycle was in order, that some natural expression of the language had more or less been established, and that in areas where this was not true, it could be established, with some work.</span></p>
<p>Let's discuss this stabilization work with regards to the topics we've followed throughout the book.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">SIMD</h1>
                </header>
            
            <article>
                
<p>In this book, we discussed thread-based concurrency. In <a href="d4802512-564b-4037-9407-b6035bd38f31.xhtml" target="_blank">Chapter 8</a>, <em>High-Level Parallelism – Threadpools, Parallel Iterators, and Processes</em>, we took to a higher level of abstraction with Rayon's parallel iterators. Underneath, as we saw, rayon uses a sophisticated work-stealing threadpool. Threads are merely one approach to concurrency on modern CPUs. In a sense, serial programs are parallel on CPUs with deep lookahead pipelines and sophisticated branch predictors, as we saw in <a href="8c61da2f-08c8-40be-89d4-d8afa8510850.xhtml" target="_blank">Chapter 2</a>, <em>Sequential Rust Performance and Testing</em>. Exploiting this parallelism is a matter of carefully structuring your data and managing access to it, among the other details we discussed. What we have not gone into in this book is how to exploit a modern CPUs' ability to perform the same operation on contiguous memory in parallel <em>without</em> resorting to threads—vectorization. We haven't looked at this because, as I write this, the language is only now getting minimal support in the standard library for vectorization instructions.</p>
<p>Vectorization comes in two variants—MIMD and SIMD. MIMD stands for multiple instructions, multiple data. SIMD stands for single instructions, single data. The basic idea is this: A modern CPU can apply an instruction to a contiguous, specifically sized block of data in parallel. That's it. Now, consider how many loops we've written in this book where we've done the exact same operation to every element of the loop. More than a few! Had we investigated string algorithms or looked into doing numeric calculations—encryption, physics simulations, or the like—we would have had more such loops in this book.</p>
<p>As I write this, the merger of SIMD vectorization into the standard library is held up behind RFC 2366 (<a href="https://github.com/rust-lang/rfcs/pull/2366">https://github.com/rust-lang/rfcs/pull/2366</a>), with ambitions to having x86 SIMD in a stable channel by year's end, with other architectures to follow. The reader may remember from <a href="5f3aec9d-fd53-48ff-9ba8-43ce13e91cff.xhtml" target="_blank">Chapter 1</a>,<em> Preliminaries – Machine Architecture and Getting Started with Rust,</em> that the memory systems of x86 and ARM are quite different. Well, this difference extends to their SIMD systems. It <em>is</em> possible to exploit SIMD instructions using stable Rust via the stdsimd (<a href="https://crates.io/crates/stdsimd">https://crates.io/crates/stdsimd</a>) and simd (<a href="https://crates.io/crates/simd">https://crates.io/crates/simd</a>) crates. The stdsimd crate is the best for programmers to target as it will eventually be the standard library implementation.</p>
<p>A discussion of SIMD is beyond the scope of this book, given the remaining <span>space</span><span>. Suffice it to say that getting the details right with SIMD is roughly on a par with the difficulty of getting the details right in atomic programming. I expect that, post stabilization, the community will build higher-level libraries to exploit SIMD approaches without requiring a great deal of pre-study, though potentially with less chance for finely tuned optimizations. The</span> faster (<a href="https://crates.io/crates/faster">https://crates.io/crates/faster</a>) <span>crate, for example, exposes SIMD-parallel iterators in the spirit of rayon. The programming model there, from an end-user perspective, is extremely convenient.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hex encoding</h1>
                </header>
            
            <article>
                
<p>Let's look at an example from the stdsimd crate. We won't dig in too deeply here; we're only out to get a sense of the structure of this kind of programming. We've pulled stdsimd at <kbd>2f86c75a2479cf051b92fc98273daaf7f151e7a1</kbd>. The file we'll examine is <kbd>examples/hex.rs</kbd>. The program's purpose is to <kbd>hex</kbd> encode stdin. For example:</p>
<pre><strong>&gt; echo hope | cargo +nightly run --release --example hex -p stdsimd
    Finished release [optimized + debuginfo] target(s) in 0.52s
     Running `target/release/examples/hex`
686f70650a</strong></pre>
<p>Note that this requires a nightly channel, and not a stable one, we've used so far in this book. I tested this on nightly, 2018-05-10. The details of the implementation are shifting so rapidly and rely on such unstable compiler features that you should make sure that you use the specific nightly channel that is listed, otherwise the example code may not compile.</p>
<p>The preamble to <kbd>examples/hex.rs</kbd> contains a good deal of information we've not seen in the book so far. Let's take it in two chunks and tease out the unknowns:</p>
<pre style="padding-left: 30px">#![feature(stdsimd)]
#![cfg_attr(test, feature(test))]
#![cfg_attr(feature = "cargo-clippy",
            allow(result_unwrap_used, print_stdout, option_unwrap_used,
                  shadow_reuse, cast_possible_wrap, cast_sign_loss,
                  missing_docs_in_private_items))]

#[cfg(any(target_arch = "x86", target_arch = "x86_64"))]
#[macro_use]
extern crate stdsimd;

#[cfg(not(any(target_arch = "x86", target_arch = "x86_64")))]
extern crate stdsimd;</pre>
<p>Because we've stuck to stable Rust, we've not seen anything like <kbd>#![feature(stdsimd)]</kbd> before. What is this? Rust allows the adventurous user to enable in-progress features that are present in rustc, but that are not yet exposed in the nightly variant of the language. I say adventurous because an in-progress feature may maintain a stable API through its development, or it may change every few days, requiring updates on the consumer side. Now, what is <kbd>cfg_attr</kbd>? This is a conditional compilation attribute, turning on features selectively. You can read all about it in <em>The Rust Reference</em>, linked in the <em>Further reading</em> section. There are many details, but the idea is simple—turn bits of code on or off depending on user directives—such as testing—or the running environment.</p>
<p>This later is what <kbd>#[cfg(any(target_arch = "x86", target_arch = "x86_64"))]</kbd> means. On either x86 or x86_64, stdsimd will be externed with macros enabled and without them enabled when on any other platform. Now, hopefully, the rest of the preamble is self-explanatory:</p>
<pre>#[cfg(test)]
#[macro_use]
extern crate quickcheck;

use std::io::{self, Read};
use std::str;

#[cfg(target_arch = "x86")]
use stdsimd::arch::x86::*;
#[cfg(target_arch = "x86_64")]
use stdsimd::arch::x86_64::*;</pre>
<p>The <kbd>main</kbd> function of this program is quite brief:</p>
<pre>fn main() {
    let mut input = Vec::new();
    io::stdin().read_to_end(&amp;mut input).unwrap();
    let mut dst = vec![0; 2 * input.len()];
    let s = hex_encode(&amp;input, &amp;mut dst).unwrap();
    println!("{}", s);
}</pre>
<p>The whole of stdin is read into the input and a <span>brief </span><span>vector to hold the hash, called <kbd>dst</kbd>. <kbd>hex_encode</kbd>, is also used. The first portion performs input validation:</span></p>
<pre>fn hex_encode&lt;'a&gt;(src: &amp;[u8], dst: &amp;'a mut [u8]) -&gt; Result&lt;&amp;'a str, usize&gt; {
    let len = src.len().checked_mul(2).unwrap();
    if dst.len() &lt; len {
        return Err(len);
    }</pre>
<p>Note the return—a <kbd>&amp;str</kbd> on success and a <kbd>usize</kbd>—the faulty length—on failure. The rest of the function is a touch more complicated:</p>
<pre>    #[cfg(any(target_arch = "x86", target_arch = "x86_64"))]
    {
        if is_x86_feature_detected!("avx2") {
            return unsafe { hex_encode_avx2(src, dst) };
        }
        if is_x86_feature_detected!("sse4.1") {
            return unsafe { hex_encode_sse41(src, dst) };
        }
    }

    hex_encode_fallback(src, dst)
}</pre>
<p>This introduces conditional compilation, which we saw previously, and contains a new thing too—<kbd>is_x86_feature_detected!</kbd>. This macro is new to the SIMD feature, and tests at runtime whether the given CPU feature is available at runtime via libcpuid. If it is, the feature is used. On an ARM processor, for instance, <kbd>hex_encode_fallback</kbd> will be executed. On an x86 processor, one or the other avx2 or sse4.1, implementations will be followed, depending on the chip's exact capabilities. Which implementation is called is determined at runtime. Let's look into the implementation of <kbd>hex_encode_fallback</kbd> first:</p>
<pre>fn hex_encode_fallback&lt;'a&gt;(
    src: &amp;[u8], dst: &amp;'a mut [u8]
) -&gt; Result&lt;&amp;'a str, usize&gt; {
    fn hex(byte: u8) -&gt; u8 {
        static TABLE: &amp;[u8] = b"0123456789abcdef";
        TABLE[byte as usize]
    }

    for (byte, slots) in src.iter().zip(dst.chunks_mut(2)) {
        slots[0] = hex((*byte &gt;&gt; 4) &amp; 0xf);
        slots[1] = hex(*byte &amp; 0xf);
    }

    unsafe { Ok(str::from_utf8_unchecked(&amp;dst[..src.len() * 2])) }
}</pre>
<p>The interior function <kbd>hex</kbd> acts as a static lookup table, and the for loop zips together the bytes from the <kbd>src</kbd> and a two-chunk pull from <kbd>dst</kbd>, updating <kbd>dst</kbd> as the loop proceeds. Finally, <kbd>dst</kbd> is converted into a <kbd>&amp;str</kbd> and returned. It's safe to use <kbd>from_utf8_unchecked</kbd> here as we can verify that no non-utf8 characters are possible in <kbd>dst</kbd>, saving us a check.</p>
<p>Okay, now that's the fallback. How do the SIMD variants read? We'll look at <kbd>hex_encode_avx2</kbd>, leaving the sse4.1 variant to the ambitious reader. First, we see the reappearance of conditional compilation and the familiar function types:</p>
<pre>#[target_feature(enable = "avx2")]
#[cfg(any(target_arch = "x86", target_arch = "x86_64"))]
unsafe fn hex_encode_avx2&lt;'a&gt;(
    mut src: &amp;[u8], dst: &amp;'a mut [u8]
) -&gt; Result&lt;&amp;'a str, usize&gt; {</pre>
<p>So far, so good. Now, look at the following:</p>
<pre>    let ascii_zero = _mm256_set1_epi8(b'0' as i8);
    let nines = _mm256_set1_epi8(9);
    let ascii_a = _mm256_set1_epi8((b'a' - 9 - 1) as i8);
    let and4bits = _mm256_set1_epi8(0xf);</pre>
<p>Well, the function <kbd>_mm256_set1_epi8</kbd> is certainly new! The documentation (<a href="https://rust-lang-nursery.github.io/stdsimd/x86_64/stdsimd/arch/x86_64/fn._mm256_set1_epi8.html">https://rust-lang-nursery.github.io/stdsimd/x86_64/stdsimd/arch/x86_64/fn._mm256_set1_epi8.html</a>) states the following:</p>
<div class="packt_quote">
<p>"Broadcast 8-bit integer a to all elements of returned vector."</p>
</div>
<p>Okay. What vector? Many SIMD instructions are carried out in terms of vectors with implicit sizes, the size being defined by the CPU architecture. For instance, the function that we have been looking at returns an <kbd>stdsimd::arch::x86_64::__m256i</kbd>, a 256-bit-wide vector. So, <kbd>ascii_zero</kbd> is 256-bits-worth of zeros, <kbd>ascii_a</kbd> 256-bits-worth of <kbd>a</kbd>, and so forth. Each of these functions and types, incidentally, follows Intel's naming scheme. It's my understanding that the LLVM uses their own naming scheme, but Rust—in an effort to reduce developer confusion—maintains a mapping from the architecture names to LLVM's names. By keeping the architecture names, Rust makes it real easy to look up details about each intrinsic. You can see this in the <em>Intel Intrinsics Guide</em> at <kbd>mm256_set1_epi8</kbd> (<a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set1_epi8&amp;expand=4676">https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set1_epi8&amp;expand=4676</a>).</p>
<p>The implementation then enters a loop, processing <kbd>src</kbd> in 256-bit-wide chunks until there are only 32 or fewer bits left:</p>
<pre>    let mut i = 0_isize;
    while src.len() &gt;= 32 {
        let invec = _mm256_loadu_si256(src.as_ptr() as *const _);</pre>
<p>The variable <kbd>invec</kbd> is now 256 bits of <kbd>src</kbd>, with <kbd>_mm256_loadu_si256</kbd> being responsible for performing a load. SIMD instructions have to operate on their native types, you see. In the actual reality of the machine, there is no type, but specialized registers. What comes next is complex, but we can reason through it:</p>
<pre>        let masked1 = _mm256_and_si256(invec, and4bits);
        let masked2 = _mm256_and_si256(<br/>            _mm256_srli_epi64(invec, 4),<br/>            and4bits<br/>        );</pre>
<p>Okay, <kbd>_mm256_and_si256</kbd> performs a bitwise boolean-and of two <kbd>__m256i</kbd>, returning the result. <kbd>masked1</kbd> is the bitwise boolean-and of <kbd>invec</kbd> and <kbd>and4bits</kbd>. The same is true for <kbd>masked2</kbd>, except we need to determine what <kbd>_mm256_srli_epi64</kbd> does. The Intel document state the following:</p>
<div class="packt_quote">
<p>"Shift packed 64-bit integers in a right by imm8 while shifting in zeros, and store the results in dst."</p>
</div>
<p><kbd>invec</kbd> is subdivided into four 64-bit integers, and these are shifted right by four bits. That is then boolean-and'ed with <kbd>and4bits</kbd>, and <kbd>masked2</kbd> pops out. If you refer back to <kbd>hex_encode_fallback</kbd> and squint a little, you can start to see the relationship to <kbd>(*byte &gt;&gt; 4) &amp; 0xf</kbd>. Next up, two comparison masks are put together:</p>
<pre>        let cmpmask1 = _mm256_cmpgt_epi8(masked1, nines);
        let cmpmask2 = _mm256_cmpgt_epi8(masked2, nines);</pre>
<p><kbd>_mm256_cmpgt_epi8</kbd> greater-than compares 8-bit chunks of the 256-bit vector, returning the larger byte. The comparison masks are then used to control the blending of <kbd>ascii_zero</kbd> and <kbd>ascii_a</kbd>, and the result of that is added to <kbd>masked1</kbd>:</p>
<pre>        let masked1 = _mm256_add_epi8(
            masked1,
            _mm256_blendv_epi8(ascii_zero, ascii_a, cmpmask1),
        );
        let masked2 = _mm256_add_epi8(
            masked2,
            _mm256_blendv_epi8(ascii_zero, ascii_a, cmpmask2),
        );</pre>
<p>Blending is done bytewise, with the high bit in each byte determining whether to move a byte from the first vector in the <kbd>arg</kbd> list or the second. The computed masks are then interleaved, choosing high and low halves:</p>
<pre>        let res1 = _mm256_unpacklo_epi8(masked2, masked1);
        let res2 = _mm256_unpackhi_epi8(masked2, masked1);</pre>
<p>In the high half of the interleaved masks, the destination's first 128 bits are filled with the top 128 bits from the source, the remainder being filled in with zeros. In the lower half of the interleaved masks, the destination's first 128 bits are filled from the bottom 128 bits from the source, the remainder being zeros. Then, at long last, a pointer is computed into <kbd>dst</kbd> at 64-bit strides and further subdivided into 16-bit chunks, and the <kbd>res1</kbd> and <kbd>res2</kbd> are stored there:</p>
<pre>        let base = dst.as_mut_ptr().offset(i * 2);
        let base1 = base.offset(0) as *mut _;
        let base2 = base.offset(16) as *mut _;
        let base3 = base.offset(32) as *mut _;
        let base4 = base.offset(48) as *mut _;
        _mm256_storeu2_m128i(base3, base1, res1);
        _mm256_storeu2_m128i(base4, base2, res2);
        src = &amp;src[32..];
        i += 32;
    }</pre>
<p>When the <kbd>src</kbd> is less than 32 bytes, <kbd>hex_encode_sse4</kbd> is called on it, but we'll leave the details of that to the adventurous reader:</p>
<pre>    let i = i as usize;
    let _ = hex_encode_sse41(src, &amp;mut dst[i * 2..]);

    Ok(str::from_utf8_unchecked(
        &amp;dst[..src.len() * 2 + i * 2],
    ))
}</pre>
<p>That was tough! If our aim here was to do more than get a flavor of the SIMD style of programming, then we'd probably need to work out by hand how these instructions work on a bit level. Is it worth it? This example comes from RFC 2325 (<a href="https://github.com/rust-lang/rfcs/blob/master/text/2325-stable-simd.md">https://github.com/rust-lang/rfcs/blob/master/text/2325-stable-simd.md</a>), <em>Stable SIMD</em>. Their benchmarking demonstrates that the fallback implementation we looked at first hashes at around 600 megabytes per second. The avx2 implementation hashes at 15,000 MB/s, a 60% performance bump. Not too shabby. And keep in mind that each CPU core comes equipped with SIMD capability. You could split such computations up among threads, too.</p>
<p>Like atomic programming, SIMD is not a panacea or even especially easy. If you need the speed, though, it's worth the work.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Futures and async/await</h1>
                </header>
            
            <article>
                
<p>Asynchronous I/O is a hot topic in the Rust ecosystem right now. The C10K challenge we discussed briefly in <a href="d4802512-564b-4037-9407-b6035bd38f31.xhtml" target="_blank">Chapter 8</a>, <em>High-Level Parallelism – Threadpools, Parallel Iterators, and Processes</em>, was resolved when operating systems introduced scalable I/O notification syscalls such as kqueue in the BSDs and epoll in Linux. Prior to epoll, kqueue, and friends, I/O notification suffered linear growth in processing time as the number of file descriptors increased—a real problem. The aggressively naive approach to networking we've taken in this book also suffers from linear blowup. Every TCP socket we opened for listening would need to be polled in a loop, underserving very high-volume connections and overserving all others.</p>
<p>Rust's abstraction for scalable network I/O is mio (<a href="https://crates.io/crates/mio">https://crates.io/crates/mio</a>). If you go poking around inside of mio, you'll find that it is a clean adapter for the I/O notification subsystems of a few common platforms: Windows, Unixen, and that's about it. The user of mio doesn't get much in the way of additional support. Where do you store your mio sockets? That's up to the programmer. Can you include multithreading? Up to you. Now, that's great for very specific use cases—cernan, for example, uses mio to drive its ingress I/O because we wanted very fine control over all of these details—but unless you have very specific needs, this is probably going to be a very tedious situation for you. The community has been investing very heavily in tokio (<a href="https://crates.io/crates/tokio">https://crates.io/crates/tokio</a>), a framework for doing scalable I/O with essential things such as back-pressure, transaction cancellation, and higher-level abstractions to simplify request/response protocols. Tokio's been a fast-moving project, but will almost surely be one of the <em>key</em> projects in the coming years. Tokio is, fundamentally, a reactor-based (<a href="https://en.wikipedia.org/wiki/Reactor_pattern">https://en.wikipedia.org/wiki/Reactor_pattern</a>) architecture; an I/O event becomes available and handlers you've registered are then called.</p>
<p>Reactor-based systems are simple enough to program if you have only one event to respond to. But, more often than you'd imagine, there exist dependency relationships between I/O events in a real system—an ingressing UDP packet results in a TCP round-trip to a network memory cache, which results in an egress UDP packet to the original ingress system, and another as well. Coordinating that is hard. Tokio—and the Rust ecosystem somewhat generally—has gone all-in on a promise library called futures (<a href="https://crates.io/crates/futures">https://crates.io/crates/futures</a>). Promises are a fairly tidy solution to asynchronous I/O: the underlying reactor calls a well-established interface, you implement a closure inside (or a trait to then slot inside) that interface, and everyone's code stays loosely coupled. Rust's futures are pollable, meaning you can hit the poll function on a future and it might resolve into a value or a notification that the value isn't ready yet.</p>
<p>This is similar to promise systems in other languages. But, as anyone who remembers the early introduction of NodeJS into the JavaScript ecosystem can attest to, promises without language support get <span>weird</span><span> </span><span>and</span><span> </span><span>deeply nested fast. Rust's borrow checker made the situation harder, requiring boxing or full-on arcs where, in straight-line code, this would not have been necessary.</span></p>
<p>Languages with promises as a first-class concern generally evolve async/await syntaxes. An async function is a function in Rust that will return a future and little else. A Rust future is simple trait, extracted from the futures project. The actual implementation will live outside the language's standard library and allow for alternative implementations. The interior of an async function is not executed immediately, but only when the future is polled. The interior of the function executes through the polling until an await point is hit, resulting in a notification that the value is not yet ready and that whatever is polling the future should move on.</p>
<p>As in other languages, Rust's proposed async / await syntax really is a sugar over the top of an explicit callback chaining structure. But, because the async/await syntax and futures trait is moving into the compiler, rules can be added to the borrow-checking system to remove the current boxing concerns. I expect you'll see more futures in stable Rust once they become more convenient to interact with. Indeed, as we saw in <a href="d4802512-564b-4037-9407-b6035bd38f31.xhtml" target="_blank">Chapter 8</a>, <em>High-Level Parallelism – Pools, Iterators, and Processes</em>, rayon is investing time in a futures-based interface.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Specialization</h1>
                </header>
            
            <article>
                
<p>In <a href="8c61da2f-08c8-40be-89d4-d8afa8510850.xhtml" target="_blank">Chapter 2</a>, <em>Sequential Rust Performance and Testing</em>, we noted that the technique of <em>specialization</em> would be cut off from data structure implementors if they intended to target stable Rust. Specialization is a means by which a generic structure—say, a <kbd>HashMap</kbd>—can be implemented specifically for one type. Usually, this is done to improve performance—say, by turning a <kbd>HashMap</kbd> with <kbd>u8</kbd> keys into an array—or provide a simpler implementation when possible. Specialization has been in Rust for a good while, but limited to nightly projects, such as the compiler, because of soundness issues when interacting with lifetimes. The canonical example has always been the following:</p>
<pre>trait Bad1 {
    fn bad1(&amp;self);
}

impl&lt;T&gt; Bad1 for T {
    default fn bad1(&amp;self) {
        println!("generic");
    }
}

// Specialization cannot work: trans doesn't know if T: 'static
impl&lt;T: 'static&gt; Bad1 for T {
    fn bad1(&amp;self) {
        println!("specialized");
    }
}

fn main() {
    "test".bad1()
}</pre>
<p>Lifetimes are erased at a certain point in the compilation process. String literals are static, but by the time dispatch to specialized type would occur there's not enough information in the compiler to decide. Lifetime equality in specialization also poses difficulties.</p>
<p>It seemed for a good while that specialization would miss the boat for inclusion into the language in 2018. Now, <em>maybe</em>, a more limited form of specialization will be included. A specialization will only apply if the implementation is generic with regard to lifetimes, does not introduce duplication into trait lifetimes, does not repeat generic type parameters, and all trait bounds are themselves applicable for specialization. The exact details are in flux as I write this, but do keep an eye out.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Interesting projects</h1>
                </header>
            
            <article>
                
<p>There are many interesting projects in the Rust community right now. In this section, I'd like to look at two that I didn't get a chance to discuss at length in the book.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Fuzzing</h1>
                </header>
            
            <article>
                
<p>In previous chapters we've used AFL to validate that our programs did not exhibit crashing behavior. While AFL is very commonly used, it's not the only fuzzer available for Rust. LLVM has a native library—libfuzzer (<a href="https://llvm.org/docs/LibFuzzer.html">https://llvm.org/docs/LibFuzzer.html</a>)—covering the same space, and the cargo-fuzz (<a href="https://crates.io/crates/cargo-fuzz">https://crates.io/crates/cargo-fuzz</a>) project acts as an executor. You might also be interested in honggfuzz-rs (<a href="https://crates.io/crates/honggfuzz">https://crates.io/crates/honggfuzz</a>), a fuzzer developed at Google for searching out security related violations. It is natively multithreaded—there is no need to spin up multiple processes manually—and can do network fuzzing. My preference, traditionally, has been to fuzz with AFL. The honggfuzz project has real momentum, and readers should give it a try in their own projects.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Seer, a symbolic execution engine for Rust</h1>
                </header>
            
            <article>
                
<p>Fuzzing, as we discussed previously, explores the state space of a program using random inputs and binary instrumentation. This can be, as we've seen, slow. The ambition of symbolic execution (<a href="https://en.wikipedia.org/wiki/Symbolic_execution">https://en.wikipedia.org/wiki/Symbolic_execution</a>) is to allow the same exploration of state space, but without random probing. Searching for program crashes is one area of application, but it can also be used with proof tools. Symbolic execution, in a carefully written program, can let you demonstrate that your program can never reach error states. Rust has a partially implemented symbolic execution tool, seer (<a href="https://github.com/dwrensha/seer">https://github.com/dwrensha/seer</a>). The project uses z3, a constraint solver, to generate branching inputs at program branches. Seer's README, as of SHA <kbd>91f12b2291fa52431f4ac73f28d3b18a0a56ff32</kbd>, amusingly decodes a hash. This is done by defining the decoding of the hashed data to be a crashable condition. Seer churns for a bit and then decodes the hash, crashing the program. It reports the decoded value among the error report.</p>
<p>It's still early days for seer, but the potential is there.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The community</h1>
                </header>
            
            <article>
                
<p>The Rust community is large and multi-faceted. It's so large, in fact, that it can be hard to know where to go with questions or ideas. More, books at the intermediate to advanced level will often assume that readers know as much about the language community as the author does. Having been mostly on the other side of the author, reader relationship I've always found this assumption frustrating. As an author, though, I now understand the hesitancy—none of the community information will stay up to date.</p>
<p>Oh well. Some of this information may be out of date by the time you get to it. Reader beware. </p>
<p>Throughout this book, we've referred to the crates ecosystem; crates.io (<a href="https://crates.io/">https://crates.io/</a>) is <em>the</em> location for Rust source projects. The docs.rs (<a href="https://docs.rs/">https://docs.rs/</a>) is a vital resource for understanding crates, and is run by the Rust team. You can also <kbd>cargo docs</kbd> and get a copy of your project's dependency documentation locally. I'm often without Wi-Fi, and I find this very useful.</p>
<p>As with almost all things in Rust, the community itself is documented on this web page (<a href="https://www.rust-lang.org/en-US/community.html">https://www.rust-lang.org/en-US/community.html</a>). IRC is fairly common for real-time, loose conversations, but there's a real focus on the web-facing side of Rust's communication. The Users Forum (<a href="https://users.rust-lang.org/">https://users.rust-lang.org/</a>) is a web forum intended for folks who use the language and have questions or announcements. The compiler people—as well as the standard library implementors, and so on—hang out on the Internals Forum(<a href="https://internals.rust-lang.org/">https://internals.rust-lang.org/</a>). There is a good deal of overlap in the two communities, as you might expect.</p>
<p>Throughout this book, we've referenced various RFCs. Rust evolves through a request-for-comments system, all of which are centralized (<a href="https://github.com/rust-lang/rfcs">https://github.com/rust-lang/rfcs</a>). That's just for the Rust compiler and standard library. Many community projects follow a similar system—for example, crossbeam RFCs (<a href="https://github.com/crossbeam-rs/rfcs">https://github.com/crossbeam-rs/rfcs</a>). These are well worth reading, by the way.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Should I use unsafe?</h1>
                </header>
            
            <article>
                
<p>It's not uncommon to hear some variant of the following position—<em>I won't use any library that has an</em> <kbd>unsafe</kbd> <em>block in it</em>. The reasoning behind this position is that <kbd>unsafe</kbd>, well, advertises that the crate is potentially unsafe and might crash your otherwise carefully crafted program. That's true—kind of. As we've seen in this book, it's entirely possible to put together a project using <kbd>unsafe</kbd> that is totally safe at runtime. We've also seen that it's entirely possible to put together a project without <kbd>unsafe</kbd> blocks that flame out at runtime. The existence or absence of <kbd>unsafe</kbd> blocks shouldn't reduce the original programmer's responsibilities for due diligence—writing tests, probing the implementation with fuzzing tools, and so on. Moreover, the existence or absence of <kbd>unsafe</kbd> blocks does not relieve the user of a crate from that same responsibility. Any software, at some level, should be considered suspect unless otherwise demonstrated to be safe.</p>
<p>Go ahead and use the <kbd>unsafe</kbd> keyword. Do so when it's necessary. Keep in mind that the compiler won't be able to help you as much as it normally would—that's all. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this last chapter of the book, we discussed near and midterm improvements coming to the Rust language—features being stabilized after a while out in the wilderness, the foundations of larger projects being integrated, research that may or may not pan out, and other topics. Some of these topics will surely have chapters of their own in the second edition of this book, should it get a new edition (Tell your friends to pick up a copy!). We also touched on the community, which is the place to find ongoing discussions, ask questions, and get involved. Then, we affirmed that, yes indeed, you should write in unsafe Rust when you have reason to do so.<br/>
<br/>
Okay, that's it. That's the end of the book.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Shipping specialization: a story of soundness</em>, available at <a href="https://aturon.github.io/blog/2017/07/08/lifetime-dispatch/">https://aturon.github.io/blog/2017/07/08/lifetime-dispatch/</a>. There's long been an ambition to see specialization available in stable Rust, and it's not for want of trying that it hasn't happened yet. In this blog post, Aaron Turon discusses the difficulties of specialization in 2017, introducing the Chalk logic interpreter in the discussion. Chalk is interesting in its own right, especially if you are interested in compiler internals or logic programming.</li>
<li><em>Maximally minimal specialization: always applicable impls</em>, available at <a href="http://smallcultfollowing.com/babysteps/blog/2018/02/09/maximally-minimal-specialization-always-applicable-impls/">http://smallcultfollowing.com/babysteps/blog/2018/02/09/maximally-minimal-specialization-always-applicable-impls/</a>. This blog post by Niko Matsakis extends the topics covered in Turon's <em>Shipping specialization</em> article, discussing a <em>min-max</em> solution to the specialization soundness issue. The approach seemed to be the most likely candidate for eventual implementation, but flaws were discovered. Happily, not unresolvable flaws. This post is an excellent example of the preRFC conversation in the Rust community.</li>
<li><em>Sound and ergonomic specialization for Rust</em>, available at <a href="http://aturon.github.io/2018/04/05/sound-specialization/">http://aturon.github.io/2018/04/05/sound-specialization/</a>. This blog post addresses the issues in the min-max article and proposes the implementation discussed in this chapter. It is, as of writing this book, the most likely to be implemented, unless some bright spark finds a flaw in it.</li>
<li><em>Chalk</em>, available at <a href="https://github.com/rust-lang-nursery/chalk">https://github.com/rust-lang-nursery/chalk</a>. Chalk is really a very interesting Rust project. It is, according to the project description, a <em>PROLOG-ish interpreter written in Rust</em>. To date, Chalk is being used to reason about specialization in the Rust language, but there are plans to merge Chalk into <kbd>rustc</kbd> itself someday. The project README, as of <kbd>SHA 94a1941a021842a5fcb35cd043145c8faae59f08</kbd>, has a list of excellent articles on the applications of chalk.</li>
<li><em>The Unstable Book</em>, available at <a href="https://doc.rust-lang.org/nightly/unstable-book/">https://doc.rust-lang.org/nightly/unstable-book/</a>. <kbd>rustc</kbd> has a large number of in-flight features. <em>The Unstable Book</em> is intended to be a best-effort collection of these features, the justifications behind them, and any relevant tracking issues. It is well worth a read, especially if you're looking to contribute to the compiler project.</li>
<li><em>Zero-cost futures in Rust</em>, available at <a href="http://aturon.github.io/blog/2016/08/11/futures/">http://aturon.github.io/blog/2016/08/11/futures/</a>. This post introduced the Rust community to the futures project and explains the motivation behind the introduction well. The actual implementation details of futures have changed with time, but this article is still well worth a read.</li>
<li><em>Async / Await</em>, available at  <a href="https://github.com/rust-lang/rfcs/blob/master/text/2394-async_await.md">https://github.com/rust-lang/rfcs/blob/master/text/2394-async_await.md</a>. RFC  2394 introduces the motivation for simplifying the ergonomics of futures in Rust, as well as laying out the implementation approach for it. The RFC is, itself, a fine example of how Rust evolves—community desire turns into experimentation which then turns into support from the compiler.</li>
</ul>
<p> </p>


            </article>

            
        </section>
    </div>



  </body></html>
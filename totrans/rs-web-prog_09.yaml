- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing Our Application Endpoints and Components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our to-do Rust application now fully works. We are happy with our first version
    as it manages authentication, different users, and their to-do lists, and logs
    our processes for inspection. However, a web developer’s job is never done.
  prefs: []
  type: TYPE_NORMAL
- en: While we have now come to the end of adding features to our application, we
    know that the journey does not stop here. In future iterations beyond this book,
    we may want to add teams, new statuses, multiple lists per user, and so on. However,
    as we add these features, we must ensure that our old application’s behavior stays
    the same unless we actively change it. This is done by building tests.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll build tests that check our existing behavior, laying
    down traps that will throw errors that report to us if the app’s behavior changes
    without us actively changing it. This prevents us from breaking the application
    and pushing it to a server after adding a new feature or altering the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Building our unit tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building JWT unit tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing functional API tests in Postman
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating Postman tests with Newman
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an entire automated testing pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, we will understand how to build unit tests in Rust,
    inspecting our structs in detail with a range of edge cases. If our structs behave
    in a way we do not expect, our unit tests will report it to us.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll build on the code built in [*Chapter 8*](B18722_08.xhtml#_idTextAnchor168),
    *Building RESTful Services*. This can be found at [https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter08/caching](https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter08/caching).
  prefs: []
  type: TYPE_NORMAL
- en: Node and NPM are also needed for installing and running the automated API tests,
    which can be found at [https://nodejs.org/en/download/](https://nodejs.org/en/download/).
  prefs: []
  type: TYPE_NORMAL
- en: We will also be running a part of the automated testing pipeline in Python.
    Python can be downloaded and installed at [https://www.python.org/downloads/](https://www.python.org/downloads/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the full source code used in this chapter here: [https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter09](https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter09).'
  prefs: []
  type: TYPE_NORMAL
- en: Building our unit tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore the concept of unit tests and how to build
    unit test modules that contain tests as functions. Here, we are not going to achieve
    100% unit test coverage for our application. There are places in our application
    that can be covered by our functional tests, such as API endpoints and JSON serialization.
    However, unit tests are still important in some parts of our application.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests enable us to look at some of our processes in more detail. As we
    saw with our logging in [*Chapter 8*](B18722_08.xhtml#_idTextAnchor168), *Building
    RESTful Services*, a functional test might work the way we want it to end-to-end,
    but there might be edge cases and behaviors that we do not want. This was seen
    in the previous chapter, where we saw our application make two `GET` calls when
    one was enough.
  prefs: []
  type: TYPE_NORMAL
- en: In our unit tests, we will break down the processes one by one, mock certain
    parameters, and test the outcomes. These tests are fully isolated. The advantage
    of this is that we get to test a range of parameters quickly, without having to
    run a full process each time. This also helps us pinpoint exactly where the application
    is failing and with what configuration. Unit testing is also useful for test-driven
    development, where we build the components of a feature bit by bit, running the
    unit tests and altering the components as and when the test outcomes require.
  prefs: []
  type: TYPE_NORMAL
- en: In big, complex systems, this saves a lot of time as you do not have to spin
    up the app and run the full system to spot a typo or failure to account for an
    edge case.
  prefs: []
  type: TYPE_NORMAL
- en: However, before we get too excited, we must acknowledge that unit testing is
    a tool, not a lifestyle, and there are some fallbacks to using it. The tests are
    only as good as their mocks. If we do not mock realistic interactions, then a
    unit test could pass but the application could fail. Unit tests are important,
    but they also must be accompanied by functional tests.
  prefs: []
  type: TYPE_NORMAL
- en: Rust is still a new language, so at this point, unit testing support is not
    as advanced as with other languages such as Python or Java. For instance, with
    Python, we can mock any object from any file with ease at any point in the test.
    With these mocks, we can define outcomes and monitor interactions. While Rust
    does not have these mocks so readily available, this does not mean we cannot unit
    test.
  prefs: []
  type: TYPE_NORMAL
- en: '*A bad craftsman always blames their tools*. The craftsmanship behind successful
    unit testing is constructing our code in such a way that individual code pieces
    won’t depend on each other, giving the pieces will have as much autonomy as possible.
    Because of this lack of dependency, testing can easily be performed without the
    necessity of having a complex mocking system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can test our to-do structs. As you’ll remember, we have the `done`
    and `pending` structs, which inherit a `base` struct. We can start by unit testing
    the struct that has no dependencies and then move down to other structs that have
    dependencies. In our `src/to_do/structs/base.rs` file, we can define our unit
    tests for the `base` struct at the bottom of the file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we merely create a struct and assess the fields of that
    struct, ensuring that they are what we expect them to be. We can see that we created
    our `test` module, which is annotated with a `#[cfg(test)]` attribute. The `#[cfg(test)]`
    attribute is a conditional check where the code is only active if we run `cargo
    test`. If we do not run `cargo test`, the code annotated with `#[cfg(test)]` is
    not compiled.
  prefs: []
  type: TYPE_NORMAL
- en: nside the module, we will import the `Base` struct from the file outside of
    the `base_tests` module, which is still in the file. In the Rust world, it is
    typical to import what we are testing using `super`. There is a well-established
    standard to have the testing code right under the code that is being tested in
    the same file. We will then test the `Base::new` function by decorating our `new`
    function with a `#[``test]` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: This is the first time we have covered attributes. An **attribute** is simply
    metadata applied to modules and functions. This metadata aids the compiler by
    giving it information. In this case, it is telling the compiler that this module
    is a test module and that the function is an individual test.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we run the preceding code, it would not work. This is because the
    `Eq` trait is not implemented in the `TaskStatus` enum, meaning that we cannot
    execute the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This also means that we cannot use the `==` operator between two `TaskStatus`
    enums. Therefore, before we try and run our test, we will have to implement the
    `Eq` trait on the `TaskStatus` enum in the `src/to_do/structs/enums.rs` file with
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that we have implemented the `Eq` and `Debug` traits, which are
    needed for the `assert_eq!` macro. However, our test still will not run because
    we have not defined the rules around equating two `TaskStatus` enums. We could
    implement the `PartialEq` trait by simply adding the `PartialEq` trait to our
    derive annotation. However, we should explore how to write our own custom logic.
    To define the equating rules, we implement the `eq` function under the `PartialEq`
    trait with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that we manage to confirm if the `TaskStatus` enum is equal
    to the other `TaskStatus` enum being compared using two match statements. It seems
    more intuitive to use the `==` operator in the `eq` function; however, using the
    `==` operator calls the `eq` function resulting in an infinite loop. The code
    will still compile if you use the `==` operator in the `eq` function but if you
    run it you will get the following unhelpful error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now essentially created a new `base` struct and then checked to see
    if the fields are what we expected. To run this, run the `cargo test` functionality,
    pointing it to the file we want to test, which is denoted by the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that our test was run and that it passed. Now, we’ll move on to
    writing tests for the rest of the module, which are the `Done` and `Pending` structs.
    Now is the time to see if you can write a basic unit test in the `src/to_do/structs/done.rs`
    file. If you have attempted to write a unit test for the `Done` struct in the
    `src/to_do/structs/done.rs` file, your code should look like the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can run both tests with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Running `cargo test` runs all the tests across all Rust files. We can see that
    all our tests have now run and passed.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have done some basic testing, let’s look at the other modules that
    we can test. Our JSON serialization and views can be tested in our functional
    tests with **Postman**. Our database models do not have any advanced functionality
    that we have purposefully defined.
  prefs: []
  type: TYPE_NORMAL
- en: Building JWT unit tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All our models do is read and write to the database. This has been shown to
    work. The only module left that we’ll unit test is the `auth` module. Here, we
    have some logic that has multiple outcomes based on the inputs. We also must do
    some mocking as some of the functions accept `actix_web` structs, which have certain
    fields and functions. Luckily for us, `actix_web` has a test module that enables
    us to mock requests.
  prefs: []
  type: TYPE_NORMAL
- en: Building a configuration for tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we start building our unit tests for the JWT, we must remember that
    there is a dependency on the `config` file to get the secret key. Unit tests must
    be isolated. They should not need to have the correct parameters passed into them
    to work. They should work every time in isolation. Because of this, we are going
    to have to build a `new` function for our `Config` struct in our `src/config.rs`
    file. The outline for the coding tests will look like the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding outline shows that there are two `new` functions. Our new `new`
    function gets compiled if tests are being run, and the old `new` function gets
    compiled if the server is running as normal. Our test `new` function has the standard
    values hardcoded in with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: These default functions are the same as our development `config` file; however,
    we know that these variables are going to be consistent. We do not need to pass
    in anything when running the tests and we do not run the risk of reading another
    file. Now that our tests have been configured, we can define the requirements,
    including the configuration for our JWT tests.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the requirements for JWT tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have secured our `Config` struct for tests, we can go to our `src/jwt.rs`
    file and define the imports for our tests with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: With the preceding code, we can import a range of `actix_web` structs and functions
    to enable us to create fake HTTP requests and send them to a fake application
    to test how the `JwToken` struct works during the HTTP request process. We will
    also define a `ResponseFromTest` struct that can be processed to and from JSON
    to extract the user ID from the HTTP request as the `JwToken` struct houses the
    user ID. The `ResponseFromTest` struct is the HTTP response we are expecting to
    have so we are closely mocking the response object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have imported all that we need, we can define the outline of our
    tests with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that we test the getting of the key and the encoding and decoding
    of the token. They are native functions to the `JwToken` struct and, with what
    we have covered previously, you should be able to write them yourself. The other
    functions are decorated with `#[actix_web::test]`. This means that we are going
    to create fake HTTP requests to test how our `JwToken` implements the `FromRequest`
    trait. Now, there is nothing stopping us from writing the tests, which we will
    cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building basic function tests for JWT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start with the most basic test, getting the key, which takes the following
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We must remember that `"secret"` is the hardcoded key defined in the `Config::new`
    function for test implementations. If the test `Config::new` function works, the
    aforementioned test will work. Getting the expiry can also be important. Because
    we directly rely on the expiration minutes to be extracted from `config`, the
    following test will ensure that we are returning 120 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now move on to test how invalid tokens are handled with the following
    test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, we pass in an `"invalid_token"` string that should fail the decoding process
    because it is clearly not a valid token. We will then match the outcome. If the
    outcome is an error, we will then assert that the message is that the error is
    a result of an invalid token. If there is any other output apart from an error,
    then we throw an error failing the test because we expect the decode to fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have written two tests for our `JwToken` struct functions, this
    is a good time for you to attempt to write the test for encoding and decoding
    a token. If you have attempted to write the encoding and decoding test, it should
    look like the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The preceding test essentially boils down the login and authenticated request
    process around the token. We create a new token with a user ID, encode the token,
    and then decode the token testing to see if the data we passed into the token
    is the same as we get out when we decode it. If we don’t, then the test will fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have finished testing the functions for the `JwToken` struct, we
    can move on to testing how the `JwToken` struct implements the `FromRequest` trait.
    Before we do this, we must define a basic view function that will merely handle
    the authentication of `JwToken` and then returns the user ID from the token with
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This is nothing new, in fact, this outline is also how we define our views in
    our application. With our basic tests defined, we can move on to building tests
    for web requests.
  prefs: []
  type: TYPE_NORMAL
- en: Building tests for web requests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now test our test view to see how it handles a request with no token
    in the header with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can see that we can create a fake server and attach
    our `test_handler` test view to it. We can then create a fake request that does
    not have any token in the header. We will then call the server with the fake request,
    then assert that the response code of the request is unauthorized. We can now
    create a test that inserts a valid token with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that we create a valid token. We can create our fake server
    and attach our `test_handler` function to that fake server. We will then create
    a request that can be mutated. Then, we will insert the token into the header
    and call the fake server with the fake request, using the `call_and_read_body_json`
    function. It must be noted that when we call the `call_and_read_body_json` function,
    we declare that the type returned under the `resp` variable name to be `ResponseFromTest`.
    We then assert that the user ID is from the request response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have seen how to create a fake HTTP request with a header, this
    is a good opportunity for you to try and build the test that makes a request with
    a fake token that cannot be decoded. If you have attempted this, it should look
    like the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the following code, we can see that we inserted a false token into
    the header using the approach laid out in the passing token request test with
    the unauthorized assertion used in the test with no token provided. If we run
    all the tests now, we should get the following printout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding output, our `jwt` and `to_do` modules are now fully unit-tested.
    Considering that Rust is still a new language, we have managed to painlessly unit
    test our code because we structured our code in a modular fashion.
  prefs: []
  type: TYPE_NORMAL
- en: The `tests` crate that `actix_web` provided enabled us to test edge cases quickly
    and easily. In this section, we tested how our functions processed requests with
    missing tokens, false tokens, and correct tokens. We have seen first-hand how
    Rust enables us to run unit tests on our code.
  prefs: []
  type: TYPE_NORMAL
- en: Everything is configured with `cargo`. We do not have to set up paths, install
    extra modules, or configure environment variables. All we must do is define modules
    with the `test` attribute and run the `cargo test` command. However, we must remember
    that our views and JSON serialization code are not unit-tested. This is where
    we switch to Postman to test our API endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Writing tests in Postman
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be implementing functional integration tests using
    Postman to test our API endpoints. This will test our JSON processing and database
    access. To do this, we will follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to have to create a test user for our Postman tests. We can do
    this with the JSON body shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to add a `POST` request to the `http://127.0.0.1:8000/v1/user/create`
    URL. Once we have done this, we can use our login endpoint for our Postman tests.
    Now that we have created our test user, we must get the token from the response
    header of the `POST` request to the `http://127.0.0.1:8000/v1/auth/login` URL
    with the JSON request body:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following Postman layout:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Creating a new use Postman request](img/Figure_9.1_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Creating a new use Postman request
  prefs: []
  type: TYPE_NORMAL
- en: 'With this token, we have all the information needed to create our Postman collection.
    Postman is a collection of API requests. In this collection, we can bunch all
    our to-do item API calls together using the user token as authentication. The
    result of the call is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Creating new use Postman response](img/Figure_9.2_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Creating new use Postman response
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create our collection with the following Postman button, that is, **+**
    **New Collection**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Creating new Postman collection](img/Figure_9.3_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Creating new Postman collection
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have clicked this, we must make sure that our user token is defined
    for the collection, as all to-do item API calls need the token. This can be done
    by using the **Authorization** configuration for our API calls, as seen in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Defining AUTH credentials in a new Postman collection](img/Figure_9.4_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Defining AUTH credentials in a new Postman collection
  prefs: []
  type: TYPE_NORMAL
- en: We can see that we have merely copied and pasted our token into the value with
    **token** as the key, which will be inserted into the header of the requests.
    This should now be passed in all our requests in the collection. This collection
    is now stored on the left-hand side navigation bar under the **Collections** tab.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now configured our collection and can now add requests under the collection
    by clicking the grayed-out **Add Request** button shown in this screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Creating a new request for our Postman collection](img/Figure_9.5_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Creating a new request for our Postman collection
  prefs: []
  type: TYPE_NORMAL
- en: Now, we must think about our approach to testing the flow of testing as this
    has to be self-contained.
  prefs: []
  type: TYPE_NORMAL
- en: Writing ordered requests for tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our requests will take the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Create**: Create a to-do item, then check the return to see if it is stored
    correctly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create**: Create another to-do item, checking the return to see if the previous
    one is stored and that the process can handle two items.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create**: Create another to-do item with the same title as one of the other
    items, checking the response to ensure that our application is not storing duplicate
    to-do items with the same title.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Edit**: Edit an item, checking the response to see if the edited item has
    been changed to *done* and that it is stored in the correct list.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Edit**: Edit the second item to see if the *edit* effect is permanent and
    that the *done* list supports both items.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Edit**: Edit an item that is not present in the application to see if the
    application handles this correctly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Delete**: Delete one to-do item to see if the response no longer returns
    the deleted to-do item, meaning that it is no longer stored in the database.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Delete**: Delete the final to-do item, checking the response to see if there
    are no items left, showing that the *delete* action is permanent.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to run the preceding tests for them to work as they rely on the previous
    action being correct. When we create a request for the collection, we must be
    clear about what the request is doing, which step it is on, and what type of request
    it is. For instance, creating our first *create* test will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Creating our first Postman create request](img/Figure_9.6_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Creating our first Postman create request
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the step is appended with the type by an underscore. We then
    put the description of the test from the list in the **Request description (Optional)**
    field. When defining the request, you may realize that the API key is not in the
    header of the request.
  prefs: []
  type: TYPE_NORMAL
- en: This is because it is in the hidden autogenerated headers of the request. Our
    first request must be a `POST` request with the `http://127.0.0.1:8000/v1/item/create/washing`
    URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'This creates the to-do item *washing*. However, before we click the **Send**
    button, we must move over to the **Tests** tab in our Postman request, just to
    the left of the **Settings** tab, to write our tests as seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Accessing the tests script in Postman](img/Figure_9.7_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Accessing the tests script in Postman
  prefs: []
  type: TYPE_NORMAL
- en: 'Our tests must be written in JavaScript. However, we can get access to Postman’s
    `test` library by typing `pm` into the test script. First, at the top of the test
    script, we need to process the request, which is done with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding line, we can access the response JSON throughout the test
    script. To comprehensively test our request, we need to follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to check the basic content of the response. Our first test is
    to check to see if the response is `200`. This can be done with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we define the test description. Then, the function that the test runs
    is defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we check the length of data in the response. After the preceding test,
    we will define our test to check if the pending item has a length of one via the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we do a simple check of the length and throw an error
    if the length is not one as we only expect one pending item in the `pending_items`
    list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we inspect the title and status of the pending item in the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we throw an error if the status or title does not match
    what we want. Now we have satisfied our tests for the pending items, we can move
    on to the tests for the done items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Seeing as our done items should be zero, the tests have the following definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we are merely ensuring that the `done_items` array has
    a length of zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must check the counts of our done and pending items. This is done in
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that our tests are built, we can make the request by clicking the **SEND**
    button in Postman to get the following output for the tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Postman tests output](img/Figure_9.8_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Postman tests output
  prefs: []
  type: TYPE_NORMAL
- en: We can see that our test descriptions and the status of the test are highlighted.
    If you get an error the status will be red with a **FAIL**. Now that our first
    create test has been done, we can create our second create test.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a test for an HTTP request
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can then create the `2_create` test with this URL: `http://127.0.0.1:8000/v1/item/create/cooking`.
    This is a good opportunity to try and build the test yourself with the testing
    methods that we have explored in the previous step. If you have attempted to build
    the tests, they should look like the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: We can see that we have added a couple of extra tests on the second pending
    item. The preceding tests also directly apply to the `3_create` test as a duplicate
    creation will be the same as we will be using the same URL as `2_create`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding tests require a fair amount of repetition in these tests, slightly
    altering the length of arrays, item counts, and attributes within these arrays.
    This is a good opportunity to practice basic Postman tests. If you need to cross-reference
    your tests with mine, you can assess them in the JSON file at the following URL:
    [https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/blob/main/chapter09/building_test_pipeline/web_app/scripts/to_do_items.postman_collection.json](https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/blob/main/chapter09/building_test_pipeline/web_app/scripts/to_do_items.postman_collection.json).'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have put in a series of steps for Postman to test when an
    API call is made. This is not just useful for our application. Postman can hit
    any API on the internet it has access to. Therefore, you can use Postman tests
    to monitor live servers and third-party APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Now, running all these tests can be arduous if it must be done manually every
    time. We can automate the running and checking of all the tests in this collection
    using **Newman**. If we automate these collections, we can run tests at certain
    times every day on live servers and third-party APIs we rely on, alerting us to
    when our servers or the third-party API breaks.
  prefs: []
  type: TYPE_NORMAL
- en: Newman will give us a good foundation for further development in this area.
    In the next section, we’ll export the collection and run all the API tests in
    the exported collection in sequence using Newman.
  prefs: []
  type: TYPE_NORMAL
- en: Automating Postman tests with Newman
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To automate the series of tests, in this section, we will export our to-do
    item Postman collection in the correct sequence. But first, we must export the
    collection as a JSON file. This can be done by clicking on our collection in Postman
    on the left-hand navigation bar and clicking the grayed-out **Export** button,
    as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Exporting our Postman collection](img/Figure_9.9_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Exporting our Postman collection
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have exported the collection, we can quickly inspect it to see
    how the file is structured. The following code defines the header of the suite
    of tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code tells Postman what schema is needed to run the tests. If
    the code is imported into Postman, the ID and name will be visible. The file then
    goes on to define the individual tests via the code given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we can see that our tests, method, URL, headers, and
    more are all defined in an array. A quick inspection of the `item` array will
    show that the tests will be executed in the order that we want.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can simply run it with Newman. We can install Newman with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'It must be noted that the preceding command is a global install, which can
    sometimes have issues. To avoid this, you can setup a `package.json` file with
    the following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'With this `package.json`, we have defined the test command and the Newman dependency.
    We can install our dependencies locally with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'This then installs all we need under the `node_modules` directory. Instead
    of running the Newman test command directly, we can use the test command defined
    in `package.json` with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have installed Newman, we can run the collection of tests against
    the exported collection JSON file with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command runs all the tests and gives us a status report. Each
    description is printed out and the status is also denoted by the side of the test.
    The following is a typical printout of an API test being assessed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding output gives us the name, method, URL, and response. Here, all
    of them passed. If one did not, then the test description would sport a *cross*
    instead of a *tick*. We also get the following summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Newman summary](img/Figure_9.10_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Newman summary
  prefs: []
  type: TYPE_NORMAL
- en: We can see that all our tests passed. With this, we have managed to automate
    our functional testing, enabling us to test a full workflow with minimal effort.
    However, what we have done is not maintainable. For instance, our token will expire,
    meaning that if we run tests later in the month, they will fail. In the next section,
    we will build an entire automated pipeline that will build our server, update
    our token, and run our tests.
  prefs: []
  type: TYPE_NORMAL
- en: Building an entire automated testing pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to development and testing, we need an environment that can be
    torn down and recreated easily. There is nothing worse than building up data in
    a database on your local machine to be able to develop further features using
    that data. However, the database container might be deleted by accident, or you
    may write some code that corrupts the data. Then, you must spend a lot of time
    recreating the data before you can get back to where you were. If the system is
    complex and there is missing documentation, you might forget the steps needed
    to recreate your data. If you are not comfortable with destroying your local database
    and starting again when developing and testing, there is something wrong and it
    is only a matter of time before you get caught out. In this section, we are going
    to create a single Bash script that carries out the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: Starts database Docker containers in the background.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compiles the Rust server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Runs unit tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Starts running the Rust server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Runs migrations to the database running in Docker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Makes an HTTP request to create a user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Makes an HTTP request to log in and get a token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Updates the Newman JSON file with the token from the login.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Runs the Newman tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removes the files produced in this whole process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stops the Rust server from running.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stops and destroy the Docker containers that were running for the whole process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are a lot of steps laid out in the preceding list. Glancing at this list,
    it would seem intuitive to break the code blocks that we are going to explore
    into steps; however, we are going to run nearly all the steps in one Bash script.
    A lot of the preceding steps outlined can be achieved in one line of Bash code
    each. It would be excessive to break the code down into steps. Now that we have
    all the steps needed, we can set up our testing infrastructure. First, we need
    to set up a `scripts` directory alongside the `src` directory in the root of `web_app`.
    Inside the `scripts` directory, we then need to have a `run_test_pipeline.sh`
    script that will run the main testing process. We also need to put our Newman
    JSON `config` file in the `scripts` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use `bash` to orchestrate the entire testing pipeline, which is the
    best tool for orchestrating testing tasks. In our `srcipts/run_test_pipeline.sh`
    script, we will start out with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we told the computer that the code block is a Bash script
    with the `#!/bin/bash` shebang line. Bash scripts run from the current working
    directory of where the Bash script it called from. We can call the script from
    multiple directories so we need to ensure that we get the directory of where the
    script is housed, which is the `scripts` directory, assign that to a variable
    called `SCRIPTPATH`, move to that directory, and then move out one with the `cd..`
    command to be in the main directory where the Docker, config, and Cargo files
    are. We can then spin up our Docker containers in the background with the `-d`
    flag and loop until the database is accepting connections with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our Docker containers are running, we can now move on to building
    our Rust server. First, we can compile the Rust server and run our unit tests
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the unit tests have been run, we can then run our server in the background
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'With `&` at the end of the command, the `cargo run config.yml` runs in the
    background. We then get the process ID of the `cargo run config.yml` command and
    assign it to the variable `SERVER_PID`. We then sleep for 5 seconds to be sure
    that the server is ready to accept connections. Before we make any API calls to
    our server, we must run our migrations to the database with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'We then move back into our `scripts` directory and make an API call to our
    server that creates a user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are wondering how to use `curl` to make HTTP requests in Bash, you can
    autogenerate them using your Postman tool. On the right-hand side of the Postman
    tool, you can see a **Code** button, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Code generation tool](img/Figure_9.11_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – Code generation tool
  prefs: []
  type: TYPE_NORMAL
- en: Once you have clicked on the code tag, there is a drop-down menu where you can
    select from a range of languages. Once you have selected the language you want,
    your API call will be displayed in a code snippet for your chosen language, which
    you can then copy and paste.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have created our user, we can log in and store the token in the
    `fresh_token.json` file with the following code; however, it must be noted that
    `curl` first needs to be installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'What is happening here is that we can wrap the result of the API call into
    a variable with `$(...)`. We then echo this and write it to the file using `echo
    $(...) > ./fresh_token.json`. We can then insert the fresh token into the Newman
    data and run the Newman API tests with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Our testing is now done. We can clean up the files created when running the
    tests, destroy the Docker containers, and stop our server running with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`curl` and `jq` both need to be installed before we can run the Bash script.
    If you are using Linux, you might need to run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then run our testing script with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Showing the whole printout would just needlessly fill up the book. However,
    we can see the end of the test printout in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – The testing pipeline output](img/Figure_9.12_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – The testing pipeline output
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the printout makes it clear that the Newman tests have run and passed.
    After the tests were completed, the server was shut down and the Docker containers
    that were supporting the server were stopped and removed. If you want to write
    this log to a `txt` file, you can do so with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: There you have it! A fully working test pipeline that automates the setting
    up, testing, and clean-up of our server. Because we have written it in a simple
    Bash test pipeline, we could integrate these steps in automation pipelines such
    as Travis, Jenkins, or GitHub Actions. These pipeline tools fire automatically
    when a `pull` request and merges are performed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through the workflows and components of our application,
    breaking them down so we could pick the right tools for the right part. We used
    unit testing so we could inspect several edge cases quickly to see how each function
    and struct interacted with others.
  prefs: []
  type: TYPE_NORMAL
- en: We also directly inspected our custom structs with unit tests. We then used
    the `actix_web` test structs to mock requests to see how the functions that use
    the structs and process the requests work. However, when we came to the main API
    views module, we switched to Postman.
  prefs: []
  type: TYPE_NORMAL
- en: This is because our API endpoints were simple. They created, edited, and deleted
    to-do items. We could directly assess this process by making API calls and inspecting
    the responses. Out of the box we managed to assess the JSON processing for accepting
    and returning data. We were also able to assess the querying, writing, and updating
    of the data in the database with these Postman tests.
  prefs: []
  type: TYPE_NORMAL
- en: Postman enabled us to test a range of processes quickly and efficiently. We
    even sped up this testing process by automating it via Newman. However, it must
    be noted that this approach is not a one-size-fits-all approach. If the API view
    functions become more complex, with more moving parts, such as communicating with
    another API or service, then the Newman approach would have to be redesigned.
    Environment variables that trigger mocking such processes would have to be considered
    so we can quickly test a range of edge cases.
  prefs: []
  type: TYPE_NORMAL
- en: Mocking objects will be needed if the system grows as the dependencies of our
    structs will grow. This is where we create a fake struct or function and define
    the output for a test. To do this, we will need an external crate such as `mockall`.
    The documentation on this crate is covered in the *Further reading* section of
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Our application now fully runs and has a range of tests. Now, all we have left
    is to deploy our application on a server.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will set up a server on **Amazon** **Web** **Services**
    (**AWS**), utilizing *Docker* to deploy our application on a server. We will cover
    the process of setting up the AWS configuration, running tests, and then deploying
    our application on our server if the tests pass.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why do we bother with unit tests if we can just manually play with the application?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between unit tests and functional tests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the advantages of unit tests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the disadvantages of unit tests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the advantages of functional tests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the disadvantages of functional tests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a sensible approach to building unit tests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to manual testing, you may forget to run a certain procedure.
    Running tests standardizes our standards and enables us to integrate them into
    continuous integration tools to ensure new code will not break the server as continuous
    integration can block new code merges if the code fails.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unit tests isolate individual components such as functions and structs. These
    functions and structs are then assessed with a range of fake inputs to assess
    how the component interacts with different inputs. Functional tests assess the
    system, hitting API endpoints, and checking the response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unit tests are lightweight and do not need an entire system to run. They can
    test a whole set of edge cases quickly. Unit tests can also isolate exactly where
    the error is.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unit tests are essentially isolated tests with made-up inputs. If the type of
    input is changed in the system but not updated in the unit test, then this test
    will essentially pass when it should fail. Unit tests also do not assess how the
    system runs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Functional tests ensure that the entire infrastructure works together as it
    should. For instance, there could be an issue with how we configure and connect
    to the database. With unit tests, such problems can be missed. Also, although
    mocking ensures isolated tests, unit test mocks might be out of date. This means
    that a mocked function might return data that the updated version does not. As
    a result, the unit test will pass but the functional tests will not as they test
    everything.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Functional tests need to have the infrastructure to run like a database. There
    also must be a setup and teardown function. For instance, a functional test will
    affect the data stored in the database. At the end of the test, the database needs
    to be wiped before running the test again. This can increase the complications
    and can require “glue” code between different operations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We start off with testing structs and functions that do not have any dependencies.
    Once these have been tested, we know that we are comfortable with them. We then
    move on to the functions and structs that have the dependencies we previously
    tested. Using this approach, we know that the current test we are writing does
    not fail due to a dependency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mockall documentation: [https://docs.rs/mockall/0.9.0/mockall/](https://docs.rs/mockall/0.9.0/mockall/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Github Actions documentation: [https://github.com/features/actions](https://github.com/features/actions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Travis documentation: [https://docs.travis-ci.com/user/for-beginners/](https://docs.travis-ci.com/user/for-beginners/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Circle CI documentation: [https://circleci.com/docs/](https://circleci.com/docs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jenkins documentation: [https://www.jenkins.io/doc/](https://www.jenkins.io/doc/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

<html><head></head><body>
		<div><h1 id="_idParaDest-144"><em class="italic">Chapter 9</em>: <a id="_idTextAnchor150"/>Managing Concurrency</h1>
			<p>Concurrent systems are all around us. When you download a file, listen to streaming music, initiate a text chat with a friend, and print something in the background on your computer, <em class="italic">all at the same time</em>, you are experiencing the magic of concurrency in action. The operating system manages all these for you in the background, scheduling tasks across available processors (CPUs).</p>
			<p>But do you know how to write a program that can do multiple things at the same time? More importantly, do you know how to do it in a way that is both memory- and thread-safe, while ensuring optimal use of system resources? Concurrent programming is one way to achieve this. But concurrent programming is considered to be a difficult topic in most programming languages due to challenges in <em class="italic">synchronizing tasks</em> and <em class="italic">sharing data safely across multiple threads of execution</em>. In this chapter, you'll learn about the basics of concurrency in Rust and how Rust makes it easier to prevent common pitfalls and enables us to write concurrent programs in a safe manner. This chapter is structured as shown here:</p>
			<ul>
				<li>Reviewing concurrency basics</li>
				<li>Spawning and configuring threads</li>
				<li>Error handling in threads</li>
				<li>Message passing between threads</li>
				<li>Achieving concurrency with shared state</li>
				<li>Pausing thread execution with timers</li>
			</ul>
			<p>By the end of this chapter, you'll have learned how to write concurrent programs in Rust by spawning new threads, handling thread errors, transferring and sharing data safely across threads to synchronize tasks, understanding the basics of thread-safe data types, and pausing the execution of current threads for synchronization.</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor151"/>Technical requirements</h1>
			<p>Verify that <code>rustup</code>, <code>rustc</code>, and <code>cargo</code> have been installed correctly with the following commands: </p>
			<pre>rustup --version
rustc --version 
cargo --version</pre>
			<p>The Git repo for the code in this chapter can be found at: <a href="https://github.com/PacktPublishing/Practical-System-Programming-for-Rust-Developers/tree/master/Chapter09">https://github.com/PacktPublishing/Practical-System-Programming-for-Rust-Developers/tree/master/Chapter09</a>.</p>
			<p>Let's get started with some basic concepts of concurrency.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor152"/>Reviewing concurrency basics</h1>
			<p>In this <a id="_idIndexMarker642"/>section, we'll cover the basics of <strong class="bold">multi-threading</strong> and clarify the terminology around <strong class="bold">concurrency</strong> and <strong class="bold">parallelism</strong>.</p>
			<p>To appreciate the value of concurrent programming, we have to understand the need of today's programs to make decisions quickly or process a large amount of data in a short period of time. Several use cases become impossible to achieve if we strictly rely on sequential execution. Let's consider a few examples of systems that must perform multiple things simultaneously.</p>
			<p>An autonomous car needs to perform many tasks at the same time, such as processing inputs from a wide array of sensors (to construct an internal map of its surroundings), plotting the path of the vehicle, and sending instructions to the vehicle's actuators (to control the brakes, acceleration, and steering). It needs to process continually arriving input events, and respond in tenths of a second.</p>
			<p>There are also other, more mundane examples. A web browser handles user inputs while simultaneously rendering a web page incrementally, as new data is received. A website handles requests from multiple simultaneous users. A web crawler has to access many thousands of sites simultaneously to gather information about the websites and their contents. It is impractical to do all these things sequentially.</p>
			<p>We've so far seen a few use cases that require multiple tasks to be performed simultaneously. But there is also a technical reason that is driving concurrency in programming, which is that CPU clock speeds on a single core are hitting upper practical limits. So, it is becoming <a id="_idIndexMarker643"/>necessary to add more CPU cores, and more processors on a single machine. This is in turn driving the need for software that can efficiently utilize the additional CPU cores. To achieve this, portions of a program should be executable concurrently on different CPU cores, rather than being constrained by the sequential execution of instructions on a single CPU core.</p>
			<p>These factors have resulted in the increased use of multi-threading concepts in programming. Here, there are two related terms that need to be understood – <em class="italic">concurrency</em> and <em class="italic">parallelism</em>. Let's take a closer look at this.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor153"/>Concurrency versus parallelism</h2>
			<p>In this <a id="_idIndexMarker644"/>section, we'll review <a id="_idIndexMarker645"/>the fundamentals of multi-threading and understand the differences between <em class="italic">concurrent</em> and <em class="italic">parallel</em> execution models of a program.</p>
			<div><div><img src="img/Figure_9.1_B16405.jpg" alt="Figure 9.1 – Concurrency basics"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Concurrency basics</p>
			<p><em class="italic">Figure 9.1</em> shows three different computation scenarios within a Unix/Linux process:</p>
			<ul>
				<li><strong class="bold">Sequential execution</strong>: Let's assume that a <a id="_idIndexMarker646"/>process has two tasks <strong class="bold">A</strong> and <strong class="bold">B</strong>. <strong class="bold">Task A</strong> has three subtasks <strong class="bold">A1</strong>, <strong class="bold">A2</strong>, and <strong class="bold">A3</strong>, which are executed sequentially. Likewise, <strong class="bold">Task B</strong> has two tasks, <strong class="bold">B1</strong> and <strong class="bold">B2</strong>, that are executed one after the other. Overall, the process executes all tasks of process <em class="italic">A</em> before taking on process <em class="italic">B</em> tasks. There is a challenge in this model. Assume the case where task <strong class="bold">A2</strong> involves waiting for an external network or user input, or for a system resource to become available. Here, all <a id="_idIndexMarker647"/>tasks lined up after task <strong class="bold">A2</strong> will be blocked until <strong class="bold">A2</strong> completes. This is not an efficient use of the CPU and causes a delay in the completion of all the scheduled tasks that belong to the process.</li>
				<li><strong class="bold">Concurrent execution</strong>: Sequential <a id="_idIndexMarker648"/>programs are limited as they do not have the ability to deal with multiple simultaneous inputs. This is the reason many modern applications are <em class="italic">concurrent</em> where there are multiple threads of execution running concurrently.<p>In the concurrent model, the process interleaves the tasks, that is, alternates between the execution of <strong class="bold">Task A</strong> and <strong class="bold">Task B</strong>, until both of them are complete. Here, even if <strong class="bold">A2</strong> is blocked, it allows progress with the other sub-tasks. Each sub-task, <strong class="bold">A1</strong>, <strong class="bold">A2</strong>, <strong class="bold">A3</strong>, <strong class="bold">B1</strong>, and <strong class="bold">B2</strong>, can be scheduled on separate execution threads. These threads could run either on a single processor or scheduled across multiple processor cores. One thing to bear in mind is that concurrency is about <em class="italic">order-independent</em> computations as opposed to sequential execution, which relies on steps executed in a specific order to arrive at the correct program outcome. Writing programs to accommodate <em class="italic">order-independent</em> computations is more challenging than writing sequential programs.</p></li>
				<li><strong class="bold">Parallel execution</strong>: This is a <a id="_idIndexMarker649"/>variant of the <em class="italic">concurrent execution</em> model. In this model, the process executes <strong class="bold">Task A</strong> and <strong class="bold">Task B</strong> truly in parallel, on separate CPU processors or cores. This assumes, of course, that the software is written in a way that such parallel execution is possible, and there are no dependencies between <strong class="bold">Task A</strong> and <strong class="bold">Task B</strong> that could stall the execution or corrupt the data.<p>Parallel computing <a id="_idIndexMarker650"/>is a broad term. <em class="italic">Parallelism</em> can be achieved either <a id="_idIndexMarker651"/>within a single machine by having <strong class="bold">multi-cores</strong> or <strong class="bold">multi-processors</strong> or there can be clusters of different computers that can cooperatively perform a set of tasks.</p><p class="callout-heading">When to use concurrent versus parallel execution?</p><p class="callout">A program or a <a id="_idIndexMarker652"/>function is <em class="italic">compute-intensive</em> when it involves a lot of computations such as in graphics, meteorological, or genome processing. Such programs spend the bulk of their time using CPU cycles and will benefit from having better and faster CPUs.</p><p class="callout">A program is <em class="italic">I/O-intensive</em> when a bulk of the processing involves communicating with input/output devices such as network sockets, filesystems, and other devices. Such programs benefit from having faster I/O subsystems, such as for disk or network access.</p><p class="callout">Broadly, <em class="italic">parallel execution</em> (true parallelism) is more relevant for increasing the throughput of programs in <em class="italic">compute-intensive</em> use cases, while <em class="italic">concurrent processing</em> (or pseudo-parallelism) can be suitable for increasing throughput and reducing latency in <em class="italic">I/O-intensive</em> use cases.</p></li>
			</ul>
			<p>In this section, we've seen two ways to write concurrent programs – <em class="italic">concurrency</em> and <em class="italic">parallelism</em>, and how these differ from sequential models of execution. Both these models use <em class="italic">multi-threading</em> as the foundational concept. Let's talk more about this in the next section.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor154"/>Concepts of multi-threading</h2>
			<p>In this section, we'll deep-dive into <a id="_idIndexMarker653"/>how multi-threading is implemented in Unix.</p>
			<p>Unix supports threads as a mechanism for a process to perform multiple tasks concurrently. A Unix process starts up with a single thread, which is the main thread of execution. But additional threads can be spawned, that can <em class="italic">execute concurrently in a single-processor system</em>, or <em class="italic">execute in parallel in a multi-processor system</em>.</p>
			<p>Each thread has access to its own <em class="italic">stack</em> for storing its own <em class="italic">local variables</em> and <em class="italic">function parameters</em>. Threads also maintain their own register state including the <em class="italic">stack pointer</em> and <em class="italic">program counter</em>. All the threads in a process share the same memory address space, which means that they share access to the <em class="italic">data</em> segments (<em class="italic">initialized data</em>, <em class="italic">uninitialized data</em>, and the <em class="italic">heap</em>). Threads also share the same <em class="italic">program code</em> (process instructions).</p>
			<p>In a multi-threaded process, multiple threads concurrently execute the same program. They may be executing different parts of a program (such as different functions) or they may be invoking the same function in different threads (working with a different set of data for processing). But note that for a function to be invoked by multiple threads at the same time, it needs to be <em class="italic">thread-safe</em>. Some ways to make a function thread-safe are to avoid the usage of <em class="italic">global</em> or <em class="italic">static</em> variables in the function, using a <em class="italic">mutex</em> to restrict usage of a function to just one thread at a time, or using <em class="italic">mutex</em> to synchronize usage of a piece of shared data.</p>
			<p>But it is a design choice to <a id="_idIndexMarker654"/>model a concurrent program either as a group of processes or as a group of threads within the same process. Let's compare the two approaches, for a Unix-like system.</p>
			<p>It is much easier to share data across threads as they are in the same process space. Threads also share common resources of a process such as <em class="italic">file descriptors</em> and <em class="italic">user/group IDs</em>. Thread creation is faster than process creation. Context switching between threads is also faster for the CPU due to their sharing the same memory space. But threads bring their own share of complexities.</p>
			<p>As discussed earlier, shared functions must be thread-safe and access to shared global data should be carefully synchronized. Also, a critical defect in one of the threads can affect other threads or even bring the entire process down. Additionally, there is no guarantee about the order in which different parts of code in different threads will run, which can lead to data races, deadlocks, or hard-to-reproduce bugs. Bugs related to concurrency are difficult to debug since factors such as CPU speed, the number of threads, and the set of running applications at a point in time, can alter the outcome of a concurrent program. In spite of these drawbacks, if one decides to proceed with the thread-based concurrency model, aspects such as code structure, the use of global variables, and thread synchronization should be carefully designed.</p>
			<p><em class="italic">Figure 9.2</em> shows the memory layout of threads within a process:</p>
			<div><div><img src="img/Figure_9.2_B16405.jpg" alt="Figure 9.2 – Memory layout of threads in a process"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – Memory layout of threads in a process</p>
			<p>The figure shows <a id="_idIndexMarker655"/>how a set of tasks in process P1 are represented in memory when they are executed in a multi-threaded model. We've seen in detail the memory layout of a process, in <a href="B16405_05_Final_NM_ePUB.xhtml#_idTextAnchor083"><em class="italic">Chapter 5</em></a>, <em class="italic">Memory Management in Rust</em>. <em class="italic">Figure 9.2</em> extends the process memory layout with details of how memory is allocated for individual threads within a process.</p>
			<p>As discussed earlier, all threads are allocated memory within the process memory space. By default, the main thread is created with its own stack. Additional threads are also assigned their own stack as and when they are created. The shared model of concurrency, which we discussed earlier in the chapter, is possible because global and static variables of a process are accessible by all threads, and each thread also can pass around pointers to memory created on the heap to other threads.</p>
			<p>The program code, however, is common for the threads. Each thread can execute a different section of the code from the program text segment, and store the local variables and function <a id="_idIndexMarker656"/>parameters within their respective thread stack. When it is the turn of a thread to execute, its program counter (containing the address of the instruction to execute) is loaded for the CPU to execute the set of instructions for a given thread.</p>
			<p>In the example shown in the diagram, if task <em class="italic">A2</em> is blocked waiting for I/O, then the CPU will switch execution to another task such as <em class="italic">B1</em> or <em class="italic">A1</em>.</p>
			<p>With this, we conclude the section on concurrency and multi-threading basics. We are now ready to get started with writing concurrent programs using the Rust Standard Library.</p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor155"/>Spawning and configuring threads</h1>
			<p>In the previous <a id="_idIndexMarker657"/>section, we reviewed the <a id="_idIndexMarker658"/>fundamentals of multi-threading that apply broadly to all user processes in the Unix environment. There is, however, another aspect of threading that is dependent on the programming language for implementation – this is the <em class="italic">threading model</em>.</p>
			<p>Rust implements a <em class="italic">1:1 model</em> of threading where each operating system thread maps to one user-level thread created by the Rust Standard Library. The alternative model is <em class="italic">M:N </em>(also <a id="_idIndexMarker659"/>known as <strong class="bold">green threads</strong>) where there are <em class="italic">M green threads</em> (user-level threads managed by a runtime) that map to <em class="italic">N kernel-level threads</em>.</p>
			<p>In this section, we'll cover the fundamentals of creating <em class="italic">1:1</em> operating system threads using the Rust Standard Library. The Rust Standard Library module for thread-related functions is <code>std::thread</code>.</p>
			<p>There are two ways to create a new thread using the Rust Standard Library. The first method uses the <code>thread::spawn</code> function, and the second method uses the builder pattern using the <code>thread::Builder</code> struct. Let's look at an example of the <code>thread::spawn</code> function first:</p>
			<pre>use std::thread;
fn main() {
    for _ in 1..5 {
        thread::spawn(|| {
            println!("Hi from thread id {:?}", 
                thread::current().id());
        });
    }
} </pre>
			<p>The <code>std::thread</code> module is used in this program. <code>thread::spawn()</code> is the function used to spawn a new thread. In the program shown, we're spawning four new child threads in the main function (which runs in the main thread in the process). Run this program with <code>cargo run</code>. Run it a few more times. What did you expect to see, and what did you actually see?</p>
			<p>You would have <a id="_idIndexMarker660"/>expected to see four lines <a id="_idIndexMarker661"/>printed to the terminal listing the <em class="italic">thread IDs</em>. But you would have noticed that the results vary each time. Sometimes you see one line printed, sometimes you see more, and sometimes none. Why is this?</p>
			<p>The reason for this inconsistency is that there is no guarantee of the order in which the threads are executed. Further, if the <code>main()</code> function completes before the child threads are executed, you won't see the expected output in your terminal.</p>
			<p>To fix this, what we need to do is to join the <em class="italic">child threads</em> that are created to the <em class="italic">main thread</em>. Then the <code>main()</code> thread waits until all the child threads have been executed. To see this in action, let's alter the program as shown:</p>
			<pre>use std::thread;
fn main() {
    <strong class="bold">let mut child_threads = Vec::new();</strong>
    for _ in 1..5 {
        <strong class="bold">let handle = thread::spawn(|| {</strong>
            println!("Hi from thread id {:?}", 
                thread::current().id());
        });
        <strong class="bold">child_threads.push(handle);</strong>
    }
    <strong class="bold">for i in child_threads {</strong>
<strong class="bold">        i.join().unwrap();</strong>
<strong class="bold">    }</strong>
}</pre>
			<p>The changes from the previous program are highlighted. <code>thread::spawn()</code> returns a thread handle that we're storing in a <code>Vec</code> collection data type. Before the end of the <code>main()</code> function, we join each child thread to the main thread. This ensures that the <code>main()</code> function waits until the completion of all the child threads before it exits.</p>
			<p>Let's run the program again. You'll notice four lines printed, one for each thread. Run the program a few more <a id="_idIndexMarker662"/>times. You'll see four lines printed every time. This is progress. It shows that <a id="_idIndexMarker663"/>joining the child threads to the main threads is helping. However, the order of thread execution (as seen by the order of print outputs on the terminal) varies with each run. This is because, when we span multiple child threads, there is no guarantee of the order in which the threads are executed. This is a feature of multi-threading (as discussed earlier), not a bug. But this is also one of the challenges of working with threads, as this brings difficulties in synchronizing activities across threads. We'll learn how to address this a little later in the chapter.</p>
			<p>We've so far seen how to use the <code>thread::spawn()</code> function to create a new thread. Let's now see the second way to create a new thread.</p>
			<p>The <code>thread::spawn()</code> function uses default parameters for thread name and stack size. If you'd like to set them explicitly, you can use <code>thread:Builder</code>. This is a <em class="italic">thread factory</em> that uses the <code>Builder</code> pattern to configure the properties of a new thread. The previous example has been rewritten here using the <code>Builder</code> pattern:</p>
			<pre>use std::thread;
fn main() {
    let mut child_threads = Vec::new();
    for <strong class="bold">i</strong> in 1..5 {
        <strong class="bold">let builder = thread::Builder::new().name(format!(</strong>
<strong class="bold">            "mythread{}", i));</strong>
        let handle = <strong class="bold">builder</strong>
<strong class="bold">            .spawn</strong>(|| {
                println!("Hi from thread id {:?}", thread::
                    current().name().unwrap());
            })
            <strong class="bold">.unwrap();</strong>
        child_threads.push(handle);
    }
 
    for i in child_threads {
        i.join().unwrap();
    }
}</pre>
			<p>The changes are <a id="_idIndexMarker664"/>highlighted in <a id="_idIndexMarker665"/>the code. We are creating a new <code>builder</code> object by using the <code>new()</code> function, and then configuring the name of the thread using the <code>name()</code> method. We're then using the <code>spawn()</code> method on an instance of the <code>Builder</code> pattern. Note that the <code>spawn()</code> method returns a <code>JoinHandle</code> type wrapped in <code>io::Result&lt;JoinHandle&lt;T&gt;&gt;</code>, so we have to unwrap the return value of the method to retrieve the child process handle.</p>
			<p>Run the <a id="_idIndexMarker666"/>code and you'll see the four thread names <a id="_idIndexMarker667"/>printed to your terminal.</p>
			<p>We've so far seen how to spawn new threads. Let's now take a look at error handling while working with threads.</p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor156"/>Error handling in threads</h1>
			<p>The Rust Standard Library <a id="_idIndexMarker668"/>contains the <code>std::thread::Result</code> type, which is a <a id="_idIndexMarker669"/>specialized <code>Result</code> type for threads. An example of how to use this is shown in the following code:</p>
			<pre>use std::fs;
use std::thread;
fn copy_file() -&gt; thread::Result&lt;()&gt; {
    thread::spawn(|| {
        fs::copy("a.txt", "b.txt").expect("Error 
            occurred");
    })
    .join()
}
fn main() {
    match copy_file() {
        Ok(_) =&gt; println!("Ok. copied"),
        Err(_) =&gt; println!("Error in copying file"),
    }
}</pre>
			<p>We have a function, <code>copy_file()</code>, that copies a source file to a destination file. This function returns a <code>thread::Result&lt;()&gt;</code> type, which we are unwrapping using a <code>match</code> statement in the <code>main()</code> function. If the <code>copy_file()</code> function returns a <code>Result::Err</code> variant, we handle it by printing an error message.</p>
			<p>Run the program with <code>cargo run</code> with an invalid source filename. You will see the error message: <code>Ok()</code> branch of the <code>match</code> clause, and the success message will be printed.</p>
			<p>This example shows us how to handle errors propagated by a thread in the calling function. What if we want a <a id="_idIndexMarker670"/>way to recognize that the current thread is <a id="_idIndexMarker671"/>panicking, even before it is propagated to the calling function. The Rust Standard Library has a function, <code>thread::panicking()</code>, available in the <code>std::thread</code> module for this. Let's learn how to use it by modifying the previous example:</p>
			<pre>use std::fs;
use std::thread;
struct Filenames {
    source: String,
    destination: String,
}
impl Drop for Filenames {
    fn drop(&amp;mut self) {
        if thread::panicking() {
            println!("dropped due to  panic");
        } else {
            println!("dropped without panic");
        }
    }
}
fn copy_file(file_struct: Filenames) -&gt; thread::Result&lt;()&gt; {
    thread::spawn(move || {
        fs::copy(&amp;file_struct.source, 
            &amp;file_struct.destination).expect(
            "Error occurred");
    })
    .join()
}
fn main() {
    let foo = Filenames {
        source: "a1.txt".into(),
        destination: "b.txt".into(),
    };
    match copy_file(foo) {
        Ok(_) =&gt; println!("Ok. copied"),
        Err(_) =&gt; println!("Error in copying file"),
    }
}</pre>
			<p>We've created a <a id="_idIndexMarker672"/>struct, <code>Filenames</code>, which contains the source <a id="_idIndexMarker673"/>and destination filenames to copy. We're initializing the source filename with an invalid value. We're also implementing the <code>Drop</code> trait for the <code>Filenames</code> struct, which gets called when an instance of the struct goes out of scope. In this <code>Drop</code> trait implementation, we are using the <code>thread::panicking()</code> function to check if the current thread is panicking, and are handling it by printing out an error message. The error is then propagated to the main function, which also handles the thread error and prints out another error message.</p>
			<p>Run the program with <code>cargo run</code> and an invalid source filename, and you will see the following messages printed to your terminal:</p>
			<pre>dropped due to  panic
Error in copying file</pre>
			<p>Also, note the use of the <code>move</code> keyword in the <code>closure</code> supplied to the <code>spawn()</code> function. This is needed for the thread to transfer ownership of the <code>file_struct</code> data structure from the <code>main</code> thread to the newly spawned thread.</p>
			<p>We've seen how to handle thread panic in the calling function and also how to detect if the current thread is <a id="_idIndexMarker674"/>panicking. Handling errors in child threads is very important to <a id="_idIndexMarker675"/>ensure that the error is isolated and does not bring the whole process down. Hence special attention is needed to design error handling for multi-threaded programs.</p>
			<p>Next, we'll move on to the topic of how to synchronize computations across threads, which is an important aspect of writing concurrent programs.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor157"/>Message passing between threads</h1>
			<p>Concurrency is a <a id="_idIndexMarker676"/>powerful feature that enables the writing of new kinds of applications. However, the <a id="_idIndexMarker677"/>execution and debugging of concurrent programs are difficult because their execution is non-deterministic. We saw this through examples in the previous section where the order of print statements varied for each run of the program. The order in which the threads will be executed is not known ahead of time. A concurrent program developer must make sure that the program will execute correctly overall, regardless of the order in which the individual threads are executed.</p>
			<p>One way to ensure program correctness in the face of the unpredictable ordering of thread execution is to introduce mechanisms for synchronizing activities across threads. One such model for concurrent programming is <em class="italic">message-passing concurrency</em>. It is a way to structure the components of a concurrent program. In our case, concurrent components are <em class="italic">threads</em> (but they can also be processes). The Rust Standard Library has implemented a <em class="italic">message-passing concurrency</em> solution <a id="_idIndexMarker678"/>called <strong class="bold">channels</strong>. <em class="italic">A channel</em> is basically like a pipe, with two parts – a <em class="italic">producer</em> and a <em class="italic">consumer</em>. The <em class="italic">producer</em> puts a message into a <em class="italic">channel</em>, and a <em class="italic">consumer</em> reads from the <em class="italic">channel</em>.</p>
			<p>Many programming languages implement the concept of channels for inter-thread communications. But Rust's implementation of <em class="italic">channels</em> has a special property – <em class="italic">multiple producer single consumer</em> (<code>mpsc</code>). This means, there can be multiple sending ends but only one consuming end. Translate this to the world of threads: we can have multiple threads that send values into a channel, but there can be only one thread that can receive and consume these values. Let's see how this works with an example that we'll build out step by step. The complete code listing is also provided in the Git repo for the chapter under <code>src/message-passing.rs</code>:</p>
			<ol>
				<li value="1">Let's first declare the module imports – the <code>mpsc</code> and <code>thread</code> modules from the standard library:<pre>use std::sync::mpsc;
use std::thread;</pre></li>
				<li>Within the <code>main()</code> function, create a new <code>mpsc</code> channel:<pre>let (transmitter1, receiver) = mpsc::channel();</pre></li>
				<li>Clone the <a id="_idIndexMarker679"/>channel so we can have two transmitting <a id="_idIndexMarker680"/>threads:<pre>let transmitter2 = mpsc::Sender::clone(&amp;transmitter1);</pre></li>
				<li>Note that we now have two transmission handles – <code>transmitter1</code> and <code>transmitter2</code>, and one receiving handle – <code>receiver</code>.</li>
				<li>Spawn a new thread moving the transmission handle <code>transmitter1</code> into the thread closure. Inside this thread, send a bunch of values into the channel using the transmission handle:<pre>    thread::spawn(move || {
        let num_vec: Vec&lt;String&gt; = vec!["One".into(), 
            "two".into(), "three".into(), 
            "four".into()];
        for num in num_vec {
            transmitter1.send(num).unwrap();
        }
    });</pre></li>
				<li>Spawn a second thread moving the transmission handle <code>transmitter2</code> into the thread closure. Inside this thread, send another bunch of values into the channel using the transmission handle:<pre>    thread::spawn(move || {
        let num_vec: Vec&lt;String&gt; =
            vec!["Five".into(), "Six".into(), 
                "Seven".into(), "eight".into()];
        for num in num_vec {
            transmitter2.send(num).unwrap();
        }
    });</pre></li>
				<li>In the main <a id="_idIndexMarker681"/>thread of the program, use the receiving handle of the channel <a id="_idIndexMarker682"/>to consume the values being written into the channel by the two child threads:<pre>    for received_val in receiver {
        println!("Received from thread: {}", 
            received_val);
    }</pre><p>The complete code listing is shown:</p><pre>use std::sync::mpsc;
use std::thread;
fn main() {
    let (transmitter1, receiver) = mpsc::channel();
    let transmitter2 = mpsc::Sender::clone(
        &amp;transmitter1);
    thread::spawn(move || {
        let num_vec: Vec&lt;String&gt; = vec!["One".into(), 
            "two".into(), "three".into(), 
            "four".into()];
        for num in num_vec {
            transmitter1.send(num).unwrap();
        }
    });
    thread::spawn(move || {
        let num_vec: Vec&lt;String&gt; =
            vec!["Five".into(), "Six".into(), 
                "Seven".into(), "eight".into()];
        for num in num_vec {
            transmitter2.send(num).unwrap();
        }
    });
    for received_val in receiver {
        println!("Received from thread: {}", 
            received_val);
    }
}</pre></li>
				<li>Run the program with <code>cargo run</code>. (<em class="italic">Note:</em> If you are running code from the Packt Git repo, use <code>cargo run --bin message-passing</code>). You'll see the values printed out in the main program thread, which are sent from the two child threads. Each time you run the program, you may get a different order in which the values are received, as the order of thread execution is <em class="italic">non-deterministic</em>.</li>
			</ol>
			<p>The <code>mpsc</code> channel offers a lightweight inter-thread synchronization mechanism that can be used for message-based <a id="_idIndexMarker683"/>communications across threads. This type of <a id="_idIndexMarker684"/>concurrent programming model is useful when you want to spawn out multiple threads for different types of computations and want to have the main thread aggregate the results.</p>
			<p>One aspect to note in <code>mpsc</code> is that once a value is sent down a channel, the sending thread no longer has ownership of it. If you want to retain ownership or continue to use a value, but still need a way to share the value with other threads, there is another concurrency model that <a id="_idIndexMarker685"/>Rust supports called <strong class="bold">sha<a id="_idTextAnchor158"/>red-state concurrency</strong>. We'll look at that next.</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor159"/>Achieving concurrency with shared state</h1>
			<p>In this section, we'll <a id="_idIndexMarker686"/>discuss the second model of <a id="_idIndexMarker687"/>concurrent programming supported in the Rust Standard Library – the <em class="italic">shared-state</em> or <em class="italic">shared-memory</em> model of concurrency. Recall that all threads in a process share the same process memory space, so why not use that as a way to communicate between threads, rather than message-passing? We'll look at how to achieve this using Rust.</p>
			<p>A combination of <code>Mutex</code> and <code>Arc</code> constitutes the primary way to implement <em class="italic">shared-state concurrency</em>. <code>Mutex</code> (mutual exclusion lock) is a mechanism that allows only one thread to access a piece of data at one time. First, a data value is wrapped in a <code>Mutex</code> type, which acts as a lock. You can visualize <code>Mutex</code> like a box with an external lock, protecting something valuable inside. To access what's in the box, first of all, we have to ask someone to open the lock and hand over the box. Once we're done, we hand over the box back and someone else asks to take charge of it.</p>
			<p>Similarly, to access or mutate a value protected by a <code>Mutex</code>, we must acquire the lock first. Asking for a lock on a <code>Mutex</code> object returns a <code>MutexGuard</code> type, which lets us access the inner value. During this time, no other thread can access this value protected by the <code>MutexGuard</code>. Once we're done using it, we have to release the <code>MutexGuard</code> (which Rust does for us automatically as the <code>MutexGuard</code> goes out of scope, without us having to call a separate <code>unlock()</code> method).</p>
			<p>But there is another issue to resolve. Protecting a value with a lock is just one part of the solution. We also have to give ownership of a value to multiple threads. To support multiple ownership of a value, Rust uses <em class="italic">reference-counted</em> <em class="italic">smart pointers</em> – <code>Rc</code> and <code>Arc</code>. <code>Rc</code> allows multiple owners for a value through its <code>clone()</code> method. But <code>Rc</code> is not safe to use across threads, and <code>Arc</code> (which stands for Atomically Reference Counted) is the thread-safe equivalent of <code>Rc</code>. So, we need to wrap the <code>Mutex</code> with an <code>Arc</code> reference-counted smart-pointer, and transfer ownership of the value across threads. Once the ownership of the Arc-protected Mutex is transferred to another thread, the receiving thread can call <code>lock()</code> on the Mutex to get exclusive access to the inner value. The Rust ownership model helps in enforcing the rules around this model.</p>
			<p>The way the <code>Arc&lt;T&gt;</code> type works is that it provides the shared ownership of a value of type<code> T</code>, allocated in the heap. By calling the associated function <code>clone()</code> on an <code>Arc</code> instance, a new <a id="_idIndexMarker688"/>instance of the <code>Arc</code> reference-counted pointer is created, which points to the same <a id="_idIndexMarker689"/>allocation on the heap as the source <code>Arc</code>, while increasing a reference count. With each <code>clone()</code>, the reference count is increased by the <code>Arc</code> smart pointer. When each <code>cloned()</code> pointer goes out of scope, the reference counter is decremented. When the last of the clones go out of scope, both the <code>Arc</code> pointer and the value it points to (in the heap) are destroyed.</p>
			<p>To summarize, <code>Mutex</code> ensures that at most one thread is able to access some data at one time, while <code>Arc</code> enables shared ownership of some data and prolongs its lifetime until all the threads have finished using it.</p>
			<p>Let's see the usage of <code>Mutex</code> with <code>Arc</code> to demonstrate shared-state concurrency with a step-by-step example. This time, we'll write a more complex example than just incrementing a shared counter value across threads. We'll take the example we wrote in <a href="B16405_06_Final_NM_ePUB.xhtml#_idTextAnchor101"><em class="italic">Chapter 6</em></a>, <em class="italic">Working with Files and Directories in Rust</em>, to compute source file stats for all Rust files in a directory tree, and modify it to make it a concurrent program. We'll define the structure of the program in the next section. The complete code for this section can be found in the Git repo under <code>src/shared-state.rs</code>.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor160"/>Defining the program structure</h2>
			<p>What we'd like to do is to take a list of directories as input to our program, compute source file statistics for each <a id="_idIndexMarker690"/>file within each of these directories, and print out a consolidated set of source code stats.</p>
			<p>Let's first create a <code>dirnames.txt</code> file in the root folder of the cargo project, containing a list of directories with a full path, one per line. We'll read each entry from this file and spawn a separate thread to compute the source file stats for the Rust files within that directory tree. So, if there are five directory-name entries in the file, there will be five threads created from the main program, each of which will recursively walk through the directory structure of the entry, and compute the consolidated Rust source file stats. Each thread will increment the computed value in a shared data structure. We'll use <code>Mutex</code> and <code>Arc</code> to protect access and update the shared data safely across threads.</p>
			<p>Let's start writing the code:</p>
			<ol>
				<li value="1">We'll start with the module imports for this program:<pre>use std::ffi::OsStr;
use std::fs;
use std::fs::File;
use std::io::{BufRead, BufReader};
use std::path::PathBuf;
use std::sync::{Arc, Mutex};
use std::thread;</pre></li>
				<li>Define a struct to store the source file stats:<pre>#[derive(Debug)]
pub struct SrcStats {
    pub number_of_files: u32,
    pub loc: u32,
    pub comments: u32,
    pub blanks: u32,
}</pre></li>
				<li>Within the <code>main()</code> function, create a new instance of <code>SrcStats</code>, protect it with a <code>Mutex</code> lock, and then wrap it inside an <code>Arc</code> type:<pre>  let src_stats = SrcStats {
        number_of_files: 0,
        loc: 0,
        comments: 0,
        blanks: 0,
   };
   let stats_counter = Arc::new(
       Mutex::new(src_stats));</pre></li>
				<li>Read the <code>dirnames.txt</code> file, and <a id="_idIndexMarker691"/>store the individual entries in a vector:<pre>    let mut dir_list = File::open(
        "./dirnames.txt").unwrap();
    let reader = BufReader::new(&amp;mut dir_list);
    let dir_lines: Vec&lt;_&gt; = reader.lines().collect();</pre></li>
				<li>Iterate through the <code>dir_lines</code> vector, and for each entry, spawn a new thread to perform the following two steps: <p>a) Accumulate the list of files from each subdirectory in the tree.</p><p>b) Then open each file and compute the stats. Update the stats in the shared-memory struct protected by <code>Mutex</code> and <code>Arc</code>.</p></li>
			</ol>
			<p>The overall skeletal structure of the code for this step looks like this:</p>
			<pre>    let mut child_handles = vec![];
    for dir in dir_lines {
        let dir = dir.unwrap();
        let src_stats = Arc::clone(&amp;stats_counter);
 
        let handle = thread::spawn(move || {
        // Do processing: A) 
        // Do processing: B)
        });
        child_handles.push(handle);
    }</pre>
			<p>In this section, we <a id="_idIndexMarker692"/>read the list of directory entries for computing source file statistics from a file. We then iterated through the list to spawn a thread to process each entry. In the next section, we'll define the processing to be done in each thread.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor161"/>Aggregating source file statistics in shared state</h2>
			<p>In this section, we'll write the <a id="_idIndexMarker693"/>code for computing source file statistics in each thread and aggregate the results in shared state. We'll look at the code in two parts – <em class="italic">sub-steps A</em> and <em class="italic">B</em>:</p>
			<ol>
				<li value="1">In <em class="italic">sub-step A</em>, let's read through each subdirectory under the directory entry, and accumulate the consolidated list of all Rust source files in the <code>file_entries</code> vector. The code for <em class="italic">sub-step A</em> is shown. Here, we are first creating two vectors to hold the directory and filenames respectively. Then we are iterating through the directory entries of each item from the <code>dirnames.txt</code> file, and accumulating the entry names into the <code>dir_entries</code> or <code>file_entries</code> vector depending upon whether it is a directory or an individual file:<pre>            let mut dir_entries = vec![PathBuf::
                from(dir)];
            let mut file_entries = vec![];
            while let Some(entry) = dir_entries.pop() 
            {            
                for inner_entry in fs::read_dir(
                    &amp;entry).unwrap() {
                    if let Ok(entry) = inner_entry {
                        if entry.path().is_dir() {
                            dir_entries.push(
                                entry.path());
                        } else {
                            if entry.path()
                                .extension() 
                               == Some(OsStr::
                                   new("rs")) 
                            {
                                println!("File name 
                                    processed is 
                                    {:?}",entry);
                                
                                file_entries.push(
                                    entry);
                            
                            }
                        }
                    }
                }
            }</pre><p>At the end of <em class="italic">sub-step A</em>, all individual filenames are stored in the <code>file_entries</code> vector, which we will use in <em class="italic">sub-step B</em> for further processing.</p></li>
				<li>In <em class="italic">sub-step B</em>, we'll read each file from the <code>file_entries</code> vector, compute the source stats <a id="_idIndexMarker694"/>for each file, and save the values in the shared memory struct. Here is the code snippet for <em class="italic">sub-step B</em>:<pre>            for file in file_entries {
                let file_contents = 
                    fs::read_to_string(
                    &amp;file.path()).unwrap();
 
                let mut stats_pointer = 
                    src_stats.lock().unwrap();
                for line in file_contents.lines() {
                    if line.len() == 0 {
                        stats_pointer.blanks += 1;
                    } else if line.starts_with("//") {
                        stats_pointer.comments += 1;
                    } else {
                        stats_pointer.loc += 1;
                    }
                }
 
                stats_pointer.number_of_files += 1;
            }</pre></li>
				<li>Let's again review the skeletal structure of the program shown next. We've so far seen the code to be executed within the thread, which includes processing for steps A and B: <pre>    let mut child_handles = vec![];
    for dir in dir_lines {
        let dir = dir.unwrap();
        let src_stats = Arc::clone(&amp;stats_counter);
 
        let handle = thread::spawn(move || {
        // Do processing: step A) 
        // Do processing: step B)
        });
           <code>child_handles</code> vector.</p></li>
				<li>Let's look at the last part of the code now. As discussed earlier, in order to ensure that the main thread does not complete before the child threads are completed, we have to join the child thread handles with the main threads. Also, let's print out the final value of the thread-safe <code>stats_counter</code> struct, which contains aggregated source stats from all the Rust source files under the directory (updated by the individual threads):<pre>    for handle in child_handles {
        handle.join().unwrap();
    }
    println!(
        "Source stats: {:?}",
        stats_counter.lock().unwrap()
    );</pre><p>The complete code listing can be found in the Git repo for the chapter in <code>src/shared-state.rs</code>.</p><p>Before running this program, ensure to create a file, <code>dirnames.txt</code>, in the root folder of the cargo project, containing a list of directory entries with a full path, each on a separate line.</p></li>
				<li>Run the project with <code>cargo run</code>. (<em class="italic">Note</em>: If you are running code from the Packt Git repo, use <code>cargo run --bin shared-state</code>.) You will see the consolidated <a id="_idIndexMarker696"/>source stats printed out. Note that we have now implemented a multi-threaded version of the project we wrote in <a href="B16405_06_Final_NM_ePUB.xhtml#_idTextAnchor101"><em class="italic">Chapter 6</em></a>, <em class="italic">Working with Files and Directories in Rust</em>. As an exercise, alter this example to implement the same project with the <em class="italic">message-passing concurrency</em> model.</li>
			</ol>
			<p>In this section, we've seen how multiple threads can safely write to a shared value (wrapped in <code>Mutex</code> and <code>Arc</code>) that is stored in process heap memory, in a thread-safe manner.  In the next section, we will review one more mechanism available to control thread execution, which is to selectively pause the processing of the current thread.</p>
			<p class="callout-heading">Send and Sync traits</p>
			<p class="callout">We saw earlier how a data type can be shared across threads, and how messages can be passed between threads. There is <a id="_idIndexMarker697"/>another aspect of concurrency in Rust though. Rust <a id="_idIndexMarker698"/>defines data types as thread-safe or not.</p>
			<p class="callout">From a concurrency perspective, there are two categories of data types in Rust: those that are <code>Send</code> (that is, implement the <code>Send</code> trait), which means they are safe to be transferred from one thread to another. And the rest are <em class="italic">thread-unsafe</em> types. A related concept is <code>Sync</code>, which is associated with references of types. A type is considered to be <code>Sync</code> if its reference can be passed to another thread safely. So, <code>Send</code> means it is safe to transfer ownership of a type from one thread to another, while <code>Sync</code> means the data type can be shared (using references) safely by multiple threads at the same time. Note though that in <code>Send</code>, after a value has been transferred from the sending to the receiving thread, the sending thread can no longer use that value.</p>
			<p class="callout"><code>Send</code> and <code>Sync</code> are also automatically derived traits. This means that if a type consists of members that implement <code>Send</code> or <code>Sync</code> types, the type itself automatically becomes <code>Send</code> or <code>Sync</code>. The Rust primitives (almost all of them) implement <code>Send</code> and <code>Sync</code>, which means if you create a custom type from Rust primitives, your custom type also becomes <code>Send</code> or <code>Sync</code>. We've seen an example of this in the previous section, where the <code>SrcStats</code> (source stats) struct was transferred across the boundaries of threads without us having to explicitly implement <code>Send</code> or <code>Sync</code> on the struct.</p>
			<p class="callout">However, if there is a need to implement <code>Send</code> or <code>Sync</code> traits for a data type manually, it would have to be done in unsafe Rust.</p>
			<p class="callout">To <a id="_idIndexMarker699"/>summarize, in Rust, every data <a id="_idIndexMarker700"/>type is classified as either <em class="italic">thread-safe</em> or <em class="italic">thread-unsafe</em>, and the Rust compiler enforces the safe transfer or sharing of thread-safe types across threads.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor162"/>Pausing thread execution with timers</h1>
			<p>Sometimes, during <a id="_idIndexMarker701"/>the processing of a thread, there may be a need to pause execution either to wait for another event or to synchronize execution with other threads. Rust provides support for this using the <code>std::thread::sleep</code> function. This function takes a time duration of type <code>time::Duration</code> and pauses execution of the thread for the specified time. During this time, the processor time can be made available to other threads or applications running on the computer system. Let's see an example of the usage of <code>thread::sleep</code>:</p>
			<pre>use std::thread;
use std::time::Duration;
fn main() {
    let duration = Duration::new(1,0);
    println!("Going to sleep");
    thread::sleep(duration);
    println!("Woke up");
}</pre>
			<p>Using the <code>sleep()</code> function is fairly straightforward, but this blocks the current thread and it is important <a id="_idIndexMarker702"/>to make judicious use of this in a multi-threaded program. An alternative to using <code>sleep()</code> would be to use an async programming model to implement threads with non-blocking I/O.</p>
			<p class="callout-heading">Async I/O in Rust</p>
			<p class="callout">In the multi-threaded model, if there <a id="_idIndexMarker703"/>is a blocking I/O call in any thread, it blocks the <a id="_idIndexMarker704"/>program workflow. The <em class="italic">async</em> model relies on non-blocking system calls for I/O, for example, to access the filesystem or network. In the example of a web server with multiple simultaneous incoming connections, instead of spawning a separate thread to handle each connection in a blocking manner, <em class="italic">async</em> I/O relies on a runtime that does not block the current thread but instead schedules other tasks while waiting on I/O.</p>
			<p class="callout">While Rust has built-in <code>Async/Await</code> syntax, which makes it easier to write <em class="italic">async</em> code, it does not provide any asynchronous system call support. For this, we need to rely on external libraries such as <code>Tokio</code>, which provide both the <em class="italic">async runtime</em> (executor) and the <em class="italic">async</em> versions of the I/O functions that are present in the Rust Standard Library.</p>
			<p class="callout">So, when would one use <em class="italic">async</em> versus the <em class="italic">multi-threaded</em> approach to concurrency? The broad thumb-rule is that the <em class="italic">async</em> model is suited to programs that perform a lot of I/O, whereas, for computation-intensive (CPU-bound) tasks, <em class="italic">multi-threaded concurrency</em> is a better approach. Keep in mind though that it is not a binary choice, as in practice it is not uncommon to see <em class="italic">async</em> programs that also utilize <em class="italic">multi-threading</em> in a hybrid model.</p>
			<p class="callout">For more information on async in Rust, refer <a id="_idIndexMarker705"/>to the following link: <a href="https://rust-lang.github.io/async-book/">https://rust-lang.github.io/async-book/</a>.</p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor163"/>Summary</h1>
			<p>In this chapter, we covered the basics of concurrency and multi-threaded programming in Rust. We started by reviewing the need for concurrent programming models. We understood the differences between the concurrent and parallel execution of programs. We learned how to spawn new threads using two different methods. We handled errors using a special <code>Result</code> type in the thread module and also learned how to check whether the current thread is panicking. We looked at how threads are laid out in process memory. We discussed two techniques for synchronizing processing across threads – <em class="italic">message-passing concurrency</em> and <em class="italic">shared-state concurrency</em>, with practical examples. As a part of this, we learned about channels, <code>Mutex</code> and <code>Arc</code> in Rust, and the role they play in writing concurrent programs. We then discussed how Rust classifies data types as <em class="italic">thread-safe</em> or not, and saw how to pause the execution of the current thread.</p>
			<p>This concludes the chapter on managing concurrency in Rust. This also concludes <em class="italic">Section 2</em> of this book, which is on managing and controlling system resources in Rust.</p>
			<p>We will now move on to the last part of the book – <em class="italic">Section 3</em> covering <em class="italic">advanced topics</em>. In the next chapter, we will cover how to perform <em class="italic">device I/O</em> in Rust, and internalize learning through an example project.</p>
		</div>
	</body></html>
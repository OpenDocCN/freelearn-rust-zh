- en: Futurism – Near-Term Rust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've covered a lot of material in this book.
  prefs: []
  type: TYPE_NORMAL
- en: If you already knew parallel programming well from another systems programming
    language, I hope that now you've got a confident handle on the way Rust operates.
    Its ownership model is strict, and can feel foreign at first. Unsafe Rust, if
    my own experiences are any kind of general guide, feels like more familiar ground.
    I hope that you too have come to appreciate the general safety of Rust, as well
    as its ability to do fiddly things in memory when needed. I find being able to
    implement unsafe code that is then wrapped up in a safe interface nothing short
    of incredible.
  prefs: []
  type: TYPE_NORMAL
- en: If you knew parallel programming fairly well from a high-level, garbage-collected
    programming language, then I hope that this book has served as an introduction
    to parallel programming at a systems level. As you know, memory safety does not
    guarantee correct operation, hence this book's continual focus on testing—generative
    and fuzz, in addition to performance inspection—and careful reasoning about models.
    Rust, I have argued, is one of the easier systems languages to do parallel programming
    well in because of the language's focus on ownership management with lifetimes
    and the strong, static type system. Rust is not immune to bugs—in fact, it's vulnerable
    to a wide variety—but these are common to all languages on modern hardware, and
    Rust does manage to solve a swath of memory-related issues.
  prefs: []
  type: TYPE_NORMAL
- en: If you already knew Rust, but didn't know much about parallel programming, I
    very much hope that this book has managed to convince you of one thing—concurrency
    is not magic. Parallel programming is a wide, wide field of endeavor, one that
    takes a great deal of study, but it can be understood and mastered. Moreover,
    it need not be mastered all at once. The shape of this book argues for one path
    among several. May this book be the first of many on your journey.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever your background, thank you for reading through to the end. I know that,
    at times, the material covered was not easy and it must have taken some real willpower
    to get through it all. I appreciate it. I wrote this book to be what I would have
    wanted for myself ten years ago. It was a real pleasure getting the opportunity
    to write it out for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, we will have:'
  prefs: []
  type: TYPE_NORMAL
- en: Discussed the future of SIMD in Rust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussed the future of async IO in Rust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussed the possibility of specialization being stablized and in what fashion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussed more extensive testing methods for Rust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduced various avenues to get involved with the Rust community
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has no software requirements.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the source code for this book's projects at [https://github.com/PacktPublishing/Hands-On-Concurrency-with-Rust](https://github.com/PacktPublishing/Hands-On-Concurrency-with-Rust).
    Chapter 10 has its source code under `Chapter10/`.
  prefs: []
  type: TYPE_NORMAL
- en: Near-term improvements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The focus of 2018's Rust development has been stabilization. Since the 1.0 days
    in 2015, many important crates were nightly-only, whether because of modifications
    to the compiler or because of the fear of standardizing too quickly on awkward
    APIs. This fear was reasonable. At the time, Rust was a new language and it changed
    *very* drastically in the lead-up to 1.0\. A new language takes time to resolve
    into its natural style. By the end of 2017, the community had come to a general
    feeling that a stabilization cycle was in order, that some natural expression
    of the language had more or less been established, and that in areas where this
    was not true, it could be established, with some work.
  prefs: []
  type: TYPE_NORMAL
- en: Let's discuss this stabilization work with regards to the topics we've followed
    throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: SIMD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we discussed thread-based concurrency. In [Chapter 8](d4802512-564b-4037-9407-b6035bd38f31.xhtml),
    *High-Level Parallelism – Threadpools, Parallel Iterators, and Processes*, we
    took to a higher level of abstraction with Rayon's parallel iterators. Underneath,
    as we saw, rayon uses a sophisticated work-stealing threadpool. Threads are merely
    one approach to concurrency on modern CPUs. In a sense, serial programs are parallel
    on CPUs with deep lookahead pipelines and sophisticated branch predictors, as
    we saw in [Chapter 2](8c61da2f-08c8-40be-89d4-d8afa8510850.xhtml), *Sequential
    Rust Performance and Testing*. Exploiting this parallelism is a matter of carefully
    structuring your data and managing access to it, among the other details we discussed.
    What we have not gone into in this book is how to exploit a modern CPUs' ability
    to perform the same operation on contiguous memory in parallel *without* resorting
    to threads—vectorization. We haven't looked at this because, as I write this,
    the language is only now getting minimal support in the standard library for vectorization
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vectorization comes in two variants—MIMD and SIMD. MIMD stands for multiple
    instructions, multiple data. SIMD stands for single instructions, single data.
    The basic idea is this: A modern CPU can apply an instruction to a contiguous,
    specifically sized block of data in parallel. That''s it. Now, consider how many
    loops we''ve written in this book where we''ve done the exact same operation to
    every element of the loop. More than a few! Had we investigated string algorithms
    or looked into doing numeric calculations—encryption, physics simulations, or
    the like—we would have had more such loops in this book.'
  prefs: []
  type: TYPE_NORMAL
- en: As I write this, the merger of SIMD vectorization into the standard library
    is held up behind RFC 2366 ([https://github.com/rust-lang/rfcs/pull/2366](https://github.com/rust-lang/rfcs/pull/2366)),
    with ambitions to having x86 SIMD in a stable channel by year's end, with other
    architectures to follow. The reader may remember from [Chapter 1](5f3aec9d-fd53-48ff-9ba8-43ce13e91cff.xhtml),* Preliminaries
    – Machine Architecture and Getting Started with Rust,* that the memory systems
    of x86 and ARM are quite different. Well, this difference extends to their SIMD
    systems. It *is* possible to exploit SIMD instructions using stable Rust via the
    stdsimd ([https://crates.io/crates/stdsimd](https://crates.io/crates/stdsimd))
    and simd ([https://crates.io/crates/simd](https://crates.io/crates/simd)) crates.
    The stdsimd crate is the best for programmers to target as it will eventually
    be the standard library implementation.
  prefs: []
  type: TYPE_NORMAL
- en: A discussion of SIMD is beyond the scope of this book, given the remaining space.
    Suffice it to say that getting the details right with SIMD is roughly on a par
    with the difficulty of getting the details right in atomic programming. I expect
    that, post stabilization, the community will build higher-level libraries to exploit
    SIMD approaches without requiring a great deal of pre-study, though potentially
    with less chance for finely tuned optimizations. The faster ([https://crates.io/crates/faster](https://crates.io/crates/faster))
    crate, for example, exposes SIMD-parallel iterators in the spirit of rayon. The
    programming model there, from an end-user perspective, is extremely convenient.
  prefs: []
  type: TYPE_NORMAL
- en: Hex encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at an example from the stdsimd crate. We won''t dig in too deeply
    here; we''re only out to get a sense of the structure of this kind of programming.
    We''ve pulled stdsimd at `2f86c75a2479cf051b92fc98273daaf7f151e7a1`. The file
    we''ll examine is `examples/hex.rs`. The program''s purpose is to `hex` encode
    stdin. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that this requires a nightly channel, and not a stable one, we've used
    so far in this book. I tested this on nightly, 2018-05-10\. The details of the
    implementation are shifting so rapidly and rely on such unstable compiler features
    that you should make sure that you use the specific nightly channel that is listed,
    otherwise the example code may not compile.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preamble to `examples/hex.rs` contains a good deal of information we''ve
    not seen in the book so far. Let''s take it in two chunks and tease out the unknowns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Because we've stuck to stable Rust, we've not seen anything like `#![feature(stdsimd)]`
    before. What is this? Rust allows the adventurous user to enable in-progress features
    that are present in rustc, but that are not yet exposed in the nightly variant
    of the language. I say adventurous because an in-progress feature may maintain
    a stable API through its development, or it may change every few days, requiring
    updates on the consumer side. Now, what is `cfg_attr`? This is a conditional compilation
    attribute, turning on features selectively. You can read all about it in *The
    Rust Reference*, linked in the *Further reading* section. There are many details,
    but the idea is simple—turn bits of code on or off depending on user directives—such
    as testing—or the running environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'This later is what `#[cfg(any(target_arch = "x86", target_arch = "x86_64"))]`
    means. On either x86 or x86_64, stdsimd will be externed with macros enabled and
    without them enabled when on any other platform. Now, hopefully, the rest of the
    preamble is self-explanatory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main` function of this program is quite brief:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The whole of stdin is read into the input and a brief vector to hold the hash,
    called `dst`. `hex_encode`, is also used. The first portion performs input validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the return—a `&str` on success and a `usize`—the faulty length—on failure.
    The rest of the function is a touch more complicated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This introduces conditional compilation, which we saw previously, and contains
    a new thing too—`is_x86_feature_detected!`. This macro is new to the SIMD feature,
    and tests at runtime whether the given CPU feature is available at runtime via
    libcpuid. If it is, the feature is used. On an ARM processor, for instance, `hex_encode_fallback`
    will be executed. On an x86 processor, one or the other avx2 or sse4.1, implementations
    will be followed, depending on the chip''s exact capabilities. Which implementation
    is called is determined at runtime. Let''s look into the implementation of `hex_encode_fallback`
    first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The interior function `hex` acts as a static lookup table, and the for loop
    zips together the bytes from the `src` and a two-chunk pull from `dst`, updating
    `dst` as the loop proceeds. Finally, `dst` is converted into a `&str` and returned.
    It's safe to use `from_utf8_unchecked` here as we can verify that no non-utf8
    characters are possible in `dst`, saving us a check.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, now that''s the fallback. How do the SIMD variants read? We''ll look
    at `hex_encode_avx2`, leaving the sse4.1 variant to the ambitious reader. First,
    we see the reappearance of conditional compilation and the familiar function types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, so good. Now, look at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, the function `_mm256_set1_epi8` is certainly new! The documentation ([https://rust-lang-nursery.github.io/stdsimd/x86_64/stdsimd/arch/x86_64/fn._mm256_set1_epi8.html](https://rust-lang-nursery.github.io/stdsimd/x86_64/stdsimd/arch/x86_64/fn._mm256_set1_epi8.html)) states
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Broadcast 8-bit integer a to all elements of returned vector."'
  prefs: []
  type: TYPE_NORMAL
- en: Okay. What vector? Many SIMD instructions are carried out in terms of vectors
    with implicit sizes, the size being defined by the CPU architecture. For instance,
    the function that we have been looking at returns an `stdsimd::arch::x86_64::__m256i`,
    a 256-bit-wide vector. So, `ascii_zero` is 256-bits-worth of zeros, `ascii_a` 256-bits-worth
    of `a`, and so forth. Each of these functions and types, incidentally, follows
    Intel's naming scheme. It's my understanding that the LLVM uses their own naming
    scheme, but Rust—in an effort to reduce developer confusion—maintains a mapping
    from the architecture names to LLVM's names. By keeping the architecture names,
    Rust makes it real easy to look up details about each intrinsic. You can see this
    in the *Intel Intrinsics Guide* at `mm256_set1_epi8` ([https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set1_epi8&expand=4676](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_set1_epi8&expand=4676)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation then enters a loop, processing `src` in 256-bit-wide chunks
    until there are only 32 or fewer bits left:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The variable `invec` is now 256 bits of `src`, with `_mm256_loadu_si256` being
    responsible for performing a load. SIMD instructions have to operate on their
    native types, you see. In the actual reality of the machine, there is no type,
    but specialized registers. What comes next is complex, but we can reason through
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Okay, `_mm256_and_si256` performs a bitwise boolean-and of two `__m256i`, returning
    the result. `masked1` is the bitwise boolean-and of `invec` and `and4bits`. The
    same is true for `masked2`, except we need to determine what `_mm256_srli_epi64`
    does. The Intel document state the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Shift packed 64-bit integers in a right by imm8 while shifting in zeros, and
    store the results in dst."'
  prefs: []
  type: TYPE_NORMAL
- en: '`invec` is subdivided into four 64-bit integers, and these are shifted right
    by four bits. That is then boolean-and''ed with `and4bits`, and `masked2` pops
    out. If you refer back to `hex_encode_fallback` and squint a little, you can start
    to see the relationship to `(*byte >> 4) & 0xf`. Next up, two comparison masks
    are put together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`_mm256_cmpgt_epi8` greater-than compares 8-bit chunks of the 256-bit vector,
    returning the larger byte. The comparison masks are then used to control the blending
    of `ascii_zero` and `ascii_a`, and the result of that is added to `masked1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Blending is done bytewise, with the high bit in each byte determining whether
    to move a byte from the first vector in the `arg` list or the second. The computed
    masks are then interleaved, choosing high and low halves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the high half of the interleaved masks, the destination''s first 128 bits
    are filled with the top 128 bits from the source, the remainder being filled in
    with zeros. In the lower half of the interleaved masks, the destination''s first
    128 bits are filled from the bottom 128 bits from the source, the remainder being
    zeros. Then, at long last, a pointer is computed into `dst` at 64-bit strides
    and further subdivided into 16-bit chunks, and the `res1` and `res2` are stored
    there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When the `src` is less than 32 bytes, `hex_encode_sse4` is called on it, but
    we''ll leave the details of that to the adventurous reader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: That was tough! If our aim here was to do more than get a flavor of the SIMD
    style of programming, then we'd probably need to work out by hand how these instructions
    work on a bit level. Is it worth it? This example comes from RFC 2325 ([https://github.com/rust-lang/rfcs/blob/master/text/2325-stable-simd.md](https://github.com/rust-lang/rfcs/blob/master/text/2325-stable-simd.md)),
    *Stable SIMD*. Their benchmarking demonstrates that the fallback implementation
    we looked at first hashes at around 600 megabytes per second. The avx2 implementation
    hashes at 15,000 MB/s, a 60% performance bump. Not too shabby. And keep in mind
    that each CPU core comes equipped with SIMD capability. You could split such computations
    up among threads, too.
  prefs: []
  type: TYPE_NORMAL
- en: Like atomic programming, SIMD is not a panacea or even especially easy. If you
    need the speed, though, it's worth the work.
  prefs: []
  type: TYPE_NORMAL
- en: Futures and async/await
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asynchronous I/O is a hot topic in the Rust ecosystem right now. The C10K challenge
    we discussed briefly in [Chapter 8](d4802512-564b-4037-9407-b6035bd38f31.xhtml),
    *High-Level Parallelism – Threadpools, Parallel Iterators, and Processes*, was
    resolved when operating systems introduced scalable I/O notification syscalls
    such as kqueue in the BSDs and epoll in Linux. Prior to epoll, kqueue, and friends,
    I/O notification suffered linear growth in processing time as the number of file
    descriptors increased—a real problem. The aggressively naive approach to networking
    we've taken in this book also suffers from linear blowup. Every TCP socket we
    opened for listening would need to be polled in a loop, underserving very high-volume
    connections and overserving all others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rust''s abstraction for scalable network I/O is mio ([https://crates.io/crates/mio](https://crates.io/crates/mio)).
    If you go poking around inside of mio, you''ll find that it is a clean adapter
    for the I/O notification subsystems of a few common platforms: Windows, Unixen,
    and that''s about it. The user of mio doesn''t get much in the way of additional
    support. Where do you store your mio sockets? That''s up to the programmer. Can
    you include multithreading? Up to you. Now, that''s great for very specific use
    cases—cernan, for example, uses mio to drive its ingress I/O because we wanted
    very fine control over all of these details—but unless you have very specific
    needs, this is probably going to be a very tedious situation for you. The community
    has been investing very heavily in tokio ([https://crates.io/crates/tokio](https://crates.io/crates/tokio)),
    a framework for doing scalable I/O with essential things such as back-pressure,
    transaction cancellation, and higher-level abstractions to simplify request/response
    protocols. Tokio''s been a fast-moving project, but will almost surely be one
    of the *key* projects in the coming years. Tokio is, fundamentally, a reactor-based
    ([https://en.wikipedia.org/wiki/Reactor_pattern](https://en.wikipedia.org/wiki/Reactor_pattern))
    architecture; an I/O event becomes available and handlers you''ve registered are
    then called.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reactor-based systems are simple enough to program if you have only one event
    to respond to. But, more often than you''d imagine, there exist dependency relationships
    between I/O events in a real system—an ingressing UDP packet results in a TCP
    round-trip to a network memory cache, which results in an egress UDP packet to
    the original ingress system, and another as well. Coordinating that is hard. Tokio—and
    the Rust ecosystem somewhat generally—has gone all-in on a promise library called
    futures ([https://crates.io/crates/futures](https://crates.io/crates/futures)).
    Promises are a fairly tidy solution to asynchronous I/O: the underlying reactor
    calls a well-established interface, you implement a closure inside (or a trait
    to then slot inside) that interface, and everyone''s code stays loosely coupled.
    Rust''s futures are pollable, meaning you can hit the poll function on a future
    and it might resolve into a value or a notification that the value isn''t ready
    yet.'
  prefs: []
  type: TYPE_NORMAL
- en: This is similar to promise systems in other languages. But, as anyone who remembers
    the early introduction of NodeJS into the JavaScript ecosystem can attest to,
    promises without language support get weird and deeply nested fast. Rust's borrow
    checker made the situation harder, requiring boxing or full-on arcs where, in
    straight-line code, this would not have been necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Languages with promises as a first-class concern generally evolve async/await
    syntaxes. An async function is a function in Rust that will return a future and
    little else. A Rust future is simple trait, extracted from the futures project. The
    actual implementation will live outside the language's standard library and allow
    for alternative implementations. The interior of an async function is not executed
    immediately, but only when the future is polled. The interior of the function
    executes through the polling until an await point is hit, resulting in a notification
    that the value is not yet ready and that whatever is polling the future should
    move on.
  prefs: []
  type: TYPE_NORMAL
- en: As in other languages, Rust's proposed async / await syntax really is a sugar
    over the top of an explicit callback chaining structure. But, because the async/await
    syntax and futures trait is moving into the compiler, rules can be added to the
    borrow-checking system to remove the current boxing concerns. I expect you'll
    see more futures in stable Rust once they become more convenient to interact with.
    Indeed, as we saw in [Chapter 8](d4802512-564b-4037-9407-b6035bd38f31.xhtml),
    *High-Level Parallelism – Pools, Iterators, and Processes*, rayon is investing
    time in a futures-based interface.
  prefs: []
  type: TYPE_NORMAL
- en: Specialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](8c61da2f-08c8-40be-89d4-d8afa8510850.xhtml), *Sequential Rust
    Performance and Testing*, we noted that the technique of *specialization* would
    be cut off from data structure implementors if they intended to target stable
    Rust. Specialization is a means by which a generic structure—say, a `HashMap`—can
    be implemented specifically for one type. Usually, this is done to improve performance—say,
    by turning a `HashMap` with `u8` keys into an array—or provide a simpler implementation
    when possible. Specialization has been in Rust for a good while, but limited to
    nightly projects, such as the compiler, because of soundness issues when interacting
    with lifetimes. The canonical example has always been the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Lifetimes are erased at a certain point in the compilation process. String literals
    are static, but by the time dispatch to specialized type would occur there's not
    enough information in the compiler to decide. Lifetime equality in specialization
    also poses difficulties.
  prefs: []
  type: TYPE_NORMAL
- en: It seemed for a good while that specialization would miss the boat for inclusion
    into the language in 2018\. Now, *maybe*, a more limited form of specialization
    will be included. A specialization will only apply if the implementation is generic
    with regard to lifetimes, does not introduce duplication into trait lifetimes,
    does not repeat generic type parameters, and all trait bounds are themselves applicable
    for specialization. The exact details are in flux as I write this, but do keep
    an eye out.
  prefs: []
  type: TYPE_NORMAL
- en: Interesting projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many interesting projects in the Rust community right now. In this
    section, I'd like to look at two that I didn't get a chance to discuss at length
    in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Fuzzing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters we've used AFL to validate that our programs did not exhibit
    crashing behavior. While AFL is very commonly used, it's not the only fuzzer available
    for Rust. LLVM has a native library—libfuzzer ([https://llvm.org/docs/LibFuzzer.html](https://llvm.org/docs/LibFuzzer.html))—covering
    the same space, and the cargo-fuzz ([https://crates.io/crates/cargo-fuzz](https://crates.io/crates/cargo-fuzz))
    project acts as an executor. You might also be interested in honggfuzz-rs ([https://crates.io/crates/honggfuzz](https://crates.io/crates/honggfuzz)),
    a fuzzer developed at Google for searching out security related violations. It
    is natively multithreaded—there is no need to spin up multiple processes manually—and
    can do network fuzzing. My preference, traditionally, has been to fuzz with AFL.
    The honggfuzz project has real momentum, and readers should give it a try in their
    own projects.
  prefs: []
  type: TYPE_NORMAL
- en: Seer, a symbolic execution engine for Rust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fuzzing, as we discussed previously, explores the state space of a program using
    random inputs and binary instrumentation. This can be, as we've seen, slow. The
    ambition of symbolic execution ([https://en.wikipedia.org/wiki/Symbolic_execution](https://en.wikipedia.org/wiki/Symbolic_execution))
    is to allow the same exploration of state space, but without random probing. Searching
    for program crashes is one area of application, but it can also be used with proof
    tools. Symbolic execution, in a carefully written program, can let you demonstrate
    that your program can never reach error states. Rust has a partially implemented
    symbolic execution tool, seer ([https://github.com/dwrensha/seer](https://github.com/dwrensha/seer)).
    The project uses z3, a constraint solver, to generate branching inputs at program
    branches. Seer's README, as of SHA `91f12b2291fa52431f4ac73f28d3b18a0a56ff32`,
    amusingly decodes a hash. This is done by defining the decoding of the hashed
    data to be a crashable condition. Seer churns for a bit and then decodes the hash,
    crashing the program. It reports the decoded value among the error report.
  prefs: []
  type: TYPE_NORMAL
- en: It's still early days for seer, but the potential is there.
  prefs: []
  type: TYPE_NORMAL
- en: The community
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Rust community is large and multi-faceted. It's so large, in fact, that
    it can be hard to know where to go with questions or ideas. More, books at the
    intermediate to advanced level will often assume that readers know as much about
    the language community as the author does. Having been mostly on the other side
    of the author, reader relationship I've always found this assumption frustrating.
    As an author, though, I now understand the hesitancy—none of the community information
    will stay up to date.
  prefs: []
  type: TYPE_NORMAL
- en: Oh well. Some of this information may be out of date by the time you get to
    it. Reader beware.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we've referred to the crates ecosystem; crates.io ([https://crates.io/](https://crates.io/))
    is *the* location for Rust source projects. The docs.rs ([https://docs.rs/](https://docs.rs/))
    is a vital resource for understanding crates, and is run by the Rust team. You
    can also `cargo docs` and get a copy of your project's dependency documentation
    locally. I'm often without Wi-Fi, and I find this very useful.
  prefs: []
  type: TYPE_NORMAL
- en: As with almost all things in Rust, the community itself is documented on this
    web page ([https://www.rust-lang.org/en-US/community.html](https://www.rust-lang.org/en-US/community.html)).
    IRC is fairly common for real-time, loose conversations, but there's a real focus
    on the web-facing side of Rust's communication. The Users Forum ([https://users.rust-lang.org/](https://users.rust-lang.org/))
    is a web forum intended for folks who use the language and have questions or announcements.
    The compiler people—as well as the standard library implementors, and so on—hang
    out on the Internals Forum([https://internals.rust-lang.org/](https://internals.rust-lang.org/)).
    There is a good deal of overlap in the two communities, as you might expect.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we've referenced various RFCs. Rust evolves through a
    request-for-comments system, all of which are centralized ([https://github.com/rust-lang/rfcs](https://github.com/rust-lang/rfcs)).
    That's just for the Rust compiler and standard library. Many community projects
    follow a similar system—for example, crossbeam RFCs ([https://github.com/crossbeam-rs/rfcs](https://github.com/crossbeam-rs/rfcs)).
    These are well worth reading, by the way.
  prefs: []
  type: TYPE_NORMAL
- en: Should I use unsafe?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's not uncommon to hear some variant of the following position—*I won't use
    any library that has an* `unsafe` *block in it*. The reasoning behind this position
    is that `unsafe`, well, advertises that the crate is potentially unsafe and might
    crash your otherwise carefully crafted program. That's true—kind of. As we've
    seen in this book, it's entirely possible to put together a project using `unsafe`
    that is totally safe at runtime. We've also seen that it's entirely possible to
    put together a project without `unsafe` blocks that flame out at runtime. The
    existence or absence of `unsafe` blocks shouldn't reduce the original programmer's
    responsibilities for due diligence—writing tests, probing the implementation with
    fuzzing tools, and so on. Moreover, the existence or absence of `unsafe` blocks
    does not relieve the user of a crate from that same responsibility. Any software,
    at some level, should be considered suspect unless otherwise demonstrated to be
    safe.
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and use the `unsafe` keyword. Do so when it's necessary. Keep in mind
    that the compiler won't be able to help you as much as it normally would—that's
    all.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this last chapter of the book, we discussed near and midterm improvements
    coming to the Rust language—features being stabilized after a while out in the
    wilderness, the foundations of larger projects being integrated, research that
    may or may not pan out, and other topics. Some of these topics will surely have
    chapters of their own in the second edition of this book, should it get a new
    edition (Tell your friends to pick up a copy!). We also touched on the community,
    which is the place to find ongoing discussions, ask questions, and get involved.
    Then, we affirmed that, yes indeed, you should write in unsafe Rust when you have
    reason to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, that's it. That's the end of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Shipping specialization: a story of soundness*, available at [https://aturon.github.io/blog/2017/07/08/lifetime-dispatch/](https://aturon.github.io/blog/2017/07/08/lifetime-dispatch/).
    There''s long been an ambition to see specialization available in stable Rust,
    and it''s not for want of trying that it hasn''t happened yet. In this blog post,
    Aaron Turon discusses the difficulties of specialization in 2017, introducing
    the Chalk logic interpreter in the discussion. Chalk is interesting in its own
    right, especially if you are interested in compiler internals or logic programming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Maximally minimal specialization: always applicable impls*, available at [http://smallcultfollowing.com/babysteps/blog/2018/02/09/maximally-minimal-specialization-always-applicable-impls/](http://smallcultfollowing.com/babysteps/blog/2018/02/09/maximally-minimal-specialization-always-applicable-impls/).
    This blog post by Niko Matsakis extends the topics covered in Turon''s *Shipping
    specialization* article, discussing a *min-max* solution to the specialization
    soundness issue. The approach seemed to be the most likely candidate for eventual
    implementation, but flaws were discovered. Happily, not unresolvable flaws. This
    post is an excellent example of the preRFC conversation in the Rust community.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sound and ergonomic specialization for Rust*, available at [http://aturon.github.io/2018/04/05/sound-specialization/](http://aturon.github.io/2018/04/05/sound-specialization/). This
    blog post addresses the issues in the min-max article and proposes the implementation
    discussed in this chapter. It is, as of writing this book, the most likely to
    be implemented, unless some bright spark finds a flaw in it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chalk*, available at [https://github.com/rust-lang-nursery/chalk](https://github.com/rust-lang-nursery/chalk).
    Chalk is really a very interesting Rust project. It is, according to the project
    description, a *PROLOG-ish interpreter written in Rust*. To date, Chalk is being
    used to reason about specialization in the Rust language, but there are plans
    to merge Chalk into `rustc` itself someday. The project README, as of `SHA 94a1941a021842a5fcb35cd043145c8faae59f08`,
    has a list of excellent articles on the applications of chalk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Unstable Book*, available at [https://doc.rust-lang.org/nightly/unstable-book/](https://doc.rust-lang.org/nightly/unstable-book/).
    `rustc` has a large number of in-flight features. *The Unstable Book* is intended
    to be a best-effort collection of these features, the justifications behind them,
    and any relevant tracking issues. It is well worth a read, especially if you''re
    looking to contribute to the compiler project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zero-cost futures in Rust*, available at [http://aturon.github.io/blog/2016/08/11/futures/](http://aturon.github.io/blog/2016/08/11/futures/).
    This post introduced the Rust community to the futures project and explains the
    motivation behind the introduction well. The actual implementation details of
    futures have changed with time, but this article is still well worth a read.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Async / Await*, available at  [https://github.com/rust-lang/rfcs/blob/master/text/2394-async_await.md](https://github.com/rust-lang/rfcs/blob/master/text/2394-async_await.md). RFC 
    2394 introduces the motivation for simplifying the ergonomics of futures in Rust,
    as well as laying out the implementation approach for it. The RFC is, itself,
    a fine example of how Rust evolves—community desire turns into experimentation
    which then turns into support from the compiler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

<html><head></head><body>
		<div id="_idContainer010">
			<h1 id="_idParaDest-15" class="chapter-number"><a id="_idTextAnchor014"/>1</h1>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>Concurrency and Asynchronous Programming: a Detailed Overview</h1>
			<p>Asynchronous programming is one of those topics many programmers find confusing. You come to the point when you think you’ve got it, only to later realize that the rabbit hole is much deeper than you thought. If you participate in discussions, listen to enough talks, and read about the topic on the internet, you’ll probably also come across statements that seem to contradict each other. At least, this describes how I felt when I first was introduced to <span class="No-Break">the subject.</span></p>
			<p>The cause of this confusion is often a lack of context, or authors assuming a specific context without explicitly stating so, combined with terms surrounding concurrency and asynchronous programming that are rather <span class="No-Break">poorly defined.</span></p>
			<p>In this chapter, we’ll be covering a lot of ground, and we’ll divide the content into the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li><span class="No-Break">Async history</span></li>
				<li>Concurrency <span class="No-Break">and parallelism</span></li>
				<li>The operating system and <span class="No-Break">the CPU</span></li>
				<li>Interrupts, firmware, <span class="No-Break">and I/O</span></li>
			</ul>
			<p>This chapter is general in nature. It doesn’t specifically focus on <strong class="bold">Rust</strong>, or any specific programming language for that matter, but it’s the kind of background information we need to go through so we know that everyone is on the same page going forward. The upside is that this will be useful no matter what programming language you use. In my eyes, that fact also makes this one of the most interesting chapters in <span class="No-Break">this book.</span></p>
			<p>There’s not a lot of code in this chapter, so we’re off to a soft start. It’s a good time to make a cup of tea, relax, and get comfortable, as we’re about start this <span class="No-Break">journey together.</span></p>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Technical requirements</h1>
			<p>All examples will be written in Rust, and you have two alternatives for running <span class="No-Break">the examples:</span></p>
			<ul>
				<li>Write and run the examples we’ll write on the <span class="No-Break">Rust playground</span></li>
				<li>Install Rust on your machine and run the examples <span class="No-Break">locally (recommended)</span></li>
			</ul>
			<p>The ideal way to read this chapter is to clone the accompanying repository (<a href="https://github.com/PacktPublishing/Asynchronous-Programming-in-Rust/tree/main/ch01/a-assembly-dereference">https://github.com/PacktPublishing/Asynchronous-Programming-in-Rust/tree/main/ch01/a-assembly-dereference</a>) and open the <strong class="source-inline">ch01</strong> folder and keep it open while you read the book. There, you’ll find all the examples we write in this chapter and even some extra information that you might find interesting as well. You can of course also go back to the repository later if you don’t have that accessible <span class="No-Break">right now.</span></p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>An evolutionary journey of multitasking</h1>
			<p>In the beginning, computers had one CPU that executed a set of instructions written by a programmer one by one. No <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>), no scheduling, no threads, no multitasking. This was how computers worked for a long time. We’re talking back when a program was assembled in <a id="_idIndexMarker000"/>a deck of punched cards, and you got in big trouble if you were so unfortunate that you dropped the deck onto <span class="No-Break">the floor.</span></p>
			<p>There were operating systems being researched very early and when personal computing started to grow in the 80s, operating systems such as DOS were the standard on most <span class="No-Break">consumer PCs.</span></p>
			<p>These operating systems usually yielded control of the entire CPU to the program currently executing, and it was up to the programmer to make things work and implement any kind of multitasking for their program. This worked fine, but as interactive UIs using a mouse and windowed operating systems became the norm, this model simply couldn’t <span class="No-Break">work anymore.</span></p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>Non-preemptive multitasking</h2>
			<p><strong class="bold">Non-preemptive multitasking</strong> was the first method <a id="_idIndexMarker001"/>used to be able to<a id="_idIndexMarker002"/> keep a UI interactive (and running <span class="No-Break">background processes).</span></p>
			<p>This kind of multitasking put the responsibility of letting the OS run other tasks, such as responding to input from the mouse or running a background task, in the hands of <span class="No-Break">the programmer.</span></p>
			<p>Typically, the programmer <em class="italic">yielded</em> control to <span class="No-Break">the OS.</span></p>
			<p>Besides offloading a huge responsibility to every programmer writing a program for your platform, this method was<a id="_idIndexMarker003"/> naturally error-prone. A small mistake in a program’s code could halt or crash the <span class="No-Break">entire system.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Another popular term for what we call non-preemptive multitasking is <strong class="bold">cooperative multitasking</strong>. Windows 3.1 used<a id="_idIndexMarker004"/> cooperative multitasking and required programmers to yield control to the OS by using specific system calls. One badly-behaving application could thereby halt the <span class="No-Break">entire system.</span></p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Preemptive multitasking</h2>
			<p>While non-preemptive multitasking sounded like a good idea, it turned out to create serious problems as well. Letting every program and programmer out there be responsible for having a<a id="_idIndexMarker005"/> responsive UI in an operating system can <a id="_idIndexMarker006"/>ultimately lead to a bad user experience, since every bug out there could halt the <span class="No-Break">entire system.</span></p>
			<p>The solution was to place the responsibility of scheduling the CPU resources between the programs that requested it (including the OS itself) in the hands of the OS. The OS can stop the execution of a process, do something else, and <span class="No-Break">switch back.</span></p>
			<p>On such a system, if you write and run a program with a graphical user interface on a single-core machine, the OS will stop your program to update the mouse position before it switches back to your program to continue. This happens so frequently that we don’t usually observe any difference whether the CPU has a lot of work or <span class="No-Break">is idle.</span></p>
			<p>The OS is responsible for scheduling tasks and does this by switching contexts on the CPU. This process can happen many times each second, not only to keep the UI responsive but also to give some time to other background tasks and <span class="No-Break">IO events.</span></p>
			<p>This is now the prevailing way to design an <span class="No-Break">operating system.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Later in this book, we’ll write <a id="_idIndexMarker007"/>our own green threads and cover a lot of basic knowledge about context switching, threads, stacks, and scheduling that will give<a id="_idIndexMarker008"/> you more insight into this topic, so <span class="No-Break">stay tuned.</span></p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Hyper-threading</h2>
			<p>As CPUs evolved and added more<a id="_idIndexMarker009"/> functionality such as several <strong class="bold">arithmetic logic units</strong> (<strong class="bold">ALUs</strong>) and additional logic units, the<a id="_idIndexMarker010"/> CPU manufacturers realized that the entire CPU wasn't fully utilized. For example, when an operation only required <a id="_idIndexMarker011"/>some parts of the CPU, an instruction could be run on the ALU simultaneously. This became the start <span class="No-Break">of </span><span class="No-Break"><strong class="bold">hyper-threading</strong></span><span class="No-Break">.</span></p>
			<p>Your computer today, for example, may have 6 cores and 12 logical cores.. This is exactly where hyper-threading comes in. It “simulates” two cores on the same core by using unused parts of the CPU to drive progress on thread <em class="italic">2</em> and simultaneously running the code on thread <em class="italic">1</em>. It does this by using a number of smart tricks (such as the one with <span class="No-Break">the ALU).</span></p>
			<p>Now, using hyper-threading, we could actually offload some work on one thread while keeping the UI interactive by responding to events in the second thread even though we only had one CPU core, thereby utilizing our <span class="No-Break">hardware better.</span></p>
			<p class="callout-heading">You might wonder about the performance of hyper-threading</p>
			<p class="callout">It turns out that hyper-threading has<a id="_idIndexMarker012"/> been continuously improved since the 90s. Since you’re not actually running two CPUs, there will be some operations that need to wait for each other to finish. The performance gain of hyper-threading compared to multitasking in a single core seems to be somewhere close to 30% but it largely depends on <span class="No-Break">the workload.</span></p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>Multicore processors</h2>
			<p>As most know, the<a id="_idIndexMarker013"/> clock frequency of processors has been flat for a long time. Processors get faster by improving <strong class="bold">caches</strong>, <strong class="bold">branch prediction</strong>, and <strong class="bold">speculative execution</strong>, and by working on the <strong class="bold">processing pipelines</strong> of the processors, but the gains seem to <span class="No-Break">be diminishing.</span></p>
			<p>On the other hand, new <a id="_idIndexMarker014"/>processors are so small that they allow us to have<a id="_idIndexMarker015"/> many on the same chip. Now, most CPUs have many cores and most often, each core will also have the ability to <span class="No-Break">perform hyper-threading.</span></p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Do you really write synchronous code?</h2>
			<p>Like many things, this depends on <a id="_idIndexMarker016"/>your perspective. From the perspective of your process and the code you write, everything will normally happen in the order you <span class="No-Break">write it.</span></p>
			<p>From the operating system’s perspective, it might or might not interrupt your code, pause it, and run some other code in the meantime before resuming <span class="No-Break">your process.</span></p>
			<p>From the perspective of the CPU, it will mostly execute instructions one at a time.* It doesn’t care who wrote the code, though, so <a id="_idIndexMarker017"/>when a <strong class="bold">hardware interrupt</strong> happens, it will immediately stop and give control to an interrupt handler. This is how the CPU <span class="No-Break">handles concurrency.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">*However, modern CPUs can also do a lot of things in parallel. Most CPUs are pipelined, meaning that the next instruction is loaded while the current one is executing. It might have a branch predictor that tries to figure out what instructions to <span class="No-Break">load next.</span></p>
			<p class="callout">The processor can also<a id="_idIndexMarker018"/> reorder instructions by using <strong class="bold">out-of-order execution</strong> if it believes it makes things faster this way without ‘asking’ or ‘telling’ the programmer or the OS, so you might not have any guarantee that A happens <span class="No-Break">before B.</span></p>
			<p class="callout">The CPU offloads some work to separate ‘coprocessors’ such as the FPU for floating-point calculations, leaving the main CPU ready to do other tasks <span class="No-Break">et cetera.</span></p>
			<p class="callout">As a high-level overview, it’s OK to model the CPU as operating in a synchronous manner, but for now, let’s just make a mental note that this is a model with some caveats that become especially important<a id="_idIndexMarker019"/> when talking about parallelism, synchronization primitives (such as mutexes and atomics), and the security of computers and <span class="No-Break">operating systems.</span></p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor023"/>Concurrency versus parallelism</h1>
			<p>Right off the bat, we’ll dive<a id="_idIndexMarker020"/> into this subject by defining what <strong class="bold">concurrency</strong> is. Since it is quite easy to confuse <em class="italic">concurrent</em> with <em class="italic">parallel</em>, we will try to make a clear <a id="_idIndexMarker021"/>distinction between the two from <span class="No-Break">the get-go.</span></p>
			<p class="callout-heading">Important</p>
			<p class="callout">Concurrency is about <em class="italic">dealing</em> with a lot of<a id="_idIndexMarker022"/> things at the <span class="No-Break">same time.</span></p>
			<p class="callout">Parallelism is about <em class="italic">doing</em> a lot of things at the <span class="No-Break">same time.</span></p>
			<p>We call the concept of progressing multiple tasks at the same time <em class="italic">multitasking</em>. There are two ways to multitask. One is by <em class="italic">progressing</em> tasks concurrently, but not at the same time. Another is to progress tasks at the exact same time in parallel. <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.1</em> depicts the difference between the <span class="No-Break">two scenarios:</span></p>
			<div>
				<div id="_idContainer007" class="IMG---Figure">
					<img src="image/B20892_Figure_01.1.jpg" alt="Figure 1.1 – Multitasking two tasks"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Multitasking two tasks</p>
			<p>First, we need to agree on <span class="No-Break">some definitions:</span></p>
			<ul>
				<li><strong class="bold">Resource</strong>: This is <a id="_idIndexMarker023"/>something we need to be able to progress a task. Our resources <a id="_idIndexMarker024"/>are limited. This could be CPU time <span class="No-Break">or memory.</span></li>
				<li><strong class="bold">Task</strong>: This is a set <a id="_idIndexMarker025"/>of operations that requires <a id="_idIndexMarker026"/>some kind of resource to progress. A task must consist of <span class="No-Break">several sub-operations.</span></li>
				<li><strong class="bold">Parallel</strong>: This is something <a id="_idIndexMarker027"/>happening independently at the <em class="italic">exact</em> <span class="No-Break">same time.</span></li>
				<li><strong class="bold">Concurrent</strong>: These<a id="_idIndexMarker028"/> are tasks that are <em class="italic">in progress</em> at the same time, but not necessarily <span class="No-Break">progressing simultaneously.</span></li>
			</ul>
			<p>This is an important distinction. If two tasks are running concurrently, but are not running in parallel, they must be able to stop and resume their progress. We say that a task is <em class="italic">interruptible</em> if it allows for this kind <span class="No-Break">of concurrency.</span></p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>The mental model I use</h2>
			<p>I firmly believe the main reason we find parallel and concurrent programming hard to differentiate stems from how we model events in our everyday life. We tend to define these terms loosely, so our intuition is <span class="No-Break">often wrong.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">It doesn’t help that <em class="italic">concurrent</em> is defined in the dictionary as <em class="italic">operating or occurring at the same time</em>, which doesn’t really help us much when trying to describe how it differs <span class="No-Break">from </span><span class="No-Break"><em class="italic">parallel</em></span><span class="No-Break">.</span></p>
			<p>For me, this first clicked when I started to understand why we want to make a distinction between parallel and concurrent in the <span class="No-Break">first place!</span></p>
			<p>The <em class="italic">why</em> has everything to do with resource <a id="_idIndexMarker029"/>utilization <span class="No-Break">and efficiency.</span></p>
			<p><em class="italic">Efficiency is the (often measurable) ability to avoid wasting materials, energy, effort, money, and time in doing something or in producing a </em><span class="No-Break"><em class="italic">desired result.</em></span></p>
			<p><em class="italic">Parallelism</em> is increasing the resources we use to solve a task. It has nothing to do <span class="No-Break">with </span><span class="No-Break"><em class="italic">efficiency</em></span><span class="No-Break">.</span></p>
			<p><em class="italic">Concurrency</em> has everything to do<a id="_idIndexMarker030"/> with efficiency and resource utilization. Concurrency can never make <em class="italic">one single task go faster</em>. It can only help us utilize our resources better and thereby <em class="italic">finish a set of </em><span class="No-Break"><em class="italic">tasks faster</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>Let’s draw some parallels to process economics</h2>
			<p>In businesses that <a id="_idIndexMarker031"/>manufacture goods, we often talk about <strong class="bold">LEAN</strong> processes. This is pretty easy to compare with why programmers care so much about what we can achieve if we handle <span class="No-Break">tasks concurrently.</span></p>
			<p>Let’s pretend we’re running a bar. We only serve Guinness beer and nothing else, but we serve our Guinness to perfection. Yes, I know, it’s a little niche, but bear <span class="No-Break">with me.</span></p>
			<p>You are the manager of this bar, and your goal is to run it as efficiently as possible. Now, you can think of each bartender as a <em class="italic">CPU core</em>, and each order as a <em class="italic">task</em>. To manage this bar, you need to know the steps to serve a <span class="No-Break">perfect Guinness:</span></p>
			<ul>
				<li>Pour the Guinness draught into a glass tilted at 45 degrees until it’s 3-quarters full (<span class="No-Break">15 seconds).</span></li>
				<li>Allow the surge to settle for <span class="No-Break">100 seconds.</span></li>
				<li>Fill the glass completely to the top (<span class="No-Break">5 seconds).</span></li>
				<li><span class="No-Break">Serve.</span></li>
			</ul>
			<p>Since there is only one <a id="_idIndexMarker032"/>thing to order in the bar, customers only need to signal using<a id="_idIndexMarker033"/> their fingers how many they want to order, so we assume taking new orders is instantaneous. To keep things simple, the same goes for payment. In choosing how to run this bar, you have a <span class="No-Break">few alternatives.</span></p>
			<h3>Alternative 1 – Fully synchronous task execution with one bartender</h3>
			<p>You start out with only one bartender (CPU). The bartender takes one order, finishes it, and progresses to the next. The line is out the door and going two blocks down the street – great! One month later, you’re almost out of business and you <span class="No-Break">wonder why.</span></p>
			<p>Well, even though your bartender is very fast at taking new orders, they can only serve 30 customers an hour. Remember, they’re waiting for 100 seconds while the beer settles and they’re practically just standing there, and they only use 20 seconds to actually fill the glass. Only after one order is completely finished can they progress to the next customer and take <span class="No-Break">their order.</span></p>
			<p>The result is bad revenue, angry customers, and high costs. That’s not going <span class="No-Break">to work.</span></p>
			<h3>Alternative 2 – Parallel and synchronous task execution</h3>
			<p>So, you hire 12 bartenders, and you calculate that you can serve about 360 customers an hour. The line is barely going out the door now, and revenue is <span class="No-Break">looking great.</span></p>
			<p>One month goes by and again, you’re almost out of business. How can <span class="No-Break">that be?</span></p>
			<p>It turns out that having 12 bartenders is pretty expensive. Even though revenue is high, the costs are even higher. Throwing more resources at the problem doesn’t really make the bar <span class="No-Break">more efficient.</span></p>
			<h3>Alternative 3 – Asynchronous task execution with one bartender</h3>
			<p>So, we’re back to square one. Let’s think this through and find a smarter way of working instead of throwing more resources at <span class="No-Break">the problem.</span></p>
			<p>You ask your bartender <a id="_idIndexMarker034"/>whether they can start taking new orders while the<a id="_idIndexMarker035"/> beer settles so that they’re never just standing and waiting while there are customers to serve. The opening night <span class="No-Break">comes and...</span></p>
			<p>Wow! On a busy night where the bartender works non-stop for a few hours, you calculate that they now only use just over 20 seconds on an order. You’ve basically eliminated all the waiting. Your theoretical throughput is now 240 beers per hour. If you add one more bartender, you’ll have higher throughput than you did while having <span class="No-Break">12 bartenders.</span></p>
			<p>However, you realize that you didn’t actually accomplish 240 beers an hour, since orders come somewhat erratically and not evenly spaced over time. Sometimes, the bartender is busy with a new order, preventing them from topping up and serving beers that are finished almost immediately. In real life, the throughput is only 180 beers <span class="No-Break">an hour.</span></p>
			<p>Still, two bartenders could serve 360 beers an hour this way, the same amount that you served while employing <span class="No-Break">12 bartenders.</span></p>
			<p>This is good, but you ask yourself whether you can do <span class="No-Break">even better.</span></p>
			<h3>Alternative 4 – Parallel and asynchronous task execution with two bartenders</h3>
			<p>What if you hire two bartenders, and <a id="_idIndexMarker036"/>ask them to do just what we described in Alternative 3, but with one change: you allow them to steal each other’s tasks, so <em class="italic">bartender 1</em> can <a id="_idIndexMarker037"/>start pouring and set the beer down to settle, and <em class="italic">bartender 2</em> can top it up and serve it if <em class="italic">bartender 1</em> is busy pouring a new order at that time? This way, it is only rarely that both bartenders are busy at the same time as one of the beers-in-progress becomes ready to get topped up and served. Almost all orders are finished and served in the shortest amount of time possible, letting customers leave the bar with their beer faster and giving space to customers who want to make a <span class="No-Break">new order.</span></p>
			<p>Now, this way, you can increase throughput even further. You still won’t reach the theoretical maximum, but you’ll get very close. On the opening night, you realize that the bartenders now process 230 orders an hour each, giving a total throughput of 460 beers <span class="No-Break">an hour.</span></p>
			<p>Revenue looks good, customers are happy, costs are kept at a minimum, and you’re one happy manager of the weirdest bar on earth (an extremely efficient <span class="No-Break">bar, though).</span></p>
			<p class="callout-heading">The key takeaway</p>
			<p class="callout">Concurrency is about working smarter. Parallelism is a way of throwing more resources at <span class="No-Break">the problem.</span></p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>Concurrency and its relation to I/O</h2>
			<p>As you might understand from<a id="_idIndexMarker038"/> what I’ve written so far, writing async code mostly makes sense when you need to be smart to make optimal use of <span class="No-Break">your resources.</span></p>
			<p>Now, if you write a program that is working hard to solve a problem, there is often no help in concurrency. This is where parallelism comes into play, since it gives you a way to throw more resources at the problem if you can split it into parts that you can work on <span class="No-Break">in parallel.</span></p>
			<p>Consider the following two<a id="_idIndexMarker039"/> different use cases <span class="No-Break">for concurrency:</span></p>
			<ul>
				<li>When performing I/O and you need to wait for some external event <span class="No-Break">to occur</span></li>
				<li>When you need to divide your attention and prevent one task from waiting <span class="No-Break">too long</span></li>
			</ul>
			<p>The first is the classic I/O example: you have to wait for a network call, a database query, or something else to happen before you can progress a task. However, you have many tasks to do so instead of waiting, you continue to work elsewhere and either check in regularly to see whether the task is ready to progress, or make sure you are notified when that task is ready <span class="No-Break">to progress.</span></p>
			<p>The second is an example that is often the case when having a UI. Let’s pretend you only have one core. How do you prevent the whole UI from becoming unresponsive while performing other <span class="No-Break">CPU-intensive tasks?</span></p>
			<p>Well, you can stop whatever task you’re doing every 16 ms, run the <em class="italic">update UI</em> task, and then resume whatever you were doing afterward. This way, you will have to stop/resume your task 60 times a second, but you will also have a fully responsive UI that has a roughly 60 Hz <span class="No-Break">refresh rate.</span></p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>What about threads provided by the operating system?</h2>
			<p>We’ll cover threads a bit more when we talk about strategies for handling I/O later in this book, but I’ll mention<a id="_idIndexMarker040"/> them here as well. One challenge when using OS threads to understand concurrency is that they appear to be mapped to cores. That’s not necessarily a correct mental model to use, even though most operating systems will try to map one thread to one core up to the number of threads equal to the number <span class="No-Break">of cores.</span></p>
			<p>Once we create more threads than there are cores, the OS will switch between our threads and progress each of them concurrently using its scheduler to give each thread some time to run. You also must consider the fact that your program is not the only one running on the system. Other programs might spawn several threads as well, which means there will be many more threads than there are cores on <span class="No-Break">the CPU.</span></p>
			<p>Therefore, threads can be a means to perform tasks in parallel, but they can also be a means to <span class="No-Break">achieve concurrency.</span></p>
			<p>This brings me to the last part about concurrency. It needs to be defined in some sort of <span class="No-Break">reference frame.</span></p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>Choosing the right reference frame</h2>
			<p>When you write<a id="_idIndexMarker041"/> code that is perfectly synchronous from your perspective, stop for a second and consider how that looks from the operating <span class="No-Break">system perspective.</span></p>
			<p>The operating system might not run your code from start to end at all. It might stop and resume your process many times. The CPU might get interrupted and handle some inputs while you think it’s only focused on <span class="No-Break">your task.</span></p>
			<p>So, synchronous execution is only an illusion. But from the perspective of you as a programmer, it’s not, and that is the <span class="No-Break">important takeaway:</span></p>
			<p><em class="italic">When we talk about concurrency without providing any other context, we are using you as a programmer and your code (your process) as the reference frame. If you start pondering concurrency without keeping this in the back of your head, it will get confusing </em><span class="No-Break"><em class="italic">very fast.</em></span></p>
			<p>The reason I’m spending so much time on this is that once you realize the importance of having the same definitions and the same reference frame, you’ll start to see that some of the things you hear and learn that might seem contradictory really are not. You’ll just have to consider the reference <span class="No-Break">frame first.</span></p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>Asynchronous versus concurrent</h2>
			<p>So, you might wonder why we’re spending all this time talking about multitasking, concurrency, and parallelism, when the book is about <span class="No-Break">asynchronous programming.</span></p>
			<p>The main<a id="_idIndexMarker042"/> reason for this is that all these concepts are closely related to each other, and can even have the same (or overlapping) meanings, depending on the context they’re <span class="No-Break">used in.</span></p>
			<p>In an effort to make the definitions <a id="_idIndexMarker043"/>as distinct as possible, we’ll define these terms more narrowly than you’d normally see. However, just be aware that we can’t please everyone and we do this for our own sake of making the subject easier to understand. On the other hand, if you fancy heated internet debates, this is a good place to start. Just claim someone else’s definition of concurrent is 100 % wrong or that yours is 100 % correct, and off <span class="No-Break">you go.</span></p>
			<p><em class="italic">For the sake of this book, we’ll stick to this definition: asynchronous programming is the way a programming language or library abstracts over concurrent operations, and how we as users of a language or library use that abstraction to execute </em><span class="No-Break"><em class="italic">tasks concurrently.</em></span></p>
			<p>The operating system already has<a id="_idIndexMarker044"/> an existing abstraction that covers<a id="_idIndexMarker045"/> this, called <strong class="bold">threads</strong>. Using OS threads to handle asynchrony is often referred to <strong class="bold">as multithreaded programming</strong>. To <a id="_idIndexMarker046"/>avoid confusion, we’ll not refer to using OS threads directly as asynchronous programming, even though it solves the <span class="No-Break">same problem.</span></p>
			<p>Given that asynchronous programming is now scoped to be about abstractions over concurrent or parallel operations in a language or library, it’s also easier to understand that it’s just as relevant on embedded systems without an operating system as it is for programs that target a complex system with an advanced operating system. The definition itself does not imply any specific implementation even though we’ll look at a few popular ones throughout <span class="No-Break">this book.</span></p>
			<p>If this still sounds complicated, I understand. Just sitting and reflecting on concurrency is difficult, but if we try to keep these thoughts in the back of our heads when we work with async code I promise it will get less and <span class="No-Break">less confusing.</span></p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>The role of the operating system</h1>
			<p>The operating system (OS) stands in the center of everything we do as programmers (well, unless you’re<a id="_idIndexMarker047"/> writing an operating system or working in the embedded realm), so there is no way for us to discuss any kind of fundamentals in programming without talking about operating systems in a bit <span class="No-Break">of detail.</span></p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Concurrency from the operating system’s perspective</h2>
			<p>This ties into what I talked <a id="_idIndexMarker048"/>about earlier when I said that concurrency needs to be talked about within a reference frame, and I explained that the OS might stop and start your process at <span class="No-Break">any time.</span></p>
			<p>What we call synchronous code is, in most cases, code that appears synchronous to us as programmers. Neither the OS nor the CPU lives in a fully <span class="No-Break">synchronous world.</span></p>
			<p>Operating systems use preemptive multitasking and as long as the operating system you’re running is preemptively scheduling processes, you won’t have a guarantee that your code runs instruction by instruction <span class="No-Break">without interruption.</span></p>
			<p>The operating system will make sure that all important processes get some time from the CPU to <span class="No-Break">make progress.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">This is not as simple when we’re talking about modern machines with 4, 6, 8, or 12 physical cores, since you might actually execute code on one of the CPUs uninterrupted if the system is under very little load. The important part here is that you can’t know for sure and there is no guarantee that your code will be left to <span class="No-Break">run uninterrupted.</span></p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>Teaming up with the operating system</h2>
			<p>When you make a web request, you’re <a id="_idIndexMarker049"/>not asking the CPU or the network card to do something for you – you’re asking the operating system to talk to the network card <span class="No-Break">for you.</span></p>
			<p>There is no way for you as a programmer to make your system optimally efficient without playing to the strengths of the operating system. You basically don’t have access to the hardware directly. <em class="italic">You must remember that the operating system is an abstraction over </em><span class="No-Break"><em class="italic">the hardware.</em></span></p>
			<p>However, this also means that to understand everything from the ground up, you’ll also need to know how <a id="_idIndexMarker050"/>your operating system handles <span class="No-Break">these tasks.</span></p>
			<p>To be able to work with the operating system, you’ll need to know how you can communicate with it, and that’s exactly what we’re going to go <span class="No-Break">through next.</span></p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Communicating with the operating system</h2>
			<p>Communication with an operating system happens through what we call a <strong class="bold">system call</strong> (<strong class="bold">syscall</strong>). We need to know<a id="_idIndexMarker051"/> how to make system calls and<a id="_idIndexMarker052"/> understand why it’s so important for us when we want to cooperate and communicate with the operating system. We also need to understand how the basic abstractions we use every day use system calls behind the scenes. We’ll have a detailed walkthrough in <a href="B20892_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, so we’ll keep this brief <span class="No-Break">for now.</span></p>
			<p>A system call uses a public API that the operating system provides so that programs we write in ‘userland’ can communicate with <span class="No-Break">the OS.</span></p>
			<p>Most of the time, these calls are abstracted away for us as programmers by the language or the runtime <span class="No-Break">we use.</span></p>
			<p>Now, a syscall is an example of something that is unique to the kernel you’re communicating with, but<a id="_idIndexMarker053"/> the <strong class="bold">UNIX</strong> family of kernels has many similarities. UNIX systems <a id="_idIndexMarker054"/>expose this <span class="No-Break">through </span><span class="No-Break"><strong class="bold">libc</strong></span><span class="No-Break">.</span></p>
			<p>Windows, on the other hand, uses<a id="_idIndexMarker055"/> its own API, often referred to as <strong class="bold">WinAPI</strong>, and it can operate radically differently from how the UNIX-based <span class="No-Break">systems operate.</span></p>
			<p>Most often, though, there is a way to achieve the same things. In terms of functionality, you might not notice a big difference but as we’ll see later, and especially when we dig into how <strong class="bold">epoll</strong>, <strong class="bold">kqueue</strong>, and <strong class="bold">IOCP</strong> work, they can differ a lot in how this functionality <span class="No-Break">is implemented.</span></p>
			<p>However, a syscall is not the only way we interact with our operating system, as we’ll see in the <span class="No-Break">following section.</span></p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>The CPU and the operating system</h1>
			<p><em class="italic">Does the CPU cooperate with the </em><span class="No-Break"><em class="italic">operating system?</em></span></p>
			<p>If you had asked me this question when I first thought I understood how programs work, I would most likely have answered <em class="italic">no</em>. We run programs on the CPU and we can do whatever we want if we<a id="_idIndexMarker056"/> know how to do it. Now, first of all, I wouldn’t have thought this through, but unless you learn how CPUs and operating systems work together, it’s not easy to know <span class="No-Break">for sure.</span></p>
			<p>What started to make me think I was very wrong was a segment of code that looked like what you’re about to see. If you think inline assembly in Rust looks foreign and confusing, don’t worry just yet. We’ll go through a proper introduction to inline assembly a little later in this book. I’ll make sure to go through each of the following lines until you get more comfortable with <span class="No-Break">the syntax:</span></p>
			<p>Repository <span class="No-Break">reference: ch01/ac-assembly-dereference/src/main.rs</span></p>
			<pre class="source-code">
fn main() {
    let t = 100;
    let t_ptr: *const usize = &amp;t;
    let x = dereference(t_ptr);
    println!("{}", x);
}
fn dereference(ptr: *const usize) -&gt; usize {
    let mut res: usize;
    unsafe {
        asm!("mov {0}, [{1}]", out(reg) res, in(reg) ptr)
    };
    res
}</pre>			<p>What you’ve just looked at is a dereference function written <span class="No-Break">in assembly.</span></p>
			<p>The <strong class="source-inline">mov {0}, [{1}]</strong> line needs some explanation. <strong class="source-inline">{0}</strong> and <strong class="source-inline">{1}</strong> are templates that tell the compiler that we’re referring to the registers that <strong class="source-inline">out(reg)</strong> and <strong class="source-inline">in(reg)</strong> represent. The number is just an index, so if we had more inputs or outputs they would be numbered <strong class="source-inline">{2}</strong>, <strong class="source-inline">{3}</strong>, and so on. Since we only specify <strong class="source-inline">reg</strong> and not a specific register, we let the compiler <a id="_idIndexMarker057"/>choose what registers it wants <span class="No-Break">to use.</span></p>
			<p>The <strong class="source-inline">mov</strong> instruction instructs the CPU to take the first 8 bytes (if we’re on a 64-bit machine) it gets when reading the memory location that <strong class="source-inline">{1}</strong> points to and place that in the register represented by <strong class="source-inline">{0}</strong>. The <strong class="source-inline">[]</strong> brackets will instruct the CPU to treat the data in that register as a memory address, and instead of simply copying the memory address itself to <strong class="source-inline">{0}</strong>, it will fetch what’s at that memory location and move <span class="No-Break">it over.</span></p>
			<p><em class="italic">Anyway, we’re just writing instructions to the CPU here. No standard library, no syscall; just raw instructions. There is no way the OS is involved in that dereference </em><span class="No-Break"><em class="italic">function, right?</em></span></p>
			<p>If you run this program, you get what <span class="No-Break">you’d expect:</span></p>
			<pre class="source-code">
100</pre>			<p>Now, if you keep the <strong class="source-inline">dereference</strong> function but replace the <strong class="source-inline">main</strong> function with a function that creates a pointer to the <strong class="source-inline">99999999999999</strong> address, which we know is invalid, we get <span class="No-Break">this function:</span></p>
			<pre class="source-code">
fn main() {
    let t_ptr = 99999999999999 as *const usize;
    let x = dereference(t_ptr);
    println!("{}", x);
}</pre>			<p>Now, if we run that we get the <span class="No-Break">following results.</span></p>
			<p>This is the result <span class="No-Break">on Linux:</span></p>
			<pre class="source-code">
Segmentation fault (core dumped)</pre>			<p>This is the result <span class="No-Break">on Windows:</span></p>
			<pre class="source-code">
error: process didn't exit successfully: `target\debug\ac-assembly-dereference.exe` (exit code: 0xc0000005, STATUS_ACCESS_VIOLATION)</pre>			<p>We get a segmentation fault. Not surprising, really, but as you also might notice, the error we get is different on<a id="_idIndexMarker058"/> different platforms. Surely, the OS is involved somehow. Let’s take a look at what’s really <span class="No-Break">happening here.</span></p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Down the rabbit hole</h2>
			<p>It turns out that there is a great deal of cooperation between the OS and the CPU, but maybe not in the way you would <span class="No-Break">naively think.</span></p>
			<p>Many modern CPUs provide some basic infrastructure that operating systems use. This infrastructure gives us the security and stability we expect. Actually, most advanced CPUs provide a lot more options than operating systems such as Linux, BSD, and Windows <span class="No-Break">actually use.</span></p>
			<p>There are two in particular that I want to <span class="No-Break">address here:</span></p>
			<ul>
				<li>How the CPU prevents us from accessing memory we’re not supposed <span class="No-Break">to access</span></li>
				<li>How the CPU handles asynchronous events such <span class="No-Break">as I/O</span></li>
			</ul>
			<p>We’ll cover the first one here and the second in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>How does the CPU prevent us from accessing memory we’re not supposed to access?</h2>
			<p>As I mentioned, modern CPU architectures define some basic concepts by design. Some examples of this are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><span class="No-Break">Virtual memory</span></li>
				<li><span class="No-Break">Page table</span></li>
				<li><span class="No-Break">Page fault</span></li>
				<li><span class="No-Break">Exceptions</span></li>
				<li><span class="No-Break">Privilege level</span></li>
			</ul>
			<p>Exactly how this works will differ depending on the specific CPU, so we’ll treat them in general <span class="No-Break">terms here.</span></p>
			<p>Most modern CPUs have a <strong class="bold">memory management unit </strong>(<strong class="bold">MMU</strong>). This part of the CPU is often etched <a id="_idIndexMarker059"/>on the same dye, even. The MMU’s job is to translate the virtual address we use in our programs<a id="_idIndexMarker060"/> to a <span class="No-Break">physical address.</span></p>
			<p>When the OS starts a process (such as our program), it sets up a page table for our process and makes sure a special register on the CPU points to this <span class="No-Break">page table.</span></p>
			<p>Now, when we try to dereference <strong class="source-inline">t_ptr</strong> in the preceding code, the address is at some point sent for translation to the MMU, which looks it up in the page table to translate it to a physical address in the memory where it can fetch <span class="No-Break">the data.</span></p>
			<p>In the first case, it will point to a memory address on our stack that holds the <span class="No-Break">value </span><span class="No-Break"><strong class="source-inline">100</strong></span><span class="No-Break">.</span></p>
			<p>When we pass in <strong class="source-inline">99999999999999</strong> and ask it to fetch what’s stored at that address (which is what dereferencing does), it looks for the translation in the page table but can’t <span class="No-Break">find it.</span></p>
			<p>The CPU then treats this as a <span class="No-Break">page fault.</span></p>
			<p>At boot, the OS provided the<a id="_idIndexMarker061"/> CPU with an <strong class="bold">interrupt descriptor table</strong>. This table has a predefined format where the OS provides handlers for the predefined conditions the CPU <span class="No-Break">can encounter.</span></p>
			<p>Since the OS provided a pointer to a function that handles <em class="italic">page fault</em>, the CPU jumps to that function when we try to dereference <strong class="source-inline">99999999999999</strong> and thereby hands over control to the <span class="No-Break">operating system.</span></p>
			<p>The OS then prints a nice message for us, letting us know that we encountered what it calls a <strong class="bold">segmentation fault</strong>. This message will<a id="_idIndexMarker062"/> therefore vary depending on the OS you run the <span class="No-Break">code on.</span></p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>But can’t we just change the page table in the CPU?</h2>
			<p>Now, this is where the <em class="italic">privilege level</em> comes in. Most <a id="_idIndexMarker063"/>modern operating systems operate with two <em class="italic">ring levels</em>: <em class="italic">ring 0</em>, the kernel space, and <em class="italic">ring 3</em>, the <span class="No-Break">user space.</span></p>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="image/B20892_Figure_01.2.jpg" alt="Figure 1.2 – Privilege rings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Privilege rings</p>
			<p>Most CPUs have a concept of more rings than what most modern operating systems use. This has historical <a id="_idIndexMarker064"/>reasons, which is also why <em class="italic">ring 0</em> and <em class="italic">ring 3</em> are used (and not 1 <span class="No-Break">and 2).</span></p>
			<p>Every entry in the page table has additional information about it. Amongst that information is the information about which ring it belongs to. This information is set up when your OS <span class="No-Break">boots up.</span></p>
			<p>Code executed in <em class="italic">ring 0</em> has almost unrestricted access to external devices and memory, and is free to change registers that provide security at the <span class="No-Break">hardware level.</span></p>
			<p>The code you write in <em class="italic">ring 3</em> will typically have extremely restricted access to I/O and certain CPU registers (and instructions). Trying to issue an instruction or setting a register from <em class="italic">ring 3</em> to change the page table will be prevented by the CPU. The CPU will then treat this as an exception and jump to the handler for that exception provided by <span class="No-Break">the OS.</span></p>
			<p>This is also the reason why you have no other choice than to cooperate with the OS and handle I/O tasks through syscalls. The system wouldn’t be very secure if this wasn’t <span class="No-Break">the case.</span></p>
			<p>So, to sum it up: yes, the<a id="_idIndexMarker065"/> CPU and the OS cooperate a great deal. Most modern desktop CPUs are built with an OS in mind, so they provide the hooks and infrastructure that the OS latches onto upon bootup. When the OS spawns a process, it also sets its privilege level, making sure that normal processes stay within the borders it defines to maintain stability <span class="No-Break">and security.</span></p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Interrupts, firmware, and I/O</h1>
			<p>We’re nearing the end of the general CS subjects in this book, and we’ll start to dig our way out of the rabbit <span class="No-Break">hole soon.</span></p>
			<p>This part tries to tie things together and look at how the whole computer works as a system to handle I/O <span class="No-Break">and concurrency.</span></p>
			<p>Let’s get <span class="No-Break">to it!</span></p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>A simplified overview</h2>
			<p>Let’s look at some of the steps where we imagine that we read from a <span class="No-Break">network card:</span></p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="image/B20892_Figure_01.3.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p>Remember that we’re<a id="_idIndexMarker066"/> simplifying a lot here. This is a rather complex operation but we’ll focus on the parts that are of most interest to us and skip a few steps along <span class="No-Break">the way.</span></p>
			<h3>Step 1 – Our code</h3>
			<p>We register a socket. This happens by issuing a <em class="italic">syscall </em>to the OS. Depending on the OS, we either get a <em class="italic">file descriptor</em> (macOS/Linux) or a <span class="No-Break"><em class="italic">socket</em></span><span class="No-Break"> (Windows).</span></p>
			<p>The next step is that we <a id="_idIndexMarker067"/>register our interest in <strong class="source-inline">Read</strong> events on <span class="No-Break">that socket.</span></p>
			<h3>Step 2 – Registering events with the OS</h3>
			<p>This is handled in<a id="_idIndexMarker068"/> one of <span class="No-Break">three ways:</span></p>
			<ol>
				<li>We tell the operating system that we’re interested in <strong class="source-inline">Read</strong> events but we want to wait for it to happen by <strong class="source-inline">yielding</strong> control over our thread to the OS. The OS then suspends our thread by storing the register state and switches to some <span class="No-Break">other thread</span></li>
			</ol>
			<p><em class="italic">From our perspective, this will be blocking our thread until we have data </em><span class="No-Break"><em class="italic">to read.</em></span></p>
			<ol>
				<li value="2">We tell the operating <a id="_idIndexMarker069"/>system that we’re interested in <strong class="source-inline">Read</strong> events but we just want a handle to a task that we can <strong class="source-inline">poll</strong> to check whether the event is ready <span class="No-Break">or not.</span></li>
			</ol>
			<p><em class="italic">The OS will not suspend our thread, so this will not block </em><span class="No-Break"><em class="italic">our code.</em></span></p>
			<ol>
				<li value="3">We tell the operating system that we are probably going to be interested in many events, but we want to subscribe to one event queue. When we <strong class="source-inline">poll</strong> this queue, it will block our thread until one or more <span class="No-Break">events occur.</span></li>
			</ol>
			<p><em class="italic">This will block our thread while we wait for events </em><span class="No-Break"><em class="italic">to occur.</em></span></p>
			<p>Chapters 3 and 4 will go into detail about the third method, as it’s the most used method for modern async frameworks to <span class="No-Break">handle concurrency.</span></p>
			<h3>Step 3 – The network card</h3>
			<p>We’re skipping some steps here, but I don’t think they’re vital to <span class="No-Break">our understanding.</span></p>
			<p>On the network card, there is <a id="_idIndexMarker070"/>a small microcontroller running specialized firmware. We can imagine that this microcontroller is polling in a busy loop, checking whether any data <span class="No-Break">is incoming.</span></p>
			<p>The exact way the network card handles its internals is a little different from what I suggest here, and will most likely vary from vendor to vendor. The important part is that there is a very simple but specialized CPU running on the network card doing work to check whether there are <span class="No-Break">incoming events.</span></p>
			<p>Once the firmware<a id="_idIndexMarker071"/> registers incoming data, it issues a <span class="No-Break"><em class="italic">hardware interrupt</em></span><span class="No-Break">.</span></p>
			<h3>Step 4 – Hardware interrupt</h3>
			<p>A modern CPU has a set of <strong class="bold">interrupt request line</strong> (<strong class="bold">IRQs</strong>) for it to<a id="_idIndexMarker072"/> handle events that occur from external devices. A CPU has a fixed set of <span class="No-Break">interrupt lines.</span></p>
			<p>A hardware interrupt is an <a id="_idIndexMarker073"/>electrical signal that can occur at any time. The CPU immediately <em class="italic">interrupts</em> its normal workflow to handle the interrupt by saving the state of its registers and looking up the interrupt handler. The interrupt handlers are<a id="_idIndexMarker074"/> defined in the <strong class="bold">interrupt descriptor </strong><span class="No-Break"><strong class="bold">table</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">IDT</strong></span><span class="No-Break">).</span></p>
			<h3>Step 5 – Interrupt handler</h3>
			<p>The IDT is a table where the OS (or a driver) registers handlers for different interrupts that may occur. Each entry points to a<a id="_idIndexMarker075"/> handler function for a specific interrupt. The handler function for a network card would typically be registered and handled by a <em class="italic">driver</em> for <span class="No-Break">that card.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">The IDT is not stored on the CPU as it might seem in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.3</em>. It’s located in a fixed and known location in the main memory. The CPU only holds a pointer to the table in one of <span class="No-Break">its registers.</span></p>
			<h3>Step 6 – Writing the data</h3>
			<p>This is a step<a id="_idIndexMarker076"/> that might vary a lot depending on the CPU and the firmware on the network card. If the network card<a id="_idIndexMarker077"/> and the CPU support <strong class="bold">direct memory access</strong> (<strong class="bold">DMA</strong>), which should be the standard on all modern systems today, the network card will write data directly to a set of buffers that the OS already has set up in the <span class="No-Break">main memory.</span></p>
			<p>In such a system, the <em class="italic">firmware</em> on the network card might issue an <em class="italic">interrupt</em> when the data is <em class="italic">written</em> to memory. DMA is very efficient, since the CPU is only notified when the data is already in memory. On older systems, the CPU needed to devote resources to handle the data transfer from <a id="_idIndexMarker078"/>the <span class="No-Break">network card.</span></p>
			<p><em class="italic">The </em><strong class="bold">direct memory access controller</strong><em class="italic"> (</em><strong class="bold"> DMAC</strong><em class="italic">) is added to the diagram since in such a system, it would control the</em><em class="italic"><a id="_idIndexMarker079"/></em><em class="italic"> access to memory. It’s not part of the CPU as indicated in the previous diagram. We’re deep enough in the rabbit hole now, and exactly where the different parts of a system are is not really important to us right now, so let’s </em><span class="No-Break"><em class="italic">move on.</em></span></p>
			<h3>Step 7 – The driver</h3>
			<p>The <em class="italic">driver</em> would normally handle the communication between the OS and the network card. At some point, the buffers <a id="_idIndexMarker080"/>are filled and the network card issues an interrupt. The CPU then jumps to the handler of that interrupt. The interrupt handler for this exact type of interrupt is registered by the driver, so it’s actually the driver that handles this event and, in turn, informs the kernel that the data is ready to <span class="No-Break">be read.</span></p>
			<h3>Step 8 – Reading the data</h3>
			<p>Depending on whether<a id="_idIndexMarker081"/> we chose method 1, 2, or 3, the OS will do <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Wake <span class="No-Break">our thread</span></li>
				<li>Return <strong class="bold">Ready</strong> on the <span class="No-Break">next poll</span></li>
				<li>Wake the thread and return a <strong class="source-inline">Read</strong> event for the handler <span class="No-Break">we registered</span></li>
			</ul>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>Interrupts</h2>
			<p>As you know by now, there <a id="_idIndexMarker082"/>are two kinds <span class="No-Break">of interrupts:</span></p>
			<ul>
				<li><span class="No-Break">Hardware interrupts</span></li>
				<li><span class="No-Break">Software interrupts</span></li>
			</ul>
			<p>They are very different <span class="No-Break">in nature.</span></p>
			<h3>Hardware interrupts</h3>
			<p>Hardware interrupts are created<a id="_idIndexMarker083"/> by sending an electrical signal through an IRQ. These <a id="_idIndexMarker084"/>hardware lines signal the <span class="No-Break">CPU directly.</span></p>
			<h3>Software interrupts</h3>
			<p>These are interrupts issued<a id="_idIndexMarker085"/> from software instead of hardware. As in the case of a<a id="_idIndexMarker086"/> hardware interrupt, the CPU jumps to the IDT and runs the handler for the <span class="No-Break">specified interrupt.</span></p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Firmware</h2>
			<p>Firmware doesn’t get much <a id="_idIndexMarker087"/>attention from most of us; however, it’s a crucial part of the world we live in. It runs on all kinds of hardware and has all kinds of strange and peculiar ways to make the computers we program <span class="No-Break">on work.</span></p>
			<p>Now, the firmware needs a microcontroller to be able to work. Even the CPU has firmware that makes it work. That means there are many more small ‘CPUs’ on our system than the cores we <span class="No-Break">program against.</span></p>
			<p>Why is this important? Well, you remember that concurrency is all about efficiency, right? Since we have many CPUs/microcontrollers already doing work for us on our system, one of our concerns is to not replicate or duplicate that work when we <span class="No-Break">write code.</span></p>
			<p>If a network card has firmware that continually checks whether new data has arrived, it’s pretty wasteful if we duplicate that by letting our CPU continually check whether new data arrives as well. It’s much better if we either check once in a while, or even better, get notified when data <span class="No-Break">has arrived.</span></p>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor042"/>Summary</h1>
			<p>This chapter covered a lot of ground, so good job on doing all that legwork. We learned a little bit about how CPUs and operating systems have evolved from a historical perspective and the difference between non-preemptive and preemptive multitasking. We discussed the difference between concurrency and parallelism, talked about the role of the operating system, and learned that system calls are the primary way for us to interact with the host operating system. You’ve also seen how the CPU and the operating system cooperate through an infrastructure designed as part of <span class="No-Break">the CPU.</span></p>
			<p>Lastly, we went through a diagram on what happens when you issue a network call. You know there are at least three different ways for us to deal with the fact that the I/O call takes some time to execute, and we have to decide which way we want to handle that <span class="No-Break">waiting time.</span></p>
			<p>This covers most of the general background information we need so that we have the same definitions and overview before we go on. We’ll go into more detail as we progress through the book, and the first topic that we’ll cover in the next chapter is how programming languages model asynchronous program flow by looking into threads, coroutines <span class="No-Break">and futures.</span></p>
		</div>
	</body></html>
- en: Implementing Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency is the act of doing two things at the same time. On a single-core
    processor, this means **multitasking**. When multitasking, an operating system
    will switch between running processes to give each of them a share of time to
    use the processor. On a multi-core processor, concurrent processes can run simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at different models of concurrency. Some of these
    tools are relevant, others are used more for educational purposes. Here, we recommend
    and explain the thread model of concurrency. Further, we will explain how functional
    design patterns can make it easier to develop programs that use concurrency effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning outcomes will include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing and applying subprocess concurrency appropriately
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the nix fork concurrency model and its benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing and applying thread concurrency appropriately
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Rust primitive `Send` and `Sync` traits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing and applying the actor design pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A recent version of Rust is necessary to run the examples provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.rust-lang.org/en-US/install.html](https://www.rust-lang.org/en-US/install.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter''s code is also available on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Functional-Programming-in-RUST](https://github.com/PacktPublishing/Hands-On-Functional-Programming-in-RUST)'
  prefs: []
  type: TYPE_NORMAL
- en: Specific installation and build instructions are also included in each chapter's
    `README.md` file.
  prefs: []
  type: TYPE_NORMAL
- en: Using subprocess concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A subprocess is a command that is started from within another process. As a
    simple example of this, let''s create a parent process with three children. `process_a`
    will be the parent. Consider the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The child process, `process_b`, runs a loop and prints its own process ID.
    This is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run `process_a`, then you will see output from the three `process_b`
    processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you inspect the process tree starting at `process_a`, then you will find
    that three `process_b` processes are attached as children, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding commands to inspect the process tree require a Unix-like Command
    Prompt. The subprocess module itself, though, is more or less platform-independent.
  prefs: []
  type: TYPE_NORMAL
- en: Subprocess concurrency is useful if you want to run and manage other projects
    or utilities. A good example of subprocess concurrency done right is the `cron`
    utility. `cron` accepts a configuration file that specifies different commands
    to be run, and a schedule of when to run them. `cron` continues to run in the
    background and at the appropriate time starts each configured process according
    to schedule.
  prefs: []
  type: TYPE_NORMAL
- en: Subprocess concurrency is not well suited for parallel computation in general.
    No resources will be shared between parent and child processes when using the
    `subprocess::Command` interface. Also, information cannot be shared easily between
    these processes.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding nix fork concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before threads were introduced as a standard for POSIX operating systems in
    1995, the best option available for concurrency was `fork`. On these operating
    systems, `fork` was a fairly primitive command that allowed programs to create
    copies of themselves as child processes. The name `fork` comes from the idea of
    taking one process and splitting it into two.
  prefs: []
  type: TYPE_NORMAL
- en: '`fork` is not platform-independent, specifically it is not available on Windows,
    and we recommend using threads instead. However, for educational purposes, it
    is helpful to introduce some of the concepts from `fork` because they are also
    relevant to threaded programming.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is a translation of the preceding `process_a`, `process_b`
    example to use `fork`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the parent-child relationship is very similar to our first
    example. We have three children running and one parent managing them.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that forked processes share memory initially. Only when either
    process modifies its memory, will the operating system then perform an operation
    called **copy-on-write**, duplicating the memory. This behavior is a first step
    into shared memory between running processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate copy-on-write, let''s allocate 200 MB of memory and fork 500
    processes. Without copy-on-write, this would be 100 GB and would crash most personal
    computers. Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Many resources from the parent process also remain available and safe to use
    from the child process. This is very useful for server applications that listen
    on a socket in the parent process and poll for incoming connections in the child
    process. This simple trick permits server applications to distribute work across
    worker processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we start listening for connections on port `8888`. Then, after
    forking three times, we start serving responses with our worker process. Sending
    requests to the server, we can confirm that separate processes are indeed competing
    to serve requests. Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: All three workers served at least one response. Combining the first strategy
    of memory sharing with this new concept of built-in load balancing, forked processes
    effectively solve several common problems where concurrency is desired.
  prefs: []
  type: TYPE_NORMAL
- en: However, the fork concurrency model is very rigid. Both of these tricks require
    planning the application to strategically fork after resources are allocated.
    Fork does not help at all after the processes have been split. In POSIX, there
    have been additional standards created to address this problem. Sending information
    over channels or sharing memory are a common pattern, much like in Rust. However,
    none of these solutions have proved as practical as threads.
  prefs: []
  type: TYPE_NORMAL
- en: Threads implicitly permit inter-process messaging and memory sharing. The risk
    of threads is that sharing messages or memory may not be thread-safe and may lead
    to memory corruption. Rust is built from the ground up to make threaded programming
    safer.
  prefs: []
  type: TYPE_NORMAL
- en: Using thread concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rust threads have the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: Share memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Share resources, such as files or sockets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tend to be thread-safe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support inter-thread messaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are platform-independent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the preceding reasons, we suggest that Rust threads are better suited to
    most concurrency use cases than subprocesses. If you want to distribute computation,
    circumvent a blocking operation, or otherwise utilize concurrency for your application—use
    threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show the thread pattern, we can re-implement the preceding examples. Here
    are three children threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we spawn three threads and let them run. We print the process ID, but
    we must also print the thread ID because threads share the same process ID. Here
    is the output demonstrating this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The next example to port is the 500 processes and shared memory. In a threaded
    program, sharing might look something like the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The process starts 500 threads, all sharing the same memory. Also, thanks to
    the lock, we could modify this memory safely if we wanted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try the server example, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, three worker processes scrape a queue of requests that get served down
    from the parent process. All three children and the parent need to read and mutate
    the request queue. To mutate the request queue, each thread must lock the data.
    There is a dance here that the children and parent do to avoid holding the lock
    for too long. If one thread monopolizes the locked resource, then all other processes
    wanting to use the data must wait.
  prefs: []
  type: TYPE_NORMAL
- en: The trade-off of locking and waiting is called **contention**. In the worst
    case scenario, two threads can each hold a lock while waiting for the other thread
    to release the lock it holds. This is called **deadlock**.
  prefs: []
  type: TYPE_NORMAL
- en: Contention is a difficult problem associated with mutable shared state. For
    the preceding server case, it would have been better to send messages to children
    threads. Message passing does not create locks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a lock-free server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Channels work much better in this situation. This multi-threaded server has
    load balancing controlled from the parent process and does not suffer from lock
    contention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Channels are not strictly better than shared state. For example, legitimately
    contentious resources are good to handle with locks. Consider the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have a large mutable data structure (a neural network) that is broken
    into rows and columns. Each column has a thread-safe lock. Row data is all associated
    with the same lock. This pattern is useful for data and computation-heavy programs.
    Neural network training is a good example of where this technique may be relevant.
    Unfortunately, the code does not implement an actual neural network, but it does
    demonstrate how lock concurrency could be used to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Send and Sync traits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous neural network example, we used a static data structure that
    was shared between threads without being wrapped in a counter or lock. It contained
    locks, but why was the outer data structure permitted to be shared?
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer this question, let''s first review the rules of ownership:'
  prefs: []
  type: TYPE_NORMAL
- en: Each value in Rust has a variable that's called its **owner**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can only be one owner at a time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the owner goes out of scope, the value will be dropped
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these rules in mind, let''s try to share a variable across threads, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If we try to compile this, then we will get an error complaining of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This error indicates the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Referencing variable `a` from inside the closure is okay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The closure lives longer than variable `a`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Closures sent to threads must have a static lifetime. Variable `a` is a local
    variable, and thus will go out of scope before the static closure.
  prefs: []
  type: TYPE_NORMAL
- en: 'To fix this error, it is common to move the variable `a` into the closure.
    Thus, `a` will inherit the same lifetime as the closure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This program will compile and run. Ownership of the variable `a` is transferred
    to the closure and therefore lifetime issues are avoided. It should be noted that
    transferring ownership of a variable implies that the original variable is no
    longer valid. This is caused by ownership rule number `2`—there can only by one
    owner at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we try to share the variable again, we get an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling this gives us this error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This compiler error is a bit complicated. It says the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Capture of moved value**: `a`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Value moved (into closure) here
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Value captured here after move
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note—move occurs because `a` does not implement the `Copy` trait
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part four of the error tells us that if `a` implements the `Copy` trait, then
    we would not have this error. However, that would be implicitly copying the variable
    for us, meaning we would not be sharing data. So, that suggestion is not useful
    for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main problem is part one—capture of moved value `a`:'
  prefs: []
  type: TYPE_NORMAL
- en: First we move the variable `a` into the first closure. We needed to do this
    to avoid the lifetime problem and to use the variable. Using a variable in a closure
    is called a **capture**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next we use variable `a` in the second closure. This is the `value captured
    after move`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So our problem is that moving variable `a` invalidates it for further use.
    A much simpler example of this problem would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: By moving ownership of the value in `a` into `b`, we invalidate the original
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: So what do we do? Are we stuck?
  prefs: []
  type: TYPE_NORMAL
- en: In the neural network example, we used a shared data structure, so clearly there
    must be a way. If there is a way, hopefully there is also a rule to make sense
    of the problem. To fully understand thread-safety rules in Rust, you must understand
    three concepts—scope, `Send`, and `Sync`.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's address scope. Scope for threads means that variables used must
    be allowed to capture the variables that they used. Variables can be captured
    by value, by reference, or by mutable reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first example, not using `move`, almost worked. The only problem was that
    the lifetime of the variable we used went out of scope too soon. All thread closures
    must have static lifetimes, and therefore variables that they capture must also
    have static lifetimes. Adjusting for this, we can create a simple two-thread program
    that captures our variable, `A`, by reference and therefore does not move the
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Reading from static variables is safe. Mutating static variables is unsafe.
    Static variables are also disallowed from allocating heap memory directly, so
    they can be difficult to work with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `lazy_static` crate is a good way to create static variables with
    types that have memory allocation and need initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'A second way to fix scope problems is to use a reference counter, such as `Arc`.
    Here, we use `Arc` instead of `Rc` because `Arc` is thread-safe and `Rc` is not.
    Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The reference counter moves the reference into the closure. However, the internal
    data is shared, so it is then possible to reference common data.
  prefs: []
  type: TYPE_NORMAL
- en: 'If shared data should be mutated, then a `Mutex` lock can allow thread-safe
    locking. Another useful lock is the `std::sync::RwLock`. This is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: So why is mutation allowed after the lock, but not before? The answer is `Send`
    and `Sync`.
  prefs: []
  type: TYPE_NORMAL
- en: '`Send` and `Sync` are marker traits. A marker trait does not implement any
    functionality; however, it indicates that a type has some property. These two
    properties tell the compiler what behavior should be allowed with regards to sharing
    data between threads.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the rules regarding thread data sharing:'
  prefs: []
  type: TYPE_NORMAL
- en: A type is `Send` if it is safe to send it to another thread
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A type is `Sync` if it is safe to share between multiple threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make mutable data that can be shared across threads, whatever data type,
    you use must implement `Sync`. The standard Rust library has some thread-safe
    concurrency primitives, such as `Mutex`, for this purpose. If you don't like the
    options available, then you can search for another crate or make something yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement `Sync` for a type, just implement the trait with no body:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Be warned—incorrectly implementing `Send` or `Sync` can cause undefined behavior.
    The traits are always unsafe to implement. Thankfully, both of these marker traits
    are generally derived by the compiler, so you will very rarely need to manually
    derive them.
  prefs: []
  type: TYPE_NORMAL
- en: With these various rules in mind, we can see how Rust prevents many common threading
    bugs. Foremost, the ownership system prevents a lot of problems. Then, to allow
    some inter-thread communication, we find that channels and locks can help to safely
    implement most concurrency models.
  prefs: []
  type: TYPE_NORMAL
- en: This was a lot of trial and error but, in summary, we learned that `thread`,
    `move`, `channel`,  `Arc`, and `Mutex` will get us through most problems.
  prefs: []
  type: TYPE_NORMAL
- en: Using functional design for concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency forces the programmer to be more careful about information sharing.
    This difficulty coincidentally encourages good functional programming practices,
    such as immutable data and pure functions; when computation is not context-sensitive,
    it tends to also be thread-safe.
  prefs: []
  type: TYPE_NORMAL
- en: Functional programming sounds great for concurrency, but are there downsides?
  prefs: []
  type: TYPE_NORMAL
- en: In one example of good intentions with bad effects, during development of a
    functional language called **Haskell**, the development team ([https://www.infoq.com/interviews/armstrong-peyton-jones-erlang-haskell](https://www.infoq.com/interviews/armstrong-peyton-jones-erlang-haskell))
    wanted to make programs run faster using concurrency. Due to a unique trait of
    the Haskell language, it was possible to run all expressions and sub-expressions
    in new threads. The development team thought this sounded great and tested it
    out.
  prefs: []
  type: TYPE_NORMAL
- en: The result was that more time was spent spawning new threads than doing any
    computation. The idea still had merit, but it turned out that implementing concurrency
    automatically would be difficult. There are many trade-offs in concurrent programming.
    Letting the programmer make decisions regarding these trade-offs is the current
    state-of-the-art.
  prefs: []
  type: TYPE_NORMAL
- en: So, from functional programming, what patterns have proven useful?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many patterns for concurrent programming, but here we will introduce
    a few primitives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Actors**: Threads and patterns of behavior'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supervisors**: Monitor and manage actors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Routers**: Send messages between actors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monads**: Composable units of behavior'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, let''s look at actors in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Here we have two threads sending messages back and forth. Is this really much
    different than any of the previous examples?
  prefs: []
  type: TYPE_NORMAL
- en: There is a fairly common saying in functional programming that "*a closure is
    a poor man's object, and an object is a* *poor man's closure*".
  prefs: []
  type: TYPE_NORMAL
- en: According to object-oriented programming, objects have a type, fields, and methods.
    The closures we define hold their own mutable state, like fields of on an object.
    The ping and pong closures have slightly different types. The behavior inside
    the closure could be thought of as a single nameless method on the closure object.
    There are similarities here between object and closure.
  prefs: []
  type: TYPE_NORMAL
- en: However, it would be much nicer to use a normal object. The problem with attempting
    this is that the thread boundary gets in the way. Threads do not expose methods,
    only message passing. As a compromise, we could wrap the message passing into
    the form of methods. This would hide all of the channel management and would make
    programming with concurrent objects much nicer. We call this pattern the actor
    model.
  prefs: []
  type: TYPE_NORMAL
- en: An actor is very similar to an OOP object with the additional property that
    it lives in its own thread. Messages are sent to the actor, the actor processes
    the messages, and maybe sends out messages of its own. The actor model is like
    a busy city of people living and working doing different jobs but interacting
    and exchanging with one another according to their own schedules.
  prefs: []
  type: TYPE_NORMAL
- en: There are crates that attempt to provide elegant concurrent actor behavior,
    but we won't endorse any specifically. For the time being, please just squint
    your eyes and continue to pretend that closures are similar to objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next example, let''s wrap these actors into functions so that they can
    be created more easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the example, we will create three of each type of actor and store the
    channels in a vector, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have actors and a really basic supervisor for each actor group. The
    supervisor here is just a vector to keep track of communication channels for each
    actor. A good supervisor should periodically check the health of each actor, kill
    bad actors, and resupply the stock of good actors.
  prefs: []
  type: TYPE_NORMAL
- en: The last actor-based primitive that we will mention is routing. Routing is the
    method equivalent of object-oriented programming. OOP method calls were originally
    called **message passing**. The actor model is very object-oriented and accordingly
    we still call methods by actually passing messages around. We are still using
    the poor man's objects (closures), so our routing will probably look like a glorified
    `if` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start our actor router, we will define two data types—addresses and messages.
    Addresses should define all possible destinations and routing behaviors for messages.
    Messages should correspond to all possible method calls from all actors. Here
    is our extended ping pong application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we define our actors. They now need to match against the new `Message`
    type, and outgoing messages should have an `Address` in addition to a `Message`.
    Despite the changes, the code remains very similar to before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Each ping pong process loops to consume one message and send two more across.
    The last component for the program is initialization and routing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: After initializing the different actors, the main thread starts acting as the
    router. The router is a single thread with the sole responsibility of finding
    destinations, then moving, copying, cloning, and otherwise distributing messages
    to the recipient threads. This is not a complex solution, but it is effective,
    and uses only the typesafe, thread-safe, platform-independent primitives that
    we have introduced so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a more complex example, the routing `Address` will typically have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An actor role
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A method name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Argument type signatures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The message would then be the arguments according to the preceding type signature.
    Sending a message from an actor is as simple as sending your `(Address,Message)`
    to the router. The router at this time should be regularly checking each channel
    for new routing requests. When it sees the new message, it will pick an actor
    that satisfies the `Address` condition and send the message to that actor's inbox.
  prefs: []
  type: TYPE_NORMAL
- en: Watching the output, each ping pong action doubles the number of messages received.
    If each thread didn't do so much sleeping, then the program could get out of hand
    quickly. Messaging noise is one risk of overusing the actor model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the primitives of concurrent computation. Subprocesses,
    forked processes, and threads are the basic building blocks of all concurrent
    applications. In Rust threads, there are additional concerns that are introduced
    by the language to encourage type and thread safety.
  prefs: []
  type: TYPE_NORMAL
- en: In several examples, we built a concurrent web server using fork or threads.
    Later, while exploring thread behavior, we looked closely at what data can be
    shared between threads and how information can be sent between threads safely.
  prefs: []
  type: TYPE_NORMAL
- en: In the design pattern section, we introduced the actor design pattern. This
    popular technique combines some elements of object-oriented programming with other
    concepts from functional programming. The result is a programming tool designed
    specifically for complex resilient concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore performance, debugging, and metaprogramming.
    Performance can be hard to measure or compare, but we will try to introduce habits
    that are strictly good for performance. To help debugging, we will look at proactive
    and reactive techniques to solve issues. Proactive debugging is a set of techniques,
    such as proper error handling, that either prevents bugs or makes them easier
    to document and resolve. Reactive techniques are useful for difficult bugs that
    don't have an obvious cause. Finally, metaprogramming can do lots of complicated
    work behind the scenes to make ugly code look nicer.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a subprocess?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is fork called fork?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is fork still useful?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When were threads standardized?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is `move` sometimes needed for thread closures?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between `Send` and `Sync` traits?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are we allowed to lock and then mutate `Mutex` without an unsafe block?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

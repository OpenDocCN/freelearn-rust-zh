<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">FFI and Embedding – Combining Rust and Other Languages</h1>
                </header>
            
            <article>
                
<p>Up until this point in the book, we've discussed Rust more or less in isolation. Rust was intentionally designed to integrate with other programming languages by calling external programming languages through its <strong>Foreign Function Interface</strong> (<strong>FFI</strong>) and by being embedded itself. Many modern programming languages offer FFI, easy embedding, or both. Python, for instance, can very conveniently call out to libraries with C calling conventions and can be embedded with a little forethought. Lua, a high-level and garbage-collected language like Python, has a convenient FFI and can be embedded without much trouble. Erlang has a small handful of FFI interfaces but Erlang is not, itself, easily embedded into user-space environments. Amusingly, it's fairly straightforward to compile Erlang into an RTOS image.</p>
<p>In this chapter, we'll discuss calling out to foreign code in Rust and embedding Rust into foreign programming languages. We'll start off by extending the corewars evolver program —feruscore—that we covered at the tail end of <a href="d4802512-564b-4037-9407-b6035bd38f31.xhtml">Chapter 8</a>, <em>High-Level Parallelism – Threadpools, Parallel Iterators, and Processes</em>. You are encouraged to read that material before starting this chapter. After we've extended feruscore by embedding a C MARS simulator inside it, we'll move on to calling Rust functions from a C program. We'll demonstrate embedding Lua into a Rust system for convenient scripting and close the chapter by embedding Rust in high-level garbage-collected languages—Python and Erlang.</p>
<p><span>By the end of this chapter, we will have:</span></p>
<ul>
<li><span>Adapted the feruscore project from the last section to incorporate a C simulator</span></li>
<li><span>Demonstrated the inclusion of Lua code into a Rust project</span></li>
<li><span>Demonstrated the inclusion of Rust code into a C project</span></li>
<li><span>Demonstrated the inclusion of Rust code into a Python project</span></li>
<li><span>Demonstrated the inclusion of Rust code into an Erlang/Elixir project</span></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Embedding C into Rust – feruscore without processes</h1>
                </header>
            
            <article>
                
<p>When we wrapped up our discussion of feruscore in the previous chapter, we'd constructed a program that could discover corewars warriors through simulated natural selection. This was done by writing evolved warriors out to disk, using Rust's OS process interface to call out to pmars—the de facto standard MARS—and competing them to discover their relative fitness. We used Rayon—Rust's very convenient data parallelism library—to distribute the workload of competitions between available CPUs. Unfortunately, the implementation was pretty slow. Building a tournament selection criteria was maybe more difficult to express than we might have hoped—though I'm sure there a bright-spark of a reader out there who will improve that substantially and wow me. The real pain point was serializing <em>every</em> warrior to disk multiple times, allocating similar structures repeatedly to establish each round, and then eating pmars' allocation and parsing overhead. It's this last step, calling out to an external program to run competitions, is something we'll address in this chapter. We will also address the other two issues because, well, why not go all-in?</p>
<div class="packt_infobox"><span>Not all of feruscore's source code appears in this chapter. Some of it was discussed in-depth in the previous chapter, some of it—such as benchmarking code—would be a rehash of material already covered in the book. The C code is not printed in its entirety as it's very dense and very long. You can find the full listing in the book's source repository. </span></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The MARS C interface</h1>
                </header>
            
            <article>
                
<p>Corewars is a venerable game and there are many implementations of MARS out there. pMARS, like I mentioned, is the defacto standard, implementing the '94 ICWS draft and adding later additions, such as new instructions and a thing called <em>p-space</em>, that we won't go into here. Corewars has never been serious business. Many of the MARS available online today have traded code back and forth over the years, have been written with varying levels of attention to correctness, and sometimes programmed for older machines where multiprocessing was a concern for research labs. Most make no distinction between their internal library interface and executable consumer, like Rust programs do. Some blessed few are designed to be embedded, or, at least, are extractions of older MARS codebases that can be extended. Some blessed few of those don't use static globals in their implementations and can be embedded in a multiprocessing environment, such as feruscore.</p>
<p>In the grand tradition of MARS implementations, we will take an existing, public domain MARS, whittle it down to a manageable core, and put a new spin on it. Specifically, the feruscore introduced in this chapter embeds exhaust 1.9.2 (<a href="http://corewar.co.uk/pihlaja/exhaust/">http://corewar.co.uk/pihlaja/exhaust/</a>), written by M. Joonas Pihlaja in the early 2000s. Pihlaja seems to have extracted select code from pMARS, especially in and around exhaust's main and parser functions. We aren't after any of that code. What we need is the simulator. This means that we can toss out anything to do with parsing, and any support code needed for exhaust's <kbd>main</kbd> function. The extracted code we require lives in the feruscore project root, in <kbd>c_src/</kbd>. The functions we'll embed are all implemented in <kbd>c_src/sim.c</kbd>. These are from <kbd>c_src/sim.h</kbd>:</p>
<pre>void sim_free_bufs(mars_t* mars);
void sim_clear_core(mars_t* mars);

int sim_alloc_bufs(mars_t* mars);
int sim_load_warrior(mars_t* mars, uint32_t pos, <br/>                     const insn_t* const code, uint16_t len);
int sim_mw(mars_t* mars, const uint16_t * const war_pos_tab,<br/>           uint32_t *death_tab );</pre>
<p>The <kbd>sim_alloc_bufs</kbd> function is responsible for allocating the internal storage of a blank <kbd>mars_t</kbd>, the structure on which simulation is performed. <kbd>sim_clear_core</kbd> clears <kbd>mars_t</kbd> between rounds, setting core memory to <kbd>DAT</kbd>, and resetting any warrior queues. <kbd>sim_load_warrior</kbd> loads the warrior into core memory, the warrior really just being a pointer to an array of instructions—<kbd>insn_t</kbd>—with the <kbd>len</kbd> passed length. <kbd>sim_mw</kbd> runs the simulation, reading the warrior positions from <kbd>war_pos_tab</kbd>, and writing to <kbd>death_tab</kbd> when a warrior completely dies.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Creating C-structs from Rust</h1>
                </header>
            
            <article>
                
<p>Now, how do we call these functions from Rust? Moreover, how do we create instances of <kbd>mars_t</kbd> or <kbd>insn_t</kbd>? Well, recall back in <a href="605ce307-29ed-4b5a-961e-8d327467b84f.xhtml">Chapter 03</a>, <em>The Rust Memory Model – Ownership, References, and Manipulation</em>, that Rust allows control over memory layout in structures. Specifically, all Rust structures are implicitly <kbd>repr(Rust)</kbd>, types aligned to byte boundaries with structure fields being reordered as the compiler sees fit, among other details. We also noted the existence of a <kbd>repr(C)</kbd> for structures, in which Rust would lay out a structure's memory representation in the same manner as C. That knowledge now comes to bear.</p>
<p>What we will do is this. First, we'll compile our C code as a library and rig it to link into feruscore. That's done by placing a <kbd>build.rs</kbd> at the root of the project and using the cc (<a href="https://crates.io/crates/cc">https://crates.io/crates/cc</a>) crate to produce a static archive, like so:</p>
<pre>extern crate cc;

fn main() {
    cc::Build::new()
        .file("c_src/sim.c")
        .flag("-std=c11")
        .flag("-O3")
        .flag("-Wall")
        .flag("-Werror")
        .flag("-Wunused")
        .flag("-Wpedantic")
        .flag("-Wunreachable-code")
        .compile("mars");
}</pre>
<p>Cargo will produce <kbd>libmars.a</kbd> into <kbd>target/</kbd> when the project is built. But, how do we make <kbd>insn_t</kbd>? We copy the representation. The C side of this project defines <kbd>insn_t</kbd> like so:</p>
<pre>typedef struct insn_st {
  uint16_t a, b;
  uint16_t in;
} insn_t;</pre>
<p><kbd>uint16_t a</kbd> and <kbd>uint16_t b</kbd> are the <em>a</em>-field and <em>b</em>-field of the instruction,  where <kbd>uint16_t</kbd> is a compressed representation of the <kbd>OpCode</kbd>, <kbd>Modifier</kbd>, and <kbd>Modes</kbd> in an instruction. The Rust side of the project defines an instruction like so:</p>
<pre>#[derive(PartialEq, Eq, Copy, Clone, Debug, Default)]
#[repr(C)]
pub struct Instruction {
    a: u16,
    b: u16,
    ins: u16,
}</pre>
<p>This is the exact layout of the <kbd>inst_t</kbd> C. The reader will note that this is quite different from the definition of <kbd>Instruction</kbd> we saw in the previous chapter. Also, note that the field names do not matter, only the bit representation. The C structure calls the last field of the struct in, but this is a reserved keyword in Rust, so it is <kbd>ins</kbd> in the Rust side. Now, what is going on with that <kbd>ins</kbd> field? Recall that the <kbd>Mode</kbd> enumeration only had five fields. All we really need to encode a mode is three bits, converting the enumeration into numeric representation. A similar idea holds for the other components of an instruction. The layout of the <kbd>ins</kbd> field is:</p>
<pre>bit         15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00
field       |-flags-| |–-opcode–-| |–mod-| |b-mode| |a-mode|</pre>
<p>The <kbd>Mode</kbd> for a-field is encoded in bits 0, 1 and, 2. The <kbd>Mode</kbd> for b-field is in bits 3, 4, and 5, and so on for the other instruction components. The last two bits, 14 and 15, encode a <kbd>flag</kbd> that is almost always zero. A non-zero flag is an indicator to the simulator that the non-zero instruction is the <kbd>START</kbd> instruction—the 0<sup>th</sup> instruction of a warrior is not necessarily the one executed first by MARS. This compact structure requires a little more work on the part of the programmer to support it. For instance, the <kbd>Instruction</kbd> can no longer be created directly by the programmer but has to be constructed through a builder. The <kbd>InstructionBuilder</kbd>, defined in <kbd>src/instruction.rs</kbd>, is:</p>
<pre>pub struct InstructionBuilder {
    core_size: u16,
    ins: u16,
    a: u16,
    b: u16,
}</pre>
<p>As always, we have to keep track of the core size. Building the builder is straightforward enough, by this point:</p>
<pre>impl InstructionBuilder {
    pub fn new(core_size: u16) -&gt; Self {
        InstructionBuilder {
            core_size,
            ins: 0_u16,
            a: 0,
            b: 0,
        }
    }</pre>
<p>Writing a field into the instruction requires a little bit manipulation. Here's writing a <kbd>Modifier</kbd>:</p>
<pre>    pub fn modifier(mut self, modifier: Modifier) -&gt; Self {
        let modifier_no = modifier as u16;
        self.ins &amp;= !MODIFIER_MASK;
        self.ins |= modifier_no &lt;&lt; MODIFIER_MASK.trailing_zeros();
        self
    }</pre>
<p>The constant <kbd>MODIFIER_MASK</kbd> is defined in a block at the top of the source file with the other field masks:</p>
<pre>const AMODE_MASK: u16 = 0b0000_0000_0000_0111;
const BMODE_MASK: u16 = 0b0000_0000_0011_1000;
const MODIFIER_MASK: u16 = 0b0000_0001_1100_0000;
const OP_CODE_MASK: u16 = 0b0011_1110_0000_0000;
const FLAG_MASK: u16 = 0b1100_0000_0000_0000;</pre>
<p>Observe that the relevant bits in the masks are 1 bits. In <kbd>InstructionBuilder::modifier</kbd> we <kbd>&amp;=</kbd> the negation of the mask, which boolean-ands <kbd>ins</kbd> with the negation of the modifier mask, zero-ing the <kbd>Modifier</kbd> that was previously there. That done, the <kbd>Modifier</kbd> encoded as u16 is shifted left and boolean-or'ed into place. The <kbd>trailing_zeros()</kbd> function returns the total number of contiguous zeros in the lower end of a word, the exact number we need to shift by for each mask. Those readers that have done bit-manipulation work in other languages may find this to be very clean. I think so as well. Rust's explicit binary form for integers makes writing, and later, understanding, masks a breeze. Common bit-manipulation operations and queries are implemented on every basic integer type. Very useful.</p>
<p>The <kbd>OpCode</kbd> layout has changed somewhat. We don't <kbd>repr(C)</kbd> the enum, as the bit representation does not matter. What does matter, since this is enumeration is field-less, is which integer the variants cast to. First in the source maps to 0, the second to 1, and so forth. The C code has op-codes defined like so in <kbd>c_src/insn.h</kbd>:</p>
<pre>enum ex_op {
    EX_DAT,             /* must be 0 */
    EX_SPL,
    EX_MOV,
    EX_DJN,
    EX_ADD,
    EX_JMZ,
    EX_SUB,
    EX_SEQ,
    EX_SNE,
    EX_SLT,
    EX_JMN,
    EX_JMP,
    EX_NOP,
    EX_MUL,
    EX_MODM,
    EX_DIV,             /* 16 */
};</pre>
<p>The Rust version is as follows:</p>
<pre>#[derive(PartialEq, Eq, Copy, Clone, Debug, Rand)]
pub enum OpCode {
    Dat,  // 0
    Spl,  // 1
    Mov,  // 2
    Djn,  // 3
    Add,  // 4
    Jmz,  // 5
    Sub,  // 6
    Seq,  // 7
    Sne,  // 8
    Slt,  // 9
    Jmn,  // 10
    Jmp,  // 11
    Nop,  // 12
    Mul,  // 13
    Modm, // 14
    Div,  // 15
}</pre>
<p>The other instruction components have been shuffled around just a little bit to cope with the changes required by the C code. The good news is, this representation is more compact than the one from the previous chapter and should probably be maintained, even if all the C code were ported into Rust, a topic we'll get into later. But—and I'll spare you the full definition of <kbd>InstructionBuilder</kbd> because once you've seen one set-function you've seen them all—all this bit fiddling does make the implementation harder to see, and to correct at a glance. The instruction module now has QuickCheck tests to verify that all the fields get set correctly, meaning they can be ready right back out again no matter how many times fields are set and reset. You are encouraged to examine the QuickCheck tests yourself.</p>
<p>The high-level idea is this—a blank <kbd>Instruction</kbd> is made and a sequence of change orders is run over that Instruction—momentarily shifted into an <kbd>InstructionBuilder</kbd> to allow for modification—and then the changed field is read and confirmed to have become the value it was changed to. The technique is inline with what we've seen before elsewhere.</p>
<p>Now, what about that <kbd>mars_t</kbd>? The C definition, in <kbd>c_src/sim.h</kbd>, is:</p>
<pre>typedef struct mars_st {<br/>  uint32_t nWarriors;<br/><br/>  uint32_t cycles;<br/>  uint16_t coresize;<br/>  uint32_t processes;<br/><br/>  uint16_t maxWarriorLength;<br/><br/>  w_t* warTab;<br/>  insn_t* coreMem;<br/>  insn_t** queueMem;<br/>} mars_t;</pre>
<p>The <kbd>nWarriors</kbd> field sets how many warriors will be in the simulation, which for feruscore is always two cycles controls the number of cycles a round will take before ending if both warriors are still alive, processes the maximum number of processes available, and <kbd>maxWarriorLength</kbd> shows the maximum number of instructions a warrior may be. All of these are more or less familiar from the last chapter, just in a new programming language and with different names. The final three fields are pointers to arrays and are effectively private to the simulation function. These are allocated and deallocated by <kbd>sim_alloc_bufs</kbd> and <kbd>sim_free_bufs</kbd>, respectively. The Rust side of this structure looks like so, from <kbd>src/mars.rs</kbd>:</p>
<pre>#[repr(C)]
pub struct Mars {
    n_warriors: u32,
    cycles: u32,
    core_size: u16,
    processes: u32,
    max_warrior_length: u16,
    war_tab: *mut WarTable,
    core_mem: *mut Instruction,
    queue_mem: *const *mut Instruction,
}</pre>
<p>The only new type here is <kbd>WarTable</kbd>. Even though our code will never explicitly manipulate the warrior table, we do still have to be bit-compatible with C. The definition of <kbd>WarTable</kbd> is:</p>
<pre>#[repr(C)]
struct WarTable {
    tail: *mut *mut Instruction,
    head: *mut *mut Instruction,
    nprocs: u32,
    succ: *mut WarTable,
    pred: *mut WarTable,
    id: u32,
}</pre>
<p>We could have maybe got away with just making these private fields in <kbd>Mars</kbd> pointers to void in <kbd>mars_st</kbd>, but that would have reduced type information on the C side of the project and this approach might hamper future porting efforts. With the type explicit on the Rust side of the project, it's much easier to consider rewriting the C functions in Rust.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Calling C functions</h1>
                </header>
            
            <article>
                
<p>Now that we can create Rust structs with C-bit layout, we can start passing that memory down into C. It's important to understand, this is an inherently unsafe activity. The C code might fiddle with memory in a way that breaks Rust's invariants and the only way we can be sure this isn't true is auditing the C code ahead of time. This is the same with fuzzing, which we'll do too. How do we link with <kbd>libmars.a</kbd>? A small <kbd>extern</kbd> block will do it, in <kbd>src/mars.rs</kbd>:</p>
<pre>#[link(name = "mars")]
extern "C" {
    fn sim_alloc_bufs(mars: *mut Mars) -&gt; isize;
    fn sim_free_bufs(mars: *mut Mars) -&gt; ();
    fn sim_clear_core(mars: *mut Mars) -&gt; ();
    fn sim_load_warrior(mars: *mut Mars, pos: u32, <br/>                        code: *const Instruction, <br/>                        len: u16) -&gt; isize;
    fn sim_mw(mars: *mut Mars, war_pos_tab: *const u16, <br/>              death_tab: *mut u32) -&gt; isize;
}</pre>
<p>With <kbd>#[link(name = "mars")]</kbd>, we're instructing the compiler to link to the <kbd>libmars.a</kbd> produced in the build process. If we were linking to a system library, the approach would be the same. The Rust Nomicon section on FFI—referenced in the <em>Further reading</em> section at the end of this chapter—links to libsnappy, for example. The extern block informs the compiler that these functions will need to be called with the C ABI, not Rust's. Let's compare <kbd>sim_load_warrior</kbd> side by side. Here is the C version:</p>
<pre>int sim_load_warrior(mars_t* mars, uint32_t pos, <br/>                     const insn_t* const code, uint16_t len);</pre>
<p>Here is the Rust version:</p>
<pre>fn sim_load_warrior(mars: *mut Mars, pos: u32, <br/>                    code: *const Instruction, <br/>                    len: u16) -&gt; isize;</pre>
<p>These are similar. C doesn't have the same mutability concept as Rust, though there are const annotations for code that Rust picks up for free. We've enhanced that by making all of Instruction's query functions take <kbd>&amp;self</kbd>. Rule of thumb—unless the C is const-correct, you should probably assume the Rust needs a <kbd>*mut</kbd>. This isn't always true, but it saves a fair deal of headache. Now, on that notion, the careful reader may note that as I ported exhaust's code into feruscore, I adapted it to use <kbd>stdint.h</kbd>. In the unaltered code, <kbd>pos</kbd> has the unsigned int type or <kbd>usize</kbd>. The simulator C code assumes <kbd>pos</kbd> will be at least 32 bits wide, hence the explicit conversation to a 32-bit integer. This wasn't entirely necessary, but it's good form to used fixed-width types as they minimize surprises moving between CPU architectures.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Managing cross-language ownership</h1>
                </header>
            
            <article>
                
<p>Rust is now aware of the C functions, and we have our internal structs reworked to use C layout: it is time to link up the C functions that manipulate <kbd>mars_t</kbd> with our <kbd>Mars</kbd>. Because we're sharing ownership of <kbd>Mars</kbd> between the Rust allocator and the C allocator, we've got to be careful to initialize the <kbd>Mars</kbd> struct with null pointers in the fields that C will own, like so:</p>
<pre>impl MarsBuilder {
    pub fn freeze(self) -&gt; Mars {
        let mut mars = Mars {
            n_warriors: 2,
            cycles: u32::from(self.cycles.unwrap_or(10_000)),
            core_size: self.core_size.unwrap_or(8_000),
            processes: self.processes.unwrap_or(10_000),
            max_warrior_length: self.max_warrior_length.unwrap_or(100),
            war_tab: ptr::null_mut(),
            core_mem: ptr::null_mut(),
            queue_mem: ptr::null_mut(),
        };
        unsafe {
            sim_alloc_bufs(&amp;mut mars);
        }
        mars
    }</pre>
<p>This <kbd>MarsBuilder</kbd> is a builder-pattern for producing a <kbd>Mars</kbd>. The remainder of the implementation works how you might expect:</p>
<pre>    pub fn core_size(mut self, core_size: u16) -&gt; Self {
        self.core_size = Some(core_size);
        self
    }

    pub fn cycles(mut self, cycles: u16) -&gt; Self {
        self.cycles = Some(cycles);
        self
    }

    pub fn processes(mut self, processes: u32) -&gt; Self {
        self.processes = Some(processes);
        self
    }

    pub fn max_warrior_length(mut self, max_warrior_length: u16) -&gt; Self {
        self.max_warrior_length = Some(max_warrior_length);
        self
    }
}</pre>
<p>Back to <kbd>MarsBuilder::freeze(self) -&gt; Mars</kbd>. This function creates a new <kbd>Mars</kbd> and then passes it immediately into <kbd>sim_alloc_bufs</kbd>:</p>
<pre>unsafe {
    sim_alloc_bufs(&amp;mut mars);
}</pre>
<p><kbd>&amp;mut mars</kbd> automatically coerces into <kbd>*mut mars</kbd> and, as we know from previous chapters, dereferencing a raw pointer is unsafe. The fact that it's <em>C</em> dereferencing the raw pointer is icing on the cake. Now, let's take a look at <kbd>sim_alloc_bufs</kbd> and get a sense of what's going on. In <kbd>c_src/sim.c</kbd>:</p>
<pre>int sim_alloc_bufs(mars_t* mars) {
    mars-&gt;coreMem = (insn_t*)malloc(sizeof(insn_t) * mars-&gt;coresize);
    mars-&gt;queueMem = (insn_t**)malloc(sizeof(insn_t*) * <br/>                       (mars-&gt;nWarriors * mars-&gt;processes + 1));
    mars-&gt;warTab = (w_t*)malloc(sizeof(w_t)*mars-&gt;nWarriors);

    return (mars-&gt;coreMem
            &amp;&amp; mars-&gt;queueMem
            &amp;&amp; mars-&gt;warTab);
}</pre>
<p>The three fields owned by C—<kbd>coreMem</kbd>, <kbd>queueMem</kbd>, and <kbd>warTab</kbd>—are allocated according to the size of the structs being stored and the return is a boolean-and of the <kbd>malloc</kbd> returns, a short way of determining whether <kbd>malloc</kbd> ever returned <kbd>NULL</kbd>, signaling that no more memory was available on-system. Had we decided to add a new field into the Rust struct and not updated the C struct to reflect this change, these stores would be too small. Eventually, some code somewhere would reach past the bounds of an array and crash. No good, that.</p>
<p>But! We've just called C code from Rust.</p>
<p>Let's talk about ownership for a second. <kbd>Mars</kbd> is a structure not wholly owned by Rust, and not wholly owned by C. That's fine and is also not uncommon, especially if you're partially (or completely) porting a C codebase into Rust. It does mean we've got to be careful about <kbd>Drop</kbd>:</p>
<pre>impl Drop for Mars {
    fn drop(&amp;mut self) {
        unsafe { sim_free_bufs(self) }
    }
}</pre>
<p>As we've seen in previous chapters, we have to arrange for explicit <kbd>Drop</kbd> when there's raw memory in use. <kbd>Mars</kbd> is no exception. The call here to <kbd>sim_free_bufs</kbd> clears up the C-owned memory and Rust takes care of the rest. If cleanup were more difficult—as sometimes happens—you'd have to take care to avoid deferencing the C-owned pointers post-free. <kbd>sim_free_bufs</kbd> is a brief implementation:</p>
<pre>void sim_free_bufs(mars_t* mars)
{
    free(mars-&gt;coreMem);
    free(mars-&gt;queueMem);
    free(mars-&gt;warTab);
}</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Running the simulation</h1>
                </header>
            
            <article>
                
<p>All that's left to do is run warriors in simulation. In the previous chapter, an <kbd>Individual</kbd> was responsible for managing its own pmars process. Now, <kbd>Mars</kbd> will be responsible for hosting the competitors, which you may have gleaned from the C API. We simply don't have any space in <kbd>Individual</kbd> to store something, and by pushing competition into <kbd>Mars</kbd>, we can avoid allocation churn on temporary <kbd>Mars</kbd> structures.</p>
<p>The compete function that was formerly bound to <kbd>Individual</kbd> has now been moved to <kbd>Mars</kbd>:</p>
<pre>impl Mars {
    /// Competes two Individuals at random locations
    ///
    /// The return of this function indicates the winner. <br/>    /// `Winner::Right(12)` will mean that the 'right' player <br/>    /// won 12 more rounds than did 'left'. This may mean they<br/>    /// tied 40_000 times or maybe they only played 12 rounds <br/>    /// and right won each time.
    pub fn compete(&amp;mut self, rounds: u16, left: &amp;Individual, <br/>                   right: &amp;Individual) -&gt; Winner {
        let mut wins = Winner::Tie;
        for _ in 0..rounds {
            let core_size = self.core_size;
            let half_core = (core_size / 2) - self.max_warrior_length;
            let upper_core = core_size - self.max_warrior_length;
            let left_pos = thread_rng().gen_range(0, upper_core);
            let right_pos = if (left_pos + self.max_warrior_length) <br/>                               &lt; half_core {
                thread_rng().gen_range(half_core + 1, upper_core)
            } else {
                thread_rng().gen_range(0, half_core)
            };
            wins = wins + self.compete_inner(left, left_pos, <br/>                                             right, right_pos);
        }
        tally_fitness(wins);
        BATTLES.fetch_add(1, Ordering::Relaxed);
        wins
    }</pre>
<p>The C API requires that we calculate the warrior offsets and take care to not overlap them. The approach taken here is to randomly place the left Individual, determine whether it's in the upper or lower core, and then place the right Individual, taking care with both to not place them past the <kbd>end</kbd> of the core. The actual implementation of the competition is <kbd>compete_inner</kbd>:</p>
<pre>    pub fn compete_inner(
        &amp;mut self,
        left: &amp;Individual,
        left_pos: u16,
        right: &amp;Individual,
        right_pos: u16,
    ) -&gt; Winner {
        let (left_len, left_code) = left.as_ptr();
        let (right_len, right_code) = right.as_ptr();

        let warrior_position_table: Vec&lt;u16&gt; = vec![left_pos, right_pos];
        let mut deaths: Vec&lt;u32&gt; = vec![u32::max_value(), <br/>                                        u32::max_value()];</pre>
<p>We call <kbd>Individual::as_ptr() -&gt; (u16, *const Instruction)</kbd> to get a raw view of the chromosome of the Individual and its length. Without this, we've got nothing to pass down to the C functions. <kbd>warrior_position_table</kbd> informs MARS which instructions are the start of its competitors. We could search out the <kbd>START</kbd> flag in the warriors and place that in <kbd>warrior_position_table</kbd>. This is an improvement left for the reader. The deaths table will be populated by the simulator code. If both warriors die during competition, the array will be <kbd>[0, 1]</kbd>. The death table is populated with <kbd>u32::max_value()</kbd> to make distinguishing no-result from result easy enough. Before starting the competition, we have to clear the simulator—which might be filled with instructions from a previous bout:</p>
<pre>        unsafe {
            sim_clear_core(self);</pre>
<p>If you pull up the <kbd>sim_clear_core</kbd> implementation, you'll find a memset to 0 over <kbd>core_mem</kbd>. Recall that <kbd>DAT</kbd> is the 0<sup>th</sup> variant of the <kbd>Instruction</kbd> enumeration. Unlike <kbd>pmars</kbd>, this simulator must use <kbd>DAT</kbd> as a default instruction, but it does make resetting the field very fast. <kbd>sim_clear_core</kbd> also clears up the process queue and other C-owned storage. Loading the warriors is a matter of plugging in the information we've already computed:</p>
<pre>            assert_eq!(
                0,
                sim_load_warrior(self, left_pos.into(), <br/>                                 left_code, left_len)
            );
            assert_eq!(
                0,
                sim_load_warrior(self, right_pos.into(), <br/>                                 right_code, right_len)
            );</pre>
<p>If the result is non-zero, that's an indicator that some serious and non-recoverable fault has happened. <kbd>sim_load_warrior</kbd> is an array operation, writing the warrior Instructions into <kbd>core_mem</kbd> at the defined offsets. We could, very conceivably, rewrite the functions of <kbd>sim_clear_core</kbd> and <kbd>sim_load_warrior</kbd> in Rust if we wanted to. Finally, field cleared and warriors loaded, we are able to simulate:</p>
<pre>            let alive = sim_mw(self, <br/>                               warrior_position_table.as_ptr(),     <br/>                               deaths.as_mut_ptr());
            assert_ne!(-1, alive);
        }</pre>
<p>The <kbd>sim_mw</kbd> function returns the total number of warriors left alive at the end of the simulation run. If this value is <kbd>-1</kbd>, there's been some catastrophic, unrecoverable error.</p>
<p>Now, because we have a nice type system to play with, we don't really want to signal results back to the user with a vector of integers. We preserve the <kbd>Winner</kbd> type seen</p>
<p>in the previous chapter, doing a quick conversion before returning:</p>
<pre>        let left_dead = deaths[0] != u32::max_value();
        let right_dead = deaths[1] != u32::max_value();
        match (left_dead, right_dead) {
            (false, false) | (true, true) =&gt; Winner::Tie,
            (true, false) =&gt; Winner::Right(1),
            (false, true) =&gt; Winner::Left(1),
        }
    }<br/>}</pre>
<p>You may have noticed that <kbd>Winner</kbd> implements <kbd>Add</kbd>, allowing compete to signal how many rounds the warriors won. <kbd>impl Add for Winner</kbd> is also in <kbd>src/mars.rs</kbd>, should you be curious to see it. As a final sanity test around <kbd>Mars</kbd>, we confirm that the Imp will lose to Dwarf more often than other outcomes:</p>
<pre>#[cfg(test)]
mod tests {
    use super::*;
    use individual::*;

    #[test]
    fn imp_vs_dwarf() {
        let core_size = 8_000;
        let rounds = 100;
        let mut mars = MarsBuilder::default()<br/>                           .core_size(core_size).freeze();
        let imp = ringers::imp(core_size);
        let dwarf = ringers::dwarf(core_size);
        let res = mars.compete(rounds, &amp;imp, &amp;dwarf);
        println!("RES: {:?}", res);
        match res {
            Winner::Left(_) | Winner::Tie =&gt; { <br/>                panic!("imp should lose to dwarf")<br/>            },
            Winner::Right(_) =&gt; {}
        }
    }
}</pre>
<p>Recall that an Imp can't self-kill, only propagate. The Dwarf bombs core memory at regular, increasing offsets. An Imp versus Dwarf competition, then, is a race between Imp finding Dwarf's instructions and Dwarf dropping a <kbd>DAT</kbd> in the path of an Imp.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Fuzzing the simulation</h1>
                </header>
            
            <article>
                
<p>Feruscore has been changed in this chapter to include raw memory access in both Rust and through FFI. The responsible thing to do is fuzz <kbd>Mars</kbd> and make sure we're not causing segmentation faults. We'll use AFL, which we discussed back in <a href="8c61da2f-08c8-40be-89d4-d8afa8510850.xhtml" target="_blank">Chapter 2</a>, <em>Sequential Rust Performance and Testing</em>, and again in <a href="e2de1ba7-c291-494e-82da-37fee7323c1d.xhtml" target="_blank">Chapter 5</a>, <em>Locks – Mutex, Condvar, Barriers, and RWLock</em>. The fuzz target is <kbd>src/bin/fuzz_target.rs</kbd>. The trick with fuzzing is ensuring stability. That is, AFL can't really do its work if for some input applied multiple times multiple paths come out. Fuzzing is more efficient in the case of a deterministic system. We were careful to make <kbd>Mars::compete_inner</kbd> deterministic, where <kbd>Mars::compete</kbd> uses randomness to determine warrior positions. Fuzzing, then, will go through <kbd>compete_inner</kbd> only. The preamble for <kbd>fuzz_target</kbd> doesn't contain any new crates:</p>
<pre>extern crate byteorder;
extern crate feruscore;

use byteorder::{BigEndian, ReadBytesExt};
use feruscore::individual::*;
use feruscore::instruction::*;
use feruscore::mars::*;
use std::io::{self, Cursor, Read};</pre>
<p>Remember that AFL passes a byte slice through stdin and the fuzzing target is responsible for deserializing that array into something sensible for itself. We'll build up a <kbd>Config</kbd> structure:</p>
<pre>#[derive(Debug)]
struct Config {
    pub rounds: u16,
    pub core_size: u16,
    pub cycles: u16,
    pub processes: u16,
    pub max_warrior_length: u16,
    pub left_chromosome_size: u16,
    pub right_chromosome_size: u16,
    pub left: Individual,
    pub right: Individual,
    pub left_pos: u16,
    pub right_pos: u16,
}</pre>
<p>Hopefully, these fields are familiar. The <kbd>rounds</kbd>, <kbd>core_size</kbd>, <kbd>cycles</kbd>, and <kbd>processes</kbd> each affect the MARS environment. The <kbd>max_warrior_length</kbd>, <kbd>left_chromosome_size</kbd>, and <kbd>right_chromosome_size</kbd> affect the two individuals that will be made to compete. <kbd>left</kbd> and <kbd>right</kbd> are those <kbd>Individual</kbd> instances. <kbd>left_pos</kbd> and <kbd>right_pos</kbd> set where the warriors will be placed in the MARS core memory. The numbers that we'll deserialize from the byte slice won't always be entirely sensible, so there'll be some cleanup needed, like so:</p>
<pre>impl Config {
    pub fn new(rdr: &amp;mut Cursor&lt;Vec&lt;u8&gt;&gt;) -&gt; io::Result&lt;Config&gt; {
        let rounds = (rdr.read_u16::&lt;BigEndian&gt;()? % 1000).max(1);
        let core_size = (rdr.read_u16::&lt;BigEndian&gt;()? % 24_000).max(256);
        let cycles = (rdr.read_u16::&lt;BigEndian&gt;()? % 10_000).max(100);
        let processes = (rdr.read_u16::&lt;BigEndian&gt;()? % 1024).max(2);
        let max_warrior_length = (rdr.read_u16::&lt;BigEndian&gt;()? % 256).max(4);
        let left_chromosome_size = (rdr.read_u16::&lt;BigEndian&gt;()? <br/>                                       % max_warrior_length).max(2);
        let right_chromosome_size = (rdr.read_u16::&lt;BigEndian&gt;()? <br/>                                        % max_warrior_length).max(2);</pre>
<p>It doesn't make any sense for there to be 0 rounds, as an example, so we say thatthere must be at least one round. Likewise, we need two processes, desire at least four warrior instructions, and so forth. Creating the left and right warriors is a matter of passing the byte reader into <kbd>Config::mk_individual</kbd>:</p>
<pre>        let left = Config::mk_individual(rdr, <br/>                                         max_warrior_length,<br/>                                         left_chromosome_size, <br/>                                         core_size)?;
        let right =
            Config::mk_individual(rdr, <br/>                                  max_warrior_length, <br/>                                  right_chromosome_size, <br/>                                  core_size)?;</pre>
<p><kbd>Config::mk_individual</kbd> deserializes into <kbd>InstructionBuilder</kbd>. The whole thing is kind of awkward. While we can convert a field-less Enum into an integer, it's not possible to go from an integer to a field-less Enum without some hairy match statements:</p>
<pre>    fn mk_individual(
        rdr: &amp;mut Cursor&lt;Vec&lt;u8&gt;&gt;,
        max_chromosome_size: u16,
        chromosome_size: u16,
        core_size: u16,
    ) -&gt; io::Result&lt;Individual&gt; {
        assert!(chromosome_size &lt;= max_chromosome_size);
        let mut indv = IndividualBuilder::new();
        for _ in 0..(chromosome_size as usize) {
            let builder = InstructionBuilder::new(core_size);
            let a_field = rdr.read_i8()?;
            let b_field = rdr.read_i8()?;
            let a_mode = match rdr.read_u8()? % 5 {
                0 =&gt; Mode::Direct,
                1 =&gt; Mode::Immediate,
                2 =&gt; Mode::Indirect,
                3 =&gt; Mode::Decrement,
                _ =&gt; Mode::Increment,
            };
            let b_mode = match rdr.read_u8()? % 5 {
                0 =&gt; Mode::Direct,
                1 =&gt; Mode::Immediate,
                2 =&gt; Mode::Indirect,
                3 =&gt; Mode::Decrement,
                _ =&gt; Mode::Increment,
            };</pre>
<p>Here, we've established the <kbd>InstructionBuilder</kbd> and read the <kbd>Mode</kbd> for a-field and b-field out from the byte slice. If a field is added, we'll have to come through here and update the fuzzing code. It's a real pain. Reading the <kbd>Modifier</kbd> out works the same way:</p>
<pre>            let modifier = match rdr.read_u8()? % 7 {
                0 =&gt; Modifier::F,
                1 =&gt; Modifier::A,
                2 =&gt; Modifier::B,
                3 =&gt; Modifier::AB,
                4 =&gt; Modifier::BA,
                5 =&gt; Modifier::X,
                _ =&gt; Modifier::I,
            };</pre>
<p>As does reading out the <kbd>OpCode</kbd>:</p>
<pre>            let opcode = match rdr.read_u8()? % 16 {
                0 =&gt; OpCode::Dat,   // 0
                1 =&gt; OpCode::Spl,   // 1
                2 =&gt; OpCode::Mov,   // 2
                3 =&gt; OpCode::Djn,   // 3
                4 =&gt; OpCode::Add,   // 4
                5 =&gt; OpCode::Jmz,   // 5
                6 =&gt; OpCode::Sub,   // 6
                7 =&gt; OpCode::Seq,   // 7
                8 =&gt; OpCode::Sne,   // 8
                9 =&gt; OpCode::Slt,   // 9
                10 =&gt; OpCode::Jmn,  // 10
                11 =&gt; OpCode::Jmp,  // 11
                12 =&gt; OpCode::Nop,  // 12
                13 =&gt; OpCode::Mul,  // 13
                14 =&gt; OpCode::Modm, // 14
                _ =&gt; OpCode::Div,   // 15
            };</pre>
<p>Producing an instruction is simple enough, thanks to the builder pattern in use here:</p>
<pre>            let inst = builder
                .a_field(a_field)
                .b_field(b_field)
                .a_mode(a_mode)
                .b_mode(b_mode)
                .modifier(modifier)
                .opcode(opcode)
                .freeze();
            indv = indv.push(inst);
        }
        Ok(indv.freeze())
    }</pre>
<p>Moving back up to <kbd>Config::new</kbd>, we create the left and right positions:</p>
<pre>        let left_pos =
            Config::adjust_pos(core_size, <br/>                               rdr.read_u16::&lt;BigEndian&gt;()?, <br/>                               max_warrior_length);
        let right_pos =
            Config::adjust_pos(core_size, <br/>                               rdr.read_u16::&lt;BigEndian&gt;()?, <br/>                               max_warrior_length);</pre>
<p>The <kbd>adjust_pos</kbd> function is a small thing, intended to keep warrior positions properly in bounds:</p>
<pre>    fn adjust_pos(core_size: u16, mut pos: u16, space: u16) -&gt; u16 {
        pos %= core_size;
        if (pos + space) &gt; core_size {
            let past = (pos + space) - core_size;
            pos - past
        } else {
            pos
        }
    }</pre>
<p>It's entirely possible that the warriors will overlap with this calculation. That is okay. Our ambition with fuzzing is not to check the logic of the program, only to seek out crashes. In fact, if overlapping two warriors causes a crash, that's a fact we need to know. The close of <kbd>Config::new</kbd> is fairly straightforward:</p>
<pre>        Ok(Config {
            rounds,
            core_size,
            cycles,
            processes,
            max_warrior_length,
            left_chromosome_size,
            right_chromosome_size,
            left,
            right,
            left_pos,
            right_pos,
        })
    }</pre>
<p>After all that, the main function of <kbd>fuzz_target</kbd> is minimal:</p>
<pre>fn main() {
    let mut input: Vec&lt;u8&gt; = Vec::with_capacity(1024);
    let result = io::stdin().read_to_end(&amp;mut input);
    if result.is_err() {
        return;
    }
    let mut rdr = Cursor::new(input);
    if let Ok(config) = Config::new(&amp;mut rdr) {
        let mut mars = MarsBuilder::default()
            .core_size(config.core_size)
            .cycles(config.cycles)
            .processes(u32::from(config.processes))
            .max_warrior_length(config.max_warrior_length as u16)
            .freeze();
        mars.compete_inner(
            &amp;config.left,
            config.left_pos,
            &amp;config.right,
            config.right_pos,
        );
    }
}</pre>
<p><kbd>Stdin</kbd> is captured and a <kbd>Cursor</kbd> built from it, which we pass into <kbd>Config::new</kbd>, as explained earlier. The resulting <kbd>Config</kbd> is used to fuel <kbd>MarsBuilder</kbd>, and the <kbd>Mars</kbd> is then the arena for competition between the two fuzzing <kbd>Individual</kbd> instances that may or may not overlap. Remember, before running AFL, be sure to run <kbd>cargo afl build</kbd>—release and not cargo build —release. Both will work, but the first is significantly faster to discover crashes, as AFL's instrumentation will be inlined in the executable. I've found that even a single instance of <kbd>cargo afl fuzz -i /tmp/in -o /tmp/out target/release/fuzz_target</kbd> will run through AFL cycles at a good clip. There aren't many branches in the code and, so, few paths for AFL to probe.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The feruscore executable</h1>
                </header>
            
            <article>
                
<p>The final thing left to cover is <kbd>src/bin/feruscore.rs</kbd>. The careful reader will have noted that there's been a suspicious lack of rayon in the implementation so far. In fact, rayon is not in use in this version. Here's the full <kbd>Cargo.toml</kbd> for the project:</p>
<pre>[package]
name = "feruscore"
version = "0.2.0"
authors = ["Brian L. Troutwine &lt;brian@troutwine.us&gt;"]

[dependencies]
rand = "0.4"
rand_derive = "0.3"
libc = "0.2.0"
byteorder = "1.0"
num_cpus = "1.0"

[build-dependencies]
cc = "1.0"

[dev-dependencies]
quickcheck = "0.6"
criterion = "0.2"

[[bench]]
name = "mars_bench"
harness = false

[[bin]]
name = "feruscore"

[[bin]]
name = "fuzz_target"</pre>
<p>As mentioned at the start of the chapter, there were two issues with feruscore aside from calling out to an OS process: repeat allocation of similar structures and poor control over tournaments. Viewed a certain way, comparing <kbd>Individual</kbd> fitness is a sorting function. Rayon does have a parallel sorting capability, <kbd>ParallelSliceMut&lt;T: Send&gt;::par_sort(&amp;mut self)</kbd>, where <kbd>T: Ord</kbd>. We could make use of that, defining an <kbd>Ord</kbd> for <kbd>Individual</kbd> that allocated a new <kbd>Mars</kbd> for every comparison. Many tiny allocations is a killer for speed, though. A thread-local <kbd>Mars</kbd> could reduce that to a single allocation per thread, but then we're still giving up some control here. For instance, without inspecting rayon's source, can we be sure that population chunks are going to be of roughly equal size? Usually, this is not a concern, but it is for us here. Rayon's requiring that we perform a fold and then a reduce step is also extra work we don't necessarily have to do if we adjust our ambitions some.</p>
<p>One common method to deal with parallelizing genetic algorithms, and the one we'll take now, is to make islands that undergo evolution in parallel. The user sets a global population, where  this population is split among islands, and threads are assigned an island to simulate for some number of generations. After that limit of generations is up, the island populations are merged, shuffled, and redistributed to islands. This has the benefit of reducing cross-thread communication, which potentially comes with cache locality issues.</p>
<p>The preamble of <kbd>src/bin/feruscore.rs</kbd> is straightforward:</p>
<pre>extern crate feruscore;
extern crate num_cpus;
extern crate rand;

use feruscore::individual::*;
use feruscore::mars::*;
use rand::{thread_rng, Rng};
use std::fs::{self, DirBuilder, File};
use std::io::{self, BufWriter};
use std::path::{Path, PathBuf};
use std::sync::atomic::{AtomicUsize, Ordering};
use std::sync::mpsc;
use std::{thread, time};</pre>
<p>The configuration and reporting globals are somewhat reduced, compared to the last chapter:</p>
<pre>// configuration
const POPULATION_SIZE: usize = 1_048_576 / 8; 
const CHROMOSOME_SIZE: u16 = 100;
const CORE_SIZE: u16 = 8000;
const GENE_MUTATION_CHANCE: u32 = 100;
const ROUNDS: u16 = 100;

// reporting
static GENERATIONS: AtomicUsize = AtomicUsize::new(0);</pre>
<p>The <kbd>report</kbd> function is almost entirely the same as in last chapter, except there are a few less atomic variables to read from. checkpoint is also almost entirely the same. We'll skip reprinting both functions in the interest of space. What is new is <kbd>sort_by_tournament</kbd>:</p>
<pre>fn sort_by_tournament(mars: &amp;mut Mars, <br/>                      mut population: Vec&lt;Individual&gt;) <br/>    -&gt; Vec&lt;Individual&gt; <br/>{
    let mut i = population.len() - 1;
    while i &gt;= (population.len() / 2) {
        let left_idx = thread_rng().gen_range(0, i);
        let mut right_idx = left_idx;
        while left_idx == right_idx {
            right_idx = thread_rng().gen_range(0, i);
        }
        match mars.compete(ROUNDS, <br/>                           &amp;population[left_idx], <br/>                           &amp;population[right_idx]) <br/>        {
            Winner::Right(_) =&gt; {
                population.swap(i, right_idx);
            }
            Winner::Left(_) =&gt; {
                population.swap(i, left_idx);
            }
            Winner::Tie =&gt; {}
        }
        i -= 1;
    }
    population
}</pre>
<p>This function sorts a population by fitness, based on the result of competition inside the passed <kbd>Mars</kbd>. The reader will note that it's not a true sort in that the last element is the most fit, but that the last element is the winner of the first tournament, the second to last the winner of the second tournament, and so forth. Only <kbd>population.len() / 2</kbd> competitions are held and the resulting champions compare favorably to a population precisely sorted by fitness. The reader is encouraged to experiment with their own implementation of <kbd>sort_by_tournament</kbd>. Now, let's look at the <kbd>main</kbd> function:</p>
<pre>fn main() {
    let mut out_recvs = Vec::with_capacity(num_cpus::get());
    let mut in_sends = Vec::with_capacity(num_cpus::get());

    let total_children = 128;

    let thr_portion = POPULATION_SIZE / num_cpus::get();
    for _ in 0..num_cpus::get() {
        let (in_snd, in_rcv) = mpsc::sync_channel(1);
        let (out_snd, out_rcv) = mpsc::sync_channel(1);
        let mut population: Vec&lt;Individual&gt; = (0..thr_portion)
            .into_iter()
            .map(|_| Individual::new(CHROMOSOME_SIZE))
            .collect();
        population.pop();
        population.pop();
        population.push(ringers::imp(CORE_SIZE));
        population.push(ringers::dwarf(CORE_SIZE));
        let _ = thread::spawn(move || island(in_rcv, <br/>                                             out_snd, <br/>                                             total_children));
        in_snd.send(population).unwrap();
        in_sends.push(in_snd);
        out_recvs.push(out_rcv);
    }</pre>
<p>The total number of islands in a feruscore run will vary by the number of CPUs available on-system. Each island causes two synchronous MPSC channels to be created, one for the main thread to push populations into the worker thread and one for the worker thread to push populations back to the main. The implementation refers to these as <kbd>in_*</kbd> and <kbd>out_*</kbd> senders and receivers. You can see again that we're building up a population of random <kbd>Individual</kbd> warriors and pushing the ringers in, though the island population is not <kbd>POPULATION_SIZE</kbd>, but an even split of <kbd>POPULATION_SIZE</kbd> by the number of available CPUs. The reporting thread is started after the island threads have their populations, mostly just to avoid UI spam:</p>
<pre>    let _ = thread::spawn(report);</pre>
<p>The report function is much the same as it was in the previous chapter's discussion of feruscore and I'll avoid listing it here for brievity's sake.</p>
<p>The final chunk of the <kbd>main</kbd> function is the recombining loop. When the island threads finish their competitions, they write into their out sender, which gets picked up by the recombination loop:</p>
<pre>    let mut mars = MarsBuilder::default().core_size(CORE_SIZE).freeze();
    let mut global_population: Vec&lt;Individual&gt; = <br/>        Vec::with_capacity(POPULATION_SIZE);
    loop {
        for out_rcv in &amp;mut out_recvs {
            let mut pop = out_rcv.recv().unwrap();
            global_population.append(&amp;mut pop);
        }</pre>
<p>Once all the islands have been merged together, the whole lot is shuffled:</p>
<pre>        assert_eq!(global_population.len(), POPULATION_SIZE);
        thread_rng().shuffle(&amp;mut global_population);</pre>
<p>That's one generation done. We <kbd>checkpoint</kbd>, this time doing an additional tournament to pull the save winners from the global population:</p>
<pre>        let generation = GENERATIONS.fetch_add(1, Ordering::Relaxed);
        if generation % 100 == 0 {
            global_population = sort_by_tournament(&amp;mut mars, <br/>                                                   global_population);
            checkpoint(generation, &amp;global_population)<br/>                .expect("could not checkpoint");
        }</pre>
<p>Finally, the population is split back up and sent to the island threads:</p>
<pre>        let split_idx = global_population.len() / num_cpus::get();

        for in_snd in &amp;mut in_sends {
            let idx = global_population.len() - split_idx;
            let pop = global_population.split_off(idx);
            in_snd.send(pop).unwrap();
        }
    }
}</pre>
<p>Now, what is island doing? Well, it's an infinite loop pulling from the population assignment receiver:</p>
<pre>fn island(
    recv: mpsc::Receiver&lt;Vec&lt;Individual&gt;&gt;,
    snd: mpsc::SyncSender&lt;Vec&lt;Individual&gt;&gt;,
    total_children: usize,
) -&gt; () {
    let mut mars = MarsBuilder::default().core_size(CORE_SIZE).freeze();

    while let Ok(mut population) = recv.recv() {</pre>
<p>Now a <kbd>Mars</kbd> is allocated above the loop, meaning we'll only ever do <kbd>num_cpu::get()</kbd> allocations of this structure per feruscore run. Also recall that <kbd>Receiver::recv</kbd> blocks when there is no data in the channel, so island threads don't burn up CPU when there's no work for them. The interior of the loop should be familiar. First, the <kbd>Individual</kbd> warriors are put into competition:</p>
<pre>        // tournament, fitness and selection of parents
        population = sort_by_tournament(&amp;mut mars, population);</pre>
<p>Reproduction is done by taking high-ranking members of the population and producing two children into the low reaches of the population until the total number of children needed per generation is reached:</p>
<pre>        // reproduce
        let mut child_idx = 0;
        let mut parent_idx = population.len() - 1;
        while child_idx &lt; total_children {
            let left_idx = parent_idx;
            let right_idx = parent_idx - 1;
            parent_idx -= 2;

            population[child_idx] = population[left_idx]<br/>                                    .reproduce(&amp;population[right_idx]);
            child_idx += 1;
            population[child_idx] = population[left_idx]<br/>                                    .reproduce(&amp;population[right_idx]);
            child_idx += 1;
        }</pre>
<p>New to this implementation, we also introduce random new population members just before the newly introduced children:</p>
<pre>        for i in 0..32 {
            population[child_idx + i] = Individual::new(CHROMOSOME_SIZE)
        }</pre>
<p>Random infusion can help tamp down premature convergence in a population. Remember, simulated evolution is a kind of state-space search. Something else that has changed is that all members of the population have a chance of mutation:</p>
<pre>        // mutation
        for indv in population.iter_mut() {
            indv.mutate(GENE_MUTATION_CHANCE);
        }</pre>
<p>Finally, we push the population back up to the recombination thread, having worked the population over thoroughly:</p>
<pre>        snd.send(population).expect("could not send");
    }
}</pre>
<p>With all the changes made here—cutting back on small allocations, reducing total number of competitions per generation, removing pmars parsing plus spawn overhead—the implementation is substantially faster. Recall that the implementation in the last chapter struggled to peak 500 competitions—or <kbd>BATTLES</kbd> as the UI has it—per second. Here's a report diagnostic from a recent eight-hour run I did:</p>
<pre><strong>GENERATION(5459):
    RUNTIME (sec):  31248
    BATTLES:        41181
       BATTLES/s:   14340
    FITNESS:
        00...10:    151
        11...20:    2
        21...30:    0
        31...40:    1
        41...50:    0
        51...60:    0
        61...60:    2
        71...70:    1
        81...80:    0
        91...100:   41024</strong></pre>
<p>That's 14,000 battles per second on an 8,000-sized core memory, or around 650 generations per hour. Still not blazingly fast, but enough that with some time you can start to get pretty mediocre players being produced. Reducing core memory size will improve the runtime, as will limiting the maximum length of the warriors. Those who are interested in building a better evolver would do well to investigate different ways of assessing fitness, of introducing more ringers, and seeing whether porting <kbd>sim_mw</kbd> into Rust wouldn't improve runtime some. This simulator doesn't support the full scope of instructions that pMARS does, so that's also an area for improvement.</p>
<p>I'd love to hear about what you come up with.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Embedding Lua into Rust</h1>
                </header>
            
            <article>
                
<p>Many of the programs we've discussed in this book use a <em>report</em> thread to notify the user of the running behavior of the program. <span><span>E</span></span>ach of these report functions have been coded in Rust, an unchanging part of the executable. What if, however, we wanted end-users to be able to supply their own reporting routine? Or, consider the cernan project (<a href="https://crates.io/crates/cernan">https://crates.io/crates/cernan</a>), discussed previously in this book, which supports a <em>programmable filter</em>, an online data-stream filter that can be programmed by end-users without changing the cernan binary. How do you pull such a trick off?</p>
<p>A common answer, not just in Rust but in many compiled languages, is to embed a Lua interpreter (<a href="https://www.lua.org/">https://www.lua.org/</a>) and read user programs in at startup. It's such a common answer, in fact, that there are many Lua embeddings to choose from in the crates ecosystem. We'll use rlua (<a href="https://crates.io/crates/rlua">https://crates.io/crates/rlua</a>) here as it's a safe choice and the project documentation is very good. Other Lua embeddings suit different ambitions. Cernan uses a different, not necessarily safe embedding, for example, because we need to allow end-users to define their own functions.</p>
<p>In the previous chapter, we wrote a project called <kbd>sniffer</kbd> whose purpose was threefold—collect Ethernet packets on an interface, report about them, and echo the Ethernet packet back. Let's take that program and adapt it so that users can decide how to report with custom scripts. The <kbd>Cargo.toml</kbd> file of the project is a little different, including the rlua dependency and dropping the thread-hungry alternative executable:</p>
<pre>[package]
name = "sniffer"
version = "0.2.0"
authors = ["Brian L. Troutwine &lt;brian@troutwine.us&gt;"]

[dependencies]
pnet = "0.21"
rlua = "0.13"

[[bin]]
name = "sniffer"</pre>
<p>The preamble to the sniffer program, defined in <kbd>src/bin/sniffer.rs</kbd>, holds no surprises for us:</p>
<pre style="padding-left: 30px">extern crate pnet;
extern crate rlua;

use pnet::datalink::Channel::Ethernet;
use pnet::datalink::{self, DataLinkReceiver, MacAddr, <br/>                     NetworkInterface};
use pnet::packet::ethernet::{EtherType, EthernetPacket};
use rlua::{prelude, Function, Lua};
use std::fs::File;
use std::io::{prelude::*, BufReader};
use std::path::Path;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::sync::mpsc;
use std::{env, thread};</pre>
<p>We're importing <kbd>Function</kbd> and <kbd>Lua</kbd> from the rlua crate, but that's about all that is new. The <kbd>SKIPPED_PACKETS</kbd> and <kbd>Payload</kbd> details are unchanged:</p>
<pre>static SKIPPED_PACKETS: AtomicUsize = AtomicUsize::new(0);

#[derive(Debug)]
enum Payload {
    Packet {
        source: MacAddr,
        destination: MacAddr,
        kind: EtherType,
    },
    Pulse(u64),
}</pre>
<p>I have removed the echoing of Ethernet packets in this example as it's not necessarily a kind thing to do on a busy Ethernet network. <kbd>watch_interface</kbd>, as a result, is a little more svelte than it used to be:</p>
<pre>fn watch_interface(mut rx: Box&lt;DataLinkReceiver&gt;, snd: mpsc::SyncSender&lt;Payload&gt;) {
    loop {
        match rx.next() {
            Ok(packet) =&gt; {
                let packet = EthernetPacket::new(packet).unwrap();

                let payload: Payload = Payload::Packet {
                    source: packet.get_source(),
                    destination: packet.get_destination(),
                    kind: packet.get_ethertype(),
                };
                if snd.try_send(payload).is_err() {
                    SKIPPED_PACKETS.fetch_add(1, Ordering::Relaxed);
                }
            }
            Err(e) =&gt; {
                panic!("An error occurred while reading: {}", e);
            }
        }
    }
}</pre>
<p>The <kbd>timer</kbd> function is also unchanged:</p>
<pre>fn timer(snd: mpsc::SyncSender&lt;Payload&gt;) -&gt; () {
    use std::{thread, time};
    let one_second = time::Duration::from_millis(1000);

    let mut pulses = 0;
    loop {
        thread::sleep(one_second);
        snd.send(Payload::Pulse(pulses)).unwrap();
        pulses += 1;
    }
}</pre>
<p>The <kbd>main</kbd> function now picks up an additional argument, intended to be the on-disk path of a Lua function:</p>
<pre>fn main() {
    let interface_name = env::args().nth(1).unwrap();
    let pulse_fn_file_arg = env::args().nth(2).unwrap();
    let pulse_fn_file = Path::new(&amp;pulse_fn_file_arg);
    assert!(pulse_fn_file.exists());
    let interface_names_match = |iface: &amp;NetworkInterface| {<br/>        iface.name == interface_name<br/>    };

    // Find the network interface with the provided name
    let interfaces = datalink::interfaces();
    let interface = interfaces
        .into_iter()
        .filter(interface_names_match)
        .next()
        .unwrap();</pre>
<p>The function is intended to handle the <kbd>Payload::Pulse</kbd> that comes in from the timer thread. The gather function from the previous chapter no longer exists. To actually do something with the user-supplied function, we'll have to create a new Lua VM via <kbd>rlua::Lua::new()</kbd> and then load the function into it, like so:</p>
<pre>    let lua = Lua::new();
    let pulse_fn = eval(&amp;lua, pulse_fn_file, Some("pulse")).unwrap();</pre>
<p>The <kbd>eval</kbd> function is brief and mostly to do with reading from disk:</p>
<pre>fn eval&lt;'a&gt;(lua: &amp;'a Lua, path: &amp;Path, name: Option&lt;&amp;str&gt;) <br/>    -&gt; prelude::LuaResult&lt;Function&lt;'a&gt;&gt; <br/>{
    let f = File::open(path).unwrap();
    let mut reader = BufReader::new(f);

    let mut buf = Vec::new();
    reader.read_to_end(&amp;mut buf).unwrap();
    let s = ::std::str::from_utf8(&amp;buf).unwrap();

    lua.eval(s, name)
}</pre>
<p>The lua-specific bit there is <kbd>lua.eval(s, name)</kbd>. This evaluates the Lua code read from disk and returns function, a Rust-callable bit of Lua. Any user-supplied name in the Lua source itself is ignored and the name exists to add context to error messages on the Rust side of things. rlua does not expose loadfile in its Rust API, though other Lua embeddings do.</p>
<p>The next bit of the <kbd>main</kbd> function is mostly unchanged, though the packet buffer channel has been increased from 10 elements to <kbd>100</kbd>:</p>
<pre>    let (snd, rcv) = mpsc::sync_channel(100);

    let timer_snd = snd.clone();
    let _ = thread::spawn(move || timer(timer_snd));

    let _iface_handler = match datalink::channel(&amp;interface, <br/>                                                 Default::default()) <br/>    {
        Ok(Ethernet(_tx, rx)) =&gt; {
            let snd = snd.clone();
            thread::spawn(|| watch_interface(rx, snd))
        }
        Ok(_) =&gt; panic!("Unhandled channel type"),
        Err(e) =&gt; panic!(
            "An error occurred when creating the datalink channel: {}",
            e
        ),
    };</pre>
<p>Now, things are going to change. First, we create three Lua tables for storing the components of the <kbd>Payload::Packet</kbd>:</p>
<pre>    let destinations = lua.create_table().unwrap();
    let sources = lua.create_table().unwrap();
    let kinds = lua.create_table().unwrap();</pre>
<p>In the previous chapter, we used three <kbd>HashMap</kbd>s but, now, we need something we can easily pass into Lua. The main thread is responsible for the role that gather used to play: collecting <kbd>Payload</kbd> instances. This saves a thread and means we don't have to carefully arrange for sending the Lua VM across thread boundaries:</p>
<pre>    while let Ok(payload) = rcv.recv() {
        match payload {
            Payload::Packet {
                source,
                destination,
                kind,
            } =&gt; {
                let d_cnt = destinations<br/>                                .get(destination.to_string())<br/>                                .unwrap_or(0);
                destinations
                    .set(destination.to_string(), d_cnt + 1)
                    .unwrap();

                let s_cnt = sources.get(source.to_string()).unwrap_or(0);
                sources.set(source.to_string(), s_cnt + 1).unwrap();

                let k_cnt = kinds.get(kind.to_string()).unwrap_or(0);
                kinds.set(kind.to_string(), k_cnt + 1).unwrap();
            }</pre>
<p>Here, we've started pulling payloads from the receiver and have handled <kbd>Payload::Packets</kbd> coming across. Notice how the tables we created just before this loop are now being populated. The same basic aggregation is in play; keep a running tally of the <kbd>Packet</kbd> components coming in. A more adventurous reader might extend this program to allow for the Lua side to build its own aggregation. Now all that remains is to handle <kbd>Payload::Pulse</kbd>:</p>
<pre>            Payload::Pulse(id) =&gt; {
                let skipped_packets = SKIPPED_PACKETS<br/>                                      .swap(0, Ordering::Relaxed);
                pulse_fn
                    .call::&lt;_, ()&gt;((
                        id,
                        skipped_packets,
                        destinations.clone(),
                        sources.clone(),
                        kinds.clone(),
                    ))
                    .unwrap()
            }
        }
    }
}</pre>
<p>The <kbd>pulse_fn</kbd> created earlier is called with a four-tuple of arguments—the pulse ID, the <kbd>SKIPPED_PACKETS</kbd>, and the aggregation tables. We don't expect <kbd>pulse_fn</kbd> to have any kind of return value, hence the <kbd>&lt;_, ()&gt;</kbd> bit. This version of sniffer includes an example packet function, defined in <kbd>examples/pulse.lua</kbd>:</p>
<pre style="padding-left: 30px">function (id, skipped_packets, dest_tbl, src_tbl, kind_tbl)
   print("ID: ", id)
   print("SKIPPED PACKETS: ", skipped_packets)
   print("DESTINATIONS:")
   for k,v in pairs(dest_tbl) do
      print(k, v)
   end
   print("SOURCES:")
   for k,v in pairs(src_tbl) do
      print(k, v)
   end
   print("KINDS:")
   for k,v in pairs(kind_tbl) do
      print(k, v)
   end
end</pre>
<p>The reader will note that it does basically the same work as the former gather function once did. Running sniffer works the same way as before. Be sure to <kbd>cargo build --release</kbd> and then:</p>
<pre><strong>&gt; ./target/release/sniffer en0 examples/pulse.lua
ID:     0
SKIPPED PACKETS:    0
DESTINATIONS:
5c:f9:38:8b:4a:b6   11
8c:59:73:1d:c8:73   16
SOURCES:
5c:f9:38:8b:4a:b6   16
8c:59:73:1d:c8:73   11
KINDS:
Ipv4    27
ID:     1
SKIPPED PACKETS:    0
DESTINATIONS:
5c:f9:38:8b:4a:b6   14
8c:59:73:1d:c8:73   20
SOURCES:
5c:f9:38:8b:4a:b6   20
8c:59:73:1d:c8:73   14
KINDS:
Ipv4    34</strong></pre>
<p>The allocation-conscious reader will have noticed there's a lot of cloning going on here. That's true. The primary compilation is interop with Lua's GC. A safe Rust interface must assume that data passed down to Lua will be garbage-collected. But, Lua does support a concept of user data, in which the programmer associates opaque blobs with Lua functions to manipulate it. Lua also supports light user data, which is very similar to user data except that the light variant is associated with a pointer to memory. The UserData type in RLua is quite well done and the ambitious reader might do well to build a <kbd>PacketAggregation</kbd> type that implements <kbd>UserData</kbd> to avoid all the cloning.</p>
<p>Combining a high-level language into a systems language is often a game of trade-offs between memory management complexity, end-user burden, and initial programming difficulty. rlua does an excellent job of landing on the safer side of these trade-offs. Something like mond, in use in cernan, less so, but with more flexibility in use.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Embedding Rust</h1>
                </header>
            
            <article>
                
<p>So far, we've seen how to embed C and Lua into Rust. But, what if you want to combine Rust into other programming languages? Doing so is a very handy technique for improving runtime performance in interpreted languages, making memory-safe extensions where once you might have been using C or C++. If your target high-level language has difficulty with concurrency embedding, Rust is a further win. Python programs suffer in this regard—at least those implemented on CPython or PyPy—because of the Global Interpreter Lock, an internal mutex that locks objects' bytecode. Offloading computation of large blocks of data into a Rust + Rayon extension, for example, can be both straightforward to program and improve computation speed.</p>
<p>Well, great. How do we make this sort of thing happen? Rust's approach is simple: if you can embed C, you can embed Rust.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Into C</h1>
                </header>
            
            <article>
                
<p>Let's embed some Rust into C. The quantiles library (<a href="https://crates.io/crates/quantiles">https://crates.io/crates/quantiles</a>)—discussed in <a href="5a332d94-37e4-4748-8920-1679b07e2880.xhtml" target="_blank">Chapter 4</a>, <em>Sync and Send – the Foundation of Rust Concurrency</em>—implements online summarization algorithms. These algorithms are easy to get wrong, in terms of producing incorrect results, but more often in storing more points than strictly necessary. A good deal of work has gone into quantiles to ensure the algorithms implemented there are near the theoretical minimum storage requirements, and so it makes sense to reuse this library for online summarization in C, rather than redo all that work.</p>
<p>Specifically, let's expose a <kbd>quantiles::ckms::CKMS&lt;f32&gt;</kbd> to C (<a href="https://docs.rs/quantiles/0.7.1/quantiles/ckms/struct.CKMS.html">https://docs.rs/quantiles/0.7.1/quantiles/ckms/struct.CKMS.html</a>). We have to make the type concrete as C lacks any manner of generics in its types, but that's okay.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The Rust side</h1>
                </header>
            
            <article>
                
<p>There are new things in the <kbd>Cargo.toml</kbd> file that we need to discuss:</p>
<pre>[package]
name = "embed_quantiles"
version = "0.1.0"
authors = ["Brian L. Troutwine &lt;brian@troutwine.us&gt;"]

[dependencies]
quantiles = "0.7"

[lib]
name = "embed_quantiles"
crate-type = ["staticlib"]</pre>
<p>Note specifically that we're building the embed_quantiles library with <kbd>crate-type = ["staticlib"]</kbd>. What does that mean? The Rust compiler is capable of making many kinds of linkages, and usually they're just implicit. For instance, a binary is <kbd>crate-type = ["bin"]</kbd>. There's a longish list of different link types, which I've included in the <em>Further reading</em> section. The interested reader is encouraged to look through them. What we'd like to produce here is a statically linked library, otherwise every C programmer that tries to make use of embed_quantiles is going to need Rust's shared libraries installed on their system. Not… not great. A staticlib crate will archive all the bits of the Rust standard library needed by the Rust code. The archive can then be linked to C as normal.</p>
<p>Okay, now, if we're going to produce a static archive that C can call, we've got to export Rust functions with a C ABI. Put another way, we've got to put a C skin on top of Rust. C++ programmers will be familiar with this tactic. The sole Rust file in this project is <kbd>src/lib.rs</kbd> and its preamble is what you might expect:</p>
<pre style="padding-left: 30px">extern crate quantiles;

use quantiles::ckms::CKMS;</pre>
<p>We've pulled in the quantiles library and have imported <kbd>CKMS</kbd>. No big deal. Check this out, though:</p>
<pre style="padding-left: 30px">#[no_mangle]
pub extern "C" fn alloc_ckms(error: f64) -&gt; *mut CKMS&lt;f32&gt; {
    let ckms = Box::new(CKMS::new(error));
    Box::into_raw(ckms)
}</pre>
<p>Hey! There's a bunch of new things. First, <kbd>#[no_mangle]</kbd>? A static library has to export a symbol for the linker to, well, link. These symbols are functions, static variables, and so forth. Now, usually Rust is free to fiddle with the names that go into a symbol to include information, such as module location or, really, whatever else the compiler wants to do. The exact semantics of mangling are undefined, as of this writing. If we're going to be calling a function from C, we have to have the exact symbol name to refer to. <kbd>no_mangle</kbd> turns off mangling, leaving us with our name as written. It does mean we have to be careful not to cause symbol collisions. Similar to importing functions, the <kbd>extern C</kbd> here means that this function should be written out to obey the C ABI. Technically, we could also have written this as <kbd>extern fn</kbd>, leaving the C off as the C ABI is the implicit default.</p>
<p><kbd>alloc_ckms</kbd> allocates a new <kbd>CKMS</kbd>, returning a mutable pointer to it. Interop with C requires raw pointers, which, you know, makes sense. We do have to be very conscious of memory ownership when embedding Rust—does Rust own the memory, implying we need to provide a free function? Or, does the other language own the memory? More often than not, it's easier to keep ownership with Rust, because to free memory, the compiler will need to know the type's size in memory. By passing a pointer out, as we're doing here, we've kept C in the dark about the size of <kbd>CKMS</kbd>. All the C side of this project knows is that it has an <em>opaque struct</em> to deal with. This is a common tactic in C libraries, for good reason. Here's freeing a <kbd>CKMS</kbd>:</p>
<pre style="padding-left: 30px">#[no_mangle]
pub extern "C" fn free_ckms(ckms: *mut CKMS&lt;f32&gt;) -&gt; () {
    unsafe {
        let ckms = Box::from_raw(ckms);
        drop(ckms);
    }
}</pre>
<p>Notice that in <kbd>alloc_ckms</kbd>, we're boxing the <kbd>CKMS</kbd>—forcing it to the heap—and in <kbd>free_ckms</kbd> we're building a boxed <kbd>CKMS</kbd> from its pointer. We discussed boxing and freeing memory in the context of raw pointers extensively in <a href="605ce307-29ed-4b5a-961e-8d327467b84f.xhtml" target="_blank">Chapter 3</a>, <em>The Rust Memory Model – Ownership, References and Manipulation</em>. Inserting a value into the <kbd>CKMS</kbd> is straightforward enough:</p>
<pre style="padding-left: 30px">#[no_mangle]
pub extern "C" fn ckms_insert(ckms: &amp;mut CKMS&lt;f32&gt;, value: f32) -&gt; () {
    ckms.insert(value)
}</pre>
<p>Querying requires a little explanation:</p>
<pre style="padding-left: 30px">#[no_mangle]
pub extern "C" fn query(ckms: &amp;mut CKMS&lt;f32&gt;, q: f64, <br/>                        quant: *mut f32) <br/>    -&gt; i8 <br/>{
    unsafe {
        if let Some((_, res)) = ckms.query(q) {
            *quant = res;
            0
        } else {
            -1
        }
    }
}</pre>
<p>Signaling error conditions in a C API is tricky. In Rust, we return some kind of compound type, such as an <kbd>Option</kbd>. There's no such thing in C without building an error signaling struct for your API. In addition to the error-struct approach it's common to either write well-known nonsense into the pointer where the answer will be written or return a negative value. C expects to have its answer written into a 32-bit float being pointed to by quant, and there's no easy way to write nonsense into a numeric value. So, query returns an <kbd>i8</kbd>; zero on success, negative on a failure. A more elaborate API would differentiate failures by returning different negative values.</p>
<p>That's it! When you run <kbd>cargo build --release</kbd>, a static library obeying the C ABI will get kicked out into <kbd>target/release</kbd>. We're ready to link it into a C program.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The C side</h1>
                </header>
            
            <article>
                
<p>Our C program is <kbd>c_src/main.c</kbd>. We need a few system headers before we can define the <kbd>embed_quantiles</kbd> interface:</p>
<pre>#include &lt;stdint.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;time.h&gt;</pre>
<p>The only slightly unusual one is <kbd>time.h</kbd>. We pull that in because we're going to be pushing random floats into the <kbd>CKMS</kbd> structure. Not cryptographically secure randomness, mind. Here's the C view of the Rust API we just created:</p>
<pre style="padding-left: 30px">struct ckms;

struct ckms* alloc_ckms(double error);
void free_ckms(struct ckms* ckms);
void ckms_insert(struct ckms* ckms, float value);
int8_t query(struct ckms* ckms, double q, float* quant);</pre>
<p>Notice the <kbd>CKMS</kbd> struct . That's the opaque struct. C is now aware there is a structure but it doesn't know how big it is. That's okay. Each of our functions only operate on a pointer to this structure, which C does know the size of. The <kbd>main</kbd> function is brief:</p>
<pre style="padding-left: 30px">int main(void) {
  srand(time(NULL));

  struct ckms* ckms = alloc_ckms(0.001);

  for (int i=1; i &lt;= 1000000; i++) {
    ckms_insert(ckms, (float)rand()/(float)(RAND_MAX/10000));
  }

  float quant = 0.0;
  if (query(ckms, 0.75, &amp;quant) &lt; 0) {
    printf("ERROR IN QUERY");
    return 1;
  }
  printf("75th percentile: %f\n", quant);

  free_ckms(ckms);
  return 0;
}</pre>
<p>We allocate a <kbd>CKMS</kbd>, whose error bound is 0.001, load 1 million random floats into the <kbd>CKMS</kbd>, and then query for the 75th quantile, which should be around 7,500. Finally, the function frees the <kbd>CKMS</kbd> and exits. Short and sweet.</p>
<p>Now, building and linking <kbd>libembed_quantiles.a</kbd> is fairly easy with a clang compiler, and a little fiddly with GCC. I've gone ahead and included a Makefile, which has been tested on OS X and Linux:</p>
<pre>CC ?= $(shell which cc)
CARGO ?= $(shell which cargo)
OS := $(shell uname)

run: clean build
        ./target/release/embed_quantiles

clean:
        $(CARGO) clean
        rm -f ./target/release/embed_quantiles

build:
        $(CARGO) build --release
ifeq ($(OS),Darwin) # Assumption: CC == Clang
        $(CC) -std=c11 -Wall -Werror -Wpedantic -O3 c_src/main.c \
                -L target/release/ -l embed_quantiles \
                -o target/release/embed_quantiles
else # Assumption: CC == GCC
        $(CC) -std=c11 -Wall -Werror -Wpedantic -O3 c_src/main.c \
                -L target/release/ -lpthread -Wl,–no-as-needed -ldl \<br/>                -lm -lembed_quantiles \
                -o target/release/embed_quantiles
endif

.PHONY: clean run</pre>
<p>I don't have a Windows machine to test with so, uh, when this inevitably doesn't work I really do apologize. Hopefully there's enough here to figure it out quickly. Once you have the Makefile in place, you should be able to run make run and see something similar to the following:</p>
<pre><strong>&gt; make run
/home/blt/.cargo/bin/cargo clean
rm -f ./target/release/embed_quantiles
/home/blt/.cargo/bin/cargo build --release
   Compiling quantiles v0.7.1
   Compiling embed_quantiles v0.1.0
    Finished release [optimized] target(s) in 0.91 secs
cc -std=c11 -Wall -Werror -Wpedantic -O3 c_src/main.c \
        -L target/release/ -lpthread -Wl,–no-as-needed -ldl \<br/>        -lm -lembed_quantiles \
        -o target/release/embed_quantiles
./target/release/embed_quantiles
75th percentile: 7499.636230</strong></pre>
<p>The percentile value will move around some, but it ought to be close to 7,500, as it is here.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Into Python</h1>
                </header>
            
            <article>
                
<p>Let's look at embedding Rust into Python. We'll write a function to sum up the trailing zeros in a cytpes array, built by Python. We can't link statically into Python as the interpreter is already compiled and linked, so we'll need to create a dynamic library. The <kbd>Cargo.toml</kbd> project reflects this:</p>
<pre>[package]
name = "zero_count"
version = "0.1.0"
authors = ["Brian L. Troutwine &lt;brian@troutwine.us&gt;"]

[dependencies]

[lib]
name = "zero_count"
crate-type = ["dylib"]</pre>
<p>The sole Rust file, <kbd>src/lib.rs</kbd>, has a single function in it:</p>
<pre style="padding-left: 30px">#[no_mangle]
pub extern "C" fn tail_zero_count(arr: *const u16, len: usize) -&gt; u64 {
    let mut zeros: u64 = 0;
    unsafe {
        for i in 0..len {
            zeros += (*arr.offset(i as isize)).trailing_zeros() as u64
        }
    }
    zeros
}</pre>
<p>The <kbd>tail_zero_count</kbd> function takes a pointer to an array of <kbd>u16</kbd>s and the length of that array. It zips through the array, calling <kbd>trailing_zeros()</kbd> on each <kbd>u16</kbd> and adding the value to a global sum: <kbd>zeros</kbd>. This global sum is then returned. In the top of the project, run <kbd>cargo build --release</kbd> and you'll find the project's dynamic library—possibly <kbd>libzero_count.dylib</kbd>, <kbd>libzero_count.dll</kbd>, or <kbd>libzero_count.so</kbd>, depending on your host—in <kbd>target/release</kbd>. So far, so good.</p>
<p>Calling this function is now up to Python. Here's a small example, which lives at <kbd>zero_count.py</kbd> in the root of the project:</p>
<pre>import ctypes
import random

length = 1000000
lzc = ctypes.cdll.LoadLibrary("target/release/libzero_count.dylib")
arr = (ctypes.c_uint16 * length)(*[random.randint(0, 65000) for _ in range(0, length)])
print(lzc.tail_zero_count(ctypes.pointer(arr), length))</pre>
<p>We import the cytpes and random libraries, then load the shared library—here, pegged to OS X's naming convention—and bind it to <kbd>lzc</kbd>. Do edit this to read <kbd>[so|dll]</kbd> if you're running this on an operating system other than OS X. Once <kbd>lzc</kbd> is bound, we use the ctypes API to create a random array of <kbd>u16</kbd> values and call <kbd>tail_zero_count</kbd> on that array. Python is forced to allocate the full array before passing it into Rust using this approach, so don't increase the length too much. Running the program is a matter of calling Python's <kbd>zero_count.py</kbd>, like so:</p>
<pre><strong>&gt; python zero_count.py
1000457
&gt;  python zero_count.py
999401
&gt; python zero_count.py
1002295</strong></pre>
<p>Coping with opaque struct pointers—as we did in the C example—is well-documented in the ctypes documentation. Rust will not know the difference; it's just kicking out objects conforming to the C ABI.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Into Erlang/Elixir</h1>
                </header>
            
            <article>
                
<p>In this last section of the chapter, we'll investigate embedding Rust into the BEAM, the virtual machine that underpins Erlang and Elixir. The BEAM is a sophisticated work-stealing scheduler system that happens to be programmable. Erlang processes are small C structs that bounce around between scheduler threads and carry enough internal information to allow interpretation for a fixed number of instructions. There's no concept of shared memory in Erlang/Elixir: communication between different concurrent actors in the system <em>must</em> happen via message passing. There are many benefits to this and the VM does a great deal of work to avoid copying when possible. Erlang/Elixir processes receive messages into a message queue, a double-ended threadsafe queue of the kind we've discussed throughout the book.</p>
<p>Erlang and Elixir are rightly known for their efficiency at handling high-scale IO in a real-time fashion. Erlang was invented at Ericsson to serve as telephony control software, after all. What these languages are not known for is serial performance. Sequential computations in Erlang are relatively slow, it's just that it's so <em>easy</em> to get concurrent computations going that this is sort of made up for. Sort of. Sometimes, you need raw, serial performance.</p>
<p>Erlang has a few answers for this. A <em>Port</em> opens up an external OS process with a bidirectional byte stream. We saw an approach much like this in the previous chapter with feruscore embedding pmars. A <em>Port Driver</em> is a linked shared object, often written in C. The Port Driver interface has facilities for giving interrupt hints to BEAM's schedulers and has much to recommend it. A Port Driver <em>is</em> a process and must be able to handle the things an Erlang process normally does: servicing its message queue, dealing with special interrupt signals, cooperatively scheduling itself, and so on. These are non-trivial to write. Finally, Erlang supports a Natively Implemented Function (NIF) concept. These are simpler than Port Drivers and are synchronous, being simply callable functions that happen to be written in a language other than Erlang. NIFs are shared libraries and are often implemented in C. Both Port Drivers and NIFs have a serious downside: memory issues will corrupt the BEAM and knock your application offline. Erlang systems tend to be deployed where fault-tolerance is a major factor and segfaulting the VM is a big no-no.</p>
<p>As a result, there's a good deal of interest in the Erlang/Elixir communities towards Rust. The Rustler project (<a href="https://crates.io/crates/rustler">https://crates.io/crates/rustler</a>) aims to make combining Rust into an Elixir project as NIFs a simple matter. Let's take a look at a brief example project, presented by Sonny Scroggin at Code BEAM 2018 in San Francisco—beamcoin (<a href="https://github.com/blt/beamcoin">https://github.com/blt/beamcoin</a>). We'll discuss the project at SHA <kbd>3f510076990588c51e4feb1df990ce54ff921a06</kbd>.</p>
<div class="packt_infobox"><span>The beamcoin project is not listed in its entirety. We've mostly dropped the build configuration. You can find the full listing in this book's source repository.</span></div>
<p>The build system for Elixir—the BEAM language that Rustler targets natively—is called Mix. Its configuration file is <kbd>mix.exs</kbd> at the root of the project:</p>
<pre>defmodule Beamcoin.Mixfile do
  use Mix.Project

  def project do
    [app: :beamcoin,
     version: "0.1.0",
     elixir: "~&gt; 1.5",
     start_permanent: Mix.env == :prod,
     compilers: [:rustler] ++ Mix.compilers(),
     deps: deps(),
     rustler_crates: rustler_crates()]
  end

  def application do
    [extra_applications: [:logger]]
  end

  defp deps do
    [{:rustler, github: "hansihe/rustler", sparse: "rustler_mix"}]
  end

  defp rustler_crates do
    [beamcoin: [
      path: "native/beamcoin",
      mode: mode(Mix.env)
    ]]
  end

  defp mode(:prod), do: :release
  defp mode(_), do: :debug
end</pre>
<p>There's a fair bit here that we won't get into. Note, however, this section:</p>
<pre>  defp rustler_crates do
    [beamcoin: [
      path: "native/beamcoin",
      mode: mode(Mix.env)
    ]]
  end</pre>
<p>Embedded in the project under <kbd>native/beamcoin</kbd> is the Rust library we'll be exploring. Its cargo configuration, at <kbd>native/beamcoin/Cargo.toml</kbd>, is:</p>
<pre>[package]
name = "beamcoin"
version = "0.1.0"
authors = []

[lib]
name = "beamcoin"
path = "src/lib.rs"
crate-type = ["dylib"]

[dependencies]
rustler = { git = "https://github.com/hansihe/rustler", branch = "master" }
rustler_codegen = { git = "https://github.com/hansihe/rustler", branch = "master" }
lazy_static = "0.2"
num_cpus = "1.0"
scoped-pool = "1.0.0"
sha2 = "0.7"</pre>
<p>Nothing too surprising here. A dynamic library, called libbeamcoin, will be produced when Rust compiles this and we've seen almost all the dependencies before. rustler and <kbd>rustler_codegen</kbd> are the interface and compiler generators for Rustler, respectively. <kbd>rustler_codegen</kbd> removes a lot of boilerplate C extern work we might otherwise have to do. sha2 is a crate from the RustCrypto project that, well, implements the sha2 hashing algorithm. Beamcoin is kind of a joke project. The idea is to distribute numbers between threads and count the zeros at the back of the sha2-256 hash, mining those with a pre-set number of zeros. This would be a <em>very</em> slow thing to do in Erlang but, as we'll see, is a relatively fast computation in Rust. The scoped-pool crate is a threadpooling library that is Sendable, meaning it can be placed in a <kbd>lazy_static!</kbd></p>
<p>The preamble for <kbd>src/lib.rs</kbd> is straightforward enough:</p>
<pre>#[macro_use]
extern crate lazy_static;
extern crate num_cpus;
extern crate scoped_pool;
extern crate sha2;
#[macro_use]
extern crate rustler;

use rustler::{thread, Encoder, Env, Error, Term};
use scoped_pool::Pool;
use sha2::Digest;
use std::mem;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::{mpsc, Arc};</pre>
<p>We've seen much of this before. The Rustler imports are analogs of the Erlang C NIF API. The difficulty of mining is controlled by a top-level usize, called <kbd>DIFFICULTY</kbd>:</p>
<pre>const DIFFICULTY: usize = 6;</pre>
<p>This value controls the total number of zeros that are required to be found at the back of a hash before it is declared minable. Here's our thread pool:</p>
<pre>lazy_static! {
    static ref POOL: Pool = Pool::new(num_cpus::get());
}</pre>
<p>I mentioned earlier that the BEAM maintains its own scheduler threads. This is true. The beamcoin NIF also maintains its own, separate pool of threads to distribute mining over. Now, Rustler reduces boilerplate but cannot totally remove it. We must, for instance, tell BEAM which interpreted functions to associate with that symbol and pre-define <em>atoms</em> for use:</p>
<pre style="padding-left: 30px">rustler_atoms! {
    atom ok;
    atom error;
}

rustler_export_nifs! {
    "Elixir.Beamcoin",
    [("native_mine", 0, mine)],
    None
}</pre>
<p>An Erlang atom is a named constant. These are extremely common data types in Erlang/Elixir programs. The threads in the pool are each given a chunk of the positive integers to search, stripped according to their thread number. The pool workers use an MPSC channel to communicate back to the mining thread that a result has been found:</p>
<pre style="padding-left: 30px">#[derive(Debug)]
struct Solution(u64, String);

fn mine&lt;'a&gt;(caller: Env&lt;'a&gt;, _args: &amp;[Term&lt;'a&gt;]) <br/>    -&gt; Result&lt;Term&lt;'a&gt;, Error&gt; <br/>{
    thread::spawn::&lt;thread::ThreadSpawner, _&gt;(caller, move |env| {
        let is_solved = Arc::new(AtomicBool::new(false));
        let (sender, receiver) = mpsc::channel();

        for i in 0..num_cpus::get() {
            let sender_n = sender.clone();
            let is_solved = is_solved.clone();

            POOL.spawn(move || {
                search_for_solution(i as u64, num_cpus::get() as u64, <br/>                                    sender_n, is_solved);
            });
        }

        match receiver.recv() {
            Ok(Solution(i, hash)) =&gt; (atoms::ok(), i, hash).encode(env),
            Err(_) =&gt; (
                atoms::error(),
                "Worker threads disconnected before the \<br/>                 solution was found".to_owned(),
            ).encode(env),
        }
    });

    Ok(atoms::ok().encode(caller))
}</pre>
<p>The <kbd>search_for_solution</kbd> function is a small loop:</p>
<pre style="padding-left: 30px">fn search_for_solution(
    mut number: u64,
    step: u64,
    sender: mpsc::Sender&lt;Solution&gt;,
    is_solved: Arc&lt;AtomicBool&gt;,
) -&gt; () {
    let id = number;
    while !is_solved.load(Ordering::Relaxed) {
        if let Some(solution) = verify_number(number) {
            if let Ok(_) = sender.send(solution) {
                is_solved.store(true, Ordering::Relaxed);
                break;
            } else {
                println!("Worker {} has shut down without \<br/>                         finding a solution.", id);
            }
        }
        number += step;
    }
}</pre>
<p>At the top of the loop, every thread in the pool checks to see whether any other thread has mined a beamcoin. If one has, the function exits and the thread is available for new work in the pool. Otherwise, <kbd>verify_number</kbd> is called. That function is:</p>
<pre style="padding-left: 30px">fn verify_number(number: u64) -&gt; Option&lt;Solution&gt; {
    let number_bytes: [u8; 8] = unsafe { <br/>        mem::transmute::&lt;u64, [u8; 8]&gt;(number) <br/>    };
    let hash = sha2::Sha256::digest(&amp;number_bytes);

    let top_idx = hash.len() - 1;<br/>    // Hex chars are 16 bits, we have 8 bits. /2 is conversion.
    let trailing_zero_bytes = DIFFICULTY / 2;
<br/>    let mut jackpot = true;
    for i in 0..trailing_zero_bytes {
        jackpot &amp;= hash[top_idx - i] == 0;
    }

    if jackpot {
        Some(Solution(number, format!("{:X}", hash)))
    } else {
        None
    }
}</pre>
<p>The number is passed in, transmuted into a byte array of 8 members, and hashed. If the hash has the appropriate number of trailing zero bytes, jackpot is true at the end of the function, and the number plus its hash are returned. The careful reader will have noted that the Rust module exports a NIF named <kbd>native_mine</kbd>. Erlang NIFs, generally speaking, have a system language component with a BEAM-native implementation as a fallback. The system language NIF implementation is called <kbd>native_*</kbd> by tradition.</p>
<p>The final piece here is the Elixir module to wrap the native NIF bits. This module is called <kbd>Beamcoin</kbd> and is <kbd>lib/beamcoin.ex</kbd>:</p>
<pre style="padding-left: 30px">defmodule Beamcoin do
  use Rustler, otp_app: :beamcoin

  require Logger

  def mine do
    :ok = native_mine()

    start = System.system_time(:seconds)
    receive do
      {:ok, number, hash} -&gt;
        done = System.system_time(:seconds)
        total = done - start
        Logger.info("Solution found in #{total} seconds")
        Logger.info("The number is: #{number}")
        Logger.info("The hash is: #{hash}")
      {:error, reason} -&gt;
        Logger.error(inspect reason)
    end
  end

  def native_mine, do: :erlang.nif_error(:nif_not_loaded)
end</pre>
<p>After installing Elixir (<a href="https://elixir-lang.org/install.html">https://elixir-lang.org/install.html</a>), you can move to the root of the project, execute <kbd>mix deps.get</kbd> to get the project's dependencies, and then use <kbd>MIX_ENV=prod iex -S mix</kbd> to bring up the Elixir repl. That latter command should look something like this:</p>
<pre><strong>&gt; MIX_ENV=prod iex -S mix
Erlang/OTP 20 [erts-9.3] [source] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:10] [hipe] [kernel-poll:false] [dtrace]

Compiling NIF crate :beamcoin (native/beamcoin)...
    Finished release [optimized] target(s) in 0.70 secs
Interactive Elixir (1.6.4) - press Ctrl+C to exit (type h() ENTER for help)
iex(1)&gt;</strong></pre>
<p>At the prompt, type <kbd>Beamcoin.mine</kbd> and hit <em>Enter</em>. After a few seconds, you should see:</p>
<pre><strong>iex(1)&gt; Beamcoin.mine

23:30:45.243 [info]  Solution found in 3 seconds

23:30:45.243 [info]  The number is: 10097471

23:30:45.243 [info]  The hash is: 2354BB63E90673A357F53EBC96141D5E95FD26B3058AFAD7B1F7BACC9D000000
:ok</strong></pre>
<p>Or, something like it.</p>
<p>Building NIFs in the BEAM is a complicated topic, one we've hardly touched on here. Sonny Scroggin's talk, included in the <em>Further reading</em> section, covers that nuance in detail. If you're curious about how the BEAM functions, I've included a talk of mine on that subject in the <em>Further reading</em> section as well. Decades of careful effort have gone into the BEAM and it really shows.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed embedding languages in Rust and vice versa. Rust is an incredibly useful programming language in its own right, but has been designed with care to interoperate with the existing language ecosystem. Delegating difficult concurrency work in a memory-unsafe environment into Rust is a powerful model. Mozilla's work on Firefox has shown that path to be fruitful. Likewise, there are decades' worth of well-tested libraries niche domains—weather modeling, physics, amusing programming games from the 1980s—that could, theoretically, be rewritten in Rust but are probably better incorporated behind safe interfaces.</p>
<p>This chapter is the last that aims to teach you a new, broad skill. If you've made it this far in the book, thank you. It's been a real pleasure writing for you. You should, hopefully, now have a solid foundation for doing low-level concurrency in Rust and the confidence to read through most Rust codebases you come across. There's a lot going on in Rust and I hope it seems more familiar now. In the next, and last, chapter of the book, we'll discuss the future of Rust, what language features apropos this book are coming soon, and how they might bring new capabilities to us.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><em>FFI examples written in Rust</em>, available at <a href="https://github.com/alexcrichton/rust-ffi-examples">https://github.com/alexcrichton/rust-ffi-examples</a>. This repository by Alex Crichton is a well-done collection of FFI examples in Rust. The documentation in this book on this topic is quite good, but it never hurts to pour through working code.</li>
<li><em>Hacker's Delight</em>, Henry Warren Jr. If you enjoyed the bit fiddling present in this chapter's take on feruscore, you'll love Hacker's Delight. It's old now and some of its algorithms no longer function on 64-bit words, but it's still well worth reading, especially if you, like me, work to keep fixed-width types as small as possible.</li>
<li><em>Foreign Function Interface</em>, available at <a href="https://doc.rust-lang.org/nomicon/ffi.html">https://doc.rust-lang.org/nomicon/ffi.html</a>. The Nomicon builds a higher-level wrapper for the compression library snappy. This wrapper is extended in ways we did not touch on here, specifically with regard to C callbacks and vardic function calls.</li>
<li><em>Global Interpreter Lock</em>, available at <a href="https://wiki.python.org/moin/GlobalInterpreterLock">https://wiki.python.org/moin/GlobalInterpreterLock</a>. The GIL has long been the bane of multiprocessing software written in Python. This wiki entry discusses the technical details mandating the GIL and the historical attempts to remedy the situation.</li>
<li><em>Guided Tour</em>, available at <a href="https://github.com/chucklefish/rlua/blob/master/examples/guided_tour.rs">https://github.com/chucklefish/rlua/blob/master/examples/guided_tour.rs</a>. The rlua crate includes a guided tour module, which is well-documented and runnable. I haven't seen this approach to documentation in other projects and I warmly encourage you to check it out. First, it's helpful for learning rlua. Second, it's well-written and empathetic to the reader: a fine example of technical writing.</li>
<li><em>Linkage</em>, available at <a href="https://doc.rust-lang.org/reference/linkage.html">https://doc.rust-lang.org/reference/linkage.html</a>. This is the Rust Reference chapter on linking. The details here are very specific, but that's often necessary when being explicit about linking. The common reader will more or less use the information we've covered in this chapter but there's always some new domain requiring specific knowledge.</li>
<li><em>Rust Inside Other Languages</em>, available at <a href="https://doc.rust-lang.org/1.2.0/book/rust-inside-other-languages.html">https://doc.rust-lang.org/1.2.0/book/rust-inside-other-languages.html</a>. This chapter in the Rust Book covers similar ground to this chapter—embedding Rust—but at a faster clip and with different high-level languages. Specifically, The Book covers embedding Rust into both Ruby and NodeJS, which we did not.</li>
<li><em>FFI in Rust - writing bindings for libcpuid</em>, available at <a href="http://siciarz.net/ffi-rust-writing-bindings-libcpuid/">http://siciarz.net/ffi-rust-writing-bindings-libcpuid/</a>. Zbigniew Siciarz has been writing about Rust and writing in Rust for a good while. You may know him from his <em>24 days of Rust</em> series. In this post, Sicarz documents the process of building a safe wrapper for libcpuid, a library whose job is to poll the OS for information about the user's CPU.</li>
<li><em><span> </span></em><span><em>Taking Elixir to the Metal with Rust</em>, Sonny Scroggin, available at </span><span class="Object"><a href="https://www.youtube.com/watch?v=lSLTwWqTbKQ" target="_blank">https://www.youtube.com/watch?v=lSLTwWqTbKQ</a></span><span>. In this chapter we demonstrated Beamcoin, a combination of Elixir and Rust in the same project. Integrating NIFs into a BEAM system is a complicated subject. This talk, presented at NDC London 2017, is warmly recommended as an introduction to the subject.</span><span> </span></li>
<li><em>Piecemeal Into Space: Reliability, Safety and Erlang Principles</em>, Brian L. Troutwine, available at <a href="https://www.youtube.com/watch?v=pwoaJvrJE_U">https://www.youtube.com/watch?v=pwoaJvrJE_U</a>. There's a great deal of work that's gone into the BEAM over the decades, earning those languages a key place in fault-tolerant software deployments. Exactly how the BEAM functions is something of a mystery without inspection. In this talk, I cover the BEAM's semantic model and then discuss it's implementation at a high-level.</li>
</ul>


            </article>

            
        </section>
    </div>



  </body></html>
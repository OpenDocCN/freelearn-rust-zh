<html><head></head><body>
		<div><h1 id="_idParaDest-82" class="chapter-number"><a id="_idTextAnchor081"/>4</h1>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor082"/>Create Your Own Event Queue</h1>
			<p>In this chapter, we’ll create a simple version of an event queue using epoll. We’ll take inspiration from <code>mio</code> has the added benefit of making it easier to dive into their code base if you wish to explore how a real production-ready library works.</p>
			<p>By the end of this chapter, you should be able to understand the following:</p>
			<ul>
				<li>The difference between blocking and non-blocking I/O</li>
				<li>How to use epoll to make your own event queue</li>
				<li>The source code of cross-platform event queue libraries such as mio</li>
				<li>Why we need an abstraction layer on top of epoll, kqueue, and IOCP if we want a program or library to work across different platforms</li>
			</ul>
			<p>We’ve divided the chapter into the following sections:</p>
			<ul>
				<li>Design and introduction to epoll</li>
				<li>The <code>ffi</code> module</li>
				<li>The <code>Poll</code> module</li>
				<li>The <code>main</code> program</li>
			</ul>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor083"/>Technical requirements</h1>
			<p>This chapter focuses on epoll, which is specific to Linux. Unfortunately, epoll is not part of the <strong class="bold">Portable Operating System Interface</strong> (<strong class="bold">POSIX</strong>) standard, so this example will require you to run Linux and won’t work with macOS, BSD, or Windows operating systems.</p>
			<p>If you’re on a machine running Linux, you’re already set and can run the examples without any further steps.</p>
			<p>If you’re on Windows, my recommendation is to set up <strong class="bold">WSL</strong> (<a href="https://learn.microsoft.com/en-us/windows/wsl/install">https://learn.microsoft.com/en-us/windows/wsl/install</a>), if you haven’t already, and install Rust on the Linux operating system running on WSL.</p>
			<p>If you’re using Mac, you can create a <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) running Linux, for example, by using the <strong class="bold">QEMU</strong>-based <strong class="bold">UTM</strong> application (<a href="https://mac.getutm.app/">https://mac.getutm.app/</a>) or any other solution for managing VMs on a Mac.</p>
			<p>A last option is to rent a Linux server (there are even some providers with a free layer), install Rust, and either use an editor such as Vim or Emacs in the console or develop on the remote machine using VS Code through SSH (<a href="https://code.visualstudio.com/docs/remote/ssh">https://code.visualstudio.com/docs/remote/ssh</a>). I personally have good experience with Linode’s offering (<a href="https://www.linode.com/">https://www.linode.com/</a>), but there are many, many other options out there.</p>
			<p>It’s theoretically possible to run the examples on the Rust playground, but since we need a delay server, we would have to use a remote delay server service that accepts plain HTTP requests (not HTTPS) and modify the code so that the modules are all in one file instead. It’s possible in a clinch but not really recommended.</p>
			<p class="callout-heading">The delay server</p>
			<p class="callout">This example relies on calls made to a server that delays the response for a configurable duration. In the repository, there is a project named <code>delayserver</code> in the root folder.</p>
			<p class="callout">You can set up the server by simply entering the folder in a separate console window and writing <code>cargo run</code>. Just leave the server running in a separate, open terminal window as we’ll use it in our example.</p>
			<p class="callout">The <code>delayserver</code> program is cross-platform, so it works without any modification on all platforms that Rust supports. If you’re running WSL on Windows, I recommend running the <code>delayserver</code> program in WSL as well. Depending on your setup, you might get away with running the server in a Windows console and still be able to reach it when running the example in WSL. Just be aware that it might not work out of the box.</p>
			<p class="callout">The server will listen to port <code>8080</code> by default and the examples there assume this is the port used. You can change the listening port in the <code>delayserver</code> code before you start the server, but just remember to make the same corrections in the example code.</p>
			<p class="callout">The actual code for <code>delayserver</code> is less than 30 lines, so going through the code should only take a few minutes if you want to see what the server does.</p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor084"/>Design and introduction to epoll</h1>
			<p>Okay, so this chapter will be<a id="_idIndexMarker256"/> centered around one main example you can find in the repository under <code>ch04/a-epoll</code>. We’ll start by taking a look at how we design our example.</p>
			<p>As I mentioned at the start of this chapter, we’ll take our inspiration from <code>mio</code>. This has one big upside and one downside. The upside is that we get a gentle introduction to how <code>mio</code> is designed, making it much easier to dive into that code base if you want to learn more than what we cover in this example. The downside is that we introduce an overly thick abstraction layer over epoll, including some design decisions that are very specific to <code>mio</code>.</p>
			<p>I think the upsides outweigh the downsides for the simple reason that if you ever want to implement a production-quality event loop, you’ll probably want to look into the implementations that are already out there, and the same goes for if you want to dig deeper into the building blocks of asynchronous programming in Rust. In Rust, <code>mio</code> is one of the important libraries underpinning much of the async ecosystem, so gaining a little familiarity with it is an added bonus.</p>
			<p>It’s important to note that <code>mio</code> is a cross-platform library that creates an abstraction over epoll, kqueue, and IOCP (through Wepoll, as we described in <a href="B20892_03.xhtml#_idTextAnchor063"><em class="italic">Chapter 3</em></a>). Not only that, <code>mio</code> supports iOS and Android, and in the future, it will likely support other platforms as well. So, leaving the door open to unify an API over so many different systems is bound to also come with some compromises if you compare it to what you can achieve if you only plan to support one platform.</p>
			<p class="callout-heading">mio</p>
			<p class="callout">mio describes itself as a “<em class="italic">fast, low-level I/O library for Rust focusing on non-blocking APIs and event notification for building performance I/O apps with as little overhead as possible over the </em><em class="italic">OS abstractions</em>.”</p>
			<p class="callout">mio drives the event queue in Tokio, which is one of the most popular and widely used asynchronous runtimes in Rust. This means that mio is driving I/O for popular frameworks such as Actix Web (<a href="https://actix.rs/">https://actix.rs/</a>), Warp (<a href="https://github.com/seanmonstar/warp">https://github.com/seanmonstar/warp</a>), and Rocket (<a href="https://rocket.rs/">https://rocket.rs/</a>).</p>
			<p class="callout">The version of mio we’ll use as design inspiration in this example is version <strong class="bold">0.8.8</strong>. The API has changed in the past and may change in the future, but the parts of the API we cover here have been stable since 2019, so it’s a good bet that there will not be significant changes to it in the near future.</p>
			<p>As is the case with all cross-platform abstractions, it’s often necessary to go the route of choosing the least common denominator. Some choices will limit flexibility and efficiency on one or more platforms in the pursuit of having a unified API that works with all of them. We’ll<a id="_idIndexMarker257"/> discuss some of those choices in this chapter.</p>
			<p>Before we go further, let’s create a blank project and give it a name. We’ll refer to it as <code>a-epoll</code> going forward, but you will of course need to replace that with the name you choose.</p>
			<p>Enter the folder and type the <code>cargo </code><code>init</code> command.</p>
			<p>In this example, we’ll divide the project into a few modules, and we’ll split the code up into the following files:</p>
			<pre class="source-code">
src
 |-- ffi.rs
 |-- main.rs
 |-- poll.rs</pre>			<p>Their descriptions are as follows:</p>
			<ul>
				<li><code>ffi.rs</code>: This module will contain the code related to the syscalls we need to communicate with the host operating system</li>
				<li><code>main.rs</code>: This is the example program itself</li>
				<li><code>poll.rs</code>: This module contains the main abstraction, which is a thin layer over epoll</li>
			</ul>
			<p>Next, create the four files, mentioned in the preceding list, in the <code>src</code> folder.</p>
			<p>In <code>main.rs</code>, we need to declare the modules as well:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">a-epoll/src/main.rs</p>
			<pre class="source-code">
mod ffi;
mod poll;</pre>			<p>Now that we have our project set up, we can start by going through how we’ll design the API we’ll use. The main abstraction is in <code>poll.rs</code>, so go ahead and open that file.</p>
			<p>Let’s start by stubbing out the structures and functions we need. It’s easier to discuss them when we have <a id="_idIndexMarker258"/>them in front of us:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">a-epoll/src/poll.rs</p>
			<pre class="source-code">
use std::{io::{self, Result}, net::TcpStream, os::fd::AsRawFd};
use crate::ffi;
type Events = Vec&lt;ffi::Event&gt;;
pub struct Poll {
  registry: Registry,
}
impl Poll {
  pub fn new() -&gt; Result&lt;Self&gt; {
    todo!()
  }
  pub fn registry(&amp;self) -&gt; &amp;Registry {
    &amp;self.registry
  }
  pub fn poll(&amp;mut self, events: &amp;mut Events, timeout: Option&lt;i32&gt;) -&gt; Result&lt;()&gt; {
    todo!()
  }
}
pub struct Registry {
  raw_fd: i32,
}
impl Registry {
  pub fn register(&amp;self, source: &amp;TcpStream, token: usize, interests: i32) -&gt; Result&lt;()&gt; 
  {
    todo!()
  }
}
impl Drop for Registry {
  fn drop(&amp;mut self) {
    todo!()
  }
}</pre>			<p>We’ve replaced all the implementations with <code>todo!()</code> for now. This macro will let us compile the program even though we’ve yet to implement the function body. If our execution ever reaches <code>todo!()</code>, it will panic.</p>
			<p>The first thing you’ll notice is that we’ll pull the <code>ffi</code> module in scope in addition to some types from the standard library.</p>
			<p>We’ll also use the <code>std::io::Result</code> type as our own <code>Result</code> type. It’s convenient since most errors will stem from one of our calls into the operating system, and an operating system error can be mapped to an <code>io::Error</code> type.</p>
			<p>There are two main abstractions over epoll. One is a structure called <code>Poll</code> and the other is called <code>Registry</code>. The name and functionality of these functions are the same as they are in <code>mio</code>. Naming abstractions such as these is surprisingly difficult, and both constructs could very well have had a different name, but let’s lean on the fact that someone else has spent time on this before us and decided to go with these in our example.</p>
			<p><code>Poll</code> is a struct that <a id="_idIndexMarker259"/>represents the event queue itself. It has a few methods:</p>
			<ul>
				<li><code>new</code>: Creates a new event queue</li>
				<li><code>registry</code>: Returns a reference to the registry that we can use to register interest to be notified about new events</li>
				<li><code>poll</code>: Blocks the thread it’s called on until an event is ready or it times out, whichever occurs first</li>
			</ul>
			<p><code>Registry</code> is the other half of the equation. While <code>Poll</code> represents the event queue, <code>Registry</code> is a handle that allows us to register interest in new events.</p>
			<p><code>Registry</code> will only have one method: <code>register</code>. Again, we mimic the API <code>mio</code> uses (<a href="https://docs.rs/mio/0.8.8/mio/struct.Registry.html">https://docs.rs/mio/0.8.8/mio/struct.Registry.html</a>), and instead of accepting a predefined list of methods for registering different interests, we accept an <code>interests</code> argument, which will indicate what kind of events we want our event queue to keep track of.</p>
			<p>One more thing to note is that we won’t use a generic type for all sources. We’ll only implement this for <code>TcpStream</code>, even though there are many things we could potentially track with an event queue.</p>
			<p>This is especially true when we want to make this cross-platform since, depending on the platforms you want to support, there are many types of event sources we might want to track.</p>
			<p>mio solves this by having <code>Registry::register</code> accept an object implementing the <code>Source</code> trait that <code>mio</code> defines. As long as you implement this trait for the source, you can use the event queue to track events on it. </p>
			<p>In the following pseudo-code, you’ll get an idea of how we plan to use this API:</p>
			<pre class="source-code">
let queue = Poll::new().unwrap();
let id = 1;
// register interest in events on a TcpStream
queue.registry().register(&amp;stream, id, ...).unwrap();
let mut events = Vec::with_capacity(1);
// This will block the curren thread
queue.poll(&amp;mut events, None).unwrap();
//...data is ready on one of the tracked streams</pre>			<p>You might wonder<a id="_idIndexMarker260"/> why we need the <code>Registry</code> struct at all.</p>
			<p>To answer that question, we need to remember that <code>mio</code> abstracts over epoll, kqueue, and IOCP. It does this by making <code>Registry</code> wrap around a <code>Selector</code> object. The <code>Selector</code> object is conditionally compiled so that every platform has its own <code>Selector</code> implementation corresponding to the relevant syscalls to make IOCP, kqueue, and epoll do the same thing.</p>
			<p><code>Registry</code> implements one important method we won’t implement in our example, called <code>try_clone</code>. The reason we won’t implement this is that we don’t need it to understand how an event loop like this works and we want to keep the example simple and easy to understand. However, this method is important for understanding why the responsibility of registering events and the queue itself is divided.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">By moving the concern of registering interests to a separate struct like this, users can call <code>Registry::try_clone</code> to get an owned <code>Registry</code> instance. This instance can be passed to, or shared through <code>Arc&lt;Registry&gt;</code> with, other threads, allowing multiple threads to register interest to the same <code>Poll</code> instance even when <code>Poll</code> is blocking another thread while waiting for new events to happen in <code>Poll::poll</code>.</p>
			<p><code>Poll::poll</code> requires exclusive access since it takes a <code>&amp;mut self</code>, so when we’re waiting for events in <code>Poll::poll</code>, there is no way to register interest from a different thread at the same time if we rely on using <code>Poll</code> to register interest, since that will be prevented by Rust’s type system.</p>
			<p>It also makes it effectively impossible to have multiple threads waiting for events by calling <code>Poll::poll</code> on the same instance in any meaningful way since it would require synchronization that essentially would make each call sequential anyway.</p>
			<p>The design lets users interact with the queue from potentially many threads by registering interest, while one thread makes the blocking call and handles the notifications from <a id="_idIndexMarker261"/>the operating system.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The fact that <code>mio</code> doesn’t enable you to have multiple threads that are blocked on the same call to <code>Poll::poll</code> isn’t a limitation due to epoll, kqueue, or IOCP. They all allow for the scenario that many threads will call <code>Poll::poll</code> on the same instance and get notifications on events in the queue. epoll even allows specific flags to dictate whether the operating system should wake up only one or all threads that wait for notification (specifically the <code>EPOLLEXCLUSIVE</code> flag).</p>
			<p class="callout">The problem is partly about how the different platforms decide which threads to wake when there are many of them waiting for events on the same queue, and partly about the fact that there doesn’t seem to be a huge interest in that functionality. For example, epoll will, by default, wake all threads that block on <code>Poll</code>, while Windows, by default, will only wake up one thread. You can modify this behavior to some extent, and there have been ideas on implementing a <code>try_clone</code> method on <code>Poll</code> as well in the future. For now, the design is like we outlined, and we will stick to that in our example as well.</p>
			<p>This brings us to another<a id="_idIndexMarker262"/> topic we should cover before we start implementing our example.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor085"/>Is all I/O blocking?</h2>
			<p>Finally, a question that’s easy<a id="_idIndexMarker263"/> to answer. The answer is a big, resounding… maybe. The thing is that not all I/O operations will block in the sense that the operating system will park the calling thread and it will be more efficient to switch to another task. The reason for this is that the operating system is smart and will cache a lot of information in memory. If information is in the cache, a syscall requesting that information would simply return immediately with the data, so forcing a context switch or any rescheduling of the current task might be less efficient than just handling the data synchronously. The problem is that there is no way to know for sure whether I/O is blocking and it depends on what you’re doing.</p>
			<p>Let me give you two examples.</p>
			<h3>DNS lookup</h3>
			<p>When creating a <a id="_idIndexMarker264"/>TCP connection, one of the first things that happens is<a id="_idIndexMarker265"/> that you need to convert a typical address such as <a href="https://www.google.com">www.google.com</a> to an IP address such as <code>216.58.207.228</code>. The operating system maintains a mapping of local addresses and addresses it’s previously looked up in a cache and will be able to resolve them almost immediately. However, the first time you look up an unknown address, it might have to make a call to a DNS server, which takes a lot of time, and the OS will park the calling thread while waiting for the response if it’s not handled in a non-blocking manner.</p>
			<h3>File I/O</h3>
			<p>Files on the local <a id="_idIndexMarker266"/>filesystem are another area where the operating system<a id="_idIndexMarker267"/> performs quite a bit of caching. Smaller files that are frequently read are often cached in memory, so requesting that file might not block at all. If you have a web server that serves static files, there is most likely a rather limited set of small files you’ll be serving. The chances are that these are cached in memory. However, there is no way to know for sure – if an operating system is running low on memory, it might have to map memory pages to the hard drive, which makes what would normally be a very fast memory lookup excruciatingly slow. The same is true if there is a huge number of small files that are accessed randomly, or if you serve very large files since the operating system will only cache a limited amount of information. You’ll also encounter this kind of unpredictability if you have many unrelated processes running on the same operating system as it might not cache the information that’s important to you.</p>
			<p>A popular way of handling these cases is to forget about non-blocking I/O, and actually make a blocking call instead. You don’t want to do these calls in the same thread that runs a <code>Poll</code> instance (since every small delay will block all tasks), but you would probably relegate that<a id="_idIndexMarker268"/> task to a <strong class="bold">thread pool</strong>. In the thread pool, you have a limited number of threads that are tasked with making regular blocking calls for things such as DNS lookups or file I/O.</p>
			<p>An example of a runtime that does exactly this is <code>libuv</code> (<a href="http://docs.libuv.org/en/v1.x/threadpool.html#threadpool">http://docs.libuv.org/en/v1.x/threadpool.html#threadpool</a>). <code>libuv</code> is the asynchronous I/O library that Node.js is built upon.</p>
			<p>While its scope is larger than <code>mio</code> (which only cares about non-blocking I/O), <code>libuv</code> is to <code>Node</code> in JavaScript what <code>mio</code> is to Tokio in Rust.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The reason for doing file I/O in a thread pool is that there have historically been poor cross-platform APIs for non-blocking file I/O. While it’s true that many runtimes choose to relegate this task to a thread pool making blocking calls to the OS, it might not be true in the future as the OS APIs evolve over time.</p>
			<p>Creating a thread pool to handle these cases is outside the scope of this example (even <code>mio</code> considers this outside its scope, just to be clear). We’ll focus on showing how epoll works and<a id="_idIndexMarker269"/> mention<a id="_idIndexMarker270"/> these topics in the text, even though we won’t actually implement a solution for them in this example.</p>
			<p>Now that we’ve covered a lot of basic information about epoll, mio, and the design of our example, it’s time to write some code and see for ourselves how this all works in practice.</p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor086"/>The ffi module</h1>
			<p>Let’s start with the <a id="_idIndexMarker271"/>modules that don’t depend on any others and work our way from there. The <code>ffi</code> module contains mappings to the syscalls and data structures we need to communicate with the operating system. We’ll also explain how epoll works in detail once we have presented the syscalls.</p>
			<p>It’s only a few lines of code, so I’ll place the first part here so it’s easier to keep track of where we are in the file since there’s quite a bit to explain. Open the <code>ffi.rs</code> file and write the following lines of code:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/ffi.rs</p>
			<pre class="source-code">
pub const EPOLL_CTL_ADD: i32 = 1;
pub const EPOLLIN: i32 = 0x1;
pub const EPOLLET: i32 = 1 &lt;&lt; 31;
#[link(name = "c")]
extern "C" {
  pub fn epoll_create(size: i32) -&gt; i32;
  pub fn close(fd: i32) -&gt; i32;
  pub fn epoll_ctl(epfd: i32, op: i32, fd: i32, event: *mut Event) -&gt; i32;
  pub fn epoll_wait(epfd: i32, events: *mut Event, maxevents: i32, timeout: i32) -&gt; i32;
}</pre>			<p>The first thing you’ll notice is that we declare a few constants called <code>EPOLL_CTL_ADD</code>, <code>EPOLLIN</code>, and <code>EPOLLET</code>.</p>
			<p>I’ll get back to explaining what these constants are in a moment. Let’s first take a look at the syscalls we need to make. Fortunately, we’ve already covered syscalls in detail, so you already know the basics of <code>ffi</code> and why we link to C in the preceding code:</p>
			<ul>
				<li><code>epoll_create</code> is the syscall we make to create an epoll queue. You can find the documentation for it at <a href="https://man7.org/linux/man-pages/man2/epoll_create.2.html">https://man7.org/linux/man-pages/man2/epoll_create.2.html</a>. This method accepts one argument called <code>size</code>, but <code>size</code> is there only for historical reasons. The argument will be ignored but must have a value larger than <em class="italic">0</em>.</li>
				<li><code>close</code> is the syscall we need to close the file descriptor we get when we create our <code>epoll</code> instance, so we release our resources properly. You can read the documentation for the syscall at <a href="https://man7.org/linux/man-pages/man2/close.2.html">https://man7.org/linux/man-pages/man2/close.2.html</a>.</li>
				<li><code>epoll_ctl</code> is the control interface we use to perform operations on our epoll instance. This is the call we use to register interest in events on a source. It supports three main operations: <em class="italic">add</em>, <em class="italic">modify</em>, or <em class="italic">delete</em>. The first argument, <code>epfd</code>, is the epoll file descriptor we want to perform operations on. The second argument, <code>op</code>, is the argument where we specify whether we want to perform an <em class="italic">add</em>, <em class="italic">modify</em>, or <em class="italic">delete</em> operation</li>
				<li>In our case, we’re<a id="_idIndexMarker272"/> only interested in adding interest for events, so we’ll only pass in <code>EPOLL_CTL_ADD</code>, which is the value to indicate that we want to perform an <em class="italic">add</em> operation. <code>epoll_event</code> is a little more complicated, so we’ll discuss it in more detail. It does two important things for us: first, the <code>events</code> field indicates what kind of events we want to be notified of and it can also modify the behavior of <em class="italic">how</em> and <em class="italic">when</em> we get notified. Second, the <code>data</code> field passes on a piece of data to the kernel that it will return to us when an event occurs. The latter is important since we need this data to identify exactly what event occurred since that’s the only information we’ll receive in return that can identify what source we got the notification for. You can find the documentation for this syscall here: <a href="https://man7.org/linux/man-pages/man2/epoll_ctl.2.html">https://man7.org/linux/man-pages/man2/epoll_ctl.2.html</a>.</li>
				<li><code>epoll_wait</code> is the call that will block the current thread and wait until one of two things happens: we receive a notification that an event has occurred or it times out. <code>epfd</code> is the epoll file descriptor identifying the queue we made with <code>epoll_create</code>. <code>events</code> is an array of the same <code>Event</code> structure we used in <code>epoll_ctl</code>. The difference is that the <code>events</code> field now gives us information about what event <em class="italic">did</em> occur, and importantly the <code>data</code> field contains the same data that we passed in when we registered interest</li>
				<li>For example, the <code>data</code> field lets us identify which file descriptor has data that’s ready to be read. The <code>maxevents</code> arguments tell the kernel how many events we have reserved space for in our array. Lastly, the <code>timeout</code> argument tells the kernel how long we will wait for events before it will wake us up again so we don’t potentially block forever. You can read the documentation for <code>epoll_wait</code> at <a href="https://man7.org/linux/man-pages/man2/epoll_wait.2.html">https://man7.org/linux/man-pages/man2/epoll_wait.2.html</a>.</li>
			</ul>
			<p>The last part of the <a id="_idIndexMarker273"/>code in this file is the <code>Event</code> struct:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/ffi.rs</p>
			<pre class="source-code">
#[derive(Debug)]
#[repr(C, packed)]
pub struct Event {
    pub(crate) events: u32,
    // Token to identify event
    pub(crate) epoll_data: usize,
}
impl Event {
    pub fn token(&amp;self) -&gt; usize {
        self.epoll_data
    }
}</pre>			<p>This structure is used to communicate to the operating system in <code>epoll_ctl</code>, and the operating system uses the same structure to communicate with us in <code>epoll_wait</code>.</p>
			<p>Events are defined as a <code>u32</code>, but it’s more than just a number. This field is what we call a <strong class="bold">bitmask</strong>. I’ll take<a id="_idIndexMarker274"/> the time to explain bitmasks in a later section since it’s common in most syscalls and not something everyone has encountered before. In simple terms, it’s a way to use the bit representation as a set of yes/no flags to indicate whether an option has been chosen or not.</p>
			<p>The different options are described in the link I provided for the <code>epoll_ctl</code> syscall. I won’t explain all of them <a id="_idIndexMarker275"/>in detail here, but just cover the ones we’ll use:</p>
			<ul>
				<li><code>EPOLLIN</code> represents a bitflag indicating we’re interested in read operations on the file handle</li>
				<li><code>EPOLLET</code> represents a bitflag indicating that we’re interested in getting events notified with epoll set to an edge-triggered mode</li>
			</ul>
			<p>We’ll get back to explaining bitflags, bitmasks, and what edge-triggered mode really means in a moment, but let’s just finish with the code first.</p>
			<p>The last field on the <code>Event</code> struct is <code>epoll_data</code>. This field is defined as a union in the documentation. A union is much like an enum, but in contrast to Rust’s enums, it doesn’t carry any information on what type it is, so it’s up to us to make sure we know what type of data it holds.</p>
			<p>We use this field to simply hold a <code>usize</code> so we can pass in an integer identifying each event when we register interest using <code>epoll_ctl</code>. It would be perfectly fine to pass in a pointer instead – just as long as we make sure that the pointer is still valid when it’s returned to us in <code>epoll_wait</code>.</p>
			<p>We can think of this field as a token, which is exactly what <code>mio</code> does, and to keep the API as similar as possible, we copy <code>mio</code> and provide a <code>token</code> method on the struct to get this value.</p>
			<p class="callout-heading">What does #[repr(packed)] do?</p>
			<p class="callout">The <code>#[repr(packed)]</code> annotation is new to us. Usually, a struct will have padding either between fields or at the end of the struct. This happens even when we’ve specified <code>#[repr(C)]</code>.</p>
			<p class="callout">The reason has to do with efficient access to the data stored in the struct by not having to make multiple fetches to get the data stored in a struct field. In the case of the <code>Event</code> struct, the usual padding would be adding 4 bytes of padding at the end of the <code>events</code> field. When the operating system expects a packed struct for <code>Event</code>, and we give it a padded one, it will write parts of <code>event_data</code> to the padding between the fields. When you try to read <code>event_data</code> later on, you’ll end up only reading the last part of <code>event_data</code>, which happened to overlap and get the wrong data</p>
			<div><div><img src="img/B20892_04_0.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="callout">The fact that the<a id="_idIndexMarker276"/> operating systemexpects a packed <code>Event</code> struct isn’t obvious by reading the manpages for Linux, so you have to read the appropriate C header files to know for sure. You could of course simply rely on the <code>libc</code> crate (<a href="https://github.com/rust-lang/libc">https://github.com/rust-lang/libc</a>), which we would do too if we weren’t here to learn things like this for ourselves.</p>
			<p>So, now that we’ve finished walking through the code, there are a few topics that we promised to<a id="_idIndexMarker277"/> get back to.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor087"/>Bitflags and bitmasks</h2>
			<p>You’ll encounter<a id="_idIndexMarker278"/> this all the<a id="_idIndexMarker279"/> time when making<a id="_idIndexMarker280"/> syscalls (in fact, the<a id="_idIndexMarker281"/> concept of bitmasks is pretty common in low-level programming). A bitmask is a way to treat each bit as a switch, or a flag, to indicate that an option is either enabled or disabled.</p>
			<p>An integer, such as <code>i32</code>, can be expressed as 32 bits. <code>EPOLLIN</code> has the hex value of <code>0x1</code> (which is simply 1 in decimal). Represented in bits, this would look like <code>00000000000000000000000000000001</code>.</p>
			<p><code>EPOLLET</code>, on the other hand, has a value of <code>1 &lt;&lt; 31</code>. This simply means the bit representation of the decimal number 1, shifted 31 bits to the left. The decimal number 1 is incidentally the same as <code>EPOLLIN</code>, so by looking at that representation and shifting the bits 31 times to the left, we get a number with the bit representation of <code>10000000000000000000000000000000</code>.</p>
			<p>The way we use bitflags is that we use the OR operator, <code>|</code>, and by OR’ing the values together, we get a bitmask with each flag we OR’ed set to 1. In our example, the bitmask would look like <code>10000000000000000000000000000001</code>.</p>
			<p>The receiver of the bitmask (in this case, the operating system) can then do an opposite operation, check which flags are set, and act accordingly.</p>
			<p>We can create a very simple example in code to show how this works in practice (you can simply run this in the Rust playground or create a new empty project for throwaway experiments such as this):</p>
			<pre class="source-code">
fn main() {
  let bitflag_a: i32 = 1 &lt;&lt; 31;
  let bitflag_b: i32 = 0x1;
  let bitmask: i32 = bitflag_a | bitflag_b;
  println!("{bitflag_a:032b}");
  println!("{bitflag_b:032b}");
  println!("{bitmask:032b}");
  check(bitmask);
}
fn check(bitmask: i32) {
  const EPOLLIN: i32 = 0x1;
  const EPOLLET: i32 = 1 &lt;&lt; 31;
  const EPOLLONESHOT: i32 = 0x40000000;
  let read = bitmask &amp; EPOLLIN != 0;
  let et = bitmask &amp; EPOLLET != 0;
  let oneshot = bitmask &amp; EPOLLONESHOT != 0;
  println!("read_event? {read}, edge_triggered: {et}, oneshot?: {oneshot}")
}</pre>			<p>This code will output the following:</p>
			<pre class="source-code">
10000000000000000000000000000000
00000000000000000000000000000001
10000000000000000000000000000001
read_event? true, edge_triggered: true, oneshot?: false</pre>			<p>The <a id="_idIndexMarker282"/>next <a id="_idIndexMarker283"/>topic <a id="_idIndexMarker284"/>we<a id="_idIndexMarker285"/> will introduce in this chapter is the concept of edge-triggered events, which probably need some explanation.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor088"/>Level-triggered versus edge-triggered events</h2>
			<p>In a perfect world, we <a id="_idIndexMarker286"/>wouldn’t<a id="_idIndexMarker287"/> need to discuss this, but <a id="_idIndexMarker288"/>when working with epoll, it’s almost impossible to avoid having to know about the difference. It’s not obvious by reading the documentation, especially not if you haven’t had previous experience with these terms before. The interesting part of this is that it allows us to create a parallel between how events are handled in epoll and how events are handled at the hardware level.</p>
			<p>epoll can notify events in a <code>events</code> bitmask on the <code>Event</code> struct, we set the <code>EPOLLET</code> flag to get notified in edge-triggered mode (the default if you specify nothing is level-triggered).</p>
			<p>This way of modeling event notification and event handling has a lot of similarities to how computers handle interrupts.</p>
			<p>Level-triggered means that the answer to the question “Has an event happened” is true as long as the electrical signal on an interrupt line is reported as high. If we translate this to our example, <em class="italic">a read event has occurred as long as there is data in the buffer associated with the </em><em class="italic">file handle.</em></p>
			<p>When handling interrupts, you would clear the interrupt by servicing whatever hardware caused it, or you could mask the interrupt, which simply disables interrupts on that line until it’s explicitly unmasked later on.</p>
			<p>In our example, we clear the <em class="italic">interrupt</em> by draining all the data in the buffer by reading it. When the buffer is drained, the answer to our question changes to <em class="italic">false</em>.</p>
			<p>When using epoll in its default mode, which is level-triggered, we can encounter a case where we get multiple notifications on the same event since we haven’t had time to drain the buffer yet (remember, as long as there is data in the buffer, epoll will notify you over and over again). This is especially apparent when we have one thread that reports events and then delegates the task of handling the event (reading from the stream) to other worker threads since epoll will happily report that an event is ready even though we’re in the process of handling it.</p>
			<p>To remedy this, epoll has a flag named <code>EPOLLONESHOT</code>.</p>
			<p><code>EPOLLONESHOT</code> tells epoll that once we receive an event on this file descriptor, it should disable the file descriptor in the interest list. It won’t remove it, but we won’t get any more notifications on that file descriptor unless we explicitly reactivate it by calling <code>epoll_ctl</code> with the <code>EPOLL_CTL_MOD</code> argument and a new bitmask.</p>
			<p>If we didn’t add this flag, the following could happen: if <em class="italic">thread 1</em> is the thread where we call <code>epoll_wait</code>, then once it receives a notification about a read event, it starts a task in <em class="italic">thread 2</em> to read from that file descriptor, and then calls <code>epoll_wait</code> again to get notifications on new events. In this case, the call to <code>epoll_wait</code> would return again and tell us that data is ready on the same file descriptor since we haven’t had the time to drain the buffer on that file descriptor yet. We know that the task is taken care of by <code>thread 2</code>, but we still get a notification. Without additional synchronization and logic, we could end up giving the task of reading from the same file descriptor to <em class="italic">thread 3</em>, which could cause problems that are quite hard to debug.</p>
			<p>Using <code>EPOLLONESHOT</code> solves this problem since <em class="italic">thread 2</em> will have to reactivate the file descriptor in the event queue once it’s done handling its task, thereby telling our epoll queue that it’s finished with it and that we are interested in getting notifications on that file descriptor again.</p>
			<p>To go back to our <a id="_idIndexMarker289"/>original analogy <a id="_idIndexMarker290"/>of<a id="_idIndexMarker291"/> hardware interrupts, <code>EPOLLONESHOT</code> could be thought of as masking an interrupt. You haven’t actually cleared the source of the event notification yet, but you don’t want further notifications until you’ve done that and explicitly unmask it. In epoll, the <code>EPOLLONESHOT</code> flag will disable notifications on the file descriptor until you explicitly enable it by calling <code>epoll_ctl</code> with the <code>op</code> argument set to <code>EPOLL_CTL_MOD</code>.</p>
			<p>Edge-triggered means that the answer to the question “Has an event happened” is true only if the electrical signal has <em class="italic">changed</em> from low to high. If we translate this to our example: a read event has occurred when the buffer has changed from <em class="italic">having no data</em> to <em class="italic">having data</em>. As long as there is data in the buffer, no new events will be reported. You still handle the event by draining all the data from the socket, but you won’t get a new notification until the buffer is fully drained and then filled with new data.</p>
			<p>Edge-triggered mode also comes with some pitfalls. The biggest one is that if you don’t drain the buffer properly, you will never receive a notification on that file handle again.</p>
			<div><div><img src="img/B20892_04_1.jpg" alt="Figure 4.1 – Edge-triggered versus level-triggered events"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Edge-triggered versus level-triggered events</p>
			<p>mio<a id="_idIndexMarker292"/> doesn’t, at<a id="_idIndexMarker293"/> the<a id="_idIndexMarker294"/> time of writing, support <code>EPOLLONESHOT</code> and uses epoll in an edge-triggered mode, which we will do as well in our example.</p>
			<p class="callout-heading">What about waiting on epoll_wait in multiple threads?</p>
			<p class="callout">As long as we only have one <code>Poll</code> instance, we avoid the problems and subtleties of having multiple threads calling <code>epoll_wait</code> on the same epoll instance. Using level-triggered events will wake up all threads that are waiting in the <code>epoll_wait</code> call, causing all of them to try to handle the event (this is often referred to as the problem of the thundering heard). epoll has another flag you can set, called <code>EPOLLEXCLUSIVE</code>, that solves this issue. Events that are set to be edge-triggered will only wake up one of the threads blocking in <code>epoll_wait</code> by default and avoid this issue.</p>
			<p class="callout">Since we only use one <code>Poll</code> instance from a single thread, this will not be an issue for us.</p>
			<p>I know and understand that this sounds very complex. The general concept of event queues is rather simple, but the details can get a bit complex. That said, epoll is one of the most complex APIs in my experience since the API has clearly been evolving over time to adapt the original design to suit modern requirements, and there is really no easy way to actually use and understand it correctly without covering at least the topics we covered here.</p>
			<p>One word of comfort here is that both kqueue and IOCP have APIs that are easier to understand. There is also the fact that Unix has a new asynchronous I/O interface called <code>io_uring</code> that will be more and more and more common in the future.</p>
			<p>Now that we’ve<a id="_idIndexMarker295"/> covered the hard<a id="_idIndexMarker296"/> part <a id="_idIndexMarker297"/>of this chapter and gotten a high-level overview of how epoll works, it’s time to implement our mio-inspired API in <code>poll.rs</code>.</p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor089"/>The Poll module</h1>
			<p>If you haven’t written<a id="_idIndexMarker298"/> or copied the code we presented in the <em class="italic">Design and introduction to epoll</em> section, it’s time to do it now. We’ll implement all the functions where we just had <code>todo!()</code> earlier.</p>
			<p>We start by implementing the methods on our <code>Poll</code> struct. First up is opening the <code>impl Poll</code> block and implementing the <code>new</code> function:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/poll.rs</p>
			<pre class="source-code">
impl Poll {
    pub fn new() -&gt; Result&lt;Self&gt; {
        let res = unsafe { ffi::epoll_create(1) };
        if res &lt; 0 {
            return Err(io::Error::last_os_error());
        }
        Ok(Self {
            registry: Registry { raw_fd: res },
        })
    }</pre>			<p>Given the thorough introduction to epoll in the <em class="italic">The ffi module</em> section, this should be pretty straightforward. We call <code>ffi::epoll_create</code> with an argument of 1 (remember, the<a id="_idIndexMarker299"/> argument is ignored but must have a non-zero value). If we get any errors, we ask the operating system to report the last error for our process and return that. If the call succeeds, we return a new <code>Poll</code> instance that simply wraps around our registry that holds the epoll file descriptor.</p>
			<p>Next up is our registry method, which simply hands out a reference to the inner <code>Registry</code> struct:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/poll.rs</p>
			<pre class="source-code">
    pub fn registry(&amp;self) -&gt; &amp;Registry {
        &amp;self.registry
    }</pre>			<p>The last method on <code>Poll</code> is the most interesting one. It’s the <code>poll</code> function, which will park the current thread and tell the operating system to wake it up when an event has happened on a source we’re tracking, or the timeout has elapsed, whichever comes first. We also close the <code>impl Poll</code> block here:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/poll.rs</p>
			<pre class="source-code">
  pub fn poll(&amp;mut self, events: &amp;mut Events, timeout: Option&lt;i32&gt;) -&gt; Result&lt;()&gt; {
    let fd = self.registry.raw_fd;
    let timeout = timeout.unwrap_or(-1);
    let max_events = events.capacity() as i32;
    let res = unsafe { ffi::epoll_wait(fd, events.as_mut_ptr(), max_events, timeout) };
    if res &lt; 0 {
      return Err(io::Error::last_os_error());
    };
    unsafe { events.set_len(res as usize) };
    Ok(())
  }
}</pre>			<p>The first thing we do is <a id="_idIndexMarker300"/>to get the raw file descriptor for the event queue and store it in the <code>fd</code> variable.</p>
			<p>Next is our <code>timeout</code>. If it’s <code>Some</code>, we unwrap that value, and if it’s <code>None</code>, we set it to <code>–1</code>, which is the value that tells the operating system that we want to block until an event occurs even though that might never happen.</p>
			<p>At the top of the file, we defined <code>Events</code> as a type alias for <code>Vec&lt;ffi::Event&gt;</code>, so the next thing we do is to get the capacity of that <code>Vec</code>. It’s important that we don’t rely on <code>Vec::len</code> since that reports how many items we have in the <code>Vec</code>. <code>Vec::capacity</code> reports the space we’ve allocated and that’s what we’re after.</p>
			<p>Next up is the call to <code>ffi::epoll_wait</code>. This call will return successfully if it has a value of 0 or larger, telling us how many events have occurred.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We would get a value of 0 if a timeout elapses before an event has happened.</p>
			<p>The last thing we do is to make an unsafe call to <code>events.set_len(res as usize)</code>. This function is unsafe since we could potentially set the length so that we would access memory that’s not been initialized yet in safe Rust. We know from the guarantee the operating system gives us that the number of events it returns is pointing to valid data in our <code>Vec</code>, so this is safe in our case.</p>
			<p>Next up is <a id="_idIndexMarker301"/>our <code>Registry</code> struct. We will only implement one method, called <code>register</code>, and lastly, we’ll implement the <code>Drop</code> trait for it, closing the epoll instance:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/poll.rs</p>
			<pre class="source-code">
impl Registry {
    pub fn register(&amp;self, source: &amp;TcpStream, token: usize, interests: i32) -&gt; Result&lt;()&gt; {
        let mut event = ffi::Event {
            events: interests as u32,
            epoll_data: token,
        };
        let op = ffi::EPOLL_CTL_ADD;
        let res = unsafe {
            ffi::epoll_ctl(self.raw_fd, op, source.as_raw_fd(), &amp;mut event)
        };
        if res &lt; 0 {
            return Err(io::Error::last_os_error());
        }
        Ok(())
    }
}</pre>			<p>The register function takes a <code>&amp;TcpStream</code> as a source, a token of type <code>usize</code>, and a bitmask named <code>interests</code>, which is of type <code>i32</code>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This is where mio does things differently. The source argument is specific to each platform. Instead of having the implementation of register on <code>Registry</code>, it’s handled in a platform-specific way in the source argument it receives.</p>
			<p>The first thing we do<a id="_idIndexMarker302"/> is to create an <code>ffi::Event</code> object. The <code>events</code> field is simply set to the bitmask we received and named <code>interests</code>, and <code>epoll_data</code> is set to the value we passed in the <code>token</code> argument.</p>
			<p>The operation we want to perform on the epoll queue is adding interest in events on a new file descriptor. Therefore, we set the <code>op</code> argument to the <code>ffi::EPOLL_CTL_ADD</code> constant value.</p>
			<p>Next up is the call to <code>ffi::epoll_ctl</code>. We pass in the file descriptor to the epoll instance first, then we pass in the <code>op</code> argument to indicate what kind of operation we want to perform. The last two arguments are the file descriptor we want the queue to track and the <code>Event</code> object we created to indicate what kind of events we’re interested in getting notifications for.</p>
			<p>The last part of the function body is simply the error handling, which should be familiar by now.</p>
			<p>The last part of <code>poll.rs</code> is the <code>Drop</code> implementation for <code>Registry</code>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/poll.rs</p>
			<pre class="source-code">
impl Drop for Registry {
    fn drop(&amp;mut self) {
        let res = unsafe { ffi::close(self.raw_fd) };
        if res &lt; 0 {
            let err = io::Error::last_os_error();
            eprintln!("ERROR: {err:?}");
        }
    }
}</pre>			<p>The <code>Drop</code> implementation simply calls <code>ffi::close</code> on the epoll file descriptor. Adding a panic to <code>drop</code> is rarely a good idea since <code>drop</code> can be called within a panic already, which will cause the process to simply abort. mio logs errors if they occur in its Drop implementation but doesn’t handle them in any other way. For our simple example, we’ll just<a id="_idIndexMarker303"/> print the error so we can see if anything goes wrong since we don’t implement any kind of logging here.</p>
			<p>The last part is the code for running our example, and that leads us to <code>main.rs</code>.</p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor090"/>The main program</h1>
			<p>Let’s see how it all <a id="_idIndexMarker304"/>works in practice. Make sure that <code>delayserver</code> is up and running, because we’ll need it for these examples to work.</p>
			<p>The goal is to send a set of requests to <code>delayserver</code> with varying delays and then use epoll to wait for the responses. Therefore, we’ll only use epoll to track <code>read</code> events in this example. The program doesn’t do much more than that for now. </p>
			<p>The first thing we do is to make sure our <code>main.rs</code> file is set up correctly:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/main.rs</p>
			<pre class="source-code">
use std::{io::{self, Read, Result, Write}, net::TcpStream};
use ffi::Event;
use poll::Poll;
mod ffi;
mod poll;</pre>			<p>We import a few types<a id="_idIndexMarker305"/> from our own crate and from the standard library, which we’ll need going forward, as well as declaring our two modules.</p>
			<p>We’ll be working directly with <code>TcpStreams</code> in this example, and that means that we’ll have to format the HTTP requests we make to our <code>delayserver</code> ourselves.</p>
			<p>The server will accept <code>GET</code> requests, so we create a small helper function to format a valid HTTP <code>GET</code> request for us:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/main.rs</p>
			<pre class="source-code">
fn get_req(path &amp;str) -&gt; Vec&lt;u8&gt; {
    format!(
        "GET {path} HTTP/1.1\r\n\
             Host: localhost\r\n\
             Connection: close\r\n\
             \r\n"
    )
}</pre>			<p>The preceding code simply takes a path as an input argument and formats a valid <code>GET</code> request with it. The <em class="italic">path</em> is the part of the URL after the scheme and host. In our case, the path would be everything in bold in the following URL: <code>http://localhost:8080</code><strong class="bold">/2000/hello-world</strong>.</p>
			<p>Next up is our <code>main</code> function. It’s divided into two parts:</p>
			<ul>
				<li>Setup and sending requests</li>
				<li>Wait and handle incoming events</li>
			</ul>
			<p>The first part of <a id="_idIndexMarker306"/>the <code>main</code> function looks like this:</p>
			<pre class="source-code">
fn main() -&gt; Result&lt;()&gt; {
    let mut poll = Poll::new()?;
    let n_events = 5;
    let mut streams = vec![];
    let addr = "localhost:8080";
    for i in 0..n_events {
        let delay = (n_events - i) * 1000;
        let url_path = format!("/{delay}/request-{i}");
        let request = get_req(&amp;url_path);
        let mut stream = std::net::TcpStream::connect(addr)?;
        stream.set_nonblocking(true)?;
        stream.write_all(request.as_bytes())?;
        poll.registry()
            .register(&amp;stream, i, ffi::EPOLLIN | ffi::EPOLLET)?;
        streams.push(stream);
    }</pre>			<p>The first thing we do is to create a new <code>Poll</code> instance. We also specify what number of events we want to create and handle in our example.</p>
			<p>The next step is creating a variable to hold a collection of <code>Vec&lt;TcpStream&gt;</code> objects.</p>
			<p>We also store the address to our local <code>delayserver</code> in a variable called <code>addr</code>.</p>
			<p>The next part is where we create a set of requests that we issue to our <code>delayserver</code>, which will eventually respond to us. For each request, we expect a read event to happen sometime later on in the <code>TcpStream</code> we sent the request on.</p>
			<p>The first thing we do in the loop is set the delay time in milliseconds. Setting the delay to <code>(n_events - i) * 1000</code> simply sets the first request we make to have the longest timeout, so <a id="_idIndexMarker307"/>we should expect the responses to arrive in the reverse order from which they were sent.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For simplicity, we use the index the event will have in the <code>streams</code> collection as its ID. This ID will be the same as the <code>i</code> variable in our loop. For example, in the first loop, <code>i</code> will be <code>0</code>; it will also be the first stream to be pushed to our <code>streams</code> collection, so the index will be <code>0</code> as well. We therefore use <code>0</code> as the identification for this stream/event throughout since retrieving the <code>TcpStream</code> associated with this event will be as simple as indexing to that location in the <code>streams</code> collection.</p>
			<p>The next line, <code>format!("/{delay}/request-{i}")</code>, formats the <em class="italic">path</em> for our <code>GET</code> request. We set the timeout as described previously, and we also set a message where we store the identifier for this event, <code>i</code>, so we can track this event on the server side as well.</p>
			<p>Next up is creating a <code>TcpStream</code>. You’ve probably noticed that the <code>TcpStream</code> in Rust doesn’t accept <code>&amp;str</code> but an argument that implements the <code>ToSocketAddrs</code> trait. This trait is implemented for <code>&amp;str</code> already, so that’s why we can simply write it like we do in this example.</p>
			<p>Before <code>Tcpstream::connect</code> actually opens a socket, it will try to parse the address we pass in as an IP address. If it fails, it will parse it as a domain address and a port number, and then ask the operating system to do a DNS lookup for that address, which it then can use to actually connect to our server. So, you see, there is potentially quite a bit going on when we do a simple connection.</p>
			<p>You probably remember that we discussed some of the nuances of the DNS lookup earlier and the fact that such a call could either be very fast since the operating system already has the information stored in memory or block while waiting for a response from the DNS server. This is a potential downside if you use <code>TcpStream</code> from the standard library if<a id="_idIndexMarker308"/> you want full control over the entire process.</p>
			<p class="callout-heading">TcpStream in Rust and Nagle’s algorithm</p>
			<p class="callout">Here is a little fact for you (I originally intended to call it a “fun fact,” but realized that’s stretching the concept of “fun” just a little too far!). In Rust’s <code>TcpStream</code>, and, more importantly, most APIs that aim to mimic the standard library’s <code>TcpStream</code> such as mio or Tokio, the stream is created with the <code>TCP_NODELAY</code> flag set to <code>false</code>. In practice, this means that Nagle’s algorithm is used, which can cause some issues with latency outliers and possibly reduced throughput on some workloads.</p>
			<p class="callout">Nagle’s algorithm is an algorithm that aims to reduce network congestion by pooling small network packages together. If you look at non-blocking I/O implementations in other languages, many, if not most, disable this algorithm by default. This is not the case in most Rust implementations and is worth being aware of. You can disable it by simply calling <code>TcpStream::set_nodelay(true)</code>. If you try to create your own async library or rely on Tokio/mio, and observe lower throughput than expected or latency problems, it’s worth checking whether this flag is set to <code>true</code> or not.</p>
			<p>To continue with the code, the next step is setting <code>TcpStream</code> to non-blocking by calling <code>Tcp</code><code> </code><code>Stream::set_nonblocking(true)</code>.</p>
			<p>After that, we write our request to the server before we register interest in read events by setting the <code>EPOLLIN</code> flag bit in the <code>interests</code> bitmask.</p>
			<p>For each iteration, we push the stream to the end of our <code>streams</code> collection.</p>
			<p>The next part of the <code>main</code> function is handling incoming events.</p>
			<p>Let’s take a look<a id="_idIndexMarker309"/> at the last part of our <code>main</code> function:</p>
			<pre class="source-code">
let mut handled_events = 0;
    while handled_events &lt; n_events {
        let mut events = Vec::with_capacity(10);
        poll.poll(&amp;mut events, None)?;
        if events.is_empty() {
            println!("TIMEOUT (OR SPURIOUS EVENT NOTIFICATION)");
            continue;
        }
        handled_events += handle_events(&amp;events, &amp;mut streams)?;
    }
    println!("FINISHED");
    Ok(())
}</pre>			<p>The first thing we do is create a variable called <code>handled_events</code> to track how many events we have handled.</p>
			<p>Next is our event loop. We loop as long as the handled events are less than the number of events we expect. Once all events are handled, we exit the loop.</p>
			<p>Inside the loop, we create a <code>Vec&lt;Event&gt;</code> with the capacity to store 10 events. It’s important that we create this using <code>Vec::with_capacity</code> since the operating system will assume that we pass it memory that we’ve allocated. We could choose any number of events here and it would work just fine, but setting too low a number would limit how many events the operating system could notify us about on each wakeup.</p>
			<p>Next is our blocking call to <code>Poll::poll</code>. As you know, this will actually tell the operating system to park our thread and wake us up when an event has occurred.</p>
			<p>If we’re woken up, but there are no events in the list, it’s either a timeout or a spurious event (which could happen, so we need a way to check whether a timeout has actually elapsed if that’s important to us). If that’s the case, we simply call <code>Poll::poll</code> once more.</p>
			<p>If there are events to be handled, we pass these on to the <code>handle_events</code> function together with a mutable reference to our <code>streams</code> collection.</p>
			<p>The last part of <code>main</code> is simply to write <code>FINISHED</code> to the console to let us know we exited <code>main</code> at that point.</p>
			<p>The last bit of code in<a id="_idIndexMarker310"/> this chapter is the <code>handle_events</code> function. This function takes two arguments, a slice of <code>Event</code> structs and a mutable slice of <code>TcpStream</code> objects.</p>
			<p>Let’s take a look at the code before we explain it:</p>
			<pre class="source-code">
fn handle_events(events: &amp;[Event], streams: &amp;mut [TcpStream]) -&gt; Result&lt;usize&gt; {
    let mut handled_events = 0;
    for event in events {
        let index = event.token();
        let mut data = vec![0u8; 4096];
        loop {
            match streams[index].read(&amp;mut data) {
                Ok(n) if n == 0 =&gt; {
                    handled_events += 1;
                    break;
                }
                Ok(n) =&gt; {
                    let txt = String::from_utf8_lossy(&amp;data[..n]);
                    println!("RECEIVED: {:?}", event);
                    println!("{txt}\n------\n");
                }
                // Not ready to read in a non-blocking manner. This could
                // happen even if the event was reported as ready
                Err(e) if e.kind() == io::ErrorKind::WouldBlock =&gt; break,
                Err(e) =&gt; return Err(e),
            }
        }
    }
    Ok(handled_events)
}</pre>			<p>The first thing we do <a id="_idIndexMarker311"/>is to create a variable, <code>handled_events</code>, to track how many events we consider handled on each wakeup. The next step is looping through the events we received.</p>
			<p>In the loop, we retrieve the <em class="italic">token</em> that identifies which <code>TcpStream</code> we received an event for. As we explained earlier in this example, this <em class="italic">token</em> is the same as the index for that particular stream in the <code>streams</code> collection, so we can simply use it to index into our <code>streams</code> collection and retrieve the right <code>TcpStream</code>.</p>
			<p>Before we start reading data, we create a buffer with a size of 4,096 bytes (you can, of course, allocate a larger or smaller buffer for this if you want to).</p>
			<p>We create a loop since we might need to call <code>read</code> multiple times to be sure that we’ve actually drained the buffer. <em class="italic">Remember how important it is to fully drain the buffer when using epoll in </em><em class="italic">edge-triggered mode</em>.</p>
			<p>We match on the result of calling <code>TcpStream::read</code> since we want to take different actions based on the result:</p>
			<ul>
				<li>If we get <code>Ok(n)</code> and the value is 0, we’ve drained the buffer; we consider the event as handled and break out of the loop.</li>
				<li>If we get <code>Ok(n)</code> with a value larger than 0, we read the data to a <code>String</code> and print it out with some formatting. We do not break out of the loop yet since we have to call <code>read</code> until 0 is returned (or an error) to be sure that we’ve drained the buffers fully.</li>
				<li>If we get <code>Err</code> and the<a id="_idIndexMarker312"/> error is of the <code>io::ErrorKind::WouldBlock</code> type, we simply break out of the loop. We don’t consider the event handled yet since<code> WouldBlock</code> indicates that the data transfer is not complete, but there is no data ready right now.</li>
				<li>If we get any other error, we simply return that error and consider it a failure.</li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">There is one more error condition you’d normally want to cover, and that is <code>io::ErrorKind::Interrupted</code>. Reading from a stream could be interrupted by a signal from the operating system. This should be expected and probably not considered a failure. The way to handle this is the same as what we do when we get an error of the <code>WouldBlock</code> type.</p>
			<p>If the <code>read</code> operation is successful, we return the number of events handled.</p>
			<p class="callout-heading">Be careful with using TcpStream::read_to_end</p>
			<p class="callout">You should be careful with using <code>TcpStream::read_to_end</code> or any other function that fully drains the buffer for you when using non-blocking buffers. If you get an error of the <code>io::WouldBlock</code> type, it will report that as an error even though you had several successful reads before you got that error. You have no way of knowing how much data you read successfully other than observing any changes to the <code>&amp;mut Vec</code> you passed in.</p>
			<p>Now, if we run<a id="_idIndexMarker313"/> our program, we should get the following output:</p>
			<pre class="source-code">
RECEIVED: Event { events: 1, epoll_data: 4 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:09 GMT
request-4
------
RECEIVED: Event { events: 1, epoll_data: 3 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:10 GMT
request-3
------
RECEIVED: Event { events: 1, epoll_data: 2 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:11 GMT
request-2
------
RECEIVED: Event { events: 1, epoll_data: 1 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:12 GMT
request-1
------
RECEIVED: Event { events: 1, epoll_data: 0 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:13 GMT
request-0
------
FINISHED</pre>			<p>As you see, the responses are sent in reverse order. You can easily confirm this by looking at the output on the terminal on running the <code>delayserver</code> instance. The output should look like this:</p>
			<pre class="source-code">
#1 - 5000ms: request-0
#2 - 4000ms: request-1
#3 - 3000ms: request-2
#4 - 2000ms: request-3
#5 - 1000ms: request-4</pre>			<p>The ordering might<a id="_idIndexMarker314"/> be different sometimes as the server receives them almost simultaneously, and can choose to handle them in a slightly different order.</p>
			<p>Say we track events on the stream with ID <code>4</code>:</p>
			<ol>
				<li>In <code>send_requests</code>, we assigned the ID <code>4</code> to the last stream we created.</li>
				<li>Socket 4 sends a request to <code>delayserver</code>, setting a delay of 1,000 ms and a message of <code>request-4</code> so we can identify it on the server side.</li>
				<li>We register socket 4 with the event queue, making sure to set the <code>epoll_data</code> field to <code>4</code> so we can identify on what stream the event occurred.</li>
				<li><code>delayserver</code> receives that request and delays the response for 1,000 ms before it sends an <code>HTTP/1.1 200 OK</code> response back, together with the message we originally sent.</li>
				<li><code>epoll_wait</code> wakes up, notifying us that an event is ready. In the <code>epoll_data</code> field of the <code>Event</code> struct, we get back the same data that we passed in when registering the event. This tells us that it was an event on stream 4 that occurred.</li>
				<li>We then read data from stream 4 and print it out.</li>
			</ol>
			<p>In this example, we’ve kept things at a very low level even though we used the standard library to handle the intricacies of establishing a connection. Even though you’ve actually made a raw HTTP request to your own local server, you’ve set up an epoll instance to track events on a <code>TcpStream</code> and you’ve used epoll and syscalls to handle incoming events.</p>
			<p>That’s no small feat – congratulations!</p>
			<p>Before we leave this example, I wanted to point out how few changes we need to make to have our example use mio as the event loop instead of the one we created.</p>
			<p>In the repository under <code>ch04/b-epoll-mio</code>, you’ll see an example where we do the exact same thing using mio instead. It only requires importing a few types from mio instead of our own <a id="_idIndexMarker315"/>modules and making <em class="italic">only five minor changes to </em><em class="italic">our code</em>!</p>
			<p>Not only have you replicated what mio does, but you pretty much know how to use mio to create an event loop as well!</p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor091"/>Summary</h1>
			<p>The concept of epoll, kqueue, and IOCP is pretty simple at a high level, but the devil is in the details. It’s just not that easy to understand and get it working correctly. Even programmers who work on these things will often specialize in one platform (epoll/kqueue or Windows). It’s rare that one person will know all the intricacies of all platforms, and you could probably write a whole book about this subject alone.</p>
			<p>If we summarize what you’ve learned and got firsthand experience with in this chapter, the list is quite impressive:</p>
			<ul>
				<li>You learned a lot about how mio is designed, enabling you to go to that repository and know what to look for and how to get started on that code base much easier than before reading this chapter</li>
				<li>You learned a lot about making syscalls on Linux</li>
				<li>You created an epoll instance, registered events with it, and handled those events</li>
				<li>You learned quite a bit about how epoll is designed and its API</li>
				<li>You learned about edge-triggering and level-triggering, which are extremely low-level, but useful, concepts to have an understanding of outside the context of epoll as well</li>
				<li>You made a raw HTTP request</li>
				<li>You saw how non-blocking sockets behave and how error codes reported by the operating system can be a way of communicating certain conditions that you’re expected to handle</li>
				<li>You learned that not all I/O is equally “blocking” by looking at DNS resolution and file I/O</li>
			</ul>
			<p>That’s pretty good for a single chapter, I think!</p>
			<p>If you dive deeper into the topics we covered here, you’ll soon realize that there are gotchas and rabbit holes everywhere – especially if you expand this example to abstract over epoll, kqueue, and IOCP. You’ll probably end up reading Linus Torvald’s emails on how edge-triggered mode was supposed to work on pipes before you know it.</p>
			<p>At least you now have a good foundation for further exploration. You can expand on our simple example and create a proper event loop that handles connecting, writing, timeouts, and scheduling; you can dive deeper into kqueue and IOCP by looking at how <code>mio</code> solves that problem; or you can be happy that you don’t have to directly deal with it again and appreciate the effort that went into libraries such as <code>mio</code>, <code>polling</code>, and <code>libuv</code>.</p>
			<p>By this point, we’ve gained a lot of knowledge about the basic building blocks of asynchronous programming, so it’s time to start exploring how different programming languages create abstractions over asynchronous operations and use these building blocks to give us as programmers efficient, expressive, and productive ways to write our asynchronous programs.</p>
			<p>First off is one of my favorite examples, where we’ll look into how fibers (or green threads) work by implementing them ourselves.</p>
			<p>You’ve earned a break now. Yeah, go on, the next chapter can wait. Get a cup of tea or coffee and reset so you can start the next chapter with a fresh mind. I promise it will be both fun and interesting.</p>
		</div>
	</body></html>
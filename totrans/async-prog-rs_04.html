<html><head></head><body>
		<div id="_idContainer021">
			<h1 id="_idParaDest-82" class="chapter-number"><a id="_idTextAnchor081"/>4</h1>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor082"/>Create Your Own Event Queue</h1>
			<p>In this chapter, we’ll create a simple version of an event queue using epoll. We’ll take inspiration from <strong class="bold">mio</strong> (<a href="https://github.com/tokio-rs/mio">https://github.com/tokio-rs/mio</a>), a low-level I/O library written in Rust that underpins much of the Rust async ecosystem. Taking inspiration from <strong class="source-inline">mio</strong> has the added benefit of making it easier to dive into their code base if you wish to explore how a real production-ready <span class="No-Break">library works.</span></p>
			<p>By the end of this chapter, you should be able to understand <span class="No-Break">the following:</span></p>
			<ul>
				<li>The difference between blocking and <span class="No-Break">non-blocking I/O</span></li>
				<li>How to use epoll to make your own <span class="No-Break">event queue</span></li>
				<li>The source code of cross-platform event queue libraries such <span class="No-Break">as mio</span></li>
				<li>Why we need an abstraction layer on top of epoll, kqueue, and IOCP if we want a program or library to work across <span class="No-Break">different platforms</span></li>
			</ul>
			<p>We’ve divided the chapter into the <span class="No-Break">following sections:</span></p>
			<ul>
				<li>Design and introduction <span class="No-Break">to epoll</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">ffi</strong></span><span class="No-Break"> module</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">Poll</strong></span><span class="No-Break"> module</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">main</strong></span><span class="No-Break"> program</span></li>
			</ul>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor083"/>Technical requirements</h1>
			<p>This chapter focuses on epoll, which is specific to Linux. Unfortunately, epoll is not part of the <strong class="bold">Portable Operating System Interface</strong> (<strong class="bold">POSIX</strong>) standard, so this example will require you to run Linux and won’t work with macOS, BSD, or Windows <span class="No-Break">operating systems.</span></p>
			<p>If you’re on a machine running Linux, you’re already set and can run the examples without any <span class="No-Break">further steps.</span></p>
			<p>If you’re on Windows, my recommendation is to set up <strong class="bold">WSL</strong> (<a href="https://learn.microsoft.com/en-us/windows/wsl/install">https://learn.microsoft.com/en-us/windows/wsl/install</a>), if you haven’t already, and install Rust on the Linux operating system running <span class="No-Break">on WSL.</span></p>
			<p>If you’re using Mac, you can create a <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) running Linux, for example, by using the <strong class="bold">QEMU</strong>-based <strong class="bold">UTM</strong> application (<a href="https://mac.getutm.app/">https://mac.getutm.app/</a>) or any other solution for managing VMs on <span class="No-Break">a Mac.</span></p>
			<p>A last option is to rent a Linux server (there are even some providers with a free layer), install Rust, and either use an editor such as Vim or Emacs in the console or develop on the remote machine using VS Code through SSH (<a href="https://code.visualstudio.com/docs/remote/ssh">https://code.visualstudio.com/docs/remote/ssh</a>). I personally have good experience with Linode’s offering (<a href="https://www.linode.com/">https://www.linode.com/</a>), but there are many, many other options <span class="No-Break">out there.</span></p>
			<p>It’s theoretically possible to run the examples on the Rust playground, but since we need a delay server, we would have to use a remote delay server service that accepts plain HTTP requests (not HTTPS) and modify the code so that the modules are all in one file instead. It’s possible in a clinch but not <span class="No-Break">really recommended.</span></p>
			<p class="callout-heading">The delay server</p>
			<p class="callout">This example relies on calls made to a server that delays the response for a configurable duration. In the repository, there is a project named <strong class="source-inline">delayserver</strong> in the <span class="No-Break">root folder.</span></p>
			<p class="callout">You can set up the server by simply entering the folder in a separate console window and writing <strong class="source-inline">cargo run</strong>. Just leave the server running in a separate, open terminal window as we’ll use it in <span class="No-Break">our example.</span></p>
			<p class="callout">The <strong class="source-inline">delayserver</strong> program is cross-platform, so it works without any modification on all platforms that Rust supports. If you’re running WSL on Windows, I recommend running the <strong class="source-inline">delayserver</strong> program in WSL as well. Depending on your setup, you might get away with running the server in a Windows console and still be able to reach it when running the example in WSL. Just be aware that it might not work out of <span class="No-Break">the box.</span></p>
			<p class="callout">The server will listen to port <strong class="source-inline">8080</strong> by default and the examples there assume this is the port used. You can change the listening port in the <strong class="source-inline">delayserver</strong> code before you start the server, but just remember to make the same corrections in the <span class="No-Break">example code.</span></p>
			<p class="callout">The actual code for <strong class="source-inline">delayserver</strong> is less than 30 lines, so going through the code should only take a few minutes if you want to see what the <span class="No-Break">server does.</span></p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor084"/>Design and introduction to epoll</h1>
			<p>Okay, so this chapter will be<a id="_idIndexMarker256"/> centered around one main example you can find in the repository under <strong class="source-inline">ch04/a-epoll</strong>. We’ll start by taking a look at how we design <span class="No-Break">our example.</span></p>
			<p>As I mentioned at the start of this chapter, we’ll take our inspiration from <strong class="source-inline">mio</strong>. This has one big upside and one downside. The upside is that we get a gentle introduction to how <strong class="source-inline">mio</strong> is designed, making it much easier to dive into that code base if you want to learn more than what we cover in this example. The downside is that we introduce an overly thick abstraction layer over epoll, including some design decisions that are very specific <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">mio</strong></span><span class="No-Break">.</span></p>
			<p>I think the upsides outweigh the downsides for the simple reason that if you ever want to implement a production-quality event loop, you’ll probably want to look into the implementations that are already out there, and the same goes for if you want to dig deeper into the building blocks of asynchronous programming in Rust. In Rust, <strong class="source-inline">mio</strong> is one of the important libraries underpinning much of the async ecosystem, so gaining a little familiarity with it is an <span class="No-Break">added bonus.</span></p>
			<p>It’s important to note that <strong class="source-inline">mio</strong> is a cross-platform library that creates an abstraction over epoll, kqueue, and IOCP (through Wepoll, as we described in <a href="B20892_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>). Not only that, <strong class="source-inline">mio</strong> supports iOS and Android, and in the future, it will likely support other platforms as well. So, leaving the door open to unify an API over so many different systems is bound to also come with some compromises if you compare it to what you can achieve if you only plan to support <span class="No-Break">one platform.</span></p>
			<p class="callout-heading">mio</p>
			<p class="callout">mio describes itself as a “<em class="italic">fast, low-level I/O library for Rust focusing on non-blocking APIs and event notification for building performance I/O apps with as little overhead as possible over the </em><span class="No-Break"><em class="italic">OS abstractions</em></span><span class="No-Break">.”</span></p>
			<p class="callout">mio drives the event queue in Tokio, which is one of the most popular and widely used asynchronous runtimes in Rust. This means that mio is driving I/O for popular frameworks such as Actix Web (<a href="https://actix.rs/">https://actix.rs/</a>), Warp (<a href="https://github.com/seanmonstar/warp">https://github.com/seanmonstar/warp</a>), and <span class="No-Break">Rocket (</span><a href="https://rocket.rs/"><span class="No-Break">https://rocket.rs/</span></a><span class="No-Break">).</span></p>
			<p class="callout">The version of mio we’ll use as design inspiration in this example is version <strong class="bold">0.8.8</strong>. The API has changed in the past and may change in the future, but the parts of the API we cover here have been stable since 2019, so it’s a good bet that there will not be significant changes to it in the <span class="No-Break">near future.</span></p>
			<p>As is the case with all cross-platform abstractions, it’s often necessary to go the route of choosing the least common denominator. Some choices will limit flexibility and efficiency on one or more platforms in the pursuit of having a unified API that works with all of them. We’ll<a id="_idIndexMarker257"/> discuss some of those choices in <span class="No-Break">this chapter.</span></p>
			<p>Before we go further, let’s create a blank project and give it a name. We’ll refer to it as <strong class="source-inline">a-epoll</strong> going forward, but you will of course need to replace that with the name <span class="No-Break">you choose.</span></p>
			<p>Enter the folder and type the <strong class="source-inline">cargo </strong><span class="No-Break"><strong class="source-inline">init</strong></span><span class="No-Break"> command.</span></p>
			<p>In this example, we’ll divide the project into a few modules, and we’ll split the code up into the <span class="No-Break">following files:</span></p>
			<pre class="source-code">
src
 |-- ffi.rs
 |-- main.rs
 |-- poll.rs</pre>			<p>Their descriptions are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">ffi.rs</strong>: This module will contain the code related to the syscalls we need to communicate with the host <span class="No-Break">operating system</span></li>
				<li><strong class="source-inline">main.rs</strong>: This is the example <span class="No-Break">program itself</span></li>
				<li><strong class="source-inline">poll.rs</strong>: This module contains the main abstraction, which is a thin layer <span class="No-Break">over epoll</span></li>
			</ul>
			<p>Next, create the four files, mentioned in the preceding list, in the <span class="No-Break"><strong class="source-inline">src</strong></span><span class="No-Break"> folder.</span></p>
			<p>In <strong class="source-inline">main.rs</strong>, we need to declare the modules <span class="No-Break">as well:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">a-epoll/src/main.rs</p>
			<pre class="source-code">
mod ffi;
mod poll;</pre>			<p>Now that we have our project set up, we can start by going through how we’ll design the API we’ll use. The main abstraction is in <strong class="source-inline">poll.rs</strong>, so go ahead and open <span class="No-Break">that file.</span></p>
			<p>Let’s start by stubbing out the structures and functions we need. It’s easier to discuss them when we have <a id="_idIndexMarker258"/>them in front <span class="No-Break">of us:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">a-epoll/src/poll.rs</p>
			<pre class="source-code">
use std::{io::{self, Result}, net::TcpStream, os::fd::AsRawFd};
use crate::ffi;
type Events = Vec&lt;ffi::Event&gt;;
pub struct Poll {
  registry: Registry,
}
impl Poll {
  pub fn new() -&gt; Result&lt;Self&gt; {
    todo!()
  }
  pub fn registry(&amp;self) -&gt; &amp;Registry {
    &amp;self.registry
  }
  pub fn poll(&amp;mut self, events: &amp;mut Events, timeout: Option&lt;i32&gt;) -&gt; Result&lt;()&gt; {
    todo!()
  }
}
pub struct Registry {
  raw_fd: i32,
}
impl Registry {
  pub fn register(&amp;self, source: &amp;TcpStream, token: usize, interests: i32) -&gt; Result&lt;()&gt; 
  {
    todo!()
  }
}
impl Drop for Registry {
  fn drop(&amp;mut self) {
    todo!()
  }
}</pre>			<p>We’ve replaced all the implementations with <strong class="source-inline">todo!()</strong> for now. This macro will let us compile the program even though we’ve yet to implement the function body. If our execution ever reaches <strong class="source-inline">todo!()</strong>, it <span class="No-Break">will panic.</span></p>
			<p>The first thing you’ll notice is that we’ll pull the <strong class="source-inline">ffi</strong> module in scope in addition to some types from the <span class="No-Break">standard library.</span></p>
			<p>We’ll also use the <strong class="source-inline">std::io::Result</strong> type as our own <strong class="source-inline">Result</strong> type. It’s convenient since most errors will stem from one of our calls into the operating system, and an operating system error can be mapped to an <span class="No-Break"><strong class="source-inline">io::Error</strong></span><span class="No-Break"> type.</span></p>
			<p>There are two main abstractions over epoll. One is a structure called <strong class="source-inline">Poll</strong> and the other is called <strong class="source-inline">Registry</strong>. The name and functionality of these functions are the same as they are in <strong class="source-inline">mio</strong>. Naming abstractions such as these is surprisingly difficult, and both constructs could very well have had a different name, but let’s lean on the fact that someone else has spent time on this before us and decided to go with these in <span class="No-Break">our example.</span></p>
			<p><strong class="source-inline">Poll</strong> is a struct that <a id="_idIndexMarker259"/>represents the event queue itself. It has a <span class="No-Break">few methods:</span></p>
			<ul>
				<li><strong class="source-inline">new</strong>: Creates a new <span class="No-Break">event queue</span></li>
				<li><strong class="source-inline">registry</strong>: Returns a reference to the registry that we can use to register interest to be notified about <span class="No-Break">new events</span></li>
				<li><strong class="source-inline">poll</strong>: Blocks the thread it’s called on until an event is ready or it times out, whichever <span class="No-Break">occurs first</span></li>
			</ul>
			<p><strong class="source-inline">Registry</strong> is the other half of the equation. While <strong class="source-inline">Poll</strong> represents the event queue, <strong class="source-inline">Registry</strong> is a handle that allows us to register interest in <span class="No-Break">new events.</span></p>
			<p><strong class="source-inline">Registry</strong> will only have one method: <strong class="source-inline">register</strong>. Again, we mimic the API <strong class="source-inline">mio</strong> uses (<a href="https://docs.rs/mio/0.8.8/mio/struct.Registry.html">https://docs.rs/mio/0.8.8/mio/struct.Registry.html</a>), and instead of accepting a predefined list of methods for registering different interests, we accept an <strong class="source-inline">interests</strong> argument, which will indicate what kind of events we want our event queue to keep <span class="No-Break">track of.</span></p>
			<p>One more thing to note is that we won’t use a generic type for all sources. We’ll only implement this for <strong class="source-inline">TcpStream</strong>, even though there are many things we could potentially track with an <span class="No-Break">event queue.</span></p>
			<p>This is especially true when we want to make this cross-platform since, depending on the platforms you want to support, there are many types of event sources we might want <span class="No-Break">to track.</span></p>
			<p>mio solves this by having <strong class="source-inline">Registry::register</strong> accept an object implementing the <strong class="source-inline">Source</strong> trait that <strong class="source-inline">mio</strong> defines. As long as you implement this trait for the source, you can use the event queue to track events on it. </p>
			<p>In the following pseudo-code, you’ll get an idea of how we plan to use <span class="No-Break">this API:</span></p>
			<pre class="source-code">
let queue = Poll::new().unwrap();
let id = 1;
// register interest in events on a TcpStream
queue.registry().register(&amp;stream, id, ...).unwrap();
let mut events = Vec::with_capacity(1);
// This will block the curren thread
queue.poll(&amp;mut events, None).unwrap();
//...data is ready on one of the tracked streams</pre>			<p>You might wonder<a id="_idIndexMarker260"/> why we need the <strong class="source-inline">Registry</strong> struct <span class="No-Break">at all.</span></p>
			<p>To answer that question, we need to remember that <strong class="source-inline">mio</strong> abstracts over epoll, kqueue, and IOCP. It does this by making <strong class="source-inline">Registry</strong> wrap around a <strong class="source-inline">Selector</strong> object. The <strong class="source-inline">Selector</strong> object is conditionally compiled so that every platform has its own <strong class="source-inline">Selector</strong> implementation corresponding to the relevant syscalls to make IOCP, kqueue, and epoll do the <span class="No-Break">same thing.</span></p>
			<p><strong class="source-inline">Registry</strong> implements one important method we won’t implement in our example, called <strong class="source-inline">try_clone</strong>. The reason we won’t implement this is that we don’t need it to understand how an event loop like this works and we want to keep the example simple and easy to understand. However, this method is important for understanding why the responsibility of registering events and the queue itself <span class="No-Break">is divided.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">By moving the concern of registering interests to a separate struct like this, users can call <strong class="source-inline">Registry::try_clone</strong> to get an owned <strong class="source-inline">Registry</strong> instance. This instance can be passed to, or shared through <strong class="source-inline">Arc&lt;Registry&gt;</strong> with, other threads, allowing multiple threads to register interest to the same <strong class="source-inline">Poll</strong> instance even when <strong class="source-inline">Poll</strong> is blocking another thread while waiting for new events to happen <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">Poll::poll</strong></span><span class="No-Break">.</span></p>
			<p><strong class="source-inline">Poll::poll</strong> requires exclusive access since it takes a <strong class="source-inline">&amp;mut self</strong>, so when we’re waiting for events in <strong class="source-inline">Poll::poll</strong>, there is no way to register interest from a different thread at the same time if we rely on using <strong class="source-inline">Poll</strong> to register interest, since that will be prevented by Rust’s <span class="No-Break">type system.</span></p>
			<p>It also makes it effectively impossible to have multiple threads waiting for events by calling <strong class="source-inline">Poll::poll</strong> on the same instance in any meaningful way since it would require synchronization that essentially would make each call <span class="No-Break">sequential anyway.</span></p>
			<p>The design lets users interact with the queue from potentially many threads by registering interest, while one thread makes the blocking call and handles the notifications from <a id="_idIndexMarker261"/>the <span class="No-Break">operating system.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">The fact that <strong class="source-inline">mio</strong> doesn’t enable you to have multiple threads that are blocked on the same call to <strong class="source-inline">Poll::poll</strong> isn’t a limitation due to epoll, kqueue, or IOCP. They all allow for the scenario that many threads will call <strong class="source-inline">Poll::poll</strong> on the same instance and get notifications on events in the queue. epoll even allows specific flags to dictate whether the operating system should wake up only one or all threads that wait for notification (specifically the <span class="No-Break"><strong class="source-inline">EPOLLEXCLUSIVE</strong></span><span class="No-Break"> flag).</span></p>
			<p class="callout">The problem is partly about how the different platforms decide which threads to wake when there are many of them waiting for events on the same queue, and partly about the fact that there doesn’t seem to be a huge interest in that functionality. For example, epoll will, by default, wake all threads that block on <strong class="source-inline">Poll</strong>, while Windows, by default, will only wake up one thread. You can modify this behavior to some extent, and there have been ideas on implementing a <strong class="source-inline">try_clone</strong> method on <strong class="source-inline">Poll</strong> as well in the future. For now, the design is like we outlined, and we will stick to that in our example <span class="No-Break">as well.</span></p>
			<p>This brings us to another<a id="_idIndexMarker262"/> topic we should cover before we start implementing <span class="No-Break">our example.</span></p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor085"/>Is all I/O blocking?</h2>
			<p>Finally, a question that’s easy<a id="_idIndexMarker263"/> to answer. The answer is a big, resounding… maybe. The thing is that not all I/O operations will block in the sense that the operating system will park the calling thread and it will be more efficient to switch to another task. The reason for this is that the operating system is smart and will cache a lot of information in memory. If information is in the cache, a syscall requesting that information would simply return immediately with the data, so forcing a context switch or any rescheduling of the current task might be less efficient than just handling the data synchronously. The problem is that there is no way to know for sure whether I/O is blocking and it depends on what <span class="No-Break">you’re doing.</span></p>
			<p>Let me give you <span class="No-Break">two examples.</span></p>
			<h3>DNS lookup</h3>
			<p>When creating a <a id="_idIndexMarker264"/>TCP connection, one of the first things that happens is<a id="_idIndexMarker265"/> that you need to convert a typical address such as <a href="https://www.google.com">www.google.com</a> to an IP address such as <strong class="source-inline">216.58.207.228</strong>. The operating system maintains a mapping of local addresses and addresses it’s previously looked up in a cache and will be able to resolve them almost immediately. However, the first time you look up an unknown address, it might have to make a call to a DNS server, which takes a lot of time, and the OS will park the calling thread while waiting for the response if it’s not handled in a <span class="No-Break">non-blocking manner.</span></p>
			<h3>File I/O</h3>
			<p>Files on the local <a id="_idIndexMarker266"/>filesystem are another area where the operating system<a id="_idIndexMarker267"/> performs quite a bit of caching. Smaller files that are frequently read are often cached in memory, so requesting that file might not block at all. If you have a web server that serves static files, there is most likely a rather limited set of small files you’ll be serving. The chances are that these are cached in memory. However, there is no way to know for sure – if an operating system is running low on memory, it might have to map memory pages to the hard drive, which makes what would normally be a very fast memory lookup excruciatingly slow. The same is true if there is a huge number of small files that are accessed randomly, or if you serve very large files since the operating system will only cache a limited amount of information. You’ll also encounter this kind of unpredictability if you have many unrelated processes running on the same operating system as it might not cache the information that’s important <span class="No-Break">to you.</span></p>
			<p>A popular way of handling these cases is to forget about non-blocking I/O, and actually make a blocking call instead. You don’t want to do these calls in the same thread that runs a <strong class="source-inline">Poll</strong> instance (since every small delay will block all tasks), but you would probably relegate that<a id="_idIndexMarker268"/> task to a <strong class="bold">thread pool</strong>. In the thread pool, you have a limited number of threads that are tasked with making regular blocking calls for things such as DNS lookups or <span class="No-Break">file I/O.</span></p>
			<p>An example of a runtime that does exactly this is <strong class="source-inline">libuv</strong> (<a href="http://docs.libuv.org/en/v1.x/threadpool.html#threadpool">http://docs.libuv.org/en/v1.x/threadpool.html#threadpool</a>). <strong class="source-inline">libuv</strong> is the asynchronous I/O library that Node.js is <span class="No-Break">built upon.</span></p>
			<p>While its scope is larger than <strong class="source-inline">mio</strong> (which only cares about non-blocking I/O), <strong class="source-inline">libuv</strong> is to <strong class="source-inline">Node</strong> in JavaScript what <strong class="source-inline">mio</strong> is to Tokio <span class="No-Break">in Rust.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">The reason for doing file I/O in a thread pool is that there have historically been poor cross-platform APIs for non-blocking file I/O. While it’s true that many runtimes choose to relegate this task to a thread pool making blocking calls to the OS, it might not be true in the future as the OS APIs evolve <span class="No-Break">over time.</span></p>
			<p>Creating a thread pool to handle these cases is outside the scope of this example (even <strong class="source-inline">mio</strong> considers this outside its scope, just to be clear). We’ll focus on showing how epoll works and<a id="_idIndexMarker269"/> mention<a id="_idIndexMarker270"/> these topics in the text, even though we won’t actually implement a solution for them in <span class="No-Break">this example.</span></p>
			<p>Now that we’ve covered a lot of basic information about epoll, mio, and the design of our example, it’s time to write some code and see for ourselves how this all works <span class="No-Break">in practice.</span></p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor086"/>The ffi module</h1>
			<p>Let’s start with the <a id="_idIndexMarker271"/>modules that don’t depend on any others and work our way from there. The <strong class="source-inline">ffi</strong> module contains mappings to the syscalls and data structures we need to communicate with the operating system. We’ll also explain how epoll works in detail once we have presented <span class="No-Break">the syscalls.</span></p>
			<p>It’s only a few lines of code, so I’ll place the first part here so it’s easier to keep track of where we are in the file since there’s quite a bit to explain. Open the <strong class="source-inline">ffi.rs</strong> file and write the following lines <span class="No-Break">of code:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/ffi.rs</p>
			<pre class="source-code">
pub const EPOLL_CTL_ADD: i32 = 1;
pub const EPOLLIN: i32 = 0x1;
pub const EPOLLET: i32 = 1 &lt;&lt; 31;
#[link(name = "c")]
extern "C" {
  pub fn epoll_create(size: i32) -&gt; i32;
  pub fn close(fd: i32) -&gt; i32;
  pub fn epoll_ctl(epfd: i32, op: i32, fd: i32, event: *mut Event) -&gt; i32;
  pub fn epoll_wait(epfd: i32, events: *mut Event, maxevents: i32, timeout: i32) -&gt; i32;
}</pre>			<p>The first thing you’ll notice is that we declare a few constants called <strong class="source-inline">EPOLL_CTL_ADD</strong>, <strong class="source-inline">EPOLLIN</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">EPOLLET</strong></span><span class="No-Break">.</span></p>
			<p>I’ll get back to explaining what these constants are in a moment. Let’s first take a look at the syscalls we need to make. Fortunately, we’ve already covered syscalls in detail, so you already know the basics of <strong class="source-inline">ffi</strong> and why we link to C in the <span class="No-Break">preceding code:</span></p>
			<ul>
				<li><strong class="source-inline">epoll_create</strong> is the syscall we make to create an epoll queue. You can find the documentation for it at <a href="https://man7.org/linux/man-pages/man2/epoll_create.2.html">https://man7.org/linux/man-pages/man2/epoll_create.2.html</a>. This method accepts one argument called <strong class="source-inline">size</strong>, but <strong class="source-inline">size</strong> is there only for historical reasons. The argument will be ignored but must have a value larger <span class="No-Break">than </span><span class="No-Break"><em class="italic">0</em></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">close</strong> is the syscall we need to close the file descriptor we get when we create our <strong class="source-inline">epoll</strong> instance, so we release our resources properly. You can read the documentation for the syscall <span class="No-Break">at </span><a href="https://man7.org/linux/man-pages/man2/close.2.html"><span class="No-Break">https://man7.org/linux/man-pages/man2/close.2.html</span></a><span class="No-Break">.</span></li>
				<li><strong class="source-inline">epoll_ctl</strong> is the control interface we use to perform operations on our epoll instance. This is the call we use to register interest in events on a source. It supports three main operations: <em class="italic">add</em>, <em class="italic">modify</em>, or <em class="italic">delete</em>. The first argument, <strong class="source-inline">epfd</strong>, is the epoll file descriptor we want to perform operations on. The second argument, <strong class="source-inline">op</strong>, is the argument where we specify whether we want to perform an <em class="italic">add</em>, <em class="italic">modify</em>, or <span class="No-Break"><em class="italic">delete</em></span><span class="No-Break"> operation</span></li>
				<li>In our case, we’re<a id="_idIndexMarker272"/> only interested in adding interest for events, so we’ll only pass in <strong class="source-inline">EPOLL_CTL_ADD</strong>, which is the value to indicate that we want to perform an <em class="italic">add</em> operation. <strong class="source-inline">epoll_event</strong> is a little more complicated, so we’ll discuss it in more detail. It does two important things for us: first, the <strong class="source-inline">events</strong> field indicates what kind of events we want to be notified of and it can also modify the behavior of <em class="italic">how</em> and <em class="italic">when</em> we get notified. Second, the <strong class="source-inline">data</strong> field passes on a piece of data to the kernel that it will return to us when an event occurs. The latter is important since we need this data to identify exactly what event occurred since that’s the only information we’ll receive in return that can identify what source we got the notification for. You can find the documentation for this syscall <span class="No-Break">here: </span><a href="https://man7.org/linux/man-pages/man2/epoll_ctl.2.html"><span class="No-Break">https://man7.org/linux/man-pages/man2/epoll_ctl.2.html</span></a><span class="No-Break">.</span></li>
				<li><strong class="source-inline">epoll_wait</strong> is the call that will block the current thread and wait until one of two things happens: we receive a notification that an event has occurred or it times out. <strong class="source-inline">epfd</strong> is the epoll file descriptor identifying the queue we made with <strong class="source-inline">epoll_create</strong>. <strong class="source-inline">events</strong> is an array of the same <strong class="source-inline">Event</strong> structure we used in <strong class="source-inline">epoll_ctl</strong>. The difference is that the <strong class="source-inline">events</strong> field now gives us information about what event <em class="italic">did</em> occur, and importantly the <strong class="source-inline">data</strong> field contains the same data that we passed in when we <span class="No-Break">registered interest</span></li>
				<li>For example, the <strong class="source-inline">data</strong> field lets us identify which file descriptor has data that’s ready to be read. The <strong class="source-inline">maxevents</strong> arguments tell the kernel how many events we have reserved space for in our array. Lastly, the <strong class="source-inline">timeout</strong> argument tells the kernel how long we will wait for events before it will wake us up again so we don’t potentially block forever. You can read the documentation for <strong class="source-inline">epoll_wait</strong> <span class="No-Break">at </span><a href="https://man7.org/linux/man-pages/man2/epoll_wait.2.html"><span class="No-Break">https://man7.org/linux/man-pages/man2/epoll_wait.2.html</span></a><span class="No-Break">.</span></li>
			</ul>
			<p>The last part of the <a id="_idIndexMarker273"/>code in this file is the <span class="No-Break"><strong class="source-inline">Event</strong></span><span class="No-Break"> struct:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/ffi.rs</p>
			<pre class="source-code">
#[derive(Debug)]
#[repr(C, packed)]
pub struct Event {
    pub(crate) events: u32,
    // Token to identify event
    pub(crate) epoll_data: usize,
}
impl Event {
    pub fn token(&amp;self) -&gt; usize {
        self.epoll_data
    }
}</pre>			<p>This structure is used to communicate to the operating system in <strong class="source-inline">epoll_ctl</strong>, and the operating system uses the same structure to communicate with us <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">epoll_wait</strong></span><span class="No-Break">.</span></p>
			<p>Events are defined as a <strong class="source-inline">u32</strong>, but it’s more than just a number. This field is what we call a <strong class="bold">bitmask</strong>. I’ll take<a id="_idIndexMarker274"/> the time to explain bitmasks in a later section since it’s common in most syscalls and not something everyone has encountered before. In simple terms, it’s a way to use the bit representation as a set of yes/no flags to indicate whether an option has been chosen <span class="No-Break">or not.</span></p>
			<p>The different options are described in the link I provided for the <strong class="source-inline">epoll_ctl</strong> syscall. I won’t explain all of them <a id="_idIndexMarker275"/>in detail here, but just cover the ones <span class="No-Break">we’ll use:</span></p>
			<ul>
				<li><strong class="source-inline">EPOLLIN</strong> represents a bitflag indicating we’re interested in read operations on the <span class="No-Break">file handle</span></li>
				<li><strong class="source-inline">EPOLLET</strong> represents a bitflag indicating that we’re interested in getting events notified with epoll set to an <span class="No-Break">edge-triggered mode</span></li>
			</ul>
			<p>We’ll get back to explaining bitflags, bitmasks, and what edge-triggered mode really means in a moment, but let’s just finish with the <span class="No-Break">code first.</span></p>
			<p>The last field on the <strong class="source-inline">Event</strong> struct is <strong class="source-inline">epoll_data</strong>. This field is defined as a union in the documentation. A union is much like an enum, but in contrast to Rust’s enums, it doesn’t carry any information on what type it is, so it’s up to us to make sure we know what type of data <span class="No-Break">it holds.</span></p>
			<p>We use this field to simply hold a <strong class="source-inline">usize</strong> so we can pass in an integer identifying each event when we register interest using <strong class="source-inline">epoll_ctl</strong>. It would be perfectly fine to pass in a pointer instead – just as long as we make sure that the pointer is still valid when it’s returned to us <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">epoll_wait</strong></span><span class="No-Break">.</span></p>
			<p>We can think of this field as a token, which is exactly what <strong class="source-inline">mio</strong> does, and to keep the API as similar as possible, we copy <strong class="source-inline">mio</strong> and provide a <strong class="source-inline">token</strong> method on the struct to get <span class="No-Break">this value.</span></p>
			<p class="callout-heading">What does #[repr(packed)] do?</p>
			<p class="callout">The <strong class="source-inline">#[repr(packed)]</strong> annotation is new to us. Usually, a struct will have padding either between fields or at the end of the struct. This happens even when we’ve <span class="No-Break">specified </span><span class="No-Break"><strong class="source-inline">#[repr(C)]</strong></span><span class="No-Break">.</span></p>
			<p class="callout">The reason has to do with efficient access to the data stored in the struct by not having to make multiple fetches to get the data stored in a struct field. In the case of the <strong class="source-inline">Event</strong> struct, the usual padding would be adding 4 bytes of padding at the end of the <strong class="source-inline">events</strong> field. When the operating system expects a packed struct for <strong class="source-inline">Event</strong>, and we give it a padded one, it will write parts of <strong class="source-inline">event_data</strong> to the padding between the fields. When you try to read <strong class="source-inline">event_data</strong> later on, you’ll end up only reading the last part of <strong class="source-inline">event_data</strong>, which happened to overlap and get the <span class="No-Break">wrong data</span></p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B20892_04_0.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="callout">The fact that the<a id="_idIndexMarker276"/> operating systemexpects a packed <strong class="source-inline">Event</strong> struct isn’t obvious by reading the manpages for Linux, so you have to read the appropriate C header files to know for sure. You could of course simply rely on the <strong class="source-inline">libc</strong> crate (<a href="https://github.com/rust-lang/libc">https://github.com/rust-lang/libc</a>), which we would do too if we weren’t here to learn things like this <span class="No-Break">for ourselves.</span></p>
			<p>So, now that we’ve finished walking through the code, there are a few topics that we promised to<a id="_idIndexMarker277"/> get <span class="No-Break">back to.</span></p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor087"/>Bitflags and bitmasks</h2>
			<p>You’ll encounter<a id="_idIndexMarker278"/> this all the<a id="_idIndexMarker279"/> time when making<a id="_idIndexMarker280"/> syscalls (in fact, the<a id="_idIndexMarker281"/> concept of bitmasks is pretty common in low-level programming). A bitmask is a way to treat each bit as a switch, or a flag, to indicate that an option is either enabled <span class="No-Break">or disabled.</span></p>
			<p>An integer, such as <strong class="source-inline">i32</strong>, can be expressed as 32 bits. <strong class="source-inline">EPOLLIN</strong> has the hex value of <strong class="source-inline">0x1</strong> (which is simply 1 in decimal). Represented in bits, this would look <span class="No-Break">like </span><span class="No-Break"><strong class="source-inline">00000000000000000000000000000001</strong></span><span class="No-Break">.</span></p>
			<p><strong class="source-inline">EPOLLET</strong>, on the other hand, has a value of <strong class="source-inline">1 &lt;&lt; 31</strong>. This simply means the bit representation of the decimal number 1, shifted 31 bits to the left. The decimal number 1 is incidentally the same as <strong class="source-inline">EPOLLIN</strong>, so by looking at that representation and shifting the bits 31 times to the left, we get a number with the bit representation <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">10000000000000000000000000000000</strong></span><span class="No-Break">.</span></p>
			<p>The way we use bitflags is that we use the OR operator, <strong class="source-inline">|</strong>, and by OR’ing the values together, we get a bitmask with each flag we OR’ed set to 1. In our example, the bitmask would look <span class="No-Break">like </span><span class="No-Break"><strong class="source-inline">10000000000000000000000000000001</strong></span><span class="No-Break">.</span></p>
			<p>The receiver of the bitmask (in this case, the operating system) can then do an opposite operation, check which flags are set, and <span class="No-Break">act accordingly.</span></p>
			<p>We can create a very simple example in code to show how this works in practice (you can simply run this in the Rust playground or create a new empty project for throwaway experiments such <span class="No-Break">as this):</span></p>
			<pre class="source-code">
fn main() {
  let bitflag_a: i32 = 1 &lt;&lt; 31;
  let bitflag_b: i32 = 0x1;
  let bitmask: i32 = bitflag_a | bitflag_b;
  println!("{bitflag_a:032b}");
  println!("{bitflag_b:032b}");
  println!("{bitmask:032b}");
  check(bitmask);
}
fn check(bitmask: i32) {
  const EPOLLIN: i32 = 0x1;
  const EPOLLET: i32 = 1 &lt;&lt; 31;
  const EPOLLONESHOT: i32 = 0x40000000;
  let read = bitmask &amp; EPOLLIN != 0;
  let et = bitmask &amp; EPOLLET != 0;
  let oneshot = bitmask &amp; EPOLLONESHOT != 0;
  println!("read_event? {read}, edge_triggered: {et}, oneshot?: {oneshot}")
}</pre>			<p>This code will output <span class="No-Break">the following:</span></p>
			<pre class="source-code">
10000000000000000000000000000000
00000000000000000000000000000001
10000000000000000000000000000001
read_event? true, edge_triggered: true, oneshot?: false</pre>			<p>The <a id="_idIndexMarker282"/>next <a id="_idIndexMarker283"/>topic <a id="_idIndexMarker284"/>we<a id="_idIndexMarker285"/> will introduce in this chapter is the concept of edge-triggered events, which probably need <span class="No-Break">some explanation.</span></p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor088"/>Level-triggered versus edge-triggered events</h2>
			<p>In a perfect world, we <a id="_idIndexMarker286"/>wouldn’t<a id="_idIndexMarker287"/> need to discuss this, but <a id="_idIndexMarker288"/>when working with epoll, it’s almost impossible to avoid having to know about the difference. It’s not obvious by reading the documentation, especially not if you haven’t had previous experience with these terms before. The interesting part of this is that it allows us to create a parallel between how events are handled in epoll and how events are handled at the <span class="No-Break">hardware level.</span></p>
			<p>epoll can notify events in a <strong class="bold">level-triggered</strong> or <strong class="bold">edge-triggered</strong> mode. If your main experience is programming in high-level languages, this must sound very obscure (it did to me when I first learned about it), but bear with me. In the <strong class="source-inline">events</strong> bitmask on the <strong class="source-inline">Event</strong> struct, we set the <strong class="source-inline">EPOLLET</strong> flag to get notified in edge-triggered mode (the default if you specify nothing <span class="No-Break">is level-triggered).</span></p>
			<p>This way of modeling event notification and event handling has a lot of similarities to how computers <span class="No-Break">handle interrupts.</span></p>
			<p>Level-triggered means that the answer to the question “Has an event happened” is true as long as the electrical signal on an interrupt line is reported as high. If we translate this to our example, <em class="italic">a read event has occurred as long as there is data in the buffer associated with the </em><span class="No-Break"><em class="italic">file handle.</em></span></p>
			<p>When handling interrupts, you would clear the interrupt by servicing whatever hardware caused it, or you could mask the interrupt, which simply disables interrupts on that line until it’s explicitly unmasked <span class="No-Break">later on.</span></p>
			<p>In our example, we clear the <em class="italic">interrupt</em> by draining all the data in the buffer by reading it. When the buffer is drained, the answer to our question changes <span class="No-Break">to </span><span class="No-Break"><em class="italic">false</em></span><span class="No-Break">.</span></p>
			<p>When using epoll in its default mode, which is level-triggered, we can encounter a case where we get multiple notifications on the same event since we haven’t had time to drain the buffer yet (remember, as long as there is data in the buffer, epoll will notify you over and over again). This is especially apparent when we have one thread that reports events and then delegates the task of handling the event (reading from the stream) to other worker threads since epoll will happily report that an event is ready even though we’re in the process of <span class="No-Break">handling it.</span></p>
			<p>To remedy this, epoll has a flag <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">EPOLLONESHOT</strong></span><span class="No-Break">.</span></p>
			<p><strong class="source-inline">EPOLLONESHOT</strong> tells epoll that once we receive an event on this file descriptor, it should disable the file descriptor in the interest list. It won’t remove it, but we won’t get any more notifications on that file descriptor unless we explicitly reactivate it by calling <strong class="source-inline">epoll_ctl</strong> with the <strong class="source-inline">EPOLL_CTL_MOD</strong> argument and a <span class="No-Break">new bitmask.</span></p>
			<p>If we didn’t add this flag, the following could happen: if <em class="italic">thread 1</em> is the thread where we call <strong class="source-inline">epoll_wait</strong>, then once it receives a notification about a read event, it starts a task in <em class="italic">thread 2</em> to read from that file descriptor, and then calls <strong class="source-inline">epoll_wait</strong> again to get notifications on new events. In this case, the call to <strong class="source-inline">epoll_wait</strong> would return again and tell us that data is ready on the same file descriptor since we haven’t had the time to drain the buffer on that file descriptor yet. We know that the task is taken care of by <strong class="source-inline">thread 2</strong>, but we still get a notification. Without additional synchronization and logic, we could end up giving the task of reading from the same file descriptor to <em class="italic">thread 3</em>, which could cause problems that are quite hard <span class="No-Break">to debug.</span></p>
			<p>Using <strong class="source-inline">EPOLLONESHOT</strong> solves this problem since <em class="italic">thread 2</em> will have to reactivate the file descriptor in the event queue once it’s done handling its task, thereby telling our epoll queue that it’s finished with it and that we are interested in getting notifications on that file <span class="No-Break">descriptor again.</span></p>
			<p>To go back to our <a id="_idIndexMarker289"/>original analogy <a id="_idIndexMarker290"/>of<a id="_idIndexMarker291"/> hardware interrupts, <strong class="source-inline">EPOLLONESHOT</strong> could be thought of as masking an interrupt. You haven’t actually cleared the source of the event notification yet, but you don’t want further notifications until you’ve done that and explicitly unmask it. In epoll, the <strong class="source-inline">EPOLLONESHOT</strong> flag will disable notifications on the file descriptor until you explicitly enable it by calling <strong class="source-inline">epoll_ctl</strong> with the <strong class="source-inline">op</strong> argument set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">EPOLL_CTL_MOD</strong></span><span class="No-Break">.</span></p>
			<p>Edge-triggered means that the answer to the question “Has an event happened” is true only if the electrical signal has <em class="italic">changed</em> from low to high. If we translate this to our example: a read event has occurred when the buffer has changed from <em class="italic">having no data</em> to <em class="italic">having data</em>. As long as there is data in the buffer, no new events will be reported. You still handle the event by draining all the data from the socket, but you won’t get a new notification until the buffer is fully drained and then filled with <span class="No-Break">new data.</span></p>
			<p>Edge-triggered mode also comes with some pitfalls. The biggest one is that if you don’t drain the buffer properly, you will never receive a notification on that file <span class="No-Break">handle again.</span></p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B20892_04_1.jpg" alt="Figure 4.1 – Edge-triggered versus level-triggered events"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Edge-triggered versus level-triggered events</p>
			<p>mio<a id="_idIndexMarker292"/> doesn’t, at<a id="_idIndexMarker293"/> the<a id="_idIndexMarker294"/> time of writing, support <strong class="source-inline">EPOLLONESHOT</strong> and uses epoll in an edge-triggered mode, which we will do as well in <span class="No-Break">our example.</span></p>
			<p class="callout-heading">What about waiting on epoll_wait in multiple threads?</p>
			<p class="callout">As long as we only have one <strong class="source-inline">Poll</strong> instance, we avoid the problems and subtleties of having multiple threads calling <strong class="source-inline">epoll_wait</strong> on the same epoll instance. Using level-triggered events will wake up all threads that are waiting in the <strong class="source-inline">epoll_wait</strong> call, causing all of them to try to handle the event (this is often referred to as the problem of the thundering heard). epoll has another flag you can set, called <strong class="source-inline">EPOLLEXCLUSIVE</strong>, that solves this issue. Events that are set to be edge-triggered will only wake up one of the threads blocking in <strong class="source-inline">epoll_wait</strong> by default and avoid <span class="No-Break">this issue.</span></p>
			<p class="callout">Since we only use one <strong class="source-inline">Poll</strong> instance from a single thread, this will not be an issue <span class="No-Break">for us.</span></p>
			<p>I know and understand that this sounds very complex. The general concept of event queues is rather simple, but the details can get a bit complex. That said, epoll is one of the most complex APIs in my experience since the API has clearly been evolving over time to adapt the original design to suit modern requirements, and there is really no easy way to actually use and understand it correctly without covering at least the topics we <span class="No-Break">covered here.</span></p>
			<p>One word of comfort here is that both kqueue and IOCP have APIs that are easier to understand. There is also the fact that Unix has a new asynchronous I/O interface called <strong class="source-inline">io_uring</strong> that will be more and more and more common in <span class="No-Break">the future.</span></p>
			<p>Now that we’ve<a id="_idIndexMarker295"/> covered the hard<a id="_idIndexMarker296"/> part <a id="_idIndexMarker297"/>of this chapter and gotten a high-level overview of how epoll works, it’s time to implement our mio-inspired API <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">poll.rs</strong></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor089"/>The Poll module</h1>
			<p>If you haven’t written<a id="_idIndexMarker298"/> or copied the code we presented in the <em class="italic">Design and introduction to epoll</em> section, it’s time to do it now. We’ll implement all the functions where we just had <span class="No-Break"><strong class="source-inline">todo!()</strong></span><span class="No-Break"> earlier.</span></p>
			<p>We start by implementing the methods on our <strong class="source-inline">Poll</strong> struct. First up is opening the <strong class="source-inline">impl Poll</strong> block and implementing the <span class="No-Break"><strong class="source-inline">new</strong></span><span class="No-Break"> function:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/poll.rs</p>
			<pre class="source-code">
impl Poll {
    pub fn new() -&gt; Result&lt;Self&gt; {
        let res = unsafe { ffi::epoll_create(1) };
        if res &lt; 0 {
            return Err(io::Error::last_os_error());
        }
        Ok(Self {
            registry: Registry { raw_fd: res },
        })
    }</pre>			<p>Given the thorough introduction to epoll in the <em class="italic">The ffi module</em> section, this should be pretty straightforward. We call <strong class="source-inline">ffi::epoll_create</strong> with an argument of 1 (remember, the<a id="_idIndexMarker299"/> argument is ignored but must have a non-zero value). If we get any errors, we ask the operating system to report the last error for our process and return that. If the call succeeds, we return a new <strong class="source-inline">Poll</strong> instance that simply wraps around our registry that holds the epoll <span class="No-Break">file descriptor.</span></p>
			<p>Next up is our registry method, which simply hands out a reference to the inner <span class="No-Break"><strong class="source-inline">Registry</strong></span><span class="No-Break"> struct:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/poll.rs</p>
			<pre class="source-code">
    pub fn registry(&amp;self) -&gt; &amp;Registry {
        &amp;self.registry
    }</pre>			<p>The last method on <strong class="source-inline">Poll</strong> is the most interesting one. It’s the <strong class="source-inline">poll</strong> function, which will park the current thread and tell the operating system to wake it up when an event has happened on a source we’re tracking, or the timeout has elapsed, whichever comes first. We also close the <strong class="source-inline">impl Poll</strong> <span class="No-Break">block here:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/poll.rs</p>
			<pre class="source-code">
  pub fn poll(&amp;mut self, events: &amp;mut Events, timeout: Option&lt;i32&gt;) -&gt; Result&lt;()&gt; {
    let fd = self.registry.raw_fd;
    let timeout = timeout.unwrap_or(-1);
    let max_events = events.capacity() as i32;
    let res = unsafe { ffi::epoll_wait(fd, events.as_mut_ptr(), max_events, timeout) };
    if res &lt; 0 {
      return Err(io::Error::last_os_error());
    };
    unsafe { events.set_len(res as usize) };
    Ok(())
  }
}</pre>			<p>The first thing we do is <a id="_idIndexMarker300"/>to get the raw file descriptor for the event queue and store it in the <span class="No-Break"><strong class="source-inline">fd</strong></span><span class="No-Break"> variable.</span></p>
			<p>Next is our <strong class="source-inline">timeout</strong>. If it’s <strong class="source-inline">Some</strong>, we unwrap that value, and if it’s <strong class="source-inline">None</strong>, we set it to <strong class="source-inline">–1</strong>, which is the value that tells the operating system that we want to block until an event occurs even though that might <span class="No-Break">never happen.</span></p>
			<p>At the top of the file, we defined <strong class="source-inline">Events</strong> as a type alias for <strong class="source-inline">Vec&lt;ffi::Event&gt;</strong>, so the next thing we do is to get the capacity of that <strong class="source-inline">Vec</strong>. It’s important that we don’t rely on <strong class="source-inline">Vec::len</strong> since that reports how many items we have in the <strong class="source-inline">Vec</strong>. <strong class="source-inline">Vec::capacity</strong> reports the space we’ve allocated and that’s what <span class="No-Break">we’re after.</span></p>
			<p>Next up is the call to <strong class="source-inline">ffi::epoll_wait</strong>. This call will return successfully if it has a value of 0 or larger, telling us how many events <span class="No-Break">have occurred.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">We would get a value of 0 if a timeout elapses before an event <span class="No-Break">has happened.</span></p>
			<p>The last thing we do is to make an unsafe call to <strong class="source-inline">events.set_len(res as usize)</strong>. This function is unsafe since we could potentially set the length so that we would access memory that’s not been initialized yet in safe Rust. We know from the guarantee the operating system gives us that the number of events it returns is pointing to valid data in our <strong class="source-inline">Vec</strong>, so this is safe in <span class="No-Break">our case.</span></p>
			<p>Next up is <a id="_idIndexMarker301"/>our <strong class="source-inline">Registry</strong> struct. We will only implement one method, called <strong class="source-inline">register</strong>, and lastly, we’ll implement the <strong class="source-inline">Drop</strong> trait for it, closing the <span class="No-Break">epoll instance:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/poll.rs</p>
			<pre class="source-code">
impl Registry {
    pub fn register(&amp;self, source: &amp;TcpStream, token: usize, interests: i32) -&gt; Result&lt;()&gt; {
        let mut event = ffi::Event {
            events: interests as u32,
            epoll_data: token,
        };
        let op = ffi::EPOLL_CTL_ADD;
        let res = unsafe {
            ffi::epoll_ctl(self.raw_fd, op, source.as_raw_fd(), &amp;mut event)
        };
        if res &lt; 0 {
            return Err(io::Error::last_os_error());
        }
        Ok(())
    }
}</pre>			<p>The register function takes a <strong class="source-inline">&amp;TcpStream</strong> as a source, a token of type <strong class="source-inline">usize</strong>, and a bitmask named <strong class="source-inline">interests</strong>, which is of <span class="No-Break">type </span><span class="No-Break"><strong class="source-inline">i32</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">This is where mio does things differently. The source argument is specific to each platform. Instead of having the implementation of register on <strong class="source-inline">Registry</strong>, it’s handled in a platform-specific way in the source argument <span class="No-Break">it receives.</span></p>
			<p>The first thing we do<a id="_idIndexMarker302"/> is to create an <strong class="source-inline">ffi::Event</strong> object. The <strong class="source-inline">events</strong> field is simply set to the bitmask we received and named <strong class="source-inline">interests</strong>, and <strong class="source-inline">epoll_data</strong> is set to the value we passed in the <span class="No-Break"><strong class="source-inline">token</strong></span><span class="No-Break"> argument.</span></p>
			<p>The operation we want to perform on the epoll queue is adding interest in events on a new file descriptor. Therefore, we set the <strong class="source-inline">op</strong> argument to the <strong class="source-inline">ffi::EPOLL_CTL_ADD</strong> <span class="No-Break">constant value.</span></p>
			<p>Next up is the call to <strong class="source-inline">ffi::epoll_ctl</strong>. We pass in the file descriptor to the epoll instance first, then we pass in the <strong class="source-inline">op</strong> argument to indicate what kind of operation we want to perform. The last two arguments are the file descriptor we want the queue to track and the <strong class="source-inline">Event</strong> object we created to indicate what kind of events we’re interested in getting <span class="No-Break">notifications for.</span></p>
			<p>The last part of the function body is simply the error handling, which should be familiar <span class="No-Break">by now.</span></p>
			<p>The last part of <strong class="source-inline">poll.rs</strong> is the <strong class="source-inline">Drop</strong> implementation <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">Registry</strong></span><span class="No-Break">:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/poll.rs</p>
			<pre class="source-code">
impl Drop for Registry {
    fn drop(&amp;mut self) {
        let res = unsafe { ffi::close(self.raw_fd) };
        if res &lt; 0 {
            let err = io::Error::last_os_error();
            eprintln!("ERROR: {err:?}");
        }
    }
}</pre>			<p>The <strong class="source-inline">Drop</strong> implementation simply calls <strong class="source-inline">ffi::close</strong> on the epoll file descriptor. Adding a panic to <strong class="source-inline">drop</strong> is rarely a good idea since <strong class="source-inline">drop</strong> can be called within a panic already, which will cause the process to simply abort. mio logs errors if they occur in its Drop implementation but doesn’t handle them in any other way. For our simple example, we’ll just<a id="_idIndexMarker303"/> print the error so we can see if anything goes wrong since we don’t implement any kind of <span class="No-Break">logging here.</span></p>
			<p>The last part is the code for running our example, and that leads us <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">main.rs</strong></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor090"/>The main program</h1>
			<p>Let’s see how it all <a id="_idIndexMarker304"/>works in practice. Make sure that <strong class="source-inline">delayserver</strong> is up and running, because we’ll need it for these examples <span class="No-Break">to work.</span></p>
			<p>The goal is to send a set of requests to <strong class="source-inline">delayserver</strong> with varying delays and then use epoll to wait for the responses. Therefore, we’ll only use epoll to track <strong class="source-inline">read</strong> events in this example. The program doesn’t do much more than that for now. </p>
			<p>The first thing we do is to make sure our <strong class="source-inline">main.rs</strong> file is set <span class="No-Break">up correctly:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/main.rs</p>
			<pre class="source-code">
use std::{io::{self, Read, Result, Write}, net::TcpStream};
use ffi::Event;
use poll::Poll;
mod ffi;
mod poll;</pre>			<p>We import a few types<a id="_idIndexMarker305"/> from our own crate and from the standard library, which we’ll need going forward, as well as declaring our <span class="No-Break">two modules.</span></p>
			<p>We’ll be working directly with <strong class="source-inline">TcpStreams</strong> in this example, and that means that we’ll have to format the HTTP requests we make to our <span class="No-Break"><strong class="source-inline">delayserver</strong></span><span class="No-Break"> ourselves.</span></p>
			<p>The server will accept <strong class="source-inline">GET</strong> requests, so we create a small helper function to format a valid HTTP <strong class="source-inline">GET</strong> request <span class="No-Break">for us:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ch04/a-epoll/src/main.rs</p>
			<pre class="source-code">
fn get_req(path &amp;str) -&gt; Vec&lt;u8&gt; {
    format!(
        "GET {path} HTTP/1.1\r\n\
             Host: localhost\r\n\
             Connection: close\r\n\
             \r\n"
    )
}</pre>			<p>The preceding code simply takes a path as an input argument and formats a valid <strong class="source-inline">GET</strong> request with it. The <em class="italic">path</em> is the part of the URL after the scheme and host. In our case, the path would be everything in bold in the following <span class="No-Break">URL: </span><span class="No-Break"><strong class="source-inline">http://localhost:8080</strong></span><span class="No-Break"><strong class="bold">/2000/hello-world</strong></span><span class="No-Break">.</span></p>
			<p>Next up is our <strong class="source-inline">main</strong> function. It’s divided into <span class="No-Break">two parts:</span></p>
			<ul>
				<li>Setup and <span class="No-Break">sending requests</span></li>
				<li>Wait and handle <span class="No-Break">incoming events</span></li>
			</ul>
			<p>The first part of <a id="_idIndexMarker306"/>the <strong class="source-inline">main</strong> function looks <span class="No-Break">like this:</span></p>
			<pre class="source-code">
fn main() -&gt; Result&lt;()&gt; {
    let mut poll = Poll::new()?;
    let n_events = 5;
    let mut streams = vec![];
    let addr = "localhost:8080";
    for i in 0..n_events {
        let delay = (n_events - i) * 1000;
        let url_path = format!("/{delay}/request-{i}");
        let request = get_req(&amp;url_path);
        let mut stream = std::net::TcpStream::connect(addr)?;
        stream.set_nonblocking(true)?;
        stream.write_all(request.as_bytes())?;
        poll.registry()
            .register(&amp;stream, i, ffi::EPOLLIN | ffi::EPOLLET)?;
        streams.push(stream);
    }</pre>			<p>The first thing we do is to create a new <strong class="source-inline">Poll</strong> instance. We also specify what number of events we want to create and handle in <span class="No-Break">our example.</span></p>
			<p>The next step is creating a variable to hold a collection of <span class="No-Break"><strong class="source-inline">Vec&lt;TcpStream&gt;</strong></span><span class="No-Break"> objects.</span></p>
			<p>We also store the address to our local <strong class="source-inline">delayserver</strong> in a variable <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">addr</strong></span><span class="No-Break">.</span></p>
			<p>The next part is where we create a set of requests that we issue to our <strong class="source-inline">delayserver</strong>, which will eventually respond to us. For each request, we expect a read event to happen sometime later on in the <strong class="source-inline">TcpStream</strong> we sent the <span class="No-Break">request on.</span></p>
			<p>The first thing we do in the loop is set the delay time in milliseconds. Setting the delay to <strong class="source-inline">(n_events - i) * 1000</strong> simply sets the first request we make to have the longest timeout, so <a id="_idIndexMarker307"/>we should expect the responses to arrive in the reverse order from which they <span class="No-Break">were sent.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">For simplicity, we use the index the event will have in the <strong class="source-inline">streams</strong> collection as its ID. This ID will be the same as the <strong class="source-inline">i</strong> variable in our loop. For example, in the first loop, <strong class="source-inline">i</strong> will be <strong class="source-inline">0</strong>; it will also be the first stream to be pushed to our <strong class="source-inline">streams</strong> collection, so the index will be <strong class="source-inline">0</strong> as well. We therefore use <strong class="source-inline">0</strong> as the identification for this stream/event throughout since retrieving the <strong class="source-inline">TcpStream</strong> associated with this event will be as simple as indexing to that location in the <span class="No-Break"><strong class="source-inline">streams</strong></span><span class="No-Break"> collection.</span></p>
			<p>The next line, <strong class="source-inline">format!("/{delay}/request-{i}")</strong>, formats the <em class="italic">path</em> for our <strong class="source-inline">GET</strong> request. We set the timeout as described previously, and we also set a message where we store the identifier for this event, <strong class="source-inline">i</strong>, so we can track this event on the server side <span class="No-Break">as well.</span></p>
			<p>Next up is creating a <strong class="source-inline">TcpStream</strong>. You’ve probably noticed that the <strong class="source-inline">TcpStream</strong> in Rust doesn’t accept <strong class="source-inline">&amp;str</strong> but an argument that implements the <strong class="source-inline">ToSocketAddrs</strong> trait. This trait is implemented for <strong class="source-inline">&amp;str</strong> already, so that’s why we can simply write it like we do in <span class="No-Break">this example.</span></p>
			<p>Before <strong class="source-inline">Tcpstream::connect</strong> actually opens a socket, it will try to parse the address we pass in as an IP address. If it fails, it will parse it as a domain address and a port number, and then ask the operating system to do a DNS lookup for that address, which it then can use to actually connect to our server. So, you see, there is potentially quite a bit going on when we do a <span class="No-Break">simple connection.</span></p>
			<p>You probably remember that we discussed some of the nuances of the DNS lookup earlier and the fact that such a call could either be very fast since the operating system already has the information stored in memory or block while waiting for a response from the DNS server. This is a potential downside if you use <strong class="source-inline">TcpStream</strong> from the standard library if<a id="_idIndexMarker308"/> you want full control over the <span class="No-Break">entire process.</span></p>
			<p class="callout-heading">TcpStream in Rust and Nagle’s algorithm</p>
			<p class="callout">Here is a little fact for you (I originally intended to call it a “fun fact,” but realized that’s stretching the concept of “fun” just a little too far!). In Rust’s <strong class="source-inline">TcpStream</strong>, and, more importantly, most APIs that aim to mimic the standard library’s <strong class="source-inline">TcpStream</strong> such as mio or Tokio, the stream is created with the <strong class="source-inline">TCP_NODELAY</strong> flag set to <strong class="source-inline">false</strong>. In practice, this means that Nagle’s algorithm is used, which can cause some issues with latency outliers and possibly reduced throughput on <span class="No-Break">some workloads.</span></p>
			<p class="callout">Nagle’s algorithm is an algorithm that aims to reduce network congestion by pooling small network packages together. If you look at non-blocking I/O implementations in other languages, many, if not most, disable this algorithm by default. This is not the case in most Rust implementations and is worth being aware of. You can disable it by simply calling <strong class="source-inline">TcpStream::set_nodelay(true)</strong>. If you try to create your own async library or rely on Tokio/mio, and observe lower throughput than expected or latency problems, it’s worth checking whether this flag is set to <strong class="source-inline">true</strong> <span class="No-Break">or not.</span></p>
			<p>To continue with the code, the next step is setting <strong class="source-inline">TcpStream</strong> to non-blocking by <span class="No-Break">calling </span><span class="No-Break"><strong class="source-inline">Tcp</strong></span><strong class="source-inline"> </strong><span class="No-Break"><strong class="source-inline">Stream::set_nonblocking(true)</strong></span><span class="No-Break">.</span></p>
			<p>After that, we write our request to the server before we register interest in read events by setting the <strong class="source-inline">EPOLLIN</strong> flag bit in the <span class="No-Break"><strong class="source-inline">interests</strong></span><span class="No-Break"> bitmask.</span></p>
			<p>For each iteration, we push the stream to the end of our <span class="No-Break"><strong class="source-inline">streams</strong></span><span class="No-Break"> collection.</span></p>
			<p>The next part of the <strong class="source-inline">main</strong> function is handling <span class="No-Break">incoming events.</span></p>
			<p>Let’s take a look<a id="_idIndexMarker309"/> at the last part of our <span class="No-Break"><strong class="source-inline">main</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
let mut handled_events = 0;
    while handled_events &lt; n_events {
        let mut events = Vec::with_capacity(10);
        poll.poll(&amp;mut events, None)?;
        if events.is_empty() {
            println!("TIMEOUT (OR SPURIOUS EVENT NOTIFICATION)");
            continue;
        }
        handled_events += handle_events(&amp;events, &amp;mut streams)?;
    }
    println!("FINISHED");
    Ok(())
}</pre>			<p>The first thing we do is create a variable called <strong class="source-inline">handled_events</strong> to track how many events we <span class="No-Break">have handled.</span></p>
			<p>Next is our event loop. We loop as long as the handled events are less than the number of events we expect. Once all events are handled, we exit <span class="No-Break">the loop.</span></p>
			<p>Inside the loop, we create a <strong class="source-inline">Vec&lt;Event&gt;</strong> with the capacity to store 10 events. It’s important that we create this using <strong class="source-inline">Vec::with_capacity</strong> since the operating system will assume that we pass it memory that we’ve allocated. We could choose any number of events here and it would work just fine, but setting too low a number would limit how many events the operating system could notify us about on <span class="No-Break">each wakeup.</span></p>
			<p>Next is our blocking call to <strong class="source-inline">Poll::poll</strong>. As you know, this will actually tell the operating system to park our thread and wake us up when an event <span class="No-Break">has occurred.</span></p>
			<p>If we’re woken up, but there are no events in the list, it’s either a timeout or a spurious event (which could happen, so we need a way to check whether a timeout has actually elapsed if that’s important to us). If that’s the case, we simply call <strong class="source-inline">Poll::poll</strong> <span class="No-Break">once more.</span></p>
			<p>If there are events to be handled, we pass these on to the <strong class="source-inline">handle_events</strong> function together with a mutable reference to our <span class="No-Break"><strong class="source-inline">streams</strong></span><span class="No-Break"> collection.</span></p>
			<p>The last part of <strong class="source-inline">main</strong> is simply to write <strong class="source-inline">FINISHED</strong> to the console to let us know we exited <strong class="source-inline">main</strong> at <span class="No-Break">that point.</span></p>
			<p>The last bit of code in<a id="_idIndexMarker310"/> this chapter is the <strong class="source-inline">handle_events</strong> function. This function takes two arguments, a slice of <strong class="source-inline">Event</strong> structs and a mutable slice of <span class="No-Break"><strong class="source-inline">TcpStream</strong></span><span class="No-Break"> objects.</span></p>
			<p>Let’s take a look at the code before we <span class="No-Break">explain it:</span></p>
			<pre class="source-code">
fn handle_events(events: &amp;[Event], streams: &amp;mut [TcpStream]) -&gt; Result&lt;usize&gt; {
    let mut handled_events = 0;
    for event in events {
        let index = event.token();
        let mut data = vec![0u8; 4096];
        loop {
            match streams[index].read(&amp;mut data) {
                Ok(n) if n == 0 =&gt; {
                    handled_events += 1;
                    break;
                }
                Ok(n) =&gt; {
                    let txt = String::from_utf8_lossy(&amp;data[..n]);
                    println!("RECEIVED: {:?}", event);
                    println!("{txt}\n------\n");
                }
                // Not ready to read in a non-blocking manner. This could
                // happen even if the event was reported as ready
                Err(e) if e.kind() == io::ErrorKind::WouldBlock =&gt; break,
                Err(e) =&gt; return Err(e),
            }
        }
    }
    Ok(handled_events)
}</pre>			<p>The first thing we do <a id="_idIndexMarker311"/>is to create a variable, <strong class="source-inline">handled_events</strong>, to track how many events we consider handled on each wakeup. The next step is looping through the events <span class="No-Break">we received.</span></p>
			<p>In the loop, we retrieve the <em class="italic">token</em> that identifies which <strong class="source-inline">TcpStream</strong> we received an event for. As we explained earlier in this example, this <em class="italic">token</em> is the same as the index for that particular stream in the <strong class="source-inline">streams</strong> collection, so we can simply use it to index into our <strong class="source-inline">streams</strong> collection and retrieve the <span class="No-Break">right </span><span class="No-Break"><strong class="source-inline">TcpStream</strong></span><span class="No-Break">.</span></p>
			<p>Before we start reading data, we create a buffer with a size of 4,096 bytes (you can, of course, allocate a larger or smaller buffer for this if you <span class="No-Break">want to).</span></p>
			<p>We create a loop since we might need to call <strong class="source-inline">read</strong> multiple times to be sure that we’ve actually drained the buffer. <em class="italic">Remember how important it is to fully drain the buffer when using epoll in </em><span class="No-Break"><em class="italic">edge-triggered mode</em></span><span class="No-Break">.</span></p>
			<p>We match on the result of calling <strong class="source-inline">TcpStream::read</strong> since we want to take different actions based on <span class="No-Break">the result:</span></p>
			<ul>
				<li>If we get <strong class="source-inline">Ok(n)</strong> and the value is 0, we’ve drained the buffer; we consider the event as handled and break out of <span class="No-Break">the loop.</span></li>
				<li>If we get <strong class="source-inline">Ok(n)</strong> with a value larger than 0, we read the data to a <strong class="source-inline">String</strong> and print it out with some formatting. We do not break out of the loop yet since we have to call <strong class="source-inline">read</strong> until 0 is returned (or an error) to be sure that we’ve drained the <span class="No-Break">buffers fully.</span></li>
				<li>If we get <strong class="source-inline">Err</strong> and the<a id="_idIndexMarker312"/> error is of the <strong class="source-inline">io::ErrorKind::WouldBlock</strong> type, we simply break out of the loop. We don’t consider the event handled yet since<strong class="source-inline"> WouldBlock</strong> indicates that the data transfer is not complete, but there is no data ready <span class="No-Break">right now.</span></li>
				<li>If we get any other error, we simply return that error and consider it <span class="No-Break">a failure.</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">There is one more error condition you’d normally want to cover, and that is <strong class="source-inline">io::ErrorKind::Interrupted</strong>. Reading from a stream could be interrupted by a signal from the operating system. This should be expected and probably not considered a failure. The way to handle this is the same as what we do when we get an error of the <span class="No-Break"><strong class="source-inline">WouldBlock</strong></span><span class="No-Break"> type.</span></p>
			<p>If the <strong class="source-inline">read</strong> operation is successful, we return the number of <span class="No-Break">events handled.</span></p>
			<p class="callout-heading">Be careful with using TcpStream::read_to_end</p>
			<p class="callout">You should be careful with using <strong class="source-inline">TcpStream::read_to_end</strong> or any other function that fully drains the buffer for you when using non-blocking buffers. If you get an error of the <strong class="source-inline">io::WouldBlock</strong> type, it will report that as an error even though you had several successful reads before you got that error. You have no way of knowing how much data you read successfully other than observing any changes to the <strong class="source-inline">&amp;mut Vec</strong> you <span class="No-Break">passed in.</span></p>
			<p>Now, if we run<a id="_idIndexMarker313"/> our program, we should get the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
RECEIVED: Event { events: 1, epoll_data: 4 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:09 GMT
request-4
------
RECEIVED: Event { events: 1, epoll_data: 3 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:10 GMT
request-3
------
RECEIVED: Event { events: 1, epoll_data: 2 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:11 GMT
request-2
------
RECEIVED: Event { events: 1, epoll_data: 1 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:12 GMT
request-1
------
RECEIVED: Event { events: 1, epoll_data: 0 }
HTTP/1.1 200 OK
content-length: 9
connection: close
content-type: text/plain; charset=utf-8
date: Wed, 04 Oct 2023 15:29:13 GMT
request-0
------
FINISHED</pre>			<p>As you see, the responses are sent in reverse order. You can easily confirm this by looking at the output on the terminal on running the <strong class="source-inline">delayserver</strong> instance. The output should look <span class="No-Break">like this:</span></p>
			<pre class="source-code">
#1 - 5000ms: request-0
#2 - 4000ms: request-1
#3 - 3000ms: request-2
#4 - 2000ms: request-3
#5 - 1000ms: request-4</pre>			<p>The ordering might<a id="_idIndexMarker314"/> be different sometimes as the server receives them almost simultaneously, and can choose to handle them in a slightly <span class="No-Break">different order.</span></p>
			<p>Say we track events on the stream with <span class="No-Break">ID </span><span class="No-Break"><strong class="source-inline">4</strong></span><span class="No-Break">:</span></p>
			<ol>
				<li>In <strong class="source-inline">send_requests</strong>, we assigned the ID <strong class="source-inline">4</strong> to the last stream <span class="No-Break">we created.</span></li>
				<li>Socket 4 sends a request to <strong class="source-inline">delayserver</strong>, setting a delay of 1,000 ms and a message of <strong class="source-inline">request-4</strong> so we can identify it on the <span class="No-Break">server side.</span></li>
				<li>We register socket 4 with the event queue, making sure to set the <strong class="source-inline">epoll_data</strong> field to <strong class="source-inline">4</strong> so we can identify on what stream the <span class="No-Break">event occurred.</span></li>
				<li><strong class="source-inline">delayserver</strong> receives that request and delays the response for 1,000 ms before it sends an <strong class="source-inline">HTTP/1.1 200 OK</strong> response back, together with the message we <span class="No-Break">originally sent.</span></li>
				<li><strong class="source-inline">epoll_wait</strong> wakes up, notifying us that an event is ready. In the <strong class="source-inline">epoll_data</strong> field of the <strong class="source-inline">Event</strong> struct, we get back the same data that we passed in when registering the event. This tells us that it was an event on stream 4 <span class="No-Break">that occurred.</span></li>
				<li>We then read data from stream 4 and print <span class="No-Break">it out.</span></li>
			</ol>
			<p>In this example, we’ve kept things at a very low level even though we used the standard library to handle the intricacies of establishing a connection. Even though you’ve actually made a raw HTTP request to your own local server, you’ve set up an epoll instance to track events on a <strong class="source-inline">TcpStream</strong> and you’ve used epoll and syscalls to handle <span class="No-Break">incoming events.</span></p>
			<p>That’s no small feat – <span class="No-Break">congratulations!</span></p>
			<p>Before we leave this example, I wanted to point out how few changes we need to make to have our example use mio as the event loop instead of the one <span class="No-Break">we created.</span></p>
			<p>In the repository under <strong class="source-inline">ch04/b-epoll-mio</strong>, you’ll see an example where we do the exact same thing using mio instead. It only requires importing a few types from mio instead of our own <a id="_idIndexMarker315"/>modules and making <em class="italic">only five minor changes to </em><span class="No-Break"><em class="italic">our code</em></span><span class="No-Break">!</span></p>
			<p>Not only have you replicated what mio does, but you pretty much know how to use mio to create an event loop <span class="No-Break">as well!</span></p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor091"/>Summary</h1>
			<p>The concept of epoll, kqueue, and IOCP is pretty simple at a high level, but the devil is in the details. It’s just not that easy to understand and get it working correctly. Even programmers who work on these things will often specialize in one platform (epoll/kqueue or Windows). It’s rare that one person will know all the intricacies of all platforms, and you could probably write a whole book about this <span class="No-Break">subject alone.</span></p>
			<p>If we summarize what you’ve learned and got firsthand experience with in this chapter, the list is <span class="No-Break">quite impressive:</span></p>
			<ul>
				<li>You learned a lot about how mio is designed, enabling you to go to that repository and know what to look for and how to get started on that code base much easier than before reading <span class="No-Break">this chapter</span></li>
				<li>You learned a lot about making syscalls <span class="No-Break">on Linux</span></li>
				<li>You created an epoll instance, registered events with it, and handled <span class="No-Break">those events</span></li>
				<li>You learned quite a bit about how epoll is designed and <span class="No-Break">its API</span></li>
				<li>You learned about edge-triggering and level-triggering, which are extremely low-level, but useful, concepts to have an understanding of outside the context of epoll <span class="No-Break">as well</span></li>
				<li>You made a raw <span class="No-Break">HTTP request</span></li>
				<li>You saw how non-blocking sockets behave and how error codes reported by the operating system can be a way of communicating certain conditions that you’re expected <span class="No-Break">to handle</span></li>
				<li>You learned that not all I/O is equally “blocking” by looking at DNS resolution and <span class="No-Break">file I/O</span></li>
			</ul>
			<p>That’s pretty good for a single chapter, <span class="No-Break">I think!</span></p>
			<p>If you dive deeper into the topics we covered here, you’ll soon realize that there are gotchas and rabbit holes everywhere – especially if you expand this example to abstract over epoll, kqueue, and IOCP. You’ll probably end up reading Linus Torvald’s emails on how edge-triggered mode was supposed to work on pipes before you <span class="No-Break">know it.</span></p>
			<p>At least you now have a good foundation for further exploration. You can expand on our simple example and create a proper event loop that handles connecting, writing, timeouts, and scheduling; you can dive deeper into kqueue and IOCP by looking at how <strong class="source-inline">mio</strong> solves that problem; or you can be happy that you don’t have to directly deal with it again and appreciate the effort that went into libraries such as <strong class="source-inline">mio</strong>, <strong class="source-inline">polling</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">libuv</strong></span><span class="No-Break">.</span></p>
			<p>By this point, we’ve gained a lot of knowledge about the basic building blocks of asynchronous programming, so it’s time to start exploring how different programming languages create abstractions over asynchronous operations and use these building blocks to give us as programmers efficient, expressive, and productive ways to write our <span class="No-Break">asynchronous programs.</span></p>
			<p>First off is one of my favorite examples, where we’ll look into how fibers (or green threads) work by implementing <span class="No-Break">them ourselves.</span></p>
			<p>You’ve earned a break now. Yeah, go on, the next chapter can wait. Get a cup of tea or coffee and reset so you can start the next chapter with a fresh mind. I promise it will be both fun <span class="No-Break">and interesting.</span></p>
		</div>
	</body></html>
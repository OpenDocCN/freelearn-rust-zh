- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Configuring HTTPS with NGINX on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to deploying our applications, a lot of tutorials and books go
    through simple deployments and smooth over the concept of encrypting traffic to
    and from the server using HTTPS. However, HTTPS is essential and usually the biggest
    hurdle that a developer must overcome to get their website or API out into the
    world. While this book’s title is *Rust Web Programming*, it is essential to dedicate
    a chapter to truly understanding how HTTPS works so that you can implement HTTPS
    locally and then on the **Amazon Web Services** (**AWS**) cloud. This chapter
    will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is HTTPS?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing HTTPS locally with `docker-compose`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attaching a URL to our deployed application on AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enforcing HTTPS on our application on AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to build infrastructure in Terraform
    code that can encrypt traffic and lock down unwanted traffic to our **Elastic
    Compute Cloud** (**EC2**) instances so that they can only accept traffic from
    the load balancer, which will be explained later in the chapter. What is best
    is that most of this is automated, and seeing as we are using Docker to deploy
    our applications, you will be able to transfer this skill to any web project that
    you want to deploy in the future. While this is not the best implementation as
    there are entire books dedicated to cloud computing, you will be able to implement
    a solid, safe deployment that will continue to serve users even if an EC2 instance
    is out of service as the load balancer can route to other instances. It will also
    be able to scale if traffic demand increases.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll build on the code built in [*Chapter 10*](B18722_10.xhtml#_idTextAnchor200),
    *Deploying Our Application on AWS*. This can be found at the following URL: [](
    https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter10/deploying_our_application_on_aws)
    [https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter10/deploying_our_application_on_aws](https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter10/deploying_our_application_on_aws).'
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter11](https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter11).
  prefs: []
  type: TYPE_NORMAL
- en: What is HTTPS?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, our frontend and backend applications have been running through HTTP.
    However, this is not secure and has some drawbacks. To secure the traffic between
    our browser and our NGINX server, we are going to have to ensure that our application
    is using the HTTP/2 protocol. The HTTP/2 protocol has the following differences
    from the standard HTTP/1 protocol:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary protocol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressed headers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistent connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiplex streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can go through the preceding laid-out points and discuss the differences.
  prefs: []
  type: TYPE_NORMAL
- en: Binary protocol
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HTTP uses a text-based protocol whereas HTTP/2 uses a binary protocol. A binary
    protocol uses bytes to transmit data as opposed to human-readable characters,
    which are encoded using the **American Standard Code for Information Interchange**
    (**ASCII**). Using bytes reduces the number of possible errors and the size needed
    to transfer data. It will also enable us to encrypt our data stream, which is
    the basis of HTTPS.
  prefs: []
  type: TYPE_NORMAL
- en: Compressed headers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HTTP/2 compresses headers when sending requests. Compressing headers has similar
    benefits to the binary protocol, which results in a lower size of data needed
    to transfer the same request. The HTTP/2 protocol uses the HPACK format.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent connections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using HTTP, our browser must make a request every time a resource is needed.
    For instance, we could get our NGINX server to serve an HTML file. This will result
    in one request to get the HTML file. Inside the HTML file, there could be a reference
    to a CSS file, and this would result in another request to the NGINX server. It
    is also not uncommon to have a reference to a JavaScript file in the HTML file.
    This would result in another request. Therefore, to load a standard web page,
    our browser requires up to three requests. This does not scale well when we are
    running a server with multiple users. With HTTP/2, we can have persistent connections.
    This means that we can make three requests for the HTML, CSS, and JavaScript files
    in one connection.
  prefs: []
  type: TYPE_NORMAL
- en: Multiplex streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Making requests using HTTP/1 means that we must send our requests sequentially.
    This means that we make one request, wait for this request to be resolved, and
    then send another request. With HTTP/2, we use multiplex streaming, which means
    that we can send multiple requests at the same time and resolve them when the
    responses are returned. Combining multiplex streaming with a persistent connection
    results in a faster loading time. Any reader who used to use the internet in the
    1990s will remember having to wait a long time for a simple page to be loaded.
    Granted—the internet connection back then was not as fast, but it also was a result
    of making multiple HTTP sequential requests with different connections to load
    multiple pictures, HTML, and CSS.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored the difference between HTTP and HTTP/2, we can explore
    HTTPS, which is built on top of HTTP/2\. Before we go forward, however, it must
    be stated that security is a field in itself. Learning the high-level concepts
    around HTTPS is enough to make us understand the importance of what we are implementing
    and why we take certain steps. However, it does not make us security experts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we explore the steps of HTTPS, we need to understand what a middleman
    attack is because this attack inspires the steps of HTTPS. A middleman attack
    is exactly what it sounds like: a malicious eavesdropper that can intercept communication
    packets between the user and the server. This also means that the eavesdropper
    can also obtain encryption if they are passed over a network. Simply googling
    “middleman attacks” will result in loads of tutorials and software that can be
    downloaded to implement such attacks. There are more caveats to security that
    are beyond the scope of this book, but to conclude, if you are hosting a website
    that you expect users to connect and log in to, there is no excuse to not use
    HTTPS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to HTTPS, there is a range of steps that need to take place.
    First, before any requests are made to the server, the owner of the server and
    domain must get a certificate from a trusted central authority. There are not
    many trusted central authorities around. What these authorities do is get some
    identification of the person who owns the domain and some proof that the person
    applying for the certificate owns the domain. This might sound like a headache,
    but a lot of URL providers such as AWS have streamlined the process using information,
    such as payment details, to send to the central trusted authority in the backend
    when you are pointing and clicking to buy the domain. We will have to fill in
    some extra forms, but if you have a working AWS account, this will not be too
    taxing. These central authorities are limited because anyone with a computer can
    create a digital certificate. For instance, if we were intercepting traffic between
    a server and a user, we could generate our own digital certificates with a key
    and forward them to the user. Because of this, mainstream browsers only recognize
    certificates that have been issued by a small number of recognized authorities.
    If the browser gets a certificate that is not recognized, these browsers will
    give a warning, such as in the following Chrome example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Chrome blocking access due to unrecognized certificate](img/Figure_11.1_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Chrome blocking access due to unrecognized certificate
  prefs: []
  type: TYPE_NORMAL
- en: It is not advised to click on **ADVANCED** to proceed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the owner of the domain and server has the certificate from the central
    authority, the user can make requests. Before any meaningful data to the application
    is exchanged, the server sends over the certificate with the public key to the
    user. The user then creates a session key and encrypts this session key and the
    public key of the certificate, which is then sent back to the server. The server
    can then decrypt the key using the private key that was not sent over the network.
    Therefore, even if the eavesdropper manages to intercept the messages and get
    the session key, it is encrypted, so they cannot use it. Both the server and client
    can check the validity of mutually encrypted messages. We can use the session
    key to send encrypted messages to and from the server and the user, as seen in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Steps required when making an HTTPS connection](img/Figure_11.2_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Steps required when making an HTTPS connection
  prefs: []
  type: TYPE_NORMAL
- en: Do not worry—there are packages and tools that will help us manage the HTTPS
    process; we will not have to implement our own protocol. You will understand why
    we must carry out certain steps and how to troubleshoot problems when they arise.
    In the next section, we will implement a basic HTTPS protocol with NGINX locally.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing HTTPS locally with docker-compose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to implementing HTTPS, most of the work is going to be achieved
    through NGINX. Although we have worked a little with NGINX, the NGINX configuration
    is a powerful tool. You can implement conditional logic, pull variables and data
    from the request and act on them, redirect traffic, and much more. In this chapter,
    we are going to do enough to implement HTTPS, but it is advised if you have time
    to read up on the fundamentals of NGINX configurations; reading material is provided
    in the *Further reading* section. For our `deployment/nginx_config.yml` file,
    we need to have the following layout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that we have two `server` scopes in our `http` scope. This
    is because we need to enforce HTTPS. We must remember that our outside port is
    `80`. However, if we want to carry out an HTTPS connection, we instead need to
    connect to port `443`, which is the standard for HTTPS. Typing `https://` into
    the browser will target port `443`, and typing `http://` into the browser will
    target port `80`. If we allow port `80` to be open, loads of users will access
    our site in an unsecured way because some people will type `http://` into the
    browser. Hackers would also spread the HTTP links around because they want as
    many people as possible to not use a secure network. However, if we block port
    `80`, people who put `http://` into the browser will just get blocked from accessing
    the website. It is unlikely that the average user is going to understand the differences
    in ports, look at what they typed in, and correct it. Instead, they are just going
    to think that the site is down. Therefore, we are going to have to listen to both
    ports `443` and `80`. However, when a request is made to port `80`, we going to
    redirect the request to port `443` instead. Our first server scope can redirect
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that we listen to port `80`, and then return the same request
    that was sent but with the HTTPS protocol, which means that it will hit our `443`
    port. We can also see that we reference `$host` and `$request_uri` variables.
    These variables are standard variables in NGINX that are automatically populated.
    We can define our own variables with the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we want our NGINX instance to work on our server and localhost, so
    using the standard variables is the best choice here. Now that we have defined
    our rerouting rule, we can move on to the next server scope; we listen to port
    `443` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the preceding code, it must be noted that we are handling and directing
    traffic for both the frontend and backend applications in the same NGINX instance.
    Alongside the port definition, we also declare that we are using the `ssl` and
    `http2` NGINX modules. This is not surprising as HTTPS is essentially SSL on top
    of HTTP/2\. We then define where our server certificate is in the NGINX container.
    We will add these later in the `docker-compose` volume. We can also see that we
    pass our HTTPS request to the appropriate application via HTTP. If we try to change
    these proxies to the HTTPS protocol, then we would get a bad gateway error. This
    is because the handshake between NGINX and our services would fail. It is not
    essential because we must remember that the ports that the frontend and backend
    applications expose are not available to anyone outside of localhost. Yes—on our
    local machine we can access them, but this is because they are running on our
    local machine. If we were to deploy our application server, it will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Traffic flow if deployed on a server](img/Figure_11.3_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Traffic flow if deployed on a server
  prefs: []
  type: TYPE_NORMAL
- en: Our NGINX configuration is not optimum. There are settings that can be tweaked
    in terms of ciphers, caching, and managing timeouts. However, this is enough to
    get an HTTPS protocol working. If you get to the point that you need to optimize
    the caching and encryption methods of your NGINX configuration, it is suggested
    that you seek further education materials on DevOps and NGINX.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have defined our NGINX configuration, we must define our certificates.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To define our own certificates, we must install the `openssl` package by following
    the steps using the next links:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linux**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://fedingo.com/how-to-install-openssl-in-ubuntu/](https://fedingo.com/how-to-install-openssl-in-ubuntu/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Windows**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://linuxhint.com/install-openssl-windows/](https://linuxhint.com/install-openssl-windows/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mac**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://yasar-yy.medium.com/installing-openssl-library-on-macos-catalina-6777a2e238a6](https://yasar-yy.medium.com/installing-openssl-library-on-macos-catalina-6777a2e238a6)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be done with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates a key with `x509`, which is the international telecommunication
    union standard. We state that the certificate will expire in 10 days and that
    the key and certificate will have the name `self`. They can be called anything;
    however, for us, it makes sense to call the certificate `self` because it is a
    self-issued certificate. The command shown in the previous code snippet will push
    several prompts. It does not matter what you say to these prompts as we will just
    be making localhost requests with them, meaning that they will never go anywhere
    outside of our local computer. We can now stash the key and certificate anywhere
    within the `deployment` directory if you can reference the key and certificate
    placed in the `docker-compose.yml` file. In our `docker-compose.yml` file, our
    NGINX service now takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that I chose to store the key and certificate inside a directory
    called `nginx_configs/ssl/`. This is because I have added several simple NGINX
    configs into the GitHub repo under the `nginx_configs` directory if you want some
    easy quick references on handling variables, conditional logic, and serving HTML
    files directly from NGINX. While where you get your key and certificate from may
    vary, it is important that you put the key and the certificate inside the `etc/nginx/ssl/`
    directory inside the NGINX container.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now at the point where you can test our application to see if the local
    HTTPS is working. If you spin up your `docker-compose` instance and then go to
    the `https://localhost` URL in your browser, you should get a warning that it
    is not secure, and you will not be able to instantly connect to the frontend.
    This is reassuring because we are not a central authority, so our browser will
    not recognize our certificate. There is a multitude of browsers, and we would
    waste a lot of space in this book describing how to get past this for every browser.
    Considering browsers are free to download, we can get around the blocking of our
    application in Chrome by going to the `flags` URL, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Allowing our application certificate to pass in Chrome](img/Figure_11.4_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Allowing our application certificate to pass in Chrome
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that I have allowed invalid certificates from localhost. Now
    that our invalid certificate is enabled in our browser, we can access our application,
    as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Accessing our application through HTTPS](img/Figure_11.5_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Accessing our application through HTTPS
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are using the HTTPS protocol; however, as we can see in the preceding
    screenshot, Chrome is complaining stating that it is not secure. We can inspect
    the reason why by clicking on the **Not Secure** statement, giving the following
    view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Explaining why the connection is not secure](img/Figure_11.6_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Explaining why the connection is not secure
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that our certificate is not valid. We expected the certificate
    to not be valid because we issued it, which makes it not officially recognized.
    However, our HTTPS connection is working! This is interesting to see how HTTPS
    works; however, it is not useful with a self-signed certificate running on our
    localhost. If we want to utilize HTTPS, we are going to have to apply it to our
    application on AWS. There are a couple of steps we need to carry out before we
    implement HTTPS on AWS. In the next section, we will assign a URL to our application.
  prefs: []
  type: TYPE_NORMAL
- en: Attaching a URL to our deployed application on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we managed to deploy our to-do application onto a
    server on AWS and access this application directly by putting the IP address of
    the server into our browser. When it comes to registering our URL, you will be
    exposed to multiple acronyms. To feel comfortable when navigating AWS routing,
    it makes sense to be familiar with URL acronyms by reading the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Anatomy of a URL](img/Figure_11.7_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Anatomy of a URL
  prefs: []
  type: TYPE_NORMAL
- en: 'When we are associating a URL with our application, we are going to be configuring
    a **Domain Name System** (**DNS**). A DNS is a system that translates a user-friendly
    URL to an IP address. For a DNS system to work, we will need the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain registrar**: An organization such as AWS, Google Cloud, Azure, GoDaddy,
    and so on that will register a domain if it receives payment for the domain and
    personal details of the person responsible for the domain. This organization will
    also handle reports of abuse if the URL is used in illegal activity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DNS record**: A registered URL can have multiple DNS records. A DNS record
    essentially defines a routing rule for the URL. For instance, a simple DNS record
    will forward the URL to an IP address of a server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zone file**: A container for DNS records (in our case, the zone file will
    be managed by AWS).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The DNS records and registrars are essential for our URL to work. Even though
    we can directly connect to IP addresses, there are a couple of middlemen if we
    want to connect to a URL, as laid out here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Steps required to connect a URL to an IP address](img/Figure_11.8_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Steps required to connect a URL to an IP address
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the preceding diagram, if we want to connect to a server,
    we send the URL to the local DNS server. This server then makes three calls in
    sequential order from top to bottom. By the end of the three requests, the local
    DNS server will have the IP address related to the URL. We can see that the registrar
    is responsible for part of the mapping. This is where our DNS records are configured.
    If we remove our DNS records, then the URL is no longer available on the internet.
    We do not have to make the calls laid out in *Figure 11**.8* every time we enter
    a URL. Our browsers and the local DNS server will cache the URL to the IP address
    mapped to reduce the number of calls to the other three servers. There is a problem,
    however; when we have been building our production server, you might have realized
    that the IP address has changed every time we tear down and spin up a production
    server. There’s nothing wrong going on here; when we create an EC2 instance, we
    must take a server that is available. A cloud provider such as AWS cannot hold
    the server aside just for us unless we want to pay for it. In the next section,
    we will keep our IP consistent with elastic IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Attaching an elastic IP to our server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Elastic IP addresses are essentially fixed IP addresses that we keep. We can
    then attach these elastic IP addresses to any one EC2 instance at one point in
    time as we see fit. This is very helpful when it comes to routing. We can set
    up the routing of a URL to an elastic IP and then switch the allocation of the
    elastic IP to the server that we need. This means that we can deploy a new application
    to another server, test it, and then switch our elastic IP to the new deployment
    server without having to touch the routing for our URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will not be creating an elastic IP every time we spin up a production server.
    Because of this, it is OK to point and click in the AWS console to create and
    attach the elastic IP address. Before we do this, however, we need to deploy our
    production server with the previous NGINX config file that does not have HTTPS
    defined, but instead has the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This should make sense to you now, as the NGINX config merely listens to HTTP
    requests through the outside port `80` and then passes them through to our applications.
    We also must remove our reference to our self-signed certificates because we will
    not need them, and we will also not be uploading those certificates to our server.
    Considering our lack of reference to our certificates, our `docker-compose` instance
    in our `deployment` directory should have the following NGINX definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to deploy our build on our production server. Remember—we can
    do this using the `deployment/run_build.py` Python script that we set up in the
    previous chapter. Once the server is built, we know that there is an EC2 instance
    live with the `"to-do production server"` tag. We are now ready to allocate an
    elastic IP address to our EC2 instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To allocate elastic IPs, we first need to navigate to the EC2 service by searching
    for EC2 in the search bar in the AWS dashboard at the top and clicking on the
    service, resulting in the following view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – EC2 dashboard view](img/Figure_11.9_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – EC2 dashboard view
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that **Elastic IPs** can be accessed on the right of the **Resources**
    panel and on the left of the screen. Once we are in the elastic IP address dashboard,
    we will have a list of elastic IP addresses that you have. On the top right of
    the screen, there will be an orange button labeled **Allocate Elastic IP address**.
    If you click this button, you will get the following creation form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10 – Allocating (creating) an elastic IP address](img/Figure_11.10_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – Allocating (creating) an elastic IP address
  prefs: []
  type: TYPE_NORMAL
- en: What we are doing here is grabbing an elastic IP address from a pool of IP addresses
    for the area you are working on. There is a limit of five elastic IP addresses
    per account. If you think this is not enough for you, you will need to get more
    creative with your networking infrastructure. You can also investigate creating
    sub-accounts for the main account. As with clean code, there are benefits to having
    clean accounts that only work on one project at a time. This will help you keep
    track of costs, and shutting down all infrastructure for a project will be clean
    as you can be sure that everything for that project has been cleared by deleting
    the account.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on to our allocation of the elastic IP to our EC2 server, we can allocate
    our elastic IP by highlighting the desired elastic IP address in the elastic IP
    dashboard and clicking on the **Actions** button at the top right of the dashboard,
    as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11 – Actions on our elastic IP](img/Figure_11.11_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – Actions on our elastic IP
  prefs: []
  type: TYPE_NORMAL
- en: 'Under **Actions**, we must click on the **Associate Elastic IP address** option,
    giving us the following display:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12 – Associating an elastic IP address](img/Figure_11.12_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – Associating an elastic IP address
  prefs: []
  type: TYPE_NORMAL
- en: 'The `"to-do production server"` tag that we defined in Terraform. However,
    if we do not have a tag, we can still choose an instance from the drop-down menu.
    We can then click the **Associate** button. Once this is done, we can go to our
    elastic IP address in our browser, and we should be able to access our to-do application,
    as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.13 – Accessing our to-do application using a static IP](img/Figure_11.13_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – Accessing our to-do application using a static IP
  prefs: []
  type: TYPE_NORMAL
- en: And here we have it—our application can be accessed with an elastic IP!! We
    can now spin up a new server if we want, test it, and redirect our elastic IP
    to the new server if we are happy, providing seamless updates without our users
    knowing. However, getting users to type in the raw IP address is not desirable.
    In the next section, we will register a domain and connect it to our elastic IP
    address.
  prefs: []
  type: TYPE_NORMAL
- en: Registering a domain name
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to registering a domain, it can all be handled in AWS with the
    Route 53 service. First, we navigate to Route 53, which is the service that handles
    routing and URL registration. On the left side of the **Route 53** dashboard web
    page, we can click on the **Registered domains** section, as seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.14 – Navigation to registered domains in the Route 53 dashboard](img/Figure_11.14_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – Navigation to registered domains in the Route 53 dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then see a list of registered domains that we already own and the option
    to register a domain, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.15 – Registered domains view](img/Figure_11.15_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – Registered domains view
  prefs: []
  type: TYPE_NORMAL
- en: If you click on the **Register Domain** button, you will be led through a straightforward
    series of forms to register your domain. Screenshotting these steps would be excessive.
    The forms ask you for the domain you want to register. They will then tell you
    if the domain and other domains such as this are available. Domains cost about
    $12 a year on average at the time of writing this book. Once you have selected
    your domain and clicked on the checkout, you will be passed through a series of
    personal information forms. These forms include a contact address and whether
    the domain is for personal use or for a company. Screenshotting these forms would
    result in excessive pages with little educational advantage as these forms are
    personal and easy to fill in. It is recommended that you select the validation
    through DNS because this is automated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once your domain is registered, you can go to the **Hosted zones** section
    in the main Route 53 dashboard. Here, we will see a list of hosted zones for each
    URL that you own. A hosted zone is essentially a collection of DNS records. If
    we click on a hosted zone for a URL, there will be two DNS records: NS and SOA
    (NS—name server; SOA—start of authority). These records should not be deleted,
    and if they are, somebody else who knows what these records are could hijack your
    URL by implementing those records themselves. DNS records are essentially records
    on how to route traffic for a domain name. Each DNS record has the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain/subdomain name**: The name of the URL the record belongs to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Record type**: The type of record (A, AAAA, CNAME, or NS)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value**: Target IP address'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Routing policy**: How Route 53 responds to queries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TLL (Time to Live)**: The amount of time the record is cached in the DNS
    resolvers in the client so that we do not have to query the Route 53 servers too
    often, with the trade-off of reducing traffic to DNS servers versus time for updates
    to roll onto clients'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The attributes just defined are self-explanatory, apart from the record type.
    There are advanced record types that we can build in Route 53\. However, the following
    record types are needed for us to route our domain to our IP:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A**: The simplest record type. Type A merely routes traffic from the URL
    to an IPv4 IP address.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AAAA**: Routes traffic from the URL to an IPv6 address.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CNAME**: Maps a hostname to another hostname (target must be an A or AAAA
    record type).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NS**: Name server for the hosted zone (controls how the traffic is routed).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For us, we are going to create an DNS `A` record by clicking on the **Create
    record** button, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.16 – Creating a DNS record](img/Figure_11.16_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 – Creating a DNS record
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have clicked on this, we get the following layout:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.17 – Creating a DNS record form](img/Figure_11.17_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 – Creating a DNS record form
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that record type `A` is the default. We can also see that we can
    add a subdomain. This gives us some flexibility. For instance, if we wanted, the
    `api.freshcutswags.com` URL could point to a different IP address from `freshcutswags.com`.
    For now, we are going to just leave the subdomain empty. We are then going to
    put the elastic IP address that we set up in the previous section in the `www`.
    Once this is done, we should have two `A` records. We can then check where our
    URL is mapping the traffic using the [www.digwebinterface.com](https://www.digwebinterface.com)
    website. Here, we can enter URLs, and the website will tell us where the URLs
    are being mapped. We can see here that our URLs are both mapping to the correct
    elastic IP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.18 – Inspecting our URL mapping](img/Figure_11.18_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – Inspecting our URL mapping
  prefs: []
  type: TYPE_NORMAL
- en: 'With the mapping result confirmed, as seen in the preceding screenshot, we
    can visit our URL and expect to see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.19 – Accessing our application through the registered URL](img/Figure_11.19_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 – Accessing our application through the registered URL
  prefs: []
  type: TYPE_NORMAL
- en: We can see that our URL is now working. However, the connection is not secure.
    In the next section, we will enforce an HTTPS protocol for our application and
    lock it down, as right now, even though we can access our application through
    the URL, there is nothing stopping us from accessing the IP of the server directly.
  prefs: []
  type: TYPE_NORMAL
- en: Enforcing HTTPS on our application on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Right now, our application kind of works, but it is a nightmare in terms of
    security. By the end of this section, we will not have the most secure application,
    as further reading of a networking and DevOps textbook is suggested to achieve
    gold-standard security. However, we will have configured security groups, locked
    down our EC2 instances so that they cannot be directly accessed by outsiders,
    and enforced encrypted traffic through a load balancer that will then direct traffic
    to our EC2 instances. The result of our efforts will be the following system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.20 – Layout of our desired system to achieve HTTPS](img/Figure_11.20_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20 – Layout of our desired system to achieve HTTPS
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve the system shown in *Figure 11**.20*, we need to carry out the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Get certificates approved for our URL and variations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create multiple EC2 instances to distribute traffic and ensure that the service
    survives outages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a load balancer to handle incoming traffic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create security groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update our Python build script to support multiple EC2 instances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attach our URL to the load balancer using the Route 53 wizard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the previous section on attaching a URL to our application on AWS, we did
    a lot of pointing and clicking. As said in the previous chapter, pointing and
    clicking should be avoided if possible as it is not repeatable and we as humans
    forget what we did. Sadly, with URL approvals pointing and clicking is the best
    option. In this section, only the first and sixth steps will require pointing
    and clicking. The rest will be achieved with Terraform and Python. We are going
    to be making some big changes to our Terraform config, so it is advised that you
    run a `terraform destroy` command before altering your Terraform config. Before
    we can do any of the coding, however, we are going to have to get our certificates
    for our URL.
  prefs: []
  type: TYPE_NORMAL
- en: Getting certificates for our URL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because we brought our URL through Route 53, which is handled by AWS, and our
    servers are running in AWS, the certification and implementation of those certificates
    is a straightforward process. We need to navigate to Certificate Manager by typing
    `certificate manager` into the services search bar and clicking on it. Once there,
    we are displayed with a page that only has one orange button labeled **Request
    a certificate**. Click on this, and we will be transported to the following page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_11.21_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.21 – Certificate Manager: start of journey'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to want our certificate to be public facing; therefore, we are
    happy with the default selection, and we click **Next**. We are then presented
    with the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.22 – Defining certificate request](img/Figure_11.22_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.22 – Defining certificate request
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we type in the URL we want to associate with the certificate. We could
    add another, but we will do a separate certificate for our URL that has prefixes
    as we want to explore how to attach multiple certificates in Terraform. We can
    also see that DNS validation is already highlighted, and this is recommended as
    we have our servers on AWS, meaning we will have to take no more action to get
    the certificate issued. We can then click on the button labeled **Request**, and
    we will be redirected to a page with the list of certificates. I find that the
    new certificate request is not present nearly every time I have done this. My
    guess is that there is a delay. Do not worry—merely refresh the page, and you
    will see the pending certificate request listed. Click on this listing, and you
    will be directed to a detailed view of this certificate request. In the middle
    of the screen on the right, you need to click the **Create records in Route 53**
    button, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.23 – Creating records for DNS confirmation](img/Figure_11.23_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.23 – Creating records for DNS confirmation
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the prompts after clicking the button, and the CNAME record will be
    created. If you do not do this, then the pending status for the ticket will go
    on indefinitely because the cloud provider needs the route to issue the certificate,
    considering we selected DNS validation. After a few minutes, the certificate should
    be issued. Once this is done, carry out the same steps for the prefix wildcard.
    Once this is done, your certificate lists should look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.24 – Issued certificates](img/Figure_11.24_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.24 – Issued certificates
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding screenshot, we can see that I have two certificates: one for
    if a user directly types in the URL with no prefix, and the wildcard that covers
    all prefixes. We are ready to use these certificates, but before we do this, we
    are going to have to carry out some other steps. Before we define rules around
    the traffic, we are going to have to build the infrastructure where the traffic
    is going. In the next section, we will build two EC2 instances.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating multiple EC2 instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be using a load balancer. Because of this, we need a minimum of two
    EC2 instances. This means that if one EC2 instance is down, we can still use the
    other EC2 instance. We can also scale our application. For instance, if everyone
    in the world suddenly realized that they needed a to-do app to sort out their
    lives, there is nothing stopping us from increasing the number of EC2 instances
    to distribute the traffic across. We can increase our EC2 instances to two by
    going into our `deployment/main.tf` file and having the following definition of
    our EC2 instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that we have added a `count` parameter and defined it as `2`.
    We have also altered the tag. We can also see that we access the number of EC2
    instances being created with `index`. The index starts at zero and increases by
    one every time a resource is made. Now that we have two instances, we must update
    the outputs at the bottom of the `deployment/main.tf` file with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that apart from the database endpoint, all the other outputs
    have been changed to lists. This is because they all reference our multiple EC2
    instances. Now that we have our EC2 instances defined, we can route traffic to
    our instances with a load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a load balancer for our traffic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is a range of different load balancers that we can pick from. We have
    already discussed NGINX, which is a popular load balancer. For this chapter, we
    are going to use the application load balancer to route the traffic to our EC2
    instances and implement the HTTPS protocol. There are multiple features that load
    balancers can offer, and they protect against `deployment/load_balancer.tf` file.
    First, we collect the data that we need with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can see that instead of `resource`, we used the `data` declaration. This
    is where we make queries to AWS for specific types of data to be used in the rest
    of the Terraform script. We get the **virtual private cloud** (**VPC**) ID. In
    Terraform, we can define and build a VPC, but throughout this book, we have been
    using the default VPC. We can get the default VPC ID for our load balancer. We
    then get the data for the certificates that we have defined in the previous section
    with the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now must define a target for our load balancer. This is done in the form
    of a target group where we can bunch a group of instances together for the load
    balancer to target with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that we define the parameters for a health check. The parameters
    for the health check are self-explanatory. The health check will alert a service
    targeting the health status of the target group. We would not want to route traffic
    to a target group that is down. We then define the protocol and port of the traffic,
    the type of resource in the target group, and the ID of the VPC. Now that our
    target group is defined, we can attach our EC2 instances to it with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that we get the IDs of the EC2 servers for the target ID. With this,
    our EC2 instances can be targeted by the load balancer. Now that we have a target,
    we can create our load balancer with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the parameters in the load balancer definition are straightforward.
    However, you may have noticed the security group definition. We are referencing
    a security group even though we have not defined any security groups. If you do
    not know what a security group is, do not worry—we will cover and build all the
    security groups we need in the next section. Before we do that, however, we might
    as well define listening and routing rules for the load balancer. First, we can
    define the HTTP listener for port `80`. If you remember the first section of this
    chapter when getting HTTPS working on our localhost, what do you think we need
    to do for the HTTP traffic? You don’t have to know the specific Terraform code,
    but what is the general behavior that we want to facilitate? With this in mind,
    we can achieve that behavior with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s right! We receive HTTPS traffic from port `80` and we then redirect
    it to port `443` with the HTTPS protocol. We can see that we have attached this
    listener using the **Amazon Resource Name** (**ARN**) of the load balancer that
    we created. We can now define our HTTPS listener with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that we accept HTTPS traffic and then forward HTTP traffic
    to our target group that we defined using the ARN of the target group. We can
    also see that we have attached one of the certificates to the listener. However,
    this does not cover all our URL combinations. Remember—we have another certificate
    that we want to attach. We can attach our second certificate with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This connection should be easy to understand. Here, we merely reference the
    ARN of the HTTPS listener and the ARN of the certificate that we want to attach.
    We have now defined everything we need for the resources of the load balancer.
    However, what of the traffic? We have defined the load balancer, EC2 instances,
    and the routing for HTTPS for the load balancer. However, what is stopping someone
    just directly connecting to the EC2 instance, bypassing the load balancer and
    HTTPS completely? This is where security groups come in. In the next section,
    we will lock down the traffic by creating security groups so that users cannot
    bypass our load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: Creating security groups to lock down and secure traffic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Security groups are essentially firewalls. We can define the traffic to and
    from a resource that has a security group implemented. The rules of the traffic
    can be fine-grained. A single security group can have multiple rules defining
    the origin (even specific IP addresses if needed) and the protocol of the traffic.
    When it comes to our security groups, we are going to need two. One will accept
    HTTP and HTTPS traffic from all IPs anywhere in the world. This is going to be
    for our load balancer because we want our application to be available to everyone.
    The other security group will be implemented by our EC2 instances; this one blocks
    all HTTP traffic apart from the first security group. We will also enable SSH
    inbound traffic because we need to SSH into the servers to deploy the applications,
    giving us the following traffic layout:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.25 – Security groups for our deployment system](img/Figure_11.25_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.25 – Security groups for our deployment system
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where you must be careful with online tutorials. There is no shortage
    of *YouTube* videos and *Medium* articles that will get a load balancer up and
    running with some pointing and clicking. However, they leave the EC2 instances
    exposed and do not bother exploring security groups. Even with this section, we
    are going to leave the database exposed. I am doing this because it is a good
    question to be asked in the *Questions* section. However, I’m highlighting it
    here because you need to be warned that it is exposed. The way to lock down the
    database will be covered in the *Answers* section of this chapter. When it comes
    to our security groups, we can define them in the `deployment/security_groups.tf`
    file. We can start with the load balancer security group with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have two inbound rules under the `ingress` tag and one outbound rule
    under the `egress` tag. Our first inbound rule is to allow HTTP data from anywhere
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `cidr` blocks of all zeros means *from anywhere*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our second inbound rule is HTTPS traffic from anywhere. How do you think this
    will be defined? It can be defined with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now for our outbound rule. We must allow all traffic and protocols out of the
    load balancer as they are coming from our resources. This can be achieved with
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It must be noted that `from_port` and `to_port` are zero, meaning that we allow
    outgoing traffic from all ports. We also have set `protocol` to `-1`, meaning
    we are allowing all protocols as outgoing traffic. We have now defined the security
    group for the load balancer. We can now move on to defining our security group
    for the EC2 instances with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The outbound rules are going to be the same as the load balancer because we
    want to return data to anywhere that is available to request it. When it comes
    to our HTTP inbound rules, we only want to accept traffic from the load balancer
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that instead of defining the `cidr` blocks, we rely on the
    security group of the load balancer. Now that all the user traffic is defined,
    we only need to define the SSH traffic for deployment with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we access SSH through port `22`. This will enable us to SSH into our
    servers and deploy our applications. Nearly everything is done. We only must attach
    our EC2 instances to our EC2 security group, which can be done with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Our Terraform scripts are now complete and will be able to spin up multiple
    EC2 instances, a database, and a load balancer while locking down traffic. This
    is a good template for other projects if you want to get a basic web app with
    HTTPS and access to a database off the ground with restricted traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have two different EC2 instances, we are going to have to change
    our deployment scripts so that both have applications installed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Updating our Python deployment script for multiple EC2 instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are ways in which we can optimize the deployment process, such as running
    multiple processes at the same time. This will speed up your deployment the more
    EC2 instances that we have. However, we must remember that this is a book on Rust
    and web programming, with some deployment chapters included so that you can use
    what you create. We could write an entire book on optimizing and improving our
    deployment pipeline. When it comes to supporting multiple EC2 instances in our
    `deployment/run_build.py` file, at the end of the file, we merely loop through
    the list of global IPs from the output with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This is it. Multiple servers are now supported. Here, we can see the effectiveness
    of separating the logic behind managing the data around the deployment in the
    Python file and the individual Bash script for deploying the applications on the
    individual server. Keeping things isolated keeps technical debt down, enabling
    easy refactoring. Now, all our code infrastructure is done! We can run this Python
    script and deploy our build onto AWS. Everything is nearly done; all we must do
    is connect our URL to our load balancer in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Attaching our URL to the load balancer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the home run. We are finally near the end. I appreciate you sticking
    with me on this chapter as it is not as exciting as coding in Rust. However, it
    is important if you want to use your Rust server. To connect our URL to the load
    balancer, we must navigate to the hosted zone for your URL. Once there, click
    on the **Create record** button. When in the **Create record** display, if you
    are not using the wizard, click on the **Switch to wizard** link at the top right
    of the **Create record** display to get the following wizard view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.26 – Create record wizard view](img/Figure_11.26_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.26 – Create record wizard view
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see a range of fancy ways to route traffic. However, we are going
    to merely select **Simple routing** as we just need to pass traffic to the load
    balancer, which is doing the distribution of traffic between EC2 instances. Selecting
    **Simple routing** gives us the following form to fill in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.27 – Define simple record wizard view](img/Figure_11.27_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.27 – Define simple record wizard view
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that I have selected `ToDoApplicationLb` load balancer to
    select. When you click the **Define simple record** button, you are then navigated
    to a list of records that are to be created. We carry out the creation wizard
    process one more time to account for all prefixes with the wildcard, and then
    confirm our creation of records. With this, our HTTPS now works with our application,
    as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.28 – HTTPS working with our application on the internet](img/Figure_11.28_B18722.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.28 – HTTPS working with our application on the internet
  prefs: []
  type: TYPE_NORMAL
- en: With this, our chapter is complete. If you try to access one of our EC2 instances
    directly through an IP address, you will be blocked. Therefore, we cannot directly
    access our EC2 instances but can access them through HTTPS through our URL. If
    you give users any variation of your URL, even an HTTP link to the URL, your users
    will happily use your application with the HTTPS protocol.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For now, we have done all we need to do to deploy a robust and secure application
    on AWS with locked-down traffic and HTTPS enforced. We have covered a lot to get
    here, and the skillset that you have gained in this chapter can be applied to
    pretty much any other project you want to deploy on AWS if you can package it
    in Docker. You now understand the advantages of HTTPS and the steps needed to
    not only achieve the HTTPS protocol but also to map your URL to the IP address
    of a server or a load balancer. What is more, we automated the attachment of certificates
    that we created using Certificate Manager to our load balancer in Terraform using
    the powerful data query resource that Terraform has to offer. Finally, this all
    came together when we managed to access our application using HTTPS and only HTTPS.
    Not only have we developed some practical skills that will become useful in many
    future projects, but we have also explored the nature of how HTTPS and DNS work,
    giving us a deeper understanding and appreciation of how the internet generally
    works when we type in a URL to the browser.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the Rocket framework. Due to how we have
    built our Rust modules in our Actix web application, we will be able to lift modules
    directly from the Actix web application and slot them into the Rocket application.
    Considering what we have done in this chapter, we will also be able to wrap our
    Rocket application in Docker and slot it into the build pipeline here by merely
    changing one line of code in the deployment `docker-compose` file. In the next
    chapter, you will see firsthand that when everything is well structured and isolated,
    changing features and frameworks is not going to be a headache and is, in fact,
    fairly joyful.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Application load balancers documentation: [https://docs.aws.amazon.com/AmazonECS/latest/developerguide/load-balancer-types.html](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/load-balancer-types.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Security group documentation: [https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we remember that our system still has the database exposed to the rest
    of the world, how would we lock it down?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why should we use an elastic IP in some cases?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could we automate the association of an existing elastic IP to an EC2 instance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we utilize security groups to lock down traffic between a URL and EC2
    instances?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s say that the traffic for our application greatly increased and our two
    instances cannot handle the pressure. What could we do to enable our system to
    handle the increase?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the basic premise of a DNS record?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a URL hosted zone?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We would create a security group for the database that only accepts traffic
    to and from the EC2 instance security group. If we were having to make a migration,
    we can SSH into an EC2 instance and use it as a proxy to connect to a database.
    You can also do this with database viewing software such as DataGrip. Sometimes
    you can have an EC2 instance that is just there for a user to use as a proxy to
    access the database. This is known as a bastion server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we destroy and create EC2 instances, the IP address of the EC2 instance
    will change. We can use an elastic IP to ensure that the IP address remains consistent,
    which can be helpful for automation pipelines as we can continue to point to that
    IP address. However, if we are using a load balancer, we do not need to use an
    elastic IP address as we will point our URL to the load balancer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can automate the association of an elastic IP to an EC2 instance using Terraform
    and its powerful data resource. This means we get the data of the existing elastic
    IP with a data query, and then attach this IP to an EC2 instance that we are creating.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that the security group for the EC2 instances in the target group that
    the load balancer is attached to can only accept HTTP traffic from the load balancer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Considering that everything in our system is automated and piped into each other,
    all we must do is increase the count in our EC2 instance definition from 2 to
    3\. This will increase the number of EC2 instances handling the traffic from 2
    to 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A DNS record is essentially a routing rule that will tell us how to route traffic
    from the URL to a server IP, URL namespace, or AWS service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A URL hosted zone is a collection of DNS records for a URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Part 5:Making Our Projects Flexible
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have a fully working application deployed on AWS with HTTPS and a database.
    However, there are a lot of moving parts to get there. In this part, we transfer
    the application that we have built over the book into a Rocket application to
    see how structuring our code well will give us flexibility when choosing a web
    framework. We then cover practices on how to keep our web application repository
    clean and flexible with build/deployment pipelines and automated migration Docker
    containers that fire once to apply database migrations and then die. We also cover
    multi-stage builds and how to build distroless server Docker images that are roughly
    50 MB.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part includes the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B18722_12.xhtml#_idTextAnchor246), *Recreating Our Application
    in Rocket*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B18722_13.xhtml#_idTextAnchor264), *Best Practices for a Clean
    Web App Repository*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

<html><head></head><body>
        

                            
                    <h1 class="header-title">Concurrency</h1>
                
            
            
                
<p>Modern day software is rarely written to perform tasks sequentially. It is more important today to be able to write programs that do more than one thing at a time and do it correctly. As transistors keep getting smaller, computer architects are unable to scale CPU clocks frequency due to quantum effects in the transistors. This has shifted focus more towards building concurrent CPU architectures that employ multiple cores. With this shift, developers need to write highly concurrent applications to maintain performance gains that they had for free when Moore's law was in effect.</p>
<p>But writing concurrent code is hard and languages that don't provide better abstractions make the situation worse. Rust attempts to make things better and safer in this space. In this chapter, we will go through the concepts and primitives that enable Rust to provide fearless concurrency to developers, allowing them to easily express their programs in a way that can safely do more than one thing at a time.</p>
<p>The topics covered in this chapter are as follows:</p>
<ul>
<li>Program execution models</li>
<li>Concurrency and associated pitfalls</li>
<li>Threads as unit of concurrency</li>
<li>How Rust provides thread-safety</li>
<li>Concurrency primitives in Rust</li>
<li>Other libraries for concurrency</li>
</ul>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Program execution models</h1>
                
            
            
                
<p>"An evolving system increases its complexity unless work is done to reduce it."                                                                                                                    </p>
<p class="CDPAlignRight CDPAlign">- <em>Meir Lehman</em></p>
<p>In the early 1960s, before multitasking was even a thing, programs written for computers were limited to a sequential execution model, where they were able to run instructions one after the other in chronological order. This was mainly due to limitations in how many instructions the hardware could process during that time. As we shifted from vacuum tubes to transistors, then to integrated chips, the modern day computer opened up possibilities to support multiple points of execution in programs. Gone are the days of sequential programming model where computers had to wait for an instruction to finish before executing the next one. Today, it's more common for computers to be able to do more than one thing at a time and do it correctly.</p>
<p>The modern day computer models a concurrent execution model, where a bunch of instructions can execute independently of each other with overlapping time periods. In this model, instructions need not wait for each other and run nearly at the same time, except when they need to share or coordinate with some data. If you look at the modern day software, it does many things that appear to happen at the same time, as in the following examples:</p>
<ul>
<li>The user interface of a desktop application continues to work normally even though the application connects to the network in the background</li>
<li>A game updates the state of thousands of entities at the same time, while playing a soundtrack in the background and keeping a consistent frame rate</li>
<li>A scientific, compute-heavy program splits computation in order to take full advantage of all of the cores in the machine</li>
<li>A web server handles more than one request at a time in order to maximize throughput</li>
</ul>
<p>These are some really compelling examples that propel the need to model our program as concurrent processes. But what does concurrency really mean? In the next section, let's define that.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Concurrency</h1>
                
            
            
                
<p>The ability of a program to manage more than one thing at a time while giving an illusion of them happening at the same time is called concurrency, and such programs are called concurrent programs. Concurrency allows you to structure your program in a way that it performs faster if you have a problem that can be split into multiple sub-problems. When talking about concurrency, another term called parallelism is often thrown in the discussion, and it is important we know the differences as the usage of these terms often overlap. Parallelism is when each task runs simultaneously on separate CPU cores  with non-overlapping time periods. The following diagram illustrates the difference between concurrency and parallelism:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/03050029-7f64-46ff-a375-a1e6de196796.png" style="width:39.08em;height:23.50em;"/></p>
<p>To put it another way, concurrency is about structuring your program to manage more than one thing at a time, while parallelism is about putting your program on multiple cores to increase the amount of work it does in a period of time. With this definition, it follows that concurrency when done right, does a better utilization of the CPU while parallelism might not in all cases. If your program runs in parallel but is only dealing with a single dedicated task, you aren't gaining much throughput. This is to say that we gain the best of both worlds when a concurrent program is made to run on multiple cores.</p>
<p class="mce-root"/>
<p>Usually, the support for concurrency is already provided at the lower levels by the operating system, and developers mostly program against the higher level abstractions provided by programming languages. On top of the low level support, there are different approaches to concurrency.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Approaches to concurrency</h1>
                
            
            
                
<p>We use concurrency to offload parts of our program to run independently. At times, these parts may depend on each other and are progressing towards a common goal or they may be embarrassingly parallel, which is a term used to refer to problems that can be split into independent stateless tasks, for instance, transforming each pixel of an image in parallel. As such, the approaches used to make a program concurrent depend on what level we are leveraging concurrency and the nature of the problem we are trying to solve. In the next section, let's discuss the available approaches to concurrency.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Kernel-based</h1>
                
            
            
                
<p>With multitasking being the norm these days, modern operating systems need to deal with more than one processes. As such, your operating system kernel already provides primitives for writing concurrent programs in one of the following forms:</p>
<ul>
<li><strong>Processes</strong>: In this approach, we can run different parts of a program by spawning separate replicas of themselves. On Linux, this can be achieved using the <kbd>fork</kbd> system call. To communicate any data with the spawned processes, one can use various <strong>Inter Process Communication</strong> (<strong>IPC</strong>) facilities such as pipes and FIFOs. Process based concurrency provides you with features such as fault isolation, but also has the overhead of starting a whole new process. There's a limited number of processes you can spawn before the OS runs out of memory and kills them. Process-based concurrency is seen in Python's multiprocessing module.</li>
<li><strong>Threads</strong>: Processes under the hood are just threads, specifically called the main thread. A process can launch or spawn one or more threads. A thread is the smallest schedulable unit of execution. Every process starts with a main thread. In addition to that, it can spawn additional threads using the APIs provided by the OS. To allow a programmer to use threads, most languages come with threading APIs in their standard library. They are lightweight compared to processes. Threads share the same address space with the parent process. They don't need to have a separate entry in the <strong>Process Control Block (PCB)</strong> in the kernel, which is updated every time we spawn a new process. But taming multiple threads within a process is a challenge because, unlike processes, they share the address space with their parent process and other child threads and, because scheduling of threads is decided by the OS, we cannot rely on the order the threads will execute and what memory they will read from or write to. These operations suddenly become hard to reason about when we go from a single-threaded program to a multi-threaded one.</li>
</ul>
<div><strong>Note</strong>: The implementation of threads and processes differ between operating systems. Under Linux, they are treated the same by the kernel, except that threads don't have their own process control block entry in the kernel and they share the address space with their parent process and any other child threads.</div>


            

            
        
    

        

                            
                    <h1 class="header-title">User-level</h1>
                
            
            
                
<p>Process- and thread-based concurrency are limited by how many of them we can spawn. A lighter and more efficient alternative is to use user space threads, popularly known as green threads. They first appeared in Java with the code name <em>green</em> and the name has stuck since then. Other languages such as Go (goroutines), and Erlang also have green threads. The primary motivation in using green threads is to reduce the overhead that comes with using process- and thread-based concurrency. Green threads are very lightweight to spawn and use less space than a thread. For instance, in Go, a goroutine takes only 4 KiB of space compared to the usual 8MB by a thread.</p>
<p>User space threads are managed and scheduled as part of the language runtime. A runtime is any extra bookeeping or managing code that's executed with every program you run. This would be your garbage collector or the thread scheduler. Internally, user space threads are implemented on top of native OS threads. Rust had green threads before the 1.0 version, but they were later removed before the language hit stable release. Having green threads would have steered away Rust's guarantee and its principle of having no runtime costs.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>User space concurrency is more efficient, but hard to get right in its implementation. Thread-based concurrency, however, is a tried and tested approach and has been popular since multi-process operating systems came into existence and it's the go- to approach for concurrency. Most mainstream languages provide threading APIs that allows users to create threads and easily offload a portion of their code for independent execution.</p>
<p>Leveraging concurrency in a program follows a multi-step process. First, we need to identify parts of our problem that can be run independently. Then, we need to look for ways to co-ordinate threads that are split into multiple sub-tasks to accomplish a shared goal. In the process, threads might also need to share data and they need synchronization for accessing or writing to shared data. With all of the benefits that concurrency brings with it, there are a new set of challenges and paradigms that developers need to care and plan for. In the next section, let's discuss the pitfalls of concurrency.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Pitfalls</h1>
                
            
            
                
<p>The advantages of concurrency abound, but it brings a whole lot of complexity and pitfalls that we have to deal with. Some issues when writing concurrent programs code are as follows:</p>
<ul>
<li><strong>Race conditions</strong>: As threads are scheduled by the operating system, we don't have a say in what order and how threads will access a shared data. A common use case in multi-threaded code is about updating a global state from multiple threads. This follows a three step process—read, modify, and write. If these three operations aren't performed atomically by threads, we may end up with a race condition.</li>
</ul>
<p>A set of operations is atomic if they execute together in an indivisble manner. For a set of operations to be atomic, it must not be pre-empted in the middle of its execution. It must execute completely or not at all.</p>
<p style="padding-left: 60px">If two threads try to update a value at a memory location at the same time, they might end up overwriting each other's values and only one of the updates will ever be written to memory or the value might not get updated at all. This is a classic example of a race condition. Both threads are racing to update the value without any co-ordination with each other. This leads to other issues such as data races.</p>
<p class="mce-root"/>
<ul>
<li><strong>Data race</strong>: When multiple threads try to write data to a certain location in memory and when both of them write at the same time, it's hard to predict what values will get written. The end result in the memory could also be garbage value. Data race is a consequence of a race condition, as read-modify-update operation must happen atomically by any thread to ensure that consistent data gets read or written by any thread.</li>
<li><strong>Memory unsafety and undefined behavior</strong>: Race conditions can also lead to undefined behavior. Consider the following pseudocode:</li>
</ul>
<pre class="mce-root">// Thread A<br/><br/>Node get(List list) {<br/>    if (list.head != NULL) {<br/>        return list.head<br/>    }<br/>}<br/><br/>// Thread B<br/>list.head = NULL</pre>
<p>We have two threads, A and B, that act on a linked list. <kbd>Thread A</kbd> tries to retrieve the head of the list. For doing this safely, it first checks the head of the list is not <kbd>NULL</kbd> and then returns it. <kbd>Thread B</kbd> sets the head of the list to a <kbd>NULL</kbd> value. Both of these run at nearly the same time and might get scheduled by the OS in different order. For instance, in one of the execution instances, the point where <kbd>Thread A</kbd> runs first and asserts that <kbd>list.head</kbd>, is not <kbd>NULL</kbd>. Right after that, <kbd>Thread A</kbd> is preempted by the OS and <kbd>Thread B</kbd> is scheduled to run. Now, <kbd>Thread B</kbd> sets <kbd>list.head</kbd> to <kbd>NULL</kbd>. Following that, when <kbd>Thread A</kbd> gets the chance to run, it will try to return <kbd>list.head</kbd> which is a <kbd>NULL</kbd> value. This would result in a segmentation fault when <kbd>list.head</kbd> is read from. In this case, memory unsafety happens because ordering is not maintained for these operations.</p>
<p>There is a common solution to the previously mentioned problems—synchronizing or serializing access to shared data or code or ensuring that the threads run critical sections atomically. This is done using synchronization primitives such as a mutex, semaphores, or conditional variables. But even using these primitives can lead to other issues such as deadlocks.</p>
<p class="mce-root"/>
<p><strong>Deadlocks</strong>: Apart from race conditions, another issue that threads face is getting starved of resources while holding a lock on a resource. Deadlock is a condition where a Thread A holding a resource a and waiting for resource b. Another Thread B is holding a resource <kbd>b</kbd> and is waiting for resource a. The following diagram depicts the situation:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/7470c731-80cc-4842-9826-0dad5073d09d.png" style="width:31.42em;height:19.25em;"/></p>
<p>Deadlocks are hard to detect but they can be solved by taking locks in the correct order. In the preceding case, if both Thread A and Thread B try to take the lock first, we can ensure that the locks are released properly.</p>
<p>With the advantages and pitfalls explored, let's go through the APIs that Rust provides to write concurrent programs.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Concurrency in Rust</h1>
                
            
            
                
<p>Rust's concurrency primitives rely on native OS threads. It provides threading APIs in the <kbd>std::thread</kbd> module in the standard library. In this section, we'll start with the basics on how to create threads to perform tasks concurrently. In subsequent sections,  we'll explore how threads can share data with each other.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Thread basics</h1>
                
            
            
                
<p>As we said, every program starts with a main thread. To create an independent execution point from anywhere in the program, the main thread can spawn a new thread, which becomes its child thread. Child threads can further spawn their own threads. Let's look at a concurrent program in Rust that uses threads in the simplest way possible:</p>
<pre class="mce-root">// thread_basics.rs<br/><br/>use std::thread;<br/><br/>fn main() {<br/>    thread::spawn(|| {<br/>        println!("Thread!");<br/>        "Much concurrent, such wow!".to_string()<br/>    });<br/>    print!("Hello ");<br/>}</pre>
<p>In <kbd>main</kbd>, we call the <kbd>spawn</kbd> function from the <kbd>thread</kbd> module which takes a no parameter closure as an argument. Within this closure, we can write any code that we want to execute concurrently as a separate thread. In our closure, we simply print some text and return <kbd>String</kbd>. Compiling and running this program gives us the following output:</p>
<pre><strong>$ rustc thread_basics.rs<br/>$ ./thread_basics</strong><br/>Hello</pre>
<p>Strange! We only get to see <kbd>"Hello"</kbd> being printed. What happened to <kbd>println!("Thread");</kbd> from the child thread ? A call to <kbd>spawn</kbd> creates the thread and returns immediately and the thread starts executing concurrently without blocking the instructions after it. The child thread is created in the detached state. Before the child thread has any chance to run its code, the program reaches the <kbd>print!("Hello");</kbd> statement and exits the program when it returns from <kbd>main</kbd>. As a result, code within the child thread doesn't execute at all. To allow the child thread to execute its code, we need to wait on the child thread. To do that, we need to first assign the value returned by <kbd>spawn</kbd> to a variable:</p>
<pre class="mce-root">let child = thread::spawn(|| {<br/>    print!("Thread!");<br/>    String::from("Much concurrent, such wow!")<br/>});</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The <kbd>spawn</kbd> function returns a <kbd>JoinHandle</kbd> type, which we store in the <kbd>child</kbd> variable. This type is a handle to the child thread, which can be used to join a thread—in other words, wait for its termination. If we ignore the <kbd>JoinHandle</kbd> type of a thread, there is no way to wait for the thread. Continuing with our code, we call the <kbd>join</kbd> method on the child before exiting from <kbd>main</kbd> as in the following:</p>
<pre>let value = child.join().expect("Failed joining child thread");</pre>
<p>Calling <kbd>join</kbd> blocks the current thread and waits for the child thread to finish before executing any line of code following the <kbd>join</kbd> call. It returns a <kbd>Result</kbd> value. Since we know that this thread does not panic, we call <kbd>expect</kbd> to unwrap the <kbd>Result</kbd> type giving us the string. Joining the thread can fail if a thread is joining itself or gets deadlocked, and, in that case, it returns an <kbd>Err</kbd> variant with the value that was passed to the <kbd>panic!</kbd> call though, in this case, the returned value is of the <kbd>Any</kbd> type which must be downcasted to a proper type. Our updated code is as follows:</p>
<pre class="mce-root">// thread_basics_join.rs<br/><br/>use std::thread;<br/><br/>fn main() {<br/>    let child = thread::spawn(|| {<br/>        println!("Thread!");<br/>        String::from("Much concurrent, such wow!")<br/>    });<br/><br/>    print!("Hello ");<br/>    let value = child.join().expect("Failed joining child thread");<br/>    println!("{}", value);<br/>}</pre>
<p>Here's the output of the program:</p>
<pre class="mce-root"><strong>$ ./thread_basics_join</strong><br/>Hello Thread!<br/>Much concurrent, such wow!</pre>
<p>Great ! We wrote our first concurrent <em>hello world</em> program. Let's explore other APIs from the <kbd>thread</kbd> module.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Customizing threads</h1>
                
            
            
                
<p>We also have APIs that can be used to configure threads by setting their properties such as the name or their stack size. For this, we have the <kbd>Builder</kbd> type from the <kbd>thread</kbd> module. Here's a simple program that creates a thread and spawns it using the <kbd>Builder</kbd> type:</p>
<pre class="mce-root">// customize_threads.rs<br/><br/>use std::thread::Builder;<br/><br/>fn main() {<br/>    let my_thread = Builder::new().name("Worker Thread".to_string())<br/>                                  .stack_size(1024 * 4);<br/>    let handle = my_thread.spawn(|| {<br/>        panic!("Oops!");<br/>    });<br/>    let child_status = handle.unwrap().join();<br/>    println!("Child status: {}", child_status);<br/>}</pre>
<p>In the preceding code, we use the <kbd>Builder::new</kbd>, method followed by calling the <kbd>name</kbd> and <kbd>stack_size</kbd> methods to add a name to our thread and its stack size respectively. We then call <kbd>spawn</kbd> on <kbd>my_thread</kbd>, which consumes the builder instance and spawns the thread. This time, within our closure, we <kbd>panic!</kbd> with an <kbd>"Oops"</kbd> message. Following is the output of this program:</p>
<pre><strong>$ ./customize_threads</strong> <br/>thread 'Worker Thread' panicked at 'Oops!', customize_threads.rs:9:9<br/>note: Run with `RUST_BACKTRACE=1` for a backtrace.<br/>Child status: Err(Any)</pre>
<p>We get to see that the thread has the same name we gave it - <kbd>"Worker Thread"</kbd>. Also, notice the <kbd>"Child status"</kbd> message that's returned as an <kbd>Any</kbd> type. Values returned from panic call in a thread are returned as an <kbd>Any</kbd> type and must be downcasted to a specific type. That's all on the basics of spawning threads.</p>
<p>But the threads we spawned in the preceding code examples aren't doing much. We use concurrency to solve problems that can be split into multiple sub-tasks. In simple cases, these sub-tasks are independent of each other such as applying a filter to each pixel of an image in parallel. In other situations, the sub-tasks running in threads might want want to co-ordinate on some shared data.</p>
<p class="mce-root"/>
<p>They might also be contributing to a computation whose end result depends on the individual results from the threads, for instance, downloading a file from multiple threads in blocks and communicating it to a parent manager thread. Other problems might be dependent on a shared state such as an HTTP client sending a <kbd>POST</kbd> request to a server that has to update the database. Here, the database is the shared state common to all threads. These are some of the most common use cases of concurrency and it's important that threads are able to share or communicate data back and forth between each other and with their parent thread.</p>
<p>Let's step up the game a bit and look at how we can access existing data from parent threads within child threads.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Accessing data from threads</h1>
                
            
            
                
<p>A thread that doesn't communicate or access data from the parent thread is not much. Let's take a very common pattern of using multiple threads to concurrently access items in a list to perform some computation. Consider the following code:</p>
<pre class="mce-root">// thread_read.rs<br/><br/>use std::thread;<br/><br/>fn main() {<br/>    let nums = vec![0, 1, 2, 3, 4];<br/>    for n in 0..5 {<br/>        thread::spawn(|| {<br/>            println!("{}", nums[n]);<br/>        });<br/>    }<br/>}</pre>
<p>In the preceding code, we have <kbd>5</kbd> numbers in <kbd>values</kbd> and we spawn <kbd>5</kbd> threads where each one of them accesses the data in <kbd>values</kbd>. Let's compile this program:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/ac1bd9b6-7a3d-4362-b894-7349fc9ea2fb.png" style="width:54.50em;height:10.67em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Interesting ! The error makes sense if you think about it from a borrowing perspective. <kbd>nums</kbd> comes from the main thread. When we spawn a thread, it is not guaranteed to exit before the parent thread and may outlive it. When the parent thread returns, the <kbd>nums</kbd> variable is gone and <kbd>Vec</kbd> it's pointing to is freed. If the preceding code was allowed by Rust, the child thread could have accessed <kbd>nums</kbd> which might have some garbage value after <kbd>main</kbd> returns and it would have undergone a segmentation fault.</p>
<p>If you look at the help message from from the compiler, it suggests us to move or capture <kbd>nums</kbd> inside the closure. This way the referenced a <kbd>nums</kbd> variable from <kbd>main</kbd> is moved inside <kbd>closure</kbd> and it won't be available in the <kbd>main</kbd> thread.</p>
<p>Here's the code that uses the <kbd>move</kbd> keyword to move the value from the parent thread in its child thread:</p>
<pre class="mce-root">// thread_moves.rs<br/><br/>use std::thread;<br/><br/>fn main() {<br/>    let my_str = String::from("Damn you borrow checker!");<br/>    let _ = thread::spawn(move || {<br/>        println!("In thread: {}", my_str);<br/>    });<br/>    println!("In main: {}", my_str);<br/>}</pre>
<p>In the preceding code, we are trying to accessed <kbd>my_str</kbd> again. This fails with the following error:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/247021c7-5046-4d3c-87b5-c143f34aa150.png" style="width:57.67em;height:11.17em;"/></p>
<p>As you can see from the preceding error message, with <kbd>move</kbd>, you don't get to use the data again, even if we are only reading <kbd>my_str</kbd> from our child thread. Here too, we are saved by the compiler. If the child thread frees the data and we access <kbd>my_str</kbd> from <kbd>main</kbd>, we'll access a freed value which is a use after free issue.</p>
<p class="mce-root"/>
<p>As you saw, the same rules of ownership and borrowing work in multi-threaded contexts too. This is one of the novel aspects of its design that doesn't require additional constructs to enforce correct concurrent code. But, how do we achieve the preceding use case of accessing data from threads? Because threads are more likely to outlive their parent, we can't have references in threads. Instead, Rust provides us with synchronization primitives that allow us to safely share and communicate data between threads. Let's explore these primitives. These types are usually composed in layers depending on the needs and you only pay for what you use.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Concurrency models with threads</h1>
                
            
            
                
<p>We mainly use threads to perform a task that can be split into sub-problems, where the threads might need to communicate or share data with each other. Now, using the threading model as the baseline, there are different ways to structure our program and control access to shared data. A concurrency model specifies how multiple threads interact with instructions and data shared between them and how they make progress over time and space (here, memory).</p>
<p>Rust does not prefer any opinionated concurrency model and frees the developer in using their own models depending on the problem they are trying to solve through third party crates. So, other models of concurrency exist that includes the actor model implemented as a library in the <kbd>actix</kbd> crate. There are other models too, such as the work stealing concurrency model implemented by the <kbd>rayon</kbd> crate. Then, there is the <kbd>crossbeam</kbd> crate, which allows concurrent threads to share data from their parent stack frame and are guaranteed to return before the parent stack is deallocated.</p>
<p>There are two popular built-in concurrency models with which Rust provides us: sharing data with synchronization and sharing data by message passing.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Shared state model</h1>
                
            
            
                
<p>Using shared state to communicate values to a thread is the most widely used approach, and the synchronization primitives to achieve this exist in most mainstream languages. Synchronization primitives are types or language constructs that allow multiple threads to access or manipulate a value in a thread-safe way. Rust also has many synchronization primitives that we can wrap around types to make them thread-safe.</p>
<p class="mce-root"/>
<p>As we saw in the previous section, we cannot have shared access to any value from multiple threads. We need shared ownership here. Back in <a href="db2c2723-8ca0-43be-b135-afd847342146.xhtml" target="_blank">Chapter 5</a>, <em>Memory Management and Safety</em>, we introduced the <kbd>Rc</kbd> type. that can provide shared ownership of values. Let's try using this type with our previous example of reading data from multiple threads:</p>
<pre class="mce-root">// thread_rc.rs<br/><br/>use std::thread;<br/>use std::rc::Rc;<br/><br/>fn main() {<br/>    let nums = Rc::new(vec![0, 1, 2, 3, 4]);<br/>    let mut childs = vec![];<br/>    for n in 0..5 {<br/>        let ns = nums.clone();<br/>        let c = thread::spawn(|| {<br/>            println!("{}", ns[n]);<br/>        });<br/>        childs.push(c);<br/>    }<br/><br/>    for c in childs {<br/>        c.join().unwrap();<br/>    }<br/>}</pre>
<p>This fails with the following error:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c89bb9a2-3bb7-4903-8e87-f93e0db940b2.png" style="width:61.42em;height:14.25em;"/></p>
<p>Rust saves us here too. This is because an <kbd>Rc</kbd> type is not thread-safe as mentioned previously, as the reference count update operation is not atomic. We can only use <kbd>Rc</kbd> in single-threaded code. If we want to have the same kind of shared ownership across multi-threaded contexts, we can use the <kbd>Arc</kbd> type, which is just like <kbd>Rc</kbd>, but has atomic reference counting capability.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Shared ownership with Arc</h1>
                
            
            
                
<p>The preceding code can be made to work with the multi-threaded <kbd>Arc</kbd> type as follows:</p>
<pre>// thread_arc.rs<br/><br/>use std::thread;<br/>use std::sync::Arc;<br/><br/>fn main() {<br/>    let nums = Arc::new(vec![0, 1, 2, 3, 4]);<br/>    let mut childs = vec![];<br/>    for n in 0..5 {<br/>        let ns = Arc::clone(&amp;nums);<br/>        let c = thread::spawn(move || {<br/>            println!("{}", ns[n]);<br/>        });<br/><br/>        childs.push(c);<br/>    }<br/><br/>    for c in childs {<br/>        c.join().unwrap();<br/>    }<br/>} </pre>
<p>In the preceding code, we simply replaced the wrapper of the vector from <kbd>Rc</kbd> to the <kbd>Arc</kbd> type. Another change is that, before we reference <kbd>nums</kbd> from a child thread, we need to clone it with <kbd>Arc::clone()</kbd>, which gives us an owned <kbd>Arc&lt;Vec&lt;i32&gt;&gt;</kbd> value that refers to the same <kbd>Vec</kbd>. With that change, our program compiles and provides safe access to the shared <kbd>Vec</kbd>, with the following output:</p>
<pre><strong>$ rustc thread_arc.rs</strong><br/><strong>$./thread_arc</strong><br/>0<br/>2<br/>1<br/>3<br/>4</pre>
<p>Now, another use case in multi-threaded code is to mutate a shared value from multiple threads. Let's see how to do that next.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Mutating shared data from threads</h1>
                
            
            
                
<p>We'll take a look at a sample program where five threads push data to a shared <kbd>Vec</kbd>. The following program tries to do the same:</p>
<pre>// thread_mut.rs<br/><br/>use std::thread;<br/>use std::sync::Arc;<br/><br/>fn main() {<br/>    let mut nums = Arc::new(vec![]);<br/>    for n in 0..5 {<br/>        let mut ns = nums.clone();<br/>        thread::spawn(move || {<br/>            nums.push(n);<br/>        });<br/>    }<br/>}</pre>
<p>We have the same <kbd>nums</kbd> wrapped with <kbd>Arc</kbd>. But we cannot mutate it, as the compiler gives the following error:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c617b0b5-c313-45fd-849a-e5d38347357b.png" style="width:33.50em;height:19.92em;"/></p>
<p class="mce-root"/>
<p>This doesn't work as cloning <kbd>Arc</kbd> hands out immutable reference to the inner value. To mutate data from multiple threads, we need to use a type that provides shared mutability just like <kbd>RefCell</kbd>. But similar to <kbd>Rc</kbd>, <kbd>RefCell</kbd> cannot be used across multiple threads. Instead, we need to use their thread-safe variants such as the <kbd>Mutex</kbd> or <kbd>RwLock</kbd> wrapper types. Let's explore them next.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Mutex</h1>
                
            
            
                
<p>When safe mutable access to a shared resource is required, the access can be provided by the use of mutex. Mutex is a portmanteau for mutual exclusion, a widely used synchronization primitive for ensuring that a piece of code is executed by only one thread at a time. A <kbd>mutex</kbd> in general is a guard object which a thread acquires to protect data that is meant to be shared or modified by multiple threads. It works by prohibiting access to a value from more than one thread at a time by locking the value. If one of the  threads has a lock on the <kbd>mutex</kbd> type, no other thread can run the same code until the thread that holds the lock is done with it.</p>
<p>The <kbd>std::sync</kbd> module from the standard library contains the <kbd>Mutex</kbd> type allowing one to mutate data from threads in thread-safe manner.</p>
<p>The following code example shows how to use the <kbd>Mutex</kbd> type from a single child thread:</p>
<pre>// mutex_basics.rs<br/><br/>use std::sync::Mutex;<br/>use std::thread;<br/><br/>fn main() {<br/>    let m = Mutex::new(0);<br/>    let c = thread::spawn(move || {<br/>        {<br/>            *m.lock().unwrap() += 1;<br/>        }<br/>        let updated = *m.lock().unwrap();<br/>        updated<br/>    });<br/>    let updated = c.join().unwrap();<br/>    println!("{:?}", updated);<br/>}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Running this works as expected. But, this won't work when multiple threads try to access the value as <kbd>Mutex</kbd> doesn't provide shared mutability. To allow a value inside a <kbd>Mutex</kbd> to be mutated from multiple threads, we need to compose it it the <kbd>Arc</kbd> type. Let's see how to do that next.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Shared mutability with Arc and Mutex</h1>
                
            
            
                
<p>Having explored the basics of Mutex in single threaded contexts, we'll revisit the example from the previous section. The following code modifies a value using a <kbd>Mutex</kbd> wrapped in an <kbd>Arc</kbd> from the multiple threads:</p>
<pre>// arc_mutex.rs<br/><br/>use std::sync::{Arc, Mutex};<br/>use std::thread;<br/><br/>fn main() {<br/>    let vec = Arc::new(Mutex::new(vec![]));<br/>    let mut childs = vec![];<br/>    for i in 0..5 {<br/>        let mut v = vec.clone();<br/>        let t = thread::spawn(move || {<br/>            let mut v = v.lock().unwrap(); <br/>            v.push(i);<br/>        });<br/>        childs.push(t);<br/>    }<br/><br/>    for c in childs {<br/>        c.join().unwrap();<br/>    }<br/><br/>    println!("{:?}", vec);<br/>}</pre>
<p>In the preceding code, we created a <kbd>Mutex</kbd> value in <kbd>m</kbd>. We then spawn a thread. The output on your machine may vary.</p>
<p class="mce-root"/>
<p>Calling <kbd>lock</kbd> on a mutex will block other threads from calling <kbd>lock</kbd> until the lock is gone. As such, it is important that we structure our code in such a way that the is granular. Compiling and running this gives the following output:</p>
<pre><strong>$ rustc arc_mutex.rs<br/>$ ./arc_mutex<br/></strong>Mutex { data: [0,1,2,3,4] }</pre>
<p>There is another similar alternative to <kbd>Mutex</kbd>, which is the <kbd>RwLock</kbd> type that is more aware on the kind of lock you have on your type, and can be more performant when reads are more often than writes. Let's explore it next.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">RwLock</h1>
                
            
            
                
<p>While Mutex is fine for most use cases, for some multi-threaded scenarios, reads happen more often than writes from multiple threads. In that case, we can use the RwLock type, which also provides shared mutability but can do so at a more granular level. RwLock stands for Reader-Writer lock. With <kbd>RwLock</kbd>, we can have many readers at the same but only one writer in a given scope. This is much better than a Mutex which agnostic of the kind of access a thread wants. Using RwLock</p>
<p>RwLock exposes two methods:</p>
<ul>
<li><kbd>read</kbd>: Gives read access to the thread. There can be many read invocations.</li>
<li><kbd>write</kbd>: Gives exclusive access to thread for writing data to the wrapped type. There can be one write access from an <kbd>RwLock</kbd> instance to a thread.</li>
</ul>
<p>Here's a sample program that demonstrates using the <kbd>RwLock</kbd> instead of <kbd>Mutex</kbd>:</p>
<pre>// thread_rwlock.rs<br/><br/>use std::sync::RwLock;<br/>use std::thread;<br/><br/>fn main() {<br/>    let m = RwLock::new(5);<br/>    let c = thread::spawn(move || {<br/>        {<br/>            *m.write().unwrap() += 1;<br/>        }<br/>        let updated = *m.read().unwrap();<br/>        updated<br/>    });<br/>    let updated = c.join().unwrap();<br/>    println!("{:?}", updated);<br/>}</pre>
<p>But <kbd>RwLock</kbd> on some systems such as Linux, suffers from the writer starvation problem. It's a situation when readers continually access the shared resource, and writer threads never get the chance to access the shared resource.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Communicating through message passing</h1>
                
            
            
                
<p>Threads can also communicate with each other through a more high level abstraction called message passing. This model of thread communication removes the need to use explicit locks by the user.</p>
<p>The standard library's <kbd>std::sync::mpsc</kbd> module provides a lock-free multi-producer, single-subscriber queue, which serves as a shared message queue for threads wanting to communicate with one another. The <kbd>mpsc</kbd> module standard library has two kinds of channels:</p>
<ul>
<li><kbd>channel</kbd>: This is an asynchronous, infinite buffer channel.</li>
<li><kbd>sync_channel</kbd>: This is a synchronous, bounded buffer channel.</li>
</ul>
<p>Channels can be used to send data from one thread to another. Let's look at asynchronous channels first.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Asynchronous channels</h1>
                
            
            
                
<p>Here is an example of a simple producer-consumer system, where the main thread produces the values <kbd>0, 1, ..., 9</kbd> and the spawned thread prints them:</p>
<pre>// async_channels.rs<br/><br/>use std::thread;
use std::sync::mpsc::channel;

fn main() {
    let (tx, rx) = channel();
    let join_handle = thread::spawn(move || {
        while let Ok(n) = rx.recv() {
            println!("Received {}", n);
        }
    });

    for i in 0..10 {
        tx.send(i).unwrap();
    }
<br/>    join_handle.join().unwrap();
}</pre>
<p>We first call the <kbd>channel</kbd> method. This returns two values, <kbd>tx</kbd> and <kbd>rx</kbd>. <kbd>tx</kbd> is the transmitter end, having type <kbd>Sender&lt;T&gt;</kbd> and <kbd>rx</kbd> is the receiver end having type <kbd>Receiver&lt;T&gt;</kbd>. Their names are just a convention and you can name them anything. Most often, you will see code bases use these names as they are concise to write.</p>
<p>Next, we spawn a thread that will receive values from the <kbd>rx</kbd> side:</p>
<pre>    let join_handle = thread::spawn(move || {
        // Keep receiving in a loop, until tx is dropped!
        while let Ok(n) = rx.recv() { // Note: `recv()` always blocks
            println!("Received {}", n);
        }
    });</pre>
<p>We use a <kbd>while let</kbd> loop. This loop will receive <kbd>Err</kbd> when <kbd>tx</kbd> is dropped. The drop happens when <kbd>main</kbd> returns.</p>
<p> </p>
<p>In the preceding code, first, to create the <kbd>mpsc</kbd> queue, we call the <kbd>channel</kbd> function, which returns to us <kbd>Sender&lt;T&gt;</kbd> and <kbd>Receiver&lt;T&gt;</kbd>.</p>
<p><kbd>Sender&lt;T&gt;</kbd> is a <kbd>Clone</kbd> type, which means it can be handed off to many threads, allowing them to send messages into the shared queue.</p>
<p>The <strong>multi producer, single consumer</strong> (<strong>mpsc</strong>) approach provides multiple writers but only a single reader. Both of these functions return a pair of generic types: a sender and a receiver. The sender can be used to push new things into the channel, while receivers can be used to get things from the channel. The sender implements the <kbd>Clone</kbd> trait while the receiver does not.</p>
<p> </p>
<p>With the default asynchronous channels, the <kbd>send</kbd> method never blocks. This is because the channel buffer is infinite, so there's always space for more. Of course, it's not really infinite, just conceptually so: your system may run out of memory if you send gigabytes to the channel without receiving anything.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Synchronous channels</h1>
                
            
            
                
<p>Synchronous channels have a bounded buffer and, when it's full, the <kbd>send</kbd> method blocks until there's more space in the channel. The usage is otherwise quite similar to asynchronous channels:</p>
<pre>// sync_channels.rs<br/><br/>use std::thread; <br/>use std::sync::mpsc; <br/><br/>fn main() { <br/>    let (tx, rx) = mpsc::sync_channel(1);<br/>    let tx_clone = tx.clone();<br/><br/>    let _ = tx.send(0);<br/><br/>    thread::spawn(move || { <br/>        let _ = tx.send(1);<br/>    }); <br/><br/>    thread::spawn(move || {<br/>        let _ = tx_clone.send(2);<br/>    }); <br/><br/>    println!("Received {} via the channel", rx.recv().unwrap());<br/>    println!("Received {} via the channel", rx.recv().unwrap());<br/>    println!("Received {} via the channel", rx.recv().unwrap());<br/>    println!("Received {:?} via the channel", rx.recv());<br/>}<br/><br/></pre>
<p>The synchronous channel size is <kbd>1</kbd>, which means that we can't have more than one item in the channel. Any send call after the first send will block in such a case. However, in the preceding code, we don't get blocks (at least, the long ones) as the two sending threads work in the background and the main thread gets to receive it without being blocked on the <kbd>send</kbd> call. For both these channel types, the <kbd>recv</kbd> call returns an <kbd>Err</kbd> value if the channel is empty.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">thread-safety in Rust</h1>
                
            
            
                
<p>In the previous section, we saw how the compiler stops us from sharing the data. If a child thread accesses data mutably, it is moved because Rust won't allow it to be used in the parent thread as the child thread might deallocate it, leading to a dangling pointer dereference in the main thread. Let's explore the idea of thread-safety and how Rust's type systems achieves that.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">What is thread-safety?</h1>
                
            
            
                
<p>thread-safety is the property of a type or a piece of code that, when executed or accessed by multiple threads, does not lead to unexpected behavior. It refers to the idea that data is consistent for reads while being safe from corruption when multiple threads write to it.</p>
<p>Rust only protects you from data races. It doesn't aim to protect against deadlocks as they are difficult to detect. It instead offloads this to third-party crates such as the <kbd>parking_lot</kbd> crate.</p>
<p>Rust has a novel approach to protecting against data races. Most of the thread-safety bits are already embedded in the <kbd>spawn</kbd> method's type signature. Let's look at its type signature:</p>
<pre class="mce-root">fn spawn&lt;F, T&gt;(f: F) -&gt; JoinHandle&lt;T&gt;<br/>    where F: FnOnce() -&gt; T,<br/>        F: Send + 'static,<br/>        T: Send + 'static</pre>
<p>That's a scary-looking type signature. Let's make it less scary by explaining what each of the parts mean.</p>
<p><kbd>spawn</kbd> is a generic function over <kbd>F</kbd> and <kbd>T</kbd> and takes a parameter, <kbd>f</kbd>, and returns a generic type called <kbd>JoinHandle&lt;T&gt;</kbd>. Following that, the <kbd>where</kbd> clause specifies multiple trait bounds:</p>
<ul>
<li><kbd>F: FnOnce() -&gt; T</kbd>: This says that <kbd>F</kbd> implements a closure that can be called only once. In other words, <kbd>f</kbd> is a closure that takes everything by value and moves items referenced from the environment.</li>
<li><kbd>F: Send + 'static</kbd>: This means that the closure must be <kbd>Send</kbd> and must have the <kbd>'static</kbd> lifetime, implying that any type referenced from within the closure in its environment must also be Send and must live for the entire duration of the program.</li>
<li><kbd>T: Send + 'static</kbd>: The return type, <kbd>T</kbd>, from the closure must also implement the <kbd>Send + 'static</kbd> trait.</li>
</ul>
<p>As we know, <kbd>Send</kbd> is a marker trait. It is just used as a type-level marker that implies that the value is safe to be sent across threads; most types are Send. Types that don't implement <kbd>Send</kbd> are pointers, references, and so on. In addition, <kbd>Send</kbd> is an auto trait or an automatically derived trait whenever applicable. Compound data types such as a struct implement <kbd>Send</kbd> if all of the fields in a struct are <kbd>Send</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Traits for thread-safety</h1>
                
            
            
                
<p>Thread-safety is the idea that, if you have data that you want to data from multiple threads, any read or write operation on that value does not lead to inconsistent results. The problem with updating a value, even with a simple increment operation such as <kbd>a += 1</kbd> is that it roughly translates in to a three-step process—<kbd>load</kbd> <kbd>increment</kbd> <kbd>store</kbd>. Data that can be safely updated is meant to be wrapped in thread-safe types such as <kbd>Arc</kbd> and <kbd>Mutex</kbd> to ensure that we have data consistency in a program.</p>
<p>In Rust, you get compile-time guarantees on types that can be safely used and referenced within a thread. These guarantees are implemented as traits, which are the <kbd>Send</kbd> and <kbd>Sync</kbd> trait.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Send</h1>
                
            
            
                
<p>A Send type is safe to send to multiple threads. This implies that the type is a <kbd>move</kbd> type. Types that aren't Send are pointer types such as <kbd>&amp;T</kbd>, unless <kbd>T</kbd> is <kbd>Sync</kbd>.</p>
<p>The <kbd>Send</kbd> trait has the following type signature in the standard library's <kbd>std::marker</kbd> module:</p>
<pre class="rust trait">pub unsafe auto trait Send { }</pre>
<p>There are three important things to notice in its definition: first, it's a marker trait without any body or item. Second, it's prefixed with the <kbd>auto</kbd> keyword as it is implemented implicitly for most types when appropriate. Thirdly, it's an unsafe trait because Rust wants to make the developer sure that they opt in explicitly and ensure that their type has thread-safe synchronization built in.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Sync</h1>
                
            
            
                
<p>The <kbd>Sync</kbd> trait has a similar type signature:</p>
<pre class="rust trait">pub unsafe auto trait Sync { }</pre>
<p>This trait signifies that types that implement this trait are safe to be shared between threads. If something is <kbd>Sync</kbd> then a reference to it in other words, <kbd>&amp;T</kbd> is <kbd>Send</kbd>. This means that we can pass references to it to many threads.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Concurrency using the actor model</h1>
                
            
            
                
<p>Another model of concurrency that is quite similar to the message passing model is the actor model. The actor model became popular with Erlang, a functional programming language popular in the telecom industry, known for its robustness and distributed by default nature.</p>
<p class="mce-root">The actor model is a conceptual model that implements concurrency at the type level using entities called actors. It was first introduced by Carl Eddie Hewitt in 1973. It removes the need for locks and synchronization and provides a cleaner way to introduce concurrency in a system. The actor model consists of three things:</p>
<ul>
<li class="mce-root"><strong>Actor:</strong> This is a core primitive in the actor model. Each actor consists of its address, using which we can send messages to an actor's and mailbox, which is just a queue to store the messages it has received. The queue is generally a <strong>First In, First Out</strong> (<strong>FIFO</strong>) queue. The address of an actor is needed so that other actors can send messages to it. The supervisor actor can create child actors that can create other child actors.</li>
<li class="mce-root"><strong>Messages:</strong> Actors communicate only via messages. They are processed asynchronously by actors. The <kbd>actix-web</kbd> framework provides a nice wrapper for synchronous operations in an asynchronous wrapper.</li>
</ul>
<p>In Rust, we have the <kbd>actix</kbd> crate that implements the actor model. The <kbd>actix</kbd> crate, uses the tokio and futures crate which we'll cover in <a href="33562886-4278-4ab1-aa0d-de533af4bb99.xhtml">Chapter 12</a>, <em>Network Programming in Rust</em>. The core objects to that crate is the Arbiter type which is simply a thread which spawns an event loop underneath and provides a handle to the event loop as an <kbd>Addr</kbd> type. Once created, we can use this handle to send messages to the actor.</p>
<p>In <kbd>actix</kbd>, creation of actor follows a simple step of creating a type, defining a message and implementing the handler for the message for the actor type. Once that is done, we can create the actor and spawn them into one of the created arbiters.</p>
<p>Each actor runs within an arbiter.</p>
<p>When we create an actor, they don't execute right away. It's when we put these actors into arbiter threads, they then start executing.</p>
<p>To keep the code example simple and to show how to setup actors and run them in actix, we'll create a actor that can add two numbers. Let's create a new project by running <kbd>cargo new actor_demo</kbd> with the following dependencies in <kbd>Cargo.toml</kbd>:</p>
<pre># actor_demo/Cargo.toml<br/><br/>[dependencies]<br/>actix = "0.7.9"<br/>futures = "0.1.25"<br/>tokio = "0.1.15"</pre>
<p>Our <kbd>main.rs</kbd> contains the following code:</p>
<pre>// actor_demo/src/main.rs<br/><br/>use actix::prelude::*;<br/>use tokio::timer::Delay;<br/>use std::time::Duration;<br/>use std::time::Instant;<br/>use futures::future::Future;<br/>use futures::future;<br/><br/>struct Add(u32, u32);<br/><br/>impl Message for Add {<br/>    type Result = Result&lt;u32, ()&gt;;<br/>}<br/><br/>struct Adder;<br/><br/>impl Actor for Adder {<br/>    type Context = SyncContext&lt;Self&gt;;<br/>}<br/><br/>impl Handler&lt;Add&gt; for Adder {<br/>    type Result = Result&lt;u32, ()&gt;;<br/><br/>    fn handle(&amp;mut self, msg: Add, _: &amp;mut Self::Context) -&gt; Self::Result {<br/>        let sum = msg.0 + msg.0;<br/>        println!("Computed: {} + {} = {}",msg.0, msg.1, sum);<br/>        Ok(msg.0 + msg.1)<br/>    }<br/>}<br/><br/>fn main() {<br/>    System::run(|| {<br/>        let addr = SyncArbiter::start(3, || Adder);<br/>        for n in 5..10 {<br/>            addr.do_send(Add(n, n+1));<br/>        }<br/><br/>        tokio::spawn(futures::lazy(|| {<br/>            Delay::new(Instant::now() + Duration::from_secs(1)).then(|_| {<br/>                System::current().stop();<br/>                future::ok::&lt;(),()&gt;(())<br/>            })<br/>        }));<br/>    });<br/>}</pre>
<p>In the preceding code, we have created an actor named <kbd>Adder</kbd>. This actor can send and receive messages of type <kbd>Add</kbd>. This is a tuple struct that encapsulates two numbers to be added. To allow <kbd>Adder</kbd> to receive and process <kbd>Add</kbd> messages, we implement the <kbd>Handler</kbd> trait for <kbd>Adder</kbd> parameterized over the <kbd>Add</kbd> message type. In the <kbd>Handler</kbd> implementation, we print the computation being performed and return the sum of the given numbers.</p>
<p>Following that, in <kbd>main</kbd>, we first create a <kbd>System</kbd> actor by calling its <kbd>run</kbd> method which takes in a closure. Within the closure, we start a <kbd>SyncArbiter</kbd> with <kbd>3</kbd> threads by calling its <kbd>start</kbd> method. This create 3 actors ready to receive messages. It returns a <kbd>Addr</kbd> type which is a handle to the event loop to which we can send messages to the <kbd>Adder</kbd> actor instance. We then send 5 messages to our arbiter address <kbd>addr</kbd>. As the System::run is an parent event loop that runs forever, we spawn a future to stop the System actor after a delay of 1 second. We can ignore the details of this part of the code as it is simply to shutdown the System actor in an asynchronous way.</p>
<p>With that said, let's take this program for a spin:</p>
<pre><strong>$ cargo run<br/>Running `target/debug/actor_demo`</strong><br/>Computed: 5 + 6 = 10<br/>Computed: 6 + 7 = 12<br/>Computed: 7 + 8 = 14<br/>Computed: 8 + 9 = 16<br/>Computed: 9 + 10 = 18</pre>
<p>Similar to the <kbd>actix</kbd> crate, there are other crates in the Rust ecosystem that implements various concurrency models suitable for different use cases.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Other crates</h1>
                
            
            
                
<p>Apart from <kbd>actix</kbd>, we have a crate named <kbd>rayon</kbd> which is a work stealing based data parallelism library that makes it dead simple to write concurrent code.</p>
<p>Another notable crate to mention is the <kbd>crossbeam</kbd> crate which allows one to write multi-threaded code that can access data from its parent stack frame and are guaranteed to terminate before the parent stack frame goes away.</p>
<p><kbd>parking_lot</kbd> is another crate that provides a faster alternative to concurrency primitives present in the standard library. If you have a use case where the standard library <kbd>Mutex</kbd> or <kbd>RwLock</kbd> is not performant enough, then you can use this crate to gain significant speedups.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>It is quite astonishing that the same ownership principle that prevents memory safety violations in single-threaded contexts also works for multithreaded contexts in composition with marker traits. Rust has easy and safe ergonomics for integrating concurrrency in your application with minimal runtime cost. In this chapter, we learned how to use the <kbd>threads</kbd> API provided by Rust's standard library and got to know how copy and move types work in the context of concurrency. We covered channels, the atomic reference counting type, <kbd>Arc</kbd>, and how to use <kbd>Arc</kbd> with <kbd>Mutex</kbd> and also explored the actor model of concurrency.</p>
<p>In the next chapter, we'll dive into metaprogramming which is all about generating code from code.<br/></p>


            

            
        
    </body></html>
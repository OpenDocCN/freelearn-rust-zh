- en: Extra Performance Enhancements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once your application avoids common performance bottlenecks, it's time to move
    to more complex performance improvements. Rust has many options that allow you
    to improve the performance of your code by using lesser-known APIs. This will
    give you parity with C/C++ and, in some scenarios, it can even improve the speed
    of most of the fastest C/C++ scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be looking into the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Compile-time checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compile-time state machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extra performance enhancements, such as using closures for avoiding runtime
    evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unstable sorting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Map hashing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard library collections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compile-time checks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rust has an amazing type system. It's so powerful that it is Turing-complete
    by itself. This means that you can write very complex programs just by using Rust's
    type system. This can help your code a lot, since the type system gets evaluated
    at compile time, making your runtime much faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting from the basics, what do we mean by *the type system*? Well, it means
    all those traits, structures, generics, and enums you can use to make your code
    very specialized at runtime. An interesting thing to know is the following: if
    you create a generic function that gets used with two different types, Rust will
    compile two specific functions, one for each type.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This might seem like code duplication but, in reality, it is usually faster
    to have a specific function for the given type than to try to generalize a function
    over multiple ones. This also allows for the creation of specialized methods that
    will take into account the data they are using. Let''s see this with an example.
    Suppose we have two structures and we want them to output a message with some
    of their information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a trait that we will implement for them that will return something
    that can be displayed in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And we implement it for our structures. Note that I have decided to return
    a reference to the data string in the case of the `StringData` structure. This
    simplifies the logic but adds some lifetimes and some extra referencing to the
    variable. This is because the reference must be valid while `StringData` is valid.
    If not, it might try to print non-existent data, and Rust prevents us from doing
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, one of them returns a string and the other returns an integer,
    so it would be very difficult to create a function that allows both of them to
    work, especially in a strongly-typed language. But since Rust will create two
    completely different functions for them, each using their own code, this can be
    solved thanks to generics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the `println!` macro will call to the specific methods of the
    `i32` and `&str` structures. We then simply create a small `main()` function to
    test everything, and you should see how it can print both structures perfectly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You might be tempted to think that this is similar to what languages such as
    Java do with their interfaces, and, functionally, it is. But talking about performance,
    our topic in this book, they are very different. Here, the generated machine code
    will effectively be different between both calls. One clear symptom is that the
    `print()` method gets ownership of the value it receives, so the caller must pass
    this in the registers of the CPU. Both structures are fundamentally different
    though. One is bigger than the other (containing the string pointer, the length,
    and capacity), so the way the call is done must be different.
  prefs: []
  type: TYPE_NORMAL
- en: So, great, Rust does not use the same structure for traits as Java does for
    interfaces. But why should you care? Well, there are a number of reasons, but
    there is one that will probably show you what this accomplishes. Let's create
    a state machine.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential state machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's first think about how to implement this in a C/C++ environment. You will
    probably have a global state and then a `while` loop that would change the state
    after each iteration. There are, of course, many ways of implementing a state
    machine but, in C, all of them require either metaprogramming or a runtime evaluation
    of the global state.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Rust, we have a Turing-complete type system, so why not try and use it to
    create that state machine? Let''s start by defining some traits that will have
    the power of creating the state machine. We first define a `StateMachine` trait,
    that will have the functionality of moving from one state to another state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, I already added a new type, `MainLogic`. This will be a trait
    representing a structure that can perform a logic in a state. The `StateMachine`
    trait itself is pretty simple. It only contains a type that will be the next state
    and an `execute()` function that consumes itself so that nobody can execute the
    same state twice without going to the next state (the next state could be itself
    again). It simply returns a new state machine. Here we have the `MainLogic` trait:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s just a function to execute the logic of the state. The main functionality
    of this state machine, that will enable it to go from one state to the next, always
    doing the proper logic, is defined in the default implementation of the `MainLogic`
    trait:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This will implement the `MainLogic` trait for any state implementing the `StateMachine`
    trait. It will simply execute the state and then call the main logic of the next
    state. If this new state is also a `StateMachine`, it will get executed and then
    the next state will be executed. This pattern is especially useful if you want
    to sequentially execute different states. The last state will be the one implementing
    `MainLogic` but not `StateMachine`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The compiler will make sure at compile time that you properly go from the first
    state to the second one, and it will force you to do so. But, more importantly,
    this will be compiled into very efficient code, as efficient as doing the sequential
    calls one by one, but with all the safety Rust gives you. In fact, as you can
    see, both `FirstState` and `Laststate` have no attributes. That is because they
    have no size. They will not occupy space in memory at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: This is the simplest state machine though. It will only allow you to advance
    from one state to the next. It's helpful if that's what you want, since it will
    make sure your flow gets checked at compile time, but it will not perform complex
    patterns. If you loop over a previous state, you will endlessly continue looping.
    This will also be useful when each state has a defined next state and when no
    other possibility comes from that state.
  prefs: []
  type: TYPE_NORMAL
- en: Complex state machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A more complex state machine, that allows you to move from one to another in
    your code while still using the type system to check for proper usage, can be
    done. Let's start by defining the state machine. We want a machine that represents
    the way a robot works in a car-building facility. Let's say its job is to install
    two doors in a car. It will first wait for the next car to come, take the door,
    put it in place, put the bolts in place, do the same for the second door, and
    then wait for the next car.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first define some functions that will use sensors and that we will
    simulate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, the real software would need to take many things into account. It
    should check that the environment is safe, the way it moves the door to the car
    should be optimal, and so on, but this simplification will do for now. We now
    define some states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And we then define the machine itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This machine will hold an internal state that can have some information attached
    to it (it can be any kind of structure) or it can have a zero-sized structure,
    and thus have a size of zero bytes. We will then implement our first transition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This will simply check every 1 second whether the car is in the proper place.
    Once it is, it will return the next state, the `TakingDoor` state. The function
    signature makes sure that you cannot return the incorrect state, even if you do
    a really complex logic inside the `from()` function. Moreover, at compile time,
    this `DoorMachine` will have zero byte size, as we saw, so it will not consume
    RAM regardless of how complex our state transitions are. Of course, the code for
    the `from()` functions will be in RAM, but the necessary checks for proper transitioning
    will all be done at compile time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then implement the next transition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, a similar thing can be done for the last state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The machine can start in any given state, and moving it from one to another
    will be as simple as writing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You might be thinking, *why don't I simply write two functions and execute them
    sequentially?* The answer is not straightforward, but it's easy to explain. This
    makes you avoid many issues at compile time. For example, if each state has only
    one possible next state, you can use a generic `into()` function without needing
    to know the current state, and it will simply work.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a more complex environment, you might find yourself doing the following
    pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Of course, if you look at it properly, we are no longer in the first state!
    What will happen if the machine tries to change the state again, thinking it's
    still in the first state? Well, here is where Rust comes handy. The `into()` function
    takes ownership of the binding, so this will simply not compile. Rust will complain
    that the `beginning_state` no longer exists since it has been already converted
    to `next_state`.
  prefs: []
  type: TYPE_NORMAL
- en: Real-life type system check example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is an example I love when talking about compile-time checks and high-performance
    computing: Philipp Oppermann wrote a type-safe paging system for a kernel with
    only two traits. Let''s first understand the problem and then try the solution.'
  prefs: []
  type: TYPE_NORMAL
- en: When a program uses the memory on a computer, it must separate virtual memory
    from physical memory. This is because each program running in the OS will think
    that the whole address space is theirs. This means that in a 64-bit machine, each
    program will think it has 16 **exbibytes** (**EiB**) of memory, 2^(64) bytes.
  prefs: []
  type: TYPE_NORMAL
- en: That is, of course, not the case for any computer in the world, so what the
    kernel does is to move memory out of RAM to the HDD/SSD, and put in RAM the required
    memory. For this to work properly, memory has to be managed in chunks, since it
    doesn't make sense to move in and out individual memory addresses. These are called
    pages, and they are usually 4 KiB in size for x86_64 processors (the case for
    most laptops and desktop computers).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the paging to be easily manageable, a paging hierarchy gets created. Every
    512 pages get added to an index called a P1 table, and every 512 P1 tables get
    added to a P2 table. That goes on recursively until all pages have been assigned,
    which will be 4 levels. That is what it''s called: 4-level paging.'
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that a kernel should be able to ask a table for one of its pages,
    and if it's a P4 table, it should be able to ask for a P3, then for a P2, then
    for a P1, and finally load the page referenced by that P1\. This address gets
    passed in a 64-bit registry, so all the data is there. The problem is that it
    could be easy to end up with tons of code duplication for each table type, or
    we could end up with a solution that works for all pages but that has to check
    the current page at runtime if it wants to return the next table (if it's a P4-P2
    table) or the actual page (if it's a P1 table).
  prefs: []
  type: TYPE_NORMAL
- en: The first case is really error-prone, and difficult to maintain, while the second
    one not only continues being error-prone, but it even requires checks at runtime,
    making it slower. Rust can do better.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution is to define a trait that all pages have, let''s call it `PageTable`,
    and a trait that only higher-order tables have (tables that cannot directly return
    a page but that they need to return another page table). Let''s call it `HighTable`.
    Since all `HighTable` types are also `PageTable`, one trait will inherit from
    the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates four enumerations representing page table levels. The reason for
    using enumerations instead of structures is that empty enumerations cannot be
    instantiated, which will avoid some typos. Then we write the `HighTable` trait:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we add an associated type to each enumeration to represent the
    next level of paging. But, of course, in the case of the last level, it will not
    have another page table below.
  prefs: []
  type: TYPE_NORMAL
- en: This allows you to define functions associated to `HighTable` that will not
    be accessible to a P1 table and so on. And it lets you create a `Page` type that
    will contain the contents of a `Page` (a byte array, more or less) that is generic
    over what level it is.
  prefs: []
  type: TYPE_NORMAL
- en: Rust will ensure that you cannot try to get the next table of a P1 table at
    compile time, and at runtime, these enumerations will disappear, as they are zero-sized.
    The logic will be safe, though, and checked at compile time with no overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Extra performance tips
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compile-time checks are not the only place where you can benefit from a performance
    enhancement at no cost. While in [Chapter 1](ad672e4d-0f5e-4c59-b823-249da183abc8.xhtml), *Common
    Performance Pitfalls*, we saw the common errors people write in Rust, we left
    the most advanced tips and tricks for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Using closures to avoid runtime evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, it might seem natural to write code that does not perform as fast
    as expected. Many times, this is due to Rust doing some extra computations at
    runtime. An example of an unnecessary computation that someone could write is
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: I have intentionally made this example simply because a real example usually
    takes really long code. The idea behind it is valid though. When you have an `Option`
    or a `Result`, you have some very useful functions to allow you get the value
    inside or a default. There is this specific function, the `unwrap_or()` function,
    that allows you specify the default value. Of course, you can pass anything you
    want to that function, but if you require a complex calculation to calculate the
    default value (and it's not a constant), the code will perform poorly.
  prefs: []
  type: TYPE_NORMAL
- en: 'This happens because when calling the `unwrap_or()` function, the value you
    pass must be calculated beforehand. This does not make much sense if most of the
    time the value will exist and the computation is not required. A better option
    is to use `unwrap_or_else()`. This function accepts a closure that will only be
    executed if the `Option`/`Result` is `None`/`Err` respectively. In this concrete
    case, since `some_complex_function()` does not have any arguments, you can directly
    use that as the closure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'But if the function requires arguments, you will need to build the closure
    yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This way, you can use a very complicated function, as complicated as you'd like,
    and you will avoid calling it if there is something inside the `Option` type.
    You will also reduce the cyclomatic complexity of the function.
  prefs: []
  type: TYPE_NORMAL
- en: Unstable sorting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is also an interesting place where some gains can be made. Usually, when
    you want to sort a vector, for example, stable sorting is used. This means that
    if two elements have the same ordering, the original ordering will be preserved.
    Let''s see it with an example. Suppose we have a list of fruits we want to order
    alphabetically, taking into account only their first letter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print exactly the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f922eee-3536-4d96-845c-da8a7b362b97.png)'
  prefs: []
  type: TYPE_IMG
- en: And in that order. Even though `Peach` and `Pear` should be before `Pome` if
    we did the sorting by the whole word, since we only take the first character into
    account, the ordering is correct. The final order depends on the one at the beginning.
    If I changed the first list and put `Pome` after `Peach`, the final order would
    have `Pome` after `Peach`. This is called **stable ordering**.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, unstable ordering doesn't try to preserve previous ordering.
    So, `Pome`, `Peach`, and `Pear` could end up in any order between them. This is
    consistent with the condition of being ordered by the first letter, but without
    preserving the original order.
  prefs: []
  type: TYPE_NORMAL
- en: This unstable sorting is actually faster than the stable sorting, and if you
    don't care about respecting the initial ordering, you can save valuable time doing
    the sorting operation, one of the most time-consuming operations. A simple example
    is ordering a list of results alphabetically. In the case of a mismatch, you usually
    don't care how they were ordered in the database, so it doesn't matter if one
    comes after the other or the other way around.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use unstable sorting, you will need to call `sort_unstable()` or `sort_unstable_by()`,
    depending on whether you want to use the default comparison of each `PartialOrd`
    element or use your own classifier, if you want a custom one or if the elements
    in the vector are not `PartialOrd`. Consider the following example using unstable
    sorting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'A possible output for this would be the following, impossible with stable sorting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f922eee-3536-4d96-845c-da8a7b362b97.png)'
  prefs: []
  type: TYPE_IMG
- en: So, summarizing, if you really need to maintain the ordering of the input, use
    stable sorting; if not, use unstable sorting, since you will make your program
    much faster.
  prefs: []
  type: TYPE_NORMAL
- en: Map hashing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rust also has another development option that allows you to make the hashing
    of the maps faster. This comes from the idea that when storing information in
    a `HashMap`, for example, the key gets hashed or a faster lookup. This is great,
    since it allows using arbitrary long and complex keys, but adds overhead when
    retrieving a value or inserting a new value, since the hash must be calculated.
  prefs: []
  type: TYPE_NORMAL
- en: Rust allows you to change the hashing method for a `HashMap`, and even create
    your own. Of course, usually, the best thing is to use the default hashing algorithm,
    since it has been thoroughly tested and avoids collisions (different keys having
    the same hash and overwriting one another). The default hasher for Rust is a very
    efficient hasher, but if you need performance and you are working with a really
    small `HashMap` or even a somehow predictable `HashMap`, it could make sense to
    use your own function or even a faster function included in Rust.
  prefs: []
  type: TYPE_NORMAL
- en: But beware—it's very risky to use one of these functions in an environment where
    a user can provide (or manipulate) keys. They could generate a collision and modify
    the value of a key they should not have access to. They could even create a denial
    of service attack using it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a different hashing is as simple as using the `with_hasher()` function
    when creating the `HashMap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Currently, only `RandomState` is available in the standard library; the rest
    have been deprecated. But you can create your own by implementing the `Hasher`
    trait:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates the `MyHasher` structure, which contains a count that can be initialized
    as you wish. The `hash` function is really simple; it just adds all the bytes
    of the key and returns a `u64` with the sum result. Generating a collision here
    is pretty easy: you just need to make your bytes sum the same. So `[45, 23]` will
    have the same hash as `[23, 45]`. But it works as an example of a hasher. The
    `BuildHasher` trait is also required, and it only needs to return an instance
    of a `Hasher`. I derived the `Clone` trait and just cloned it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be easily used, as we saw before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This will probably be faster than the default hasher, but it will also be much,
    much less secure. So be careful about what hash function you use.
  prefs: []
  type: TYPE_NORMAL
- en: Perfect hash functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If the map is known at compile time, and it does not change during the runtime,
    there is a very, very fast system that can improve by orders of magnitude the
    use of maps. It''s called **perfect hash functions**, and that''s the key to them:
    they perform the minimum required computation for a hash to know whether it''s
    stored in the hash map. This is because it maps one, and only one, integer for
    each element. And it has no collisions. Of course, this requires a constant, known
    hash map at compilation time.'
  prefs: []
  type: TYPE_NORMAL
- en: To use them, you will need the `phf` crate. With this crate, you will be able
    to define a hash map at compile time in the `build.rs` file at the same level
    as the `Cargo.toml` file and use it with no more overhead than a comparison in
    your code. Let's see how to configure it.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you will need to add the `phf_codegen` crate as a development dependency.
    For that, you will need to add a `build-dependencies` section to your `Cargo.toml`,
    with the same syntax as the `dependencies` section. Then, you will need to create
    a `build.rs` file and, inside, you will need something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Let's check what is happening here. The `build.rs` script is run before the
    compilation starts (if it's present). We have a map that is an array of key/value
    tuples. It then creates a code generation map and adds entries one by one to the
    map. This has to be done in a loop, since the compiler stack could overflow due
    to deep recursion.
  prefs: []
  type: TYPE_NORMAL
- en: 'It will write into a file, called `phf.rs`, starting with a line adding a static
    variable, and then writing the whole map into the file, ending it with a new line.
    This means that once the compilation starts, a new file named `phf.rs` will exist
    that we can use from our code. How? You will need to directly include the file
    in your code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This will print the value associated to `key1`, in this case, `value1`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that when creating the map in the `build.rs` file, the values are written
    directly, so if you want to put a string, you need to add the quotation marks
    and escape them. This enables you to add enumeration variants, for example, or
    to write code directly for values.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have learned how to use compile-time hash maps, you should understand
    the different kinds of collections the standard library allows you to use, since
    it will be crucial to the speed and memory footprint of your application.
  prefs: []
  type: TYPE_NORMAL
- en: Standard library collections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rust's standard library has eight different collection types in the `std::collections`
    module. They are divided into sequences, maps, sets, and a binary heap that does
    not fit in any group. The most well known ones are arguably `HashMap` and `Vec`,
    but each of them has a use case, and you should know about them to use the proper
    one in each moment.
  prefs: []
  type: TYPE_NORMAL
- en: The official standard library documentation is really good, so you should check
    it thoroughly. In any case, though, I will introduce the types so that you can
    familiarize yourself with them. Let's start with sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most-used dynamic sequence in Rust and in most languages is the vector,
    represented in Rust as `Vec`. You can add elements to the back of a vector with
    the `push()` method, and get the last element back with the `pop()` method. You
    can also iterate through the vector and, by default, it will go from front to
    back, but you can also reverse the iterator to go from back to front. In general,
    a vector in Rust can be compared to a stack, since it's primarily a LIFO structure.
  prefs: []
  type: TYPE_NORMAL
- en: Vectors are really useful when you want to add new elements to a list, and when
    you are fine with working with indexes in slices to get the elements. Remember
    that vectors can be referenced as slices, and can be indexed with ranges. An interesting
    thing is that you can convert a vector into a boxed slice, that is similar to
    an array, but allocated in heap instead of a stack. You only have to call the
    `into_boxed_slice()` method. This is useful when you have finished growing the
    vector and want it to occupy less RAM. A vector has a capacity, a length, and
    a pointer to the elements, while a boxed slice will only have the pointer and
    the length, avoiding some extra memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Another useful sequence is the `VecDeque` sequence. This structure is a FIFO
    queue, where you can append elements to the back of the queue using the `push_back()`
    method, and pop elements from the front by using `pop_front()`. This can be used
    as a buffer since it can be consumed from the front while you continue adding
    elements to the back. Of course, to use it as a buffer crossing thread boundaries,
    you will need to lock it with a `Mutex`, for example. Iterations in these queues
    go from front to back, the same way as in vectors. It's implemented with a growable
    ring buffer.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `LinkedList` is another sequential list where its peculiarity is
    that instead of having a chunk of elements in the memory, each element gets linked
    to the one before and the one after so that there is no need for an index. It's
    easy to iterate, and easy to remove any element in the list without leaving gaps
    or having to reorder the memory, but in general, it's not very memory-friendly
    and requires more CPU consumption.
  prefs: []
  type: TYPE_NORMAL
- en: You will, most of the time, prefer a `Vec` or a `VecDeque`. `LinkedLists` are
    usually only a good option when many inserts and removes have to be done in the
    middle of the sequence, since in that case, `Vecs` and `VecDeques` will have to
    reorder themselves, which takes a lot of time. But if you will usually only change
    the structure of the list from the back, a `Vec` is the best option; if you will
    also change it from the front, a `VecDeque`. Remember that in both you can read
    any element easily by indexing, it's just that it's more time-consuming to remove
    or add them in the middle of the list.
  prefs: []
  type: TYPE_NORMAL
- en: Maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two kinds of maps: `HashMap` and `BTreeMap`. The main difference
    between them is how they order themselves in memory. They have similar methods
    to insert and retrieve elements, but their performance changes a lot depending
    on the operation.'
  prefs: []
  type: TYPE_NORMAL
- en: '`HashMap` creates an index, where for every key a hash points to the element.
    That way, you do not need to check the whole key for every new `insert`/`delete`/`get`
    operation. You simply hash it and search it in the index. If it exists, it can
    be retrieved, modified, or deleted; if it doesn''t, it can be inserted. It is
    pretty fast to insert new elements. It''s as simple as adding it to the index.
    Retrieving it is also pretty much the same: just get the value if the hashed index
    exists; all operations are done in `O(1)`. You cannot append one `HashMap` to
    another, though, because their hashing algorithms will be different, or at least
    be in different states.'
  prefs: []
  type: TYPE_NORMAL
- en: '`BTreeMap`, on the other hand, does not create indexes. It maintains an ordered
    list of elements and, that way, when you want to insert or get a new element,
    it does a binary search. Check whether the key is bigger than the key in the middle
    of the list. If it is, divide the second half of the list into two and try it
    again with the element of the middle of the second half; if it''s not, do the
    same with the first half.'
  prefs: []
  type: TYPE_NORMAL
- en: That way, you don't have to compare each element with all elements in the map,
    and you can quickly retrieve them. Adding new elements is a similarly costly algorithm,
    and all operations can be done in `O(log n)`. You can also append another `BTreeSet`
    to this one, and the elements will be reordered for the search to be as fast as
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: Sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Both `HashMap` and `BTreeMap` have their set counterparts, called `HashSet`
    and `BTreeSet`. Both are implemented with the same idea in mind: sometimes you
    don''t need a key/value store, but just an element store, where you can retrieve
    the list of the elements by iterating through them, or where you can check whether
    an element is inside just by comparing it to the ones inside.'
  prefs: []
  type: TYPE_NORMAL
- en: Their approach is the same as with the case of their map counterparts, and you
    can think of them as their counterpart maps but with a null value, where only
    keys are the ones doing the job.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use compile-time checks to your advantage.
    You learned how Rust's type system can help you create complex and safe behaviour
    without runtime overhead. You learned how to create state machines and how to
    make your code less error-prone.
  prefs: []
  type: TYPE_NORMAL
- en: You also learned about some extra performance enhancements that complement those
    of [Chapter 1](ad672e4d-0f5e-4c59-b823-249da183abc8.xhtml), *Common Performance
    Pitfalls*. You learned about unstable sorting and map hashing, including perfect
    hash functions created at compile time, and how to create compile-time hash maps
    that will have no runtime overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned about the collections in the standard library, how they
    are classified, and which type of collection you should use depending on the situation.
    You learned about sequences, maps, and sets, and how they can be adapted for your
    code.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](71d38dd3-1f0b-408e-b454-3d342b413f7c.xhtml), *Memory Management
    in Rust*, we will talk about memory management in Rust. Even if, in Rust, you
    do not need to manually allocate and de-allocate memory, there are still plenty
    of things you can do to improve your memory footprint.
  prefs: []
  type: TYPE_NORMAL

<html><head></head><body>
        

                            
                    <h1 class="header-title">Optimization of Microservices</h1>
                
            
            
                
<p class="mce-root">Optimization is an important part of the microservice development process. With optimization, you can increase the performance of a microservice and reduce costs for infrastructure and hardware. In this chapter, we will shed light on benchmarking, along with some optimization techniques, such as caching, reusing shared data with structs without taking ownership, and how you can use a compiler's options to optimize a microservice. This chapter will help you to improve the performance of your microservices using optimization techniques.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Performance-measuring tools</li>
<li>Techniques of optimizations</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p class="mce-root">To run the examples in this chapter, you need a Rust compiler (version 1.31 and above) to build the examples for testing and to build the tools you need to measure performance.<br/></p>
<p>There are sources of the examples of this chapter on GitHub: <a href="https://github.com/PacktPublishing/Hands-On-Microservices-with-Rust/tree/master/Chapter14">https://github.com/PacktPublishing/Hands-On-Microservices-with-Rust/tree/master/Chapter14</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Performance-measuring tools</h1>
                
            
            
                
<p>Before you decide to optimize something in a microservice, you have to measure its performance. You shouldn't write optimal and fast microservices from the start, because not every microservice needs good performance, and if your microservice has bottlenecks inside, it will stumble on heavy loads.</p>
<p>Let's explore a pair of benchmarking tools. We will explore tools that are written in Rust, because they can be simply used to construct your own measuring tool if you need to test a special case with extremely high load.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Welle</h1>
                
            
            
                
<p class="mce-root">Welle is an alternative to the popular <strong>Apache Benchmarking tool</strong> (<strong>ab</strong>) that is used to benchmark HTTP servers. It can produce a batch of requests to a specified URL and measure the time to response for every request. At the end, it collects statistics about the average response time and quantity of failed requests.</p>
<p>To install this tool, use the following command:</p>
<pre><strong>cargo install welle</strong></pre>
<p>Using this tool is simple: set a URL to test with a number of requests you want to send to a server:</p>
<pre><strong>welle --num-requests 10000 http://localhost:8080</strong></pre>
<p>By default, the tool uses a single thread to send a request and wait for the response to send the next request. But you can split requests across more threads by setting a <kbd>--concurrent-requests</kbd> command-line parameter with the number of necessary threads.</p>
<p>When a measurement is finished, it will print a report similar to this:</p>
<pre>Total Requests: 10000<br/>Concurrency Count: 1<br/>Total Completed Requests: 10000<br/>Total Errored Requests: 0<br/>Total 5XX Requests: 0<br/><br/>Total Time Taken: 6.170019816s<br/>Avg Time Taken: 617.001µs<br/>Total Time In Flight: 5.47647967s<br/>Avg Time In Flight: 547.647µs<br/><br/>Percentage of the requests served within a certain time:<br/>50%: 786.541µs<br/>66%: 891.163µs<br/>75%: 947.87µs<br/>80%: 982.323µs<br/>90%: 1.052751ms<br/>95%: 1.107814ms<br/>99%: 1.210104ms<br/>100%: 2.676919ms</pre>
<p>If you want to use a HTTP method other than <kbd>GET</kbd>, you can set it using the <kbd>--method</kbd> command-line parameter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Drill</h1>
                
            
            
                
<p>Drill is more complex and lets you perform load tests on microservices. It not only sends batches of requests, it also uses a testing script to produce a sequence of activities. It helps you to perform a load test that you can use to measure performance of the entire application.</p>
<p>To install <kbd>drill</kbd>, use the following command:</p>
<pre><strong>cargo install drill</strong></pre>
<p>After it's installed, you have to configure the load testing you will perform. Create a <kbd>benchmark.yml</kbd> file and add the following load test script:</p>
<pre>---

threads: 4
base: 'http://localhost:8080'
iterations: 5
rampup: 2

plan:
  - name: Index Page
    request:
      url: /</pre>
<p>To start testing using this script, run the following command:</p>
<pre><strong>drill --benchmark benchmark.yml --stats</strong></pre>
<p>It will perform a load test of your microservice by sending HTTP request constructed with rules from the script and print a report like the following:</p>
<pre>Threads 4<br/>Iterations 5<br/>Rampup 2<br/>Base URL http://localhost:8080<br/><br/>Index Page                http://localhost:8080/ 200 OK 7ms<br/>Index Page                http://localhost:8080/ 200 OK 8ms<br/>...<br/>Index Page                http://localhost:8080/ 200 OK 1ms<br/><br/>Concurrency Level 4<br/>Time taken for tests 0.2 seconds<br/>Total requests 20<br/>Successful requests 20<br/>Failed requests 0<br/>Requests per second 126.01 [#/sec]<br/>Median time per request 1ms<br/>Average time per request 3ms<br/>Sample standard deviation 3ms</pre>
<p>Both tools are suitable for testing the performance of your microservices. Welle is good for measuring the performance of a single request type if you want to optimize the specified handler. Drill is good to produce a complex load to measure how much users can be served by an application.</p>
<p>Let's look at an example in which we add some optimizations and test the difference using Welle.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Measuring and optimizing performance</h1>
                
            
            
                
<p>In this section, we will measure the performance of an example microservice compiled with two options: without optimizations, and with optimizations made by a compiler. The microservice will send rendered index page to clients. And we will use the Welle tool to measure the performance of this microservice to see if we can improve it.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Basic example</h1>
                
            
            
                
<p>Let's create a microservice in a new crate based on the <kbd>actix-web</kbd> crate.</p>
<p>Add the following dependencies to <kbd>Cargo.toml</kbd>:</p>
<pre style="padding-left: 60px">[dependencies]<br/>actix = "0.7"<br/>actix-web = "0.7"<br/>askama = "0.6"<br/>chrono = "0.4"<br/>env_logger = "0.5"<br/>futures = "0.1"<br/><br/>[build-dependencies]<br/>askama = "0.6"</pre>
<p>We will construct a tiny server that will render an index page asynchronously with the current time, with one-minute precision. As a matter of fact, there is a shortcut function that does this:</p>
<pre>fn now() -&gt; String {<br/>    Utc::now().to_string()<br/>}</pre>
<p>We use the <kbd>askama</kbd> crate to render the template of the index page and insert the current time taken from a shared state it it. For the time value, we use a <kbd>String</kbd> instead of types directly from the <kbd>chrono</kbd> crate, in order to get a value that uses a memory heap:</p>
<pre>#[derive(Template)]<br/>#[template(path = "index.html")]<br/>struct IndexTemplate {<br/>    time: String,<br/>}</pre>
<p>For a shared state, we will use a struct with a <kbd>last_minute</kbd> value, represented as a <kbd>String</kbd> wrapped with a <kbd>Mutex</kbd>:</p>
<pre>#[derive(Clone)]<br/>struct State {<br/>    last_minute: Arc&lt;Mutex&lt;String&gt;&gt;,<br/>}</pre>
<p>As you may remember, <kbd>Mutex</kbd> provides concurrent access to any type for multiple threads. It's locked for both reading and writing the value.</p>
<p>For the index page, we will use the following handler:</p>
<pre>fn index(req: &amp;HttpRequest&lt;State&gt;) -&gt; HttpResponse {<br/>    let last_minute = req.state().last_minute.lock().unwrap();<br/>    let template = IndexTemplate { time: last_minute.to_owned() };<br/>    let body = template.render().unwrap();<br/>    HttpResponse::Ok().body(body)<br/>}</pre>
<p>The handler shown in the preceding code block locks the <kbd>last_minute</kbd> field of a <kbd>State</kbd> instance available in <kbd>HttpRequest</kbd>. Then we use this value to fill the <kbd>IndexTemplate</kbd> struct and call the <kbd>render</kbd> method to render the template and use the generated value as a body for a new <kbd>HttpResponse</kbd>.</p>
<p>We start the application with the <kbd>main</kbd> function that prepares the <kbd>Server</kbd> instance and spawns a separate thread that updates a shared <kbd>State</kbd>:</p>
<pre>fn main() {<br/>    let sys = actix::System::new("fast-service");<br/><br/>    let value = now();<br/>    let last_minute = Arc::new(Mutex::new(value));<br/><br/>    let last_minute_ref = last_minute.clone();<br/>    thread::spawn(move || {<br/>        loop {<br/>            {<br/>                let mut last_minute = last_minute_ref.lock().unwrap();<br/>                *last_minute = now();<br/>            }<br/>            thread::sleep(Duration::from_secs(3));<br/>        }<br/>    });<br/><br/>    let state = State {<br/>        last_minute,<br/>    };<br/>    server::new(move || {<br/>        App::with_state(state.clone())<br/>            .middleware(middleware::Logger::default())<br/>            .resource("/", |r| r.f(index))<br/>    })<br/>    .bind("127.0.0.1:8080")<br/>    .unwrap()<br/>    .start();<br/>    let _ = sys.run();<br/>}</pre>
<p>We used the Actix Web framework for this example. You can read more about this framework in <a href="5b7dd2c1-d623-4422-83a7-e05681230ee9.xhtml" target="_blank">Chapter 11</a>, <em>Involving Concurrency with Actors and the Actix Crate</em>. This simple example is ready to compile and start. We will also check the performance of this code using the Welle tool.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Performance</h1>
                
            
            
                
<p>First, we will build and run the code using a standard command without any flags:</p>
<pre><strong>cargo run</strong></pre>
<p>We build a binary with a lot of debugging information, which can be used with LLDB as we did in <a href="1d24de7f-9990-4afe-bd1c-9bf664f1eda3.xhtml" target="_blank">Chapter 13</a>, <em>Testing and Debugging Rust Microservices</em>. A debugging symbol reduces performance, but we will check it to compare it with a version without these symbols later.</p>
<p>Let's load the running server with <kbd>100000</kbd> requests from <kbd>10</kbd> concurrent activities. Since our server was bound to port <kbd>8080</kbd> of <kbd>localhost</kbd>, we can use the <kbd>welle</kbd> command with the following arguments to measure the performance:</p>
<pre><strong>welle --concurrent-requests 10 --num-requests 100000 http://localhost:8080</strong></pre>
<p>It takes about 30 seconds (depending on your system) and the tool will print the report:</p>
<pre>Total Requests: 100000<br/>Concurrency Count: 10<br/>Total Completed Requests: 100000<br/>Total Errored Requests: 0<br/>Total 5XX Requests: 0<br/><br/>Total Time Taken: 29.883248121s<br/>Avg Time Taken: 298.832µs<br/>Total Time In Flight: 287.14008722s<br/>Avg Time In Flight: 2.8714ms<br/><br/>Percentage of the requests served within a certain time:<br/>50%: 3.347297ms<br/>66%: 4.487828ms<br/>75%: 5.456439ms<br/>80%: 6.15643ms<br/>90%: 8.40495ms<br/>95%: 10.27307ms<br/>99%: 14.99426ms<br/>100%: 144.630208ms</pre>
<p>In the report, you can see that there is an average response time of 300 milliseconds. It was a service that was burdened with debugging. Let's recompile this example with optimizations. Set the <kbd>--release</kbd> flag on the <kbd>cargo run</kbd> command:</p>
<pre><strong>cargo run --release</strong></pre>
<p>This command passed the <kbd>-C opt-level=3</kbd> optimization flag to the <kbd>rustc</kbd> compiler. If you use <kbd>cargo</kbd> without a <kbd>--release</kbd> flag, it sets <kbd>opt-level</kbd> to <kbd>2</kbd>.</p>
<p>After the server has recompiled and started, we use the Welle tool again with the same parameters. It reports the other values:</p>
<pre>Total Requests: 100000<br/>Concurrency Count: 10<br/>Total Completed Requests: 100000<br/>Total Errored Requests: 0<br/>Total 5XX Requests: 0<br/><br/>Total Time Taken: 8.010280915s<br/>Avg Time Taken: 80.102µs<br/>Total Time In Flight: 63.961189338s<br/>Avg Time In Flight: 639.611µs<br/><br/>Percentage of the requests served within a certain time:<br/>50%: 806.717µs<br/>66%: 983.35µs<br/>75%: 1.118933ms<br/>80%: 1.215726ms<br/>90%: 1.557405ms<br/>95%: 1.972497ms<br/>99%: 3.500056ms<br/>100%: 37.844721ms</pre>
<p>As we can see, the average time taken for a request has been reduced to more than 70%. The result is already pretty good. But could we reduce it a little more? Let's try to do it with some optimizations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Optimizations</h1>
                
            
            
                
<p>We will try to apply three optimizations to the code we created in the previous section:</p>
<ul>
<li>We will try to reduce the blocking of a shared state.</li>
<li>We will reuse a value in a state by reference.</li>
<li>We will add caching of responses.</li>
</ul>
<p>After we implement them, we will check the performance after the first two improvements, and later check it again with caching.</p>
<p>In the code of this section, we will implement all optimizations gradually, step by step, but if you downloaded the example from the GitHub repository of this book, you will find the following features in the <kbd>Cargo.toml</kbd> file of the project of this chapter:</p>
<pre>[features]<br/>default = []<br/>cache = []<br/>rwlock = []<br/>borrow = []<br/>fast = ["cache", "rwlock", "borrow"]</pre>
<p>The code here uses features to provide you a capability to activate or deactivate any optimization separately. We see the following:</p>
<ul>
<li><kbd>cache</kbd>: activates the caching of requests</li>
<li><kbd>rwlock</kbd>: uses <kbd>RwLock</kbd> instead of <kbd>Mutex</kbd> for <kbd>State</kbd></li>
<li><kbd>borrow</kbd>: reuses a value by reference</li>
</ul>
<p>Let's implement all of these optimizations and apply all of them to measure differences in the performance.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">State sharing without blocking</h1>
                
            
            
                
<p>In the first optimization, we will replace <kbd>Mutex</kbd> with <kbd>RwLock</kbd>, because <kbd>Mutex</kbd> is locked for both reading and writing, but <kbd>RwLock</kbd> allows us to have a single writer or multiple readers. It allows you to avoid blocking for reading the value if no one updates the value. This applies to our example, as we rarely update a shared value, but have to read it from multiple instances of handlers.</p>
<p><kbd>RwLock</kbd> is an alternative to <kbd>Mutex</kbd> that separates readers and writers, but the usage of <kbd>RwLock</kbd> is as simple as <kbd>Mutex</kbd>. Replace <kbd>Mutex</kbd> to <kbd>RwLock</kbd> in the <kbd>State</kbd> struct:</p>
<pre>#[derive(Clone)]<br/>struct State {<br/>    // last_minute: Arc&lt;Mutex&lt;String&gt;&gt;,<br/>    last_minute: Arc&lt;RwLock&lt;String&gt;&gt;,<br/>}</pre>
<p>Also, we have to replace a creation of the <kbd>last_minute</kbd> reference counter to the corresponding type:</p>
<pre>// let last_minute = Arc::new(Mutex::new(value));<br/>let last_minute = Arc::new(RwLock::new(value));</pre>
<p>In the code of the worker, we will use the <kbd>write</kbd> method of <kbd>RwLock</kbd> to lock the value for writing to set a new time value. Its exclusive lock will block all potential readers and writers with a single writer that can change the value:</p>
<pre>// let mut last_minute = last_minute_ref.lock().unwrap();<br/>let mut last_minute = last_minute_ref.write().unwrap();</pre>
<p>Since the worker will take an exclusive lock once per three seconds, it's a small price to pay to increase the amount of simultaneous readers.</p>
<p>In the handler, we will use the <kbd>read</kbd> method to lock <kbd>RwLock</kbd> for reading:</p>
<pre>// let last_minute = req.state().last_minute.lock().unwrap();<br/>let last_minute = req.state().last_minute.read().unwrap();</pre>
<p>This code won't be blocked by other handlers, excluding the case when a worker updates a value. It allows all handlers to work simultaneously.</p>
<p>Now we can implement the second improvement—avoid cloning of values and using them by references.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reusing values by references</h1>
                
            
            
                
<p>To render a template of an index page, we use a struct with a <kbd>String</kbd> field and we have to fill the <kbd>IndexTemplate</kbd> struct to call the <kbd>render</kbd> method on it. But the template needs an ownership for the value and we have to clone it. Cloning in turn takes time. To avoid this CPU cost, we can use a reference to a value, because if we clone a value that uses a memory heap, we have to allocate a new memory space and copy bytes of the value to a new place.</p>
<p>This is how we can add a reference to a value:</p>
<pre>struct IndexTemplate&lt;'a&gt; {<br/>    // time: String,<br/>    time: &amp;'a str,<br/>}</pre>
<p>We have added the <kbd>'a</kbd> lifetime to a struct, because we use a reference inside and the struct that can't live longer than the string value we referred to.</p>
<p>Using references is not always possible for combinations of <kbd>Future</kbd> instances, because we have to construct a <kbd>Future</kbd> that generates an <kbd>HttpResponse</kbd>, but lives longer than calling a handler. In this case, you can reuse a value if you take ownership of it and use methods such as fold to pass the value through all the steps of the combinator's chain. It is valuable for large values, which can consume a lot of CPU time, to be cloned.</p>
<p>Now we can use a reference to the borrowed <kbd>last_minute</kbd> value:</p>
<pre>// let template = IndexTemplate { time: last_minute.to_owned() };<br/>let template = IndexTemplate { time: &amp;last_minute };</pre>
<p>The <kbd>to_owned</kbd> method, which we used before, cloned the value that we put to <kbd>IndexTemplate</kbd>, but we can now use a reference and avoid cloning at all.</p>
<p>All we need to do now is implement caching, which can help to avoid template rendering.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Caching</h1>
                
            
            
                
<p>We will use a cache to store rendered template and returns it as a response for future requests. Ideally, the cache should have a lifetime, because if the cache is not updated, then clients won't see any updates on the page. But for our demo application, we won't reset the cache to make sure it works, because our small microservice renders a time and we can see if it's frozen. Now we will add a new field to the <kbd>State</kbd> struct to keep a rendered template for future responses:</p>
<pre>cached: Arc&lt;RwLock&lt;Option&lt;String&gt;&gt;&gt;</pre>
<p>We will use <kbd>RwLock</kbd> because we have to update this value at least once, but for values that won't be updated and can be initialized, we can use the <kbd>String</kbd> type without any wrapper for protection from concurrent access, such as <kbd>RwLock</kbd> or <kbd>Mutex</kbd>. In other words, you can use the <kbd>String</kbd> type directly if you only read it.</p>
<p>We also have to initialize the value with <kbd>None</kbd>, because we need to render a template once to get the value for caching.</p>
<p>Add an empty value to a <kbd>State</kbd> instance:</p>
<pre>let cached = Arc::new(RwLock::new(None));<br/>let state = State {<br/>    last_minute,<br/>    cached,<br/>};</pre>
<p>Now we can use a <kbd>cached</kbd> value to construct a fast response to the user's request. But you have to take into account that not all the information can be shown to every user. The cache can separate values by some information about users, for example, it can use location information to get the same cached value for users from the same country. The following code improves the <kbd>index</kbd> handler and takes a <kbd>cached</kbd> value, and if the cached value exists, it is used to produce a new <kbd>HttpResponse</kbd>:</p>
<pre>let cached = req.state().cached.read().unwrap();<br/>if let Some(ref body) = *cached {<br/>    return HttpResponse::Ok().body(body.to_owned());<br/>}</pre>
<p>We immediately return a <kbd>cached</kbd> value because there is a rendered template stored and we don't have to spend time on rendering. But if no value exists, we can produce it with the following code and set the <kbd>cached</kbd> value at the same time:</p>
<pre>let mut cached = req.state().cached.write().unwrap();<br/>*cached = Some(body.clone());</pre>
<p>After this, we keep the original code that returns <kbd>HttpResponse</kbd> to a client.</p>
<p>Now we have implemented and compiled the code with all the optimizations and can measure performance of the new version with optimizations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Compilation with optimizations</h1>
                
            
            
                
<p>The code included three optimizations. We can use some of them to check the difference in performance. First, we will compile the code with <kbd>RwLock</kbd> and borrow the state's value features. If you use the code from the book's GitHub repository, you can run the necessary optimizations using the <kbd>--features</kbd> argument with the corresponding feature names:</p>
<pre><strong>cargo run --release --features rwlock,borrow</strong></pre>
<p>Once the server is ready, we can start running the same test we used to measure performance of this microservice before, with Welle, to measure how many incoming requests the optimized version of the server can handle.</p>
<p>After testing, the tool will print a report like this:</p>
<pre>Total Requests: 100000<br/>Concurrency Count: 10<br/>Total Completed Requests: 100000<br/>Total Errored Requests: 0<br/>Total 5XX Requests: 0<br/><br/>Total Time Taken: 7.94342667s<br/>Avg Time Taken: 79.434µs<br/>Total Time In Flight: 64.120106299s<br/>Avg Time In Flight: 641.201µs<br/><br/>Percentage of the requests served within a certain time:<br/>50%: 791.554µs<br/>66%: 976.074µs<br/>75%: 1.120545ms<br/>80%: 1.225029ms<br/>90%: 1.585564ms<br/>95%: 2.049917ms<br/>99%: 3.749288ms<br/>100%: 13.867011ms</pre>
<p>As you can see, the application is faster—it takes <kbd>79.434</kbd> microseconds instead of <kbd>80.10</kbd>. The difference is less than 1%, but it's good for a handler that already worked faster.</p>
<p>Let's try to activate all the optimizations we implemented, including caching. To do this with examples from GitHub, use the following arguments:</p>
<pre><strong>cargo run --release --features fast</strong></pre>
<p>Let's start testing again once the server is ready. With the same testing parameters, we get a better report:</p>
<pre>Total Requests: 100000<br/>Concurrency Count: 10<br/>Total Completed Requests: 100000<br/>Total Errored Requests: 0<br/>Total 5XX Requests: 0<br/><br/>Total Time Taken: 7.820692644s<br/>Avg Time Taken: 78.206µs<br/>Total Time In Flight: 62.359549787s<br/>Avg Time In Flight: 623.595µs<br/><br/>Percentage of the requests served within a certain time:<br/>50%: 787.329µs<br/>66%: 963.956µs<br/>75%: 1.099572ms<br/>80%: 1.199914ms<br/>90%: 1.530326ms<br/>95%: 1.939557ms<br/>99%: 3.410659ms<br/>100%: 10.272402ms</pre>
<p>It takes <kbd>78.206</kbd> microseconds to get a response for a request from the server. It's more than 2% faster than the original version without optimization, which takes 80.10 microseconds per request on average.</p>
<p>You may think the difference is not very big, but in reality, it is. This is a tiny example, but try to imagine the difference of optimizing a handler that makes three requests to databases, and renders a 200 KB template with arrays of values to insert. For heavy handlers, you can improve performance by 20%, or even more. But remember, you should remember the over-optimization is an extreme measure, because it makes code harder to develop and add more features without affecting achieved performance.</p>
<p>It's better not to consider any optimization as a daily task, because you may spend a lot of time on optimization of short pieces of code to get a 2% performance for features your customers don't need.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Optimization techniques </h1>
                
            
            
                
<p>In the previous section, we optimized the source code, but there are also alternative techniques of optimization by using special compilation flags and third-party tools. In this section, we will cover some of these optimization techniques. We will talk a little about reducing sizes, benchmarks, and profiling Rust code.</p>
<p>Optimization is a creative topic. There is no special recipe for optimization, but in this section, we will create a small microservice that generates an index page with the current time, and then we will try to optimize it. With this example, I hope to show you some optimization ideas for your projects.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Link-time optimizations</h1>
                
            
            
                
<p>The compiler does a lot of optimizations automatically during the compilation process, but we can also activate some optimizations after the sources compiled. This technique called <strong>Link Time Optimizations</strong> (<strong>LTO</strong>) and applied after code linked and the whole program available. You can activate this optimization for Rust program by adding an extra section to the <kbd>Cargo.toml</kbd> file in your project:</p>
<pre>[profile.release]
lto = true</pre>
<p>If you have already compiled the project with this optimization, force a complete rebuild of your project. But this option also takes much more time for compilation. This optimization does not necessarily improve the performance, but can help to reduce the size of a binary.</p>
<p>If you activate all optimization options, it doesn't mean you have made the fastest version of an application. Too many optimizations can reduce the performance of a program and you should compare the results with an original version. Most often, just using the standard <kbd>--release</kbd> helps the compiler to produces a binary with an optimal balance of compilation speed and performance.</p>
<p>Normal Rust programs use panic macros for unhandled errors and print backtraces. For optimization, you can consider turning this off. Let's look at this technique in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Abort instead of panicking</h1>
                
            
            
                
<p>Error-handling code also requires space and can affect performance. If you try to write microservices that won't panic, and will try to solve problems, and fail only when there's an unsolvable problem, you can consider using aborts (immediate termination of the program without unwinding the stack), instead of Rust's <kbd>panic</kbd>.</p>
<p>To activate it, add the following to your  <kbd>Cargo.toml</kbd> file:</p>
<pre>[profile.release]
panic = "abort"</pre>
<p>Now, if your program fails, it won't make a <kbd>panic</kbd> and will be stopped immediately without printing backtraces.</p>
<p>Aborting is dangerous. If your program will be aborted, it has less chance to write logs corectly or deliver spans to distributed tracing. For microservices, you can create a separate thread for tracing, and even if the main thread failed, wait till all available tracing records will be stored.</p>
<p class="mce-root">Sometimes, you not only need to improve the performance, but also have to reduce the size of the binary. Let's see how to do it.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reducing the size of binaries</h1>
                
            
            
                
<p>You may want to reduce the size of binaries. Often it's not necessary, but it may be useful if you have a distributed application that uses some hardware with limited space that requires tiny binaries. To reduce the size of your compiled application, you can use the <kbd>strip</kbd> command, which is part of the <strong>binutils</strong> package:</p>
<pre>strip &lt;path_to_your_binary&gt;</pre>
<p>For example, I tried to strip a compiled binary of the microservice we created in the <em>Basic example</em> section of this chapter. The binary with debugging symbols compiled with the <kbd>cargo build</kbd> command reduced from 79 MB to 12 MB. The version compiled with the <kbd>--release</kbd> flag reduced from 8.5 MB to 4.7 MB.</p>
<p>But remember that you can't debug a stripped binary, because the tool will remove all necessary information for debugging.</p>
<p class="mce-root">Sometimes, you may want to compare some ideas of optimizations and want to measure which one is better. You can use benchmarks for that. Let's look at the benchmark feature supplied with <kbd>cargo</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Isolated benchmarks</h1>
                
            
            
                
<p>Rust supports benchmark testing out of the box. You can use it to compare the performance of different solutions of the same problem or to get to know the time of execution of some parts of your application.</p>
<p>To use benchmarks, you have to add a function with the <kbd>#[bench]</kbd> attribute. The function expects a mutable reference to the <kbd>Bencher</kbd> instance. For example, let's compare cloning a <kbd>String</kbd> with taking a reference to it:</p>
<pre>#![feature(test)]<br/>extern crate test;<br/>use test::Bencher;<br/><br/>#[bench]<br/>fn bench_clone(b: &amp;mut Bencher) {<br/>    let data = "data".to_string();<br/>    b.iter(move || {<br/>        let _data = data.clone();<br/>    });<br/>}<br/><br/>#[bench]<br/>fn bench_ref(b: &amp;mut Bencher) {<br/>    let data = "data".to_string();<br/>    b.iter(move || {<br/>        let _data = &amp;data;<br/>    });<br/>}</pre>
<p class="mce-root">To benchmark, you have to provide a closure with a code you want to measure to the <kbd>iter</kbd> method of the <kbd>Bencher</kbd> instance. You also need to add a <kbd>test</kbd> feature with <kbd>#![feature(test)]</kbd> to testing module and use <kbd>extern crate test</kbd> to import <kbd>test</kbd> crate to import the <kbd>Bencher</kbd> type from this module.</p>
<p>The <kbd>bench_clone</kbd> function has a <kbd>String</kbd> value and clones it on every measurement by <kbd>Bencher</kbd>. In <kbd>bench_ref</kbd>, we take a reference to a <kbd>String</kbd> value.</p>
<p>Now you can start a benchmark test with <kbd>cargo</kbd>:</p>
<pre><strong>cargo bench</strong></pre>
<p>It compiles the code for testing (the code items with the <kbd>#[cfg(test)]</kbd>  attribute will be activated) and then runs the benchmarks. For our examples, we have the following results:</p>
<pre>running 2 tests<br/>test bench_clone ... bench:          32 ns/iter (+/- 9)<br/>test bench_ref   ... bench:           0 ns/iter (+/- 0)<br/><br/>test result: ok. 0 passed; 0 failed; 0 ignored; 2 measured; 0 filtered out</pre>
<p>As we expected, taking the reference to a <kbd>String</kbd> takes no time, but the cloning of a <kbd>String</kbd> takes <kbd>32</kbd> nanoseconds per call of the <kbd>clone</kbd> method.</p>
<p>Remember, you can do good benchmark testing for CPU-bound tasks, but not I/O-bound tasks, because I/O tasks are more dependent on the quality of hardware and operating system performance.</p>
<p class="mce-root">If you want to benchmark the operation of some functions in the running application, then you have to use a profiler. Let's try to analyze some code with a profiler.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Profiling</h1>
                
            
            
                
<p>Benchmark tests are useful for checking a portion of code, but they are not suitable for checking the performance of a working application. If you need to explore the performance of some of the functions of the code, you have to use a profiler.</p>
<p>Profilers dump information about code and function executions and record the time spans during which the code works. There is a profiler in the Rust ecosystem called <strong>flame</strong>. Let's explore how to use it.</p>
<p>Profiling takes time and you should use it as a feature to avoid affecting performance in production installations. Add the <kbd>flame</kbd> crate to your project and use it as an optional. Add a feature (such as the official examples from the <kbd>flamer</kbd> crate repository; I named the feature <kbd>flame_it</kbd>) and add the flame dependency to it:</p>
<pre>[dependencies]<br/>flame = { version = "0.2", optional = true }<br/><br/>[features]<br/>default = []<br/>flame_it = ["flame"]</pre>
<p>Now, if you want to activate profiling, you have to compile the project with the <kbd>flame_it</kbd> feature.</p>
<p>Using the <kbd>flame</kbd> crate is pretty simple and includes three scenarios:</p>
<ul>
<li>Use the <kbd>start</kbd> and <kbd>end</kbd> methods directly.</li>
<li>Use the <kbd>start_guard</kbd> method, which creates a <kbd>Span</kbd> that is used to measure execution time. A <kbd>Span</kbd> instance ends measurement automatically when it's dropped.</li>
<li>Use <kbd>span_of</kbd> to measure code isolated in a closure.</li>
</ul>
<p>We will use spans like we did in the <kbd>OpenTracing</kbd> example in <a href="1d24de7f-9990-4afe-bd1c-9bf664f1eda3.xhtml" target="_blank">Chapter 13</a>, <em>Testing and Debugging Rust Microservices</em>:</p>
<pre>use std::fs::File;<br/><br/>pub fn main() {<br/>    {<br/>        let _req_span = flame::start_guard("incoming request");<br/>        {<br/>            let _db_span = flame::start_guard("database query");<br/>            let _resp_span = flame::start_guard("generating response");<br/>        }<br/>    }<br/><br/>    flame::dump_html(&amp;mut File::create("out.html").unwrap()).unwrap();<br/>    flame::dump_json(&amp;mut File::create("out.json").unwrap()).unwrap();<br/>    flame::dump_stdout();<br/>}</pre>
<p>You don't need to collect spans or send them to <kbd>Receiver</kbd>, as we did for Jaeger, but profiling with <kbd>flame</kbd> looks like tracing.</p>
<p>At the end of the execution, you have to dump a report in the appropriate format, such as  HTML or JSON, print it to a console, or write it to a <kbd>Writer</kbd> instance. We used the first three of them. We have implemented the main function and used the <kbd>start_quard</kbd> method to create <kbd>Span</kbd> instances to measure the execution time of some pieces of the code. After this, we will write reports.</p>
<p>Compile and run this example with the activated profiling feature:</p>
<pre><strong>cargo run --features flame_it</strong></pre>
<p>The preceding command compiles and prints the report to the console:</p>
<pre>THREAD: 140431102022912<br/>| incoming request: 0.033606ms<br/>  | database query: 0.016583ms<br/>    | generating response: 0.008326ms<br/>    + 0.008257ms<br/>  + 0.017023ms</pre>
<p>As you can see, we have created three spans. You can also find two reports in files, <kbd>out.json</kbd> and <kbd>out.html</kbd>. If you open the HTML report in a browser, it renders like so:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/1ec7f264-3641-4908-80e4-9f0bc1cf6c18.png" style="width:56.58em;height:7.00em;"/></p>
<p>In the preceding screenshot, you can see the relative duration of execution of every activity of our program. A longer colored block means longer execution time. As you can see, profiling is useful for finding a slow section of code that you can optimize with other techniques.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we discussed optimizations. First, we explored tools for measuring performance—Welle, which is an alternative to the classic <strong>Apache Benchmarking tool</strong>, and Drill, which uses scripts to perform load tests.</p>
<p>Then we created a tiny microservice and measured its performance. Focusing on results, we applied some optimizations to that microservice—we avoided blocking a shared state for reading, we reused a value by a reference instead of cloning it, and we added the caching of rendered templates. Then we measured the performance of the optimized microservice and compared it with the original version.</p>
<p>In the last section of this chapter, we got acquainted with alternative techniques of optimization—using LTO, aborting execution without backtracing instead of panicking, reducing the size of a compiled binary, benchmarking small pieces of code, and using profiling for your projects.</p>
<p>In the next chapter, we will look at creating images with Rust microservices using Docker to run microservices in containers with preconfigured environments to speed up delivery of your product to customers.</p>


            

            
        
    </body></html>
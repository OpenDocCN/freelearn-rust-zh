- en: Atomics – the Primitives of Synchronization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原子 – 同步的原始元素
- en: In [Chapter 4](5a332d94-37e4-4748-8920-1679b07e2880.xhtml), *Sync and Send –
    the Foundation of Rust Concurrency*, and [Chapter 5](e2de1ba7-c291-494e-82da-37fee7323c1d.xhtml),
    *Locks – Mutex, Condvar, Barriers and RWLock*, we discussed the fundamentals of
    lock-based concurrency in Rust. However, there are some cases where lock-based
    programming is not suitable, such as when extreme performance is a concern or
    threads may *never* block. In such domains, the programmer must rely on the atomic
    synchronization primitives of modern CPUs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](5a332d94-37e4-4748-8920-1679b07e2880.xhtml)，*Sync和Send – Rust并发的基石*，以及[第5章](e2de1ba7-c291-494e-82da-37fee7323c1d.xhtml)，*锁
    – Mutex、Condvar、屏障和RWLock*中，我们讨论了Rust中基于锁的并发基础。然而，在某些情况下，基于锁的编程并不适用，例如当极端性能是关注点或线程可能*永远不会*阻塞时。在这些领域，程序员必须依赖现代CPU的原子同步原语。
- en: In this chapter, we'll be discussing the atomic primitives available to the
    Rust programmer. Programming with atomics is a complex topic and an area of active
    research. An entire book could be dedicated to the topic of atomic Rust programming.
    As such, we'll be shifting our tactics slightly for this chapter, focusing on
    more of a tutorial style than previous chapters where we've done deep dives on
    existing software. Everything presented in the prior chapters will come to bear
    here. By the end of this chapter, you ought to have a working understanding of
    atomics, being able to digest existing literature with more ease and validate
    your implementations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论Rust程序员可用的原子原语。使用原子编程是一个复杂的话题，也是一个活跃的研究领域。整本书都可以专门讨论原子Rust编程。因此，我们将为本章稍微调整策略，更多地采用教程风格，而不是之前章节中对现有软件的深入研究。前几章中介绍的所有内容都将在此处得到应用。到本章结束时，你应该对原子有实际的理解，能够更容易地消化现有文献，并验证你的实现。
- en: 'By the end of this chapter, we will have:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将：
- en: Discussed the concept of linearizability
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论了可线性化性的概念
- en: Discussed the various atomic memory orderings, along with their meanings and
    implications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论各种原子内存排序，以及它们的含义和影响
- en: Built a mutex from atomic primitives
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从原子原语构建互斥锁
- en: Built a queue from atomic primitives
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从原子原语构建队列
- en: Built a semaphore
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从原子原语构建信号量
- en: Made clear the difficulties of memory reclamation in an atomic context
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清晰地阐述了在原子上下文中内存回收的困难
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires a working Rust installation. The details of verifying
    your installation are covered in [Chapter 1](5f3aec9d-fd53-48ff-9ba8-43ce13e91cff.xhtml),
    *Preliminaries – Machine Architecture and Getting Started with Rust*. No additional
    software tools are required.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要安装一个可工作的Rust环境。验证安装的详细信息请参阅[第1章](5f3aec9d-fd53-48ff-9ba8-43ce13e91cff.xhtml)，*预备知识
    – 计算机架构和Rust入门*。不需要额外的软件工具。
- en: 'You can find the source code for this book''s projects on GitHub: [https://github.com/PacktPublishing/Hands-On-Concurrency-with-Rust](https://github.com/PacktPublishing/Hands-On-Concurrency-with-Rust).
    This chapter has its source code under `Chapter06`.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到本书项目的源代码：[https://github.com/PacktPublishing/Hands-On-Concurrency-with-Rust](https://github.com/PacktPublishing/Hands-On-Concurrency-with-Rust)。本章的源代码位于`Chapter06`。
- en: Linearizability
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可线性化性
- en: Up till this point in the book, we've avoided dipping into the formal terms
    for specifying concurrent systems, because while they are *very* useful for discussing
    ideas and reasoning, they can be difficult to learn absent some context. Now that
    we have context, it's time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书在指定并发系统时，一直避免使用正式术语，因为虽然它们在讨论思想和推理时非常有用，但如果没有一些背景知识，它们可能很难学习。现在我们已经有了背景知识，是时候了。
- en: How do we decide whether a concurrent algorithm is correct? How, if we're game
    for the algorithm, do we analyze an implementation and reason about it's correctness?
    To this point, we've used techniques to demonstrate fitness-for-purpose of an
    implementation through randomized and repeat testing as well as simulation, in
    the case of helgrind. We will continue to do so. In fact, if that's all we did,
    demonstrating fitness-for-purpose of implementations, then we'd be in pretty good
    condition. The working programmer will find themselves inventing more often than
    not, taking an algorithm and adapting it—as was seen in the previous chapter's
    discussion of hopper—to fit some novel domain. It's easy enough to do this and
    not notice.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何决定一个并发算法是否正确？如果我们对算法感兴趣，我们如何分析实现并推理其正确性？到目前为止，我们使用技术通过随机和重复测试以及模拟（在helgrind的情况下）来证明实现的适用性。我们将继续这样做。实际上，如果我们只是这样做，证明实现的适用性，那么我们会处于相当好的状态。工作的程序员会发现他们经常在发明，将算法改编以适应某些新颖的领域——正如前一章关于hopper的讨论中所见。这样做很容易，而且往往不会注意到。
- en: What we're searching for, when we sit down to reason about the systems we're
    constructing, is a unifying concept that'll separate the workable ideas from the
    goofs. That concept for us, here, in the design and construction of concurrent
    data structures, is linearizability. What we're looking to build are objects that,
    paraphrasing Nancy Lynch, make it seem like operations on them occur one at a
    time, in some sequential order, consistent with the order of operations and responses.
    Lynch's *Distributed Algorithms* is concerned with the behavior of distributed
    systems, but I've always thought that her explanation of what she refers to as
    atomic objects is brilliantly concise.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们坐下来推理我们所构建的系统时，我们正在寻找的是一个统一的概念，这个概念能够将可行的想法与错误区分开来。对我们来说，在这里，在设计并发数据结构时，这个概念就是线性化。我们想要构建的对象，用Nancy
    Lynch的话来说，使得对这些对象的操作似乎是一次一个，按照某种顺序发生，与操作的顺序和响应一致。Lynch的《分布式算法》关注分布式系统的行为，但我一直认为她对所谓的原子对象的解释非常简洁而精辟。
- en: The notion of linearizability was introduced in *Axioms for Concurrent Objects* by
    Herlihy and Wing in 1986\. Their definition is a bit more involved, done up in
    terms of histories—*a finite sequence of operation invocation and response events**—*and
    specifications*,* which are, to be quick about it, a set of axiomatic pre and
    post conditions that hold on, the object in terms of the operations defined on
    it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 线性化的概念是在1986年Herlihy和Wing的《并发对象的公理》一文中提出的。他们的定义稍微复杂一些，涉及到历史——*一系列操作调用和响应事件**——以及规范，*简而言之，就是一组关于对象上定义的操作的公理前条件和后条件。
- en: 'Let the result of an operation be called `res(op)` and the application or invocation
    of an operation be called `inv(op)`, and distinguish operations by some extra
    identifier. `op_1` is distinct from `op_2` but the identifier has no bearing on
    their ordering; it''s just a name. A history `H` is a partial order over operations
    so that `op_0 < op_1` if `res(op_0)` happens before `inv(op_1)` in `H`. Operations
    that have no ordering according to `<` are concurrent in the history. A history
    `H` is sequential if all of its operations are ordered with regard to `<`. Now,
    the history `H` is linearizable if it can be adjusted by adding zero or more operations
    into the history to make some other history `H''` so that:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让操作的成果称为 `res(op)`，操作的调用或应用称为 `inv(op)`，并通过一些额外的标识符来区分操作。`op_1` 与 `op_2` 不同，但标识符不影响它们的排序；它只是一个名字。历史
    `H` 是操作上的一个偏序，使得 `op_0 < op_1` 如果在 `H` 中 `res(op_0)` 发生在 `inv(op_1)` 之前。在 `<`
    没有排序关系的操作在历史中是并发的。如果历史 `H` 中所有操作都按照 `<` 排序，则 `H` 是顺序的。现在，如果历史 `H` 可以通过添加零个或多个操作进行调整，使得另一个历史
    `H'` 如下，则 `H` 是可线性化的：
- en: '`H''` is composed only of invocations and responses and are equivalent to some
    legal, sequential history `S`'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`H''` 只包含调用和响应，并且与某个合法的顺序历史 `S` 等价。'
- en: The ordering operation of `H'` is an inclusive subset of the ordering of `S`.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`H''` 的排序操作是 `S` 排序的包含子集。'
- en: 'Phew! An object is linearizable if you can record the order of the operations
    done to it, fiddle with them some, and find an equivalent sequential application
    of operations on the same object. We''ve actually done linearizabilty analysis
    in previous chapters, I just didn''t call it out as such. Let''s keep on with
    Herlihy and Wing and take a look at their examples of linearizable vs. unlinearizable
    histories. Here''s a history on a familiar queue:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！如果一个对象的操作顺序可以被记录下来，对其进行一些调整，并找到一个等效的同一对象上的操作序列应用，那么这个对象就是可线性化的。我们实际上在之前的章节中已经进行了线性化分析，只是我没有将其称为此类。让我们继续跟随Herlihy和Wing，看看他们关于可线性化与不可线性化历史记录的例子。以下是一个熟悉的队列的历史记录：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We have two threads, `A` and `B`, performing the operations `enq(val: T) ->
    Result((), Error)` and `deq() -> Result(T, Error)`, in pseudo-Rust types. This
    history is in fact linearizable and is equivalent to:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '我们有两个线程，`A`和`B`，执行操作`enq(val: T) -> Result((), Error)`和`deq() -> Result(T,
    Error)`，以伪Rust类型表示。这个历史记录实际上是可线性化的，并且与以下等效：'
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Notice that the last operation does not exist in the original history. It''s
    possible for a history to be linearizable, as Herlihy and Wing note it, even if
    an operation *takes effect* before its response. For example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在原始历史记录中最后一步操作并不存在。正如Herlihy和Wing所指出的，即使一个操作在其响应之前*生效*，历史记录仍然可能是可线性化的。例如：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is equivalent to the sequential history:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这与以下顺序历史记录等效：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This history is not linearizable:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个历史记录是不可线性化的：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The culprit in this case is the violation of the queue's ordering. In sequential
    history, the dequeue would receive *x* and not *y*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，罪魁祸首是队列排序的违反。在顺序历史记录中，出队操作将接收*x*而不是*y*。
- en: That's it. When we talk about a structure being *linearizable,* what we're really
    asking is, can any list of valid operations against the structure be shuffled
    around or added to in such a way that the list is indistinguishable from a sequential
    history? Each of the synchronization tools we looked at in the last chapter were
    used to force linearizability by carefully sequencing the ordering of operations
    across threads. At a different level, these tools also manipulated the ordering
    of memory loads and stores. Controlling this ordering directly, while also controlling
    the order of operations on structures, will consume the remainder of this chapter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了。当我们谈论一个结构是*可线性化的*时，我们真正询问的是，针对该结构的任何有效操作列表是否可以被重新排序或添加，以至于列表与顺序历史记录无法区分？我们在上一章中查看的每个同步工具都被用来通过仔细排序线程间操作的顺序来强制线性化。在另一个层面上，这些工具也操纵了内存加载和存储的顺序。直接控制这种顺序，同时控制结构上操作的顺序，将占据本章的剩余部分。
- en: Memory ordering – happens-before and synchronizes-with
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存排序——happens-before和synchronizes-with
- en: Each CPU architecture treats memory ordering—the dependency relationships between
    loads and stores—differently. We discussed this in detail in [Chapter 1](5f3aec9d-fd53-48ff-9ba8-43ce13e91cff.xhtml),
    *Preliminaries – Machine Architecture and Getting Started with Rust*. Suffice
    it to say here in summary, x86 is a *strongly-ordered* architecture; stores by
    some thread will be seen by all other threads in the order they were performed.
    ARM, meanwhile, is a weakly-ordered architecture with data-dependency; loads and
    stores may be re-ordered in any fashion excepting those that would violate the
    behavior of a single, isolated thread, and*,* if a load depends on the results
    of a previous load, you are guaranteed that the previous load will occur rather
    than be cached. Rust exposes its own model of memory ordering to the programmer,
    abstracting away these details. Our programs must, then, be correct according
    to Rust's model*,* and we must trust `rustc` to interpret this correctly for our
    target CPU. If we mess up in Rust's model, we might see `rustc` come and re-order
    instructions to break linearizability, even if the guarantees of our target CPU
    would otherwise have helped us skate by.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个CPU架构对内存排序——加载和存储之间的依赖关系——的处理方式不同。我们已经在[第1章](5f3aec9d-fd53-48ff-9ba8-43ce13e91cff.xhtml)中详细讨论了这一点，*预备知识
    – 机器架构和Rust入门*。在此简要说明，x86是一个*强排序*架构；某些线程的存储操作将按执行顺序被所有其他线程看到。与此同时，ARM是一个具有数据依赖性的弱排序架构；加载和存储可以以任何方式重新排序，除了那些会违反单个、孤立线程的行为之外，*并且*，如果一个加载依赖于之前加载的结果，你可以保证之前的加载将发生而不是被缓存。Rust向程序员暴露了自己的内存排序模型，抽象掉了这些细节。因此，我们的程序必须根据Rust的模型*正确*，我们必须信任`rustc`能够正确地为我们目标CPU解释这些。如果我们破坏了Rust的模型，我们可能会看到`rustc`重新排序指令以破坏线性化，即使我们的目标CPU的保证原本可以帮助我们顺利通过。
- en: We discussed Rust's memory model in detail in [Chapter 3](605ce307-29ed-4b5a-961e-8d327467b84f.xhtml),
    *The Rust Memory Model – Ownership, References and Manipulation*, noting that
    its low-level details are inherited near whole-cloth from LLVM (and thus C/C++).
    We did, however, defer a discussion of dependency relationships in the model until
    this chapter. The topic is tricky enough in its own right, as you'll see directly.
    We will not go into the full details here, owing to the major difficulty in doing
    so; see LLVM's *Memory Model for Concurrent Operations* ([http://llvm.org/docs/LangRef.html#memory-model-for-concurrent-operations](http://llvm.org/docs/LangRef.html#memory-model-for-concurrent-operations))
    and its link tree for the full, challenging read. We will instead note that, outside
    of writing optimizers or especially challenging low-level code for which you do
    need to study up on the LLVM memory model reference, it's sufficient to understand
    that when we talk about ordering in Rust we're talking about a particular kind
    of causality—*happens-before*/*synchronizes-with*. So long as we structure our
    operations according to this causality and can demonstrate linearizability, we'll
    be in good shape.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](605ce307-29ed-4b5a-961e-8d327467b84f.xhtml)“Rust内存模型 – 所有权、引用和操作”中详细讨论了Rust的内存模型，指出其底层细节几乎完全继承自LLVM（以及因此的C/C++）。然而，我们推迟了对模型中依赖关系的讨论，直到本章。这个话题本身就足够复杂，您将直接看到这一点。由于这样做存在重大困难，我们不会在这里深入细节；请参阅LLVM的*并发操作的内存模型*([http://llvm.org/docs/LangRef.html#memory-model-for-concurrent-operations](http://llvm.org/docs/LangRef.html#memory-model-for-concurrent-operations))及其链接树，以获得全面且具有挑战性的阅读材料。相反，我们将指出，除了编写优化器或特别具有挑战性的需要研究LLVM内存模型引用的低级代码之外，理解当我们在Rust中谈论排序时，我们谈论的是一种特定的因果关系——*发生之前*/*同步于*就足够了。只要我们根据这种因果关系来结构我们的操作，并能证明线性化，我们就会处于良好的状态。
- en: 'What is the happens-before/synchronizes-with causality? Each of these refers
    to a particular kind of ordering relationship. Say we have three operations `A`,
    `B`, and `C`. We say that `A` *happens-before* `B` if:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 发生之前/同步于因果关系是什么？这些中的每一个都指代一种特定的排序关系。假设我们有三个操作`A`、`B`和`C`。我们说`A` *发生之前* `B`，如果：
- en: '`A` and `B` are performed on the same thread and `A` is executed and then `B`
    is executed according to program order'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`A`和`B`在同一线程上执行，并且`A`根据程序顺序先执行，然后`B`执行'
- en: '`A` happens-before `C` and `C` happens-before `B`'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`A`发生之前`C`，且`C`发生之前`B`'
- en: '`A` *synchronizes-with* `B`'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`A` *同步于* `B`'
- en: 'Note that the first point refers to single-thread program order. Rust makes
    no guarantees about multi-threaded program order but does assert that single-threaded
    code will run in its apparent textual order, even if, in actuality, both the optimized
    Rust is re-ordered and the underlying hardware is re-ordering as well. We can
    say that `A` *synchronizes-with* `B` if all of the following are true:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一个点指的是单线程程序顺序。Rust不保证多线程程序顺序，但确实断言单线程代码将按照其明显的文本顺序运行，即使实际上优化的Rust被重新排序，底层硬件也在重新排序。如果我们说`A`
    *同步于* `B`，如果以下所有条件都成立：
- en: '`B` is a load from some atomic variable `var` with `Acquire` or `SeqCst` ordering'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`B`是从某个具有`Acquire`或`SeqCst`排序的原子变量`var`中加载'
- en: '`A` is a store to the same atomic variable `var` with `Release` or `SeqCst`
    ordering'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`A`是将具有`Release`或`SeqCst`排序的值存储到相同的原子变量`var`中'
- en: '`B` reads the value from `var`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`B`从`var`读取值'
- en: The last point exists to ensure that compilers or hardware won't optimize away
    the load in the first point. But, did some additional information get stuck in
    there? Yep! More on that in a minute.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点存在是为了确保编译器或硬件不会优化掉第一点中的加载。但是，是否有一些额外的信息卡在了那里？是的！稍后我们将详细介绍。
- en: How does linearizability link up to the causal ordering we've just described?
    It's important to understand this. A structure in Rust can obey the causal ordering
    but still not linearize under examination. Take, for instance, the discussion
    of Ring in previous chapters, where, even though the causal ordering was protected
    by a mutex, writes were stomped due to an offset bug. That implementation was
    not linearizable but it was causally ordered. In that sense, then, getting the
    causality of your program is a necessary but not sufficient condition for writing
    a fit-for-purpose concurrent data structure. Linearizability is an analysis done
    in terms of the operations on a structure, while happens-before/synchronizes-with
    is an ordering that happens inside of and between operations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 线性化是如何与我们所描述的因果排序联系起来的？理解这一点非常重要。在Rust中，一个结构可以遵循因果排序，但在检查时仍然不能线性化。以之前章节中关于环形结构的讨论为例，尽管因果排序被互斥锁保护，但由于偏移量错误，写入被覆盖。那个实现不是可线性化的，但它是有因果排序的。从这个意义上说，那么，获取你程序的因果性是编写适合用途的并发数据结构的必要条件，但不是充分条件。线性化是针对结构上的操作进行的分析，而happens-before/synchronizes-with是一种在操作内部和操作之间发生的排序。
- en: Now, we mentioned `Acquire`, `SeqCst`, and `Release`; what were they? Rust's
    memory ordering model allows for different atomic orderings to be applied to loads
    and stores. These orderings control the underlying CPU and the `rustc` optimizer's
    take on which instructions to shuffle around and when to stall pipelines waiting
    for results from a load. The orderings are defined in an enumeration called `std::sync::atomic::Ordering`.
    We'll go through them one by one.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们提到了`Acquire`、`SeqCst`和`Release`；它们是什么？Rust的内存排序模型允许对加载和存储应用不同的原子排序。这些排序控制着底层CPU和`rustc`优化器的指令调度，以及何时使流水线等待加载的结果。这些排序在名为`std::sync::atomic::Ordering`的枚举中定义。我们将逐一介绍它们。
- en: Ordering::Relaxed
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ordering::Relaxed
- en: This ordering didn't show up in the definition of happens-before/synchronizes-with.
    There's a very good reason for that. `Relaxed` ordering implies no guarantees.
    Loads and stores are free to be re-ordered around a load or store with relaxed
    ordering. That is, loads and stores can migrate *up* or *down* in program order
    from the point of view of a `Relaxed` operation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种排序并没有出现在“happens-before/synchronizes-with”的定义中。这有一个非常好的原因。“Relaxed”排序意味着没有保证。带有“Relaxed”排序的加载和存储可以自由地围绕一个加载或存储进行重新排序。也就是说，加载和存储可以从“Relaxed”操作的角度在程序顺序中向上或向下迁移。
- en: The compiler and the hardware are free to do whatever they please in the presence
    of `Relaxed` ordering. There's quite a lot of utility in this, as we'll see in
    the upcoming detailed examples. For now, consider counters in a concurrent program
    used for self-telemetry, or data structures that have been designed to be eventually
    consistent. No need for strict ordering there.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器和硬件在`Relaxed`排序出现时可以随意操作。正如我们将在接下来的详细示例中看到的那样，这有很多实用价值。现在，考虑一下用于自我遥测的并发程序中的计数器，或者设计为最终一致性的数据结构。那里不需要严格的排序。
- en: 'Before we move on, it''s worth being very clear about what we mean by the ordering
    of loads and stores. Loads and stores on *what*? Well, atomics. In Rust, these
    are exposed in `std::sync::atomic` and there are, as of writing this book, four
    stable atomic types available:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，值得非常清楚地说明我们所说的加载和存储的排序是什么意思。加载和存储在*什么*上？嗯，原子操作。在Rust中，这些在`std::sync::atomic`中公开，截至撰写本书时，有四种稳定的原子类型可用：
- en: '`AtomicUsize`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AtomicUsize`'
- en: '`AtomicIsize`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AtomicIsize`'
- en: '`AtomicPtr`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AtomicPtr`'
- en: '`AtomicBool`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AtomicBool`'
- en: There's an ongoing discussion centered around using Rust issue `#32976` to support
    smaller atomic types—you'll note, of the stable four, three are machine word sized
    and `AtomicBool` is currently also one of them—but progress there looks to have
    stalled out, for want of a champion. So, for now, these are the types. Of course,
    there's more loads and stores than just atomics—manipulation of raw pointers,
    to name a familiar example. These are data accesses and have no ordering guarantees
    beyond those that are built into Rust to begin with. Two threads may load and
    store to the same raw pointer at the exact same moment and there's nothing that
    can be done about it without prior coordination.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有一个围绕使用Rust问题`#32976`来支持更小的原子类型进行的讨论——你会注意到，在稳定的四种类型中，三种是机器字大小，而`AtomicBool`目前也是其中之一——但那里的进展似乎已经停滞，因为没有找到合适的领导者。所以，现在这些是类型。当然，除了原子操作之外，还有更多的负载和存储——以原始指针的操作为例。这些是数据访问，并且没有比Rust一开始内置的更多的排序保证。两个线程可能在同一时刻对相同的原始指针进行负载和存储，而且如果没有事先协调，对此无能为力。
- en: Ordering::Acquire
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ordering::Acquire
- en: '`Acquire` is an ordering that deals with loads. Recall that we previously said
    that if a thread, `B`, loads var with `Acquire` ordering and thread `A` then stores
    with `Release` ordering, then `A` synchronizes-with `B`, which means that `A`
    happens-before `B`. Recall that back in the previous chapter we ran into `Acquire`
    in the context of hopper:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`Acquire`是一种处理负载的排序。回想一下，我们之前说过，如果一个线程`B`以`Acquire`排序加载变量，而线程`A`随后以`Release`排序存储，那么`A`与`B`同步，这意味着`A`发生在`B`之前。回想一下，在前一章中，我们在hopper的上下文中遇到了`Acquire`：'
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The size seen there is an `AtomicUsize`, which the current thread is using
    to determine if there''s space left for it to emplace a new element. Later, once
    the element has been emplaced, the thread increases the size with `Release` ordering.
    That… seems preposterous. It''s clearly counter-intuitive to program order for
    the increase of size to happen-before the capacity check. And, that''s true. It''s
    worth keeping in mind that there''s another thread in the game:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 那里的大小是一个`AtomicUsize`，当前线程正在使用它来确定是否有空间放置新元素。一旦元素被放置，线程将使用`Release`排序来增加大小。这……似乎很荒谬。显然，增加大小的程序顺序在容量检查之前发生是不直观的。而且，这是真的。值得记住的是，游戏中还有另一个线程：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Consider what would happen if there were only two threads, `A` and `B`, and
    `A` only performed `push_back` and `B` only performed `pop_front`. The `mutexes`
    function to provide isolation for threads of the same kind—those that push or
    pop—and so are meaningless in this hypothesis and can be ignored. All that matters
    are the atomics. If the size is zero and `B` is scheduled first, it will empty
    the condvar loop on an Acquire-load of size. When `A` is scheduled and finds that
    there is spare capacity for its element, the element will be enqueued and, once
    done, the size will be `Release`-stored, meaning that some lucky loop of `B` will
    happen-after a successful push-back from `A`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑如果只有两个线程`A`和`B`，`A`只执行`push_back`操作，而`B`只执行`pop_front`操作会发生什么。`mutexes`函数用于为相同类型的线程提供隔离——那些执行推或弹操作的线程——因此在这个假设中是无关紧要的，可以忽略。所有重要的是原子操作。如果大小为零，并且`B`首先被调度，它将在大小为`Acquire`负载的`condvar`循环中清空。当`A`被调度并发现其元素有额外的容量时，该元素将被排队，一旦完成，大小将被`Release`存储，这意味着`B`中某个幸运的循环将在`A`成功执行`push_back`操作之后发生。
- en: 'The Rust documentation for `Acquire` says:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Rust文档中关于`Acquire`的说明是：
- en: '"When coupled with a load, all subsequent loads will see data written before
    a store with Release ordering on the same value in other threads."'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '"当与负载结合时，所有后续的负载都将看到在具有释放排序的其他线程中相同值的存储之前写入的数据。"'
- en: 'The LLVM documentation says:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: LLVM文档说明：
- en: '"Acquire provides a barrier of the sort necessary to acquire a lock to access
    other memory with normal loads and stores."'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '"Acquire提供了一种必要的屏障，以获取锁来使用正常负载和存储访问其他内存。"'
- en: How should we take this? An `Acquire` load keeps loads and stores those that
    come after it from migrating up, with regard to program order. Loads and stores
    prior to the `Acquire` might migrate down, with regard to program order, though.
    Think of an `Acquire` as akin to the starting line of a lock, but one that is
    porous on the top side.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何理解这一点？`Acquire`负载阻止在其之后的负载和存储迁移到更高的程序顺序。然而，在`Acquire`之前的负载和存储可能会迁移到更低的程序顺序。将`Acquire`视为类似于锁的起点，但顶部是渗透的。
- en: Ordering::Release
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ordering::Release
- en: 'If `Acquire` is the starting line of a lock, then `Release` is the finish line.
    In fact, the LLVM documentation says:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `Acquire` 是锁的起点，那么 `Release` 就是终点。实际上，LLVM 文档中提到：
- en: '"Release is similar to Acquire, but with a barrier of the sort necessary to
    release a lock."'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '"Release 与 Acquire 类似，但需要一个释放锁的屏障。"'
- en: 'It''s may be a little more subtle than `lock` might suggest, though. Here''s
    what the Rust documentation has to say:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能比 `lock` 可能暗示的要微妙一些。以下是 Rust 文档中的说明：
- en: '"When coupled with a store, all previous writes become visible to the other
    threads that perform a load with Acquire ordering on the same value."'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '"当与存储结合时，所有之前的写入对执行 Acquire 排序的同一值的加载的其他线程都变得可见。"'
- en: Where `Acquire` stopped loads and stores after itself from migrating upward,
    `Release` stops loads and stores prior to itself from migrating downward, with
    regard to program order. Like `Acquire`, `Release` does not stop loads and stores
    prior to itself from migrating up. A `Release` store is akin to the finish line
    of a lock, but one that is porous on the bottom side.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `Acquire` 停止其后的加载和存储向上迁移时，`Release` 停止其之前的加载和存储向下迁移，关于程序顺序。就像 `Acquire` 一样，`Release`
    不会阻止其之前的加载和存储向上迁移。一个 `Release` 存储类似于锁的终点，但底部是渗透性的。
- en: 'Incidentally, there''s an interesting complication here that lock intuition
    is not helpful with. Question: can two `Acquire` loads or two `Release` stores
    happen simultaneously? The answer is, well, it depends on quite a lot, but it
    is possible. In the preceding hopper example, the spinning `pop_front` thread
    does not block the `push_back` thread from performing its check of size, either
    by permanently holding the size until a `Release` came along to free the `pop_front`
    thread, even temporarily, until the while loop can be checked. Remember, the causality
    model of the language says nothing about the ordering of two `Acquire` loads or
    `Release` stores. It''s undefined what happens and is probably going to strongly
    depend on your hardware. All we do know is that the `pop_front` thread will not
    partially see the stores done to memory by `push_back`; it''ll be all-or-none.
    All of the loads and stores after an `Acquire` and before a `Release` come as
    a unit.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，这里有一个有趣的复杂情况，锁的直觉在这里并不适用。问题：两个 `Acquire` 加载或两个 `Release` 存储可以同时发生吗？答案是，嗯，这取决于很多因素，但这是可能的。在先前的
    hopper 示例中，自旋的 `pop_front` 线程并不会阻止 `push_back` 线程执行其大小检查，即使是通过永久持有大小直到 `Release`
    来释放 `pop_front` 线程，即使只是临时地，直到 while 循环可以被检查。记住，语言的因果关系模型对两个 `Acquire` 加载或 `Release`
    存储的排序一无所知。发生什么是不确定的，并且可能强烈依赖于你的硬件。我们所知道的是，`pop_front` 线程将不会部分地看到 `push_back` 对内存所做的存储；它将是全部或无的。在
    `Acquire` 和 `Release` 之后的加载和存储作为一个单元出现。
- en: Ordering::AcqRel
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ordering::AcqRel
- en: This variant is a combination of `Acquire` and `Release.` When a load is performed
    with `AcqRel`, then `Acquire` is used, and stored to `Release`. `AcqRel` is not
    just a convenience in that it combines the ordering behaviors of both `Acquire`
    and `Release`—the ordering is porous on neither side. That is, loads and stores
    after an `AcqRel` cannot move up, as with `Acquire`, and loads and stores prior
    to an `AcqRel` cannot move down, as with `Release`. Nifty trick.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个变体是 `Acquire` 和 `Release` 的组合。当一个加载操作使用 `AcqRel` 执行时，就会使用 `Acquire` 并将其存储到
    `Release`。`AcqRel` 不仅是一个便利的组合，它结合了 `Acquire` 和 `Release` 的排序行为——两边的排序都是渗透性的。也就是说，在
    `AcqRel` 之后，加载和存储不能向上移动，就像 `Acquire` 一样，而在 `AcqRel` 之前的加载和存储也不能向下移动，就像 `Release`
    一样。这是一个巧妙的技巧。
- en: 'Before moving on to the next ordering variation, it''s worth pointing out that
    so far we''ve only seen examples of a thread performing an `Acquire`/`Release`
    in a pair, in cooperation with another thread. It doesn''t have to be this way.
    One thread can always perform `Acquire` and another can always perform `Release`.
    The causality definition is specifically in terms of threads that perform one
    but not both, except in cases when `A` is equal to `B`, of course. Let''s break
    down an example:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续到下一个排序变体之前，值得指出的是，到目前为止，我们只看到了线程以成对的方式执行 `Acquire`/`Release` 的示例，与另一个线程合作。这不必是这样。一个线程始终可以执行
    `Acquire`，另一个线程始终可以执行 `Release`。因果关系定义是专门针对执行一个但不是两个的线程，当然，除非 `A` 等于 `B`。让我们分析一个例子：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here, we have three threads, `A`, `B`, and `C`, performing a mix of plain loads
    and stores with atomic loads and stores. `store[Release]` is an atomic store where
    store is not. As in the section on linearizability, we''ve numbered each one of
    the operations, but be aware that this does not represent a timeline so much as
    the numbers are convenient names. What can we say of the causality of this example?
    We know:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有三个线程`A`、`B`和`C`，它们执行了一系列的普通加载和存储操作，以及原子加载和存储操作。`store[Release]`是一个原子存储操作，而存储不是。正如线性可化性部分所讨论的，我们给每个操作都编了号，但请注意，这并不代表时间线，因为数字只是方便的名称。我们能说这个例子中的因果关系是什么吗？我们知道：
- en: '`1 happens-before 2`'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1 happens-before 2`'
- en: '`2 synchronizes-with 3`'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2 synchronizes-with 3`'
- en: '`3 happens-before 4`'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`3 happens-before 4`'
- en: '`3 happens-before 5`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`3 happens-before 5`'
- en: '`5 synchronizes-with 6`'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`5 synchronizes-with 6`'
- en: '`6 happens-before 7`'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`6 happens-before 7`'
- en: We'll see more analysis of this sort later in the chapter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面看到更多此类分析。
- en: Ordering::SeqCst
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ordering::SeqCst
- en: 'Finally, and on the opposite side of the spectrum from `Relaxed`, there is
    `SeqCst`, which stands for SEQuentially ConsiSTent. `SeqCst` is like `AcqRel`
    but with the added bonus that there''s a total order across all threads between
    sequentially consistent operations, hence the name. `SeqCst` is a pretty serious
    synchronization primitive and one that you should use only at great need, as forcing
    a total order between all threads is not cheap; every atomic operation is going
    to require a synchronization cycle, for whatever that means for your CPU. A mutex,
    for instance, is an acquire-release structure—more on that shortly—but there are
    legitimate cases for a super-mutex like `SeqCst`. For one, implementing older
    papers. In the early days of atomic programming, the exact consequence of more
    relaxed—but not `Relaxed`—memory orders were not fully understood. You''ll see
    literature that''ll make use of sequentially consistent memory and think nothing
    of it. At the very least, follow along and then relax as needed. This happens
    more than you might think, at least to your author. Your mileage may vary. There
    are cases where it seems like we''re stuck on `SeqCst`. Consider a multiple-producer,
    multiple-consumer setup like this one that has been adapted from CppReference (see
    the *Further reading* section shown in the final section):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在`Relaxed`的对立面，存在`SeqCst`，代表SEQuentially ConsiSTent（顺序一致）。`SeqCst`类似于`AcqRel`，但额外的好处是，在顺序一致的操作之间，所有线程之间都有一个总顺序，因此得名。`SeqCst`是一个相当严肃的同步原语，你只有在极有必要的情况下才应该使用它，因为强制所有线程之间的总顺序并不便宜；每个原子操作都将需要一个同步周期，无论这对你的CPU意味着什么。例如，互斥锁（mutex）是一个获取-释放结构——稍后我们会详细讨论这一点——但确实存在使用类似`SeqCst`这样的超级互斥锁的合理情况。例如，实现旧论文。在原子编程的早期，更宽松——但不是`Relaxed`——的内存顺序的确切后果并没有完全理解。你会看到一些文献会使用顺序一致的内存，但对此并不在意。至少，跟随并按需放松。这种情况可能比你想象的要多，至少对你的作者来说是这样。你的体验可能会有所不同。有些情况下，我们似乎陷入了`SeqCst`的困境。考虑一个多生产者、多消费者设置，如下所示，这是从
    CppReference 中改编的（参见最后部分显示的 *进一步阅读* 部分）：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Say we weren't enforcing `SeqCst` between the threads in our example. What then?
    The thread `c` loops until the thread `a` flips Boolean `X`, loads `Y` and, if
    true, increments `Z`. Likewise for thread `d`, except `b` flips the Boolean `Y`
    and the increment is guarded on `X`. For this program to not crash after its threads
    have hung up, the threads have to see the flips of `X` and `Y` happen in the same
    order. Note that the order itself does not matter, only so much as it is the same
    as seen from every thread. Otherwise it's possible that the stores and loads will
    interleave in such a way as to avoid the increments. The reader is warmly encouraged
    to fiddle with the ordering on their own.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们不在我们的示例线程之间强制执行`SeqCst`。那么会怎样？线程`c`会循环，直到线程`a`翻转布尔值`X`，加载`Y`，如果为真，则增加`Z`。同样，对于线程`d`，除了`b`翻转布尔值`Y`，增加操作由`X`保护。为了使程序在所有线程挂起后不会崩溃，线程必须看到`X`和`Y`的翻转以相同的顺序发生。请注意，顺序本身并不重要，重要的是它对每个线程来说都是相同的。否则，存储和加载可能会以某种方式交错，从而避免增加。强烈鼓励读者自己尝试调整顺序。
- en: Building synchronization
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建同步
- en: Now that we have a good grounding on the atomic primitives available to us in
    the Rust standard library, and have, moreover, a solid theoretical background,
    it's time for us to build on these foundations. In the past few chapters, we've
    been teasing our intention to build up mutexes and semaphores from primitives
    and, well, now the time has come.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对Rust标准库中可用的原子原语有了良好的基础，并且还有坚实的理论基础，是时候在此基础上构建了。在过去几章中，我们一直在暗示我们的意图是从原语构建互斥锁和信号量，嗯，现在时机已经成熟。
- en: Mutexes
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 互斥锁
- en: 'Now that we understand linearizability and memory orderings, let''s ask ourselves
    a question. What *exactly* is a mutex? We know its properties as an atomic object:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了线性化和内存排序，让我们问自己一个问题。互斥锁 *究竟* 是什么？我们知道它作为一个原子对象具有以下属性：
- en: Mutex supports two operations, *lock* and *unlock*.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互斥锁支持两种操作，*锁定* 和 *解锁*。
- en: A mutex is either *locked* or *unlocked*.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互斥锁要么是 *锁定* 的，要么是 *未锁定* 的。
- en: The operation *lock* will move a mutex into a *locked* state if and only if
    the mutex is *unlocked*. The thread that completes *lock* is said to hold the
    lock.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作 *锁定* 将将互斥锁移动到 *锁定* 状态，仅当互斥锁是 *未锁定* 的。完成 *锁定* 的线程被称为持有锁。
- en: The operation *unlock* will move a mutex into *unlocked* state if an only if
    the mutex is previously *locked* and the caller of *unlock* is the holder of the
    lock.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作 *解锁* 将将互斥锁移动到 *未锁定* 状态，仅当互斥锁之前是 *锁定* 的，并且 *解锁* 的调用者是锁的持有者。
- en: All loads and stores which occur after and before a *lock* in program order
    must not be moved prior to or after the *lock* operation.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在程序顺序中，在 *锁定* 之后和之前发生的所有加载和存储操作不得移动到 *锁定* 操作之前或之后。
- en: All loads and stores that occur before an *unlock* in program order must not
    be moved to after the *unlock*.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在程序顺序中，在 *解锁* 之前发生的所有加载和存储操作不得移动到 *解锁* 之后。
- en: 'The second to last point is subtle. Consider a soft-mutex whose lock only keeps
    loads and stores after migrating up. This means that load/stores could migrate
    into the soft-mutex, which is all well and good unless your migration is the lock
    of another soft-mutex. Consider the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个到最后一个点很微妙。考虑一个软互斥锁，其锁只保留迁移后的加载和存储。这意味着加载/存储可以迁移到软互斥锁中，这很好，除非你的迁移是另一个软互斥锁的锁。考虑以下情况：
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This could be re-arranged to the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以重新排列为以下内容：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, the `unlock(m0)` has migrated down in program order into the mutual exclusion
    zone of `m1`, deadlocking the program. That is a problem.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`unlock(m0)` 已经在程序顺序中迁移到 `m1` 的互斥排他区域，导致程序死锁。这是一个问题。
- en: 'You might not be surprised to learn that there''s decades of research, at this
    point, into the question of mutual exclusion. How should a mutex be made? How
    does fairness factor into the implementation? The latter is a significantly broad
    topic, avoiding thread *starvation*, and is one we''ll more or less dodge here.
    Much of the historical research is written with regard to machines that have no
    concept of concurrency control. Lamport''s Bakery algorithm—see the *Further reading* section–
    provides mutual exclusion that''s absent in any hardware support but assumes reads
    and writes are sequentially consistent, an assumption that does not hold on modern
    memory hierarchies. In no small sense do we live in a very happy time for concurrent
    programming: our machines expose synchronization primitives directly, greatly
    simplifying the production of fast and correct synchronization mechanisms.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能不会惊讶地了解到，到目前为止，关于互斥排他问题的研究已有几十年。互斥锁应该如何设计？公平性因素如何影响实现？后者是一个相当广泛的话题，避免线程 *饥饿*，我们在这里或多或少会避开这个问题。大部分历史研究都是针对没有并发控制概念的机器。Lamport的面包店算法——参见
    *进一步阅读* 部分——提供了任何硬件支持中都不存在的互斥排他，但假设读取和写入是顺序一致的，这个假设在现代内存层次结构中并不成立。在很大意义上，我们生活在一个非常美好的并发编程时代：我们的机器直接暴露同步原语，极大地简化了快速和正确同步机制的生产。
- en: Compare and set mutex
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较并设置互斥锁
- en: 'First, let''s look at a very simple mutex implementation, an atomic swap mutex
    that spins around in a loop until the correct conditions arrive. That is, let''s
    build a *spin-lock*. This mutex is so named because every thread that blocks on
    `lock` is burning up CPU, spinning on the condition that holds it from acquiring
    the lock. Anyhow, you''ll see. We''ll lay down our `Cargo.toml` first:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看一个非常简单的互斥锁实现，一个在循环中旋转直到正确条件出现的原子交换互斥锁。也就是说，让我们构建一个 *自旋锁*。这个互斥锁之所以这样命名，是因为每个在
    `lock` 上阻塞的线程都在消耗CPU，在它获得锁的条件上自旋。无论如何，你很快就会看到。我们首先放下我们的 `Cargo.toml`：
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we need `src/lib.rs`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，我们需要 `src/lib.rs`:'
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'There''s a fair bit here we''re not going to use straight away, but we''ll
    come to it shortly. Now, let''s dive into `src/swap_mutex.rs`. First, our preamble:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有相当一部分内容我们暂时不会直接使用，但我们会很快涉及到。现在，让我们深入到`src/swap_mutex.rs`文件。首先，是我们的前言：
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Very little surprise here by this point. We see the usual imports, plus a few
    new ones—`AtomicBool` and `Ordering`, as discussed previously. We implement `Send`
    and `Sync` ourselves—discussed in the previous chapter—because while *we* can
    prove that our `SwapMutex<T>` is `threadsafe`, Rust cannot. The `SwapMutex<T>`
    is small:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个阶段，这里并没有太多惊喜。我们看到通常的导入，还有一些新的导入——`AtomicBool`和`Ordering`，正如之前讨论的那样。我们自行实现了`Send`和`Sync`——在上一章中讨论过——因为虽然*我们*可以证明我们的`SwapMutex<T>`是线程安全的，但Rust不能。`SwapMutex<T>`很小：
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The field `locked` is an `AtomicBool`, which we''ll be using to provide isolation
    between threads. The idea is simple enough—if a thread shows up and finds that
    `locked` is false then the thread may acquire the lock, else it has to spin. It''s
    also worth noting that our implementation lives a little on the wild side by keeping
    a raw pointer to `T`. The Rust standard library `std::sync::Mutex` keeps its interior
    data, as of writing this book, in an `UnsafeCell`, which we don''t have access
    to in stable. In this implementation we''re just going to be storing the pointer
    and not doing any manipulation through it. Still, we''ve got to be careful about
    dropping the `SwapMutex`. Creating a `SwapMutex` happens like you might expect:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 字段`locked`是一个`AtomicBool`，我们将使用它来在线程之间提供隔离。这个想法很简单——如果一个线程出现并发现`locked`是`false`，那么这个线程可以获取锁，否则它必须自旋。还值得注意的是，我们的实现稍微有些冒险，因为它保留了对`T`的原始指针。截至撰写本书时，Rust标准库`std::sync::Mutex`将其内部数据存储在`UnsafeCell`中，而我们无法在稳定版本中访问它。在这个实现中，我们只是存储指针，并不通过它进行任何操作。尽管如此，我们仍然必须小心处理`SwapMutex`的析构。创建`SwapMutex`的方式可能正如你所预期的那样：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `T` is moved into `SwapMutex` and we box it to then get its raw pointer.
    That''s necessary to avoid a stack value being pushed into `SwapMutex` and promptly
    disappearing when the stack frame changes, all of which was discussed in great
    detail in [Chapter 2](8c61da2f-08c8-40be-89d4-d8afa8510850.xhtml), *Sequential
    Rust Performance and Testing*. A mutex always starts unlocked, else there''d be
    no way to ever acquire it. Now, let''s look at locking the mutex:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`T`被移动到`SwapMutex`中，然后我们将其装箱以获取其原始指针。这是必要的，以避免栈值被推入`SwapMutex`，然后在栈帧改变时迅速消失，所有这些都已经在[第2章](8c61da2f-08c8-40be-89d4-d8afa8510850.xhtml)中详细讨论过，*顺序Rust性能和测试*。互斥锁始终处于未锁定状态，否则将无法获取它。现在，让我们看看如何锁定互斥锁：'
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The API is a little different compared to the standard library `Mutex`. For
    one, `SwapMutex` does not track poisoning, and, as a result, if lock is invoked
    it''s guaranteed to return with a guard, called `SwapMutexGuard`. `SwapMutex`
    *is* scope-based, however; just like standard library `MutexGuard` there''s no
    need—nor ability—to ever call an explicit unlock. There is an unlock though, used
    by the drop of `SwapMutexGuard`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准库`Mutex`相比，API略有不同。一方面，`SwapMutex`不跟踪中毒，因此，如果调用锁，则保证会返回一个名为`SwapMutexGuard`的守卫。然而，`SwapMutex`是基于作用域的；就像标准库`MutexGuard`一样，没有必要——也没有能力——永远调用显式的解锁。不过，确实有一个解锁操作，用于`SwapMutexGuard`的析构：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this simple mutex, all that needs to be done to unlock is set the `locked`
    field to false, but not before confirming that, in fact, the calling thread has
    a right to unlock the mutex.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的互斥锁中，要解锁只需要将`locked`字段设置为`false`，但在确认调用线程确实有权解锁互斥锁之前不要这样做。
- en: 'Now, can we convince ourselves that this mutex is correct? Let''s go through
    our criteria:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们能否说服自己这个互斥锁是正确的？让我们逐一检查我们的标准：
- en: '"Mutex supports two operations, *lock* and *unlock*."'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: “互斥锁支持两种操作，*锁定*和*解锁*。”
- en: 'Check. Next:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 检查。接下来：
- en: '"A mutex is either *locked* or *unlocked."*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: “互斥锁要么是*锁定*的，要么是*未锁定*。”
- en: 'Check, by reason that this is flagged by a Boolean. Finally:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过布尔值标记来检查，最后：
- en: '"The operation *lock* will move a mutex into a *locked* state if and only if
    the mutex is *unlocked*. The thread that completes the *lock* is said to hold
    the *lock*."'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: “操作*锁定*将互斥锁移动到*锁定*状态，当且仅当互斥锁处于*未锁定*状态。完成*锁定*操作的线程被称为持有*锁定*。”
- en: 'Let''s talk through what `AtomicBool::swap` does. The full type is `swap(&self,
    val: bool, order: Ordering) -> bool` and the function does what you might expect
    by the name; it atomically swaps the bool at self with the bool `val` atomically,
    according to the passed ordering, and returns the previous value. Here, then,
    each thread is competing to write true into the locked flag and only one thread
    at a time will see false returned as the previous value, owing to the atomicity
    of swapping. The thread that has written false is now the holder of the lock,
    returning `SwapMutexGuard`. Locking a `SwapMutex` can only be done to an unlocked
    `SwapMutex` and it is done exclusive of other threads.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们讨论一下 `AtomicBool::swap` 的作用。完整的类型是 `swap(&self, val: bool, order: Ordering)
    -> bool`，该函数根据其名称执行你可能会期望的操作；它根据传递的排序原子地交换 `self` 中的布尔值与 `val` 中的布尔值，并返回前一个值。在这里，因此，每个线程都在竞争将
    true 写入锁定标志，并且由于交换的原子性，一次只有一个线程会看到返回的前一个值为 false。写入 false 的线程现在是锁的持有者，返回 `SwapMutexGuard`。锁定
    `SwapMutex` 只能对未锁定的 `SwapMutex` 进行，并且是排他性地对其他线程进行的。'
- en: '"The operation unlock will move a mutex into an unlocked state if and only
    if the mutex is previously locked and the caller of unlock is the holder of the
    lock."'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: “解锁操作将把互斥锁移动到未锁定状态，如果且仅当互斥锁之前是锁定状态，并且解锁的调用者是锁的持有者。”
- en: 'Let''s consider unlock. Recall first that it''s only possible to call from
    `SwapMutexGuard`, so the assertion that the caller has the right to unlock the
    `SwapMutex` is a check against a malfunctioning lock: only one thread at a time
    can hold `SwapMutex`, and as a result there will only ever be one `SwapMutexGuard`
    in memory. Because of the nature of `Release` ordering, we''re guaranteed that
    the `Relaxed` load of `locked` will occur before the store, so it''s guaranteed
    that when the store does happen the value of `locked` will be true. Only the holder
    of the lock can unlock it, and this property is satisfied as well.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑解锁。首先回忆一下，只有从 `SwapMutexGuard` 调用才有可能进行操作，所以调用者有权限解锁 `SwapMutex` 的断言是对锁故障的检查：一次只有一个线程可以持有
    `SwapMutex`，因此内存中只会存在一个 `SwapMutexGuard`。由于 `Release` 排序的性质，我们保证 `locked` 的 `Relaxed`
    加载将在存储之前发生，所以可以保证当存储发生时，`locked` 的值将是 true。只有锁的持有者才能解锁它，并且这个属性也得到了满足。
- en: '"All loads and stores that occur after and before a lock in program order must
    not be moved prior to or after the lock."'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: “在程序顺序中在锁之前和之后发生的所有加载和存储操作不得在锁之前或之后移动。”
- en: This follows directly from the behavior of `AcqRel` ordering.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这直接源于 `AcqRel` 排序的行为。
- en: '"All loads and stores that occur before an unlock in program order must not
    be moved to after the unlock."'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: “在程序顺序中在解锁之前发生的所有加载和存储操作不得移动到解锁之后。”
- en: This follows from the behavior of `Release` ordering.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这源于 `Release` 排序的行为。
- en: 'Bang, we have ourselves a mutex. It''s not an especially power-friendly mutex,
    but it does have the essential properties. Here''s the rest of the thing, for
    completeness''s sake:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，我们得到了一个互斥锁。它不是一个特别节能的互斥锁，但它确实具有基本属性。为了完整性，以下是其余的内容：
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, let''s build something with `SwapMutex`. In `src/bin/swap_mutex.rs`, we''ll
    replicate the bridge problem from the last chapter, but on top of `SwapMutex`,
    plus some fancy add-ons now that we know some clever things to do with atomics.
    Here''s the preamble:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用 `SwapMutex` 来构建一些东西。在 `src/bin/swap_mutex.rs` 中，我们将复制上一章中的桥接问题，但这次是在
    `SwapMutex` 上，再加上一些现在我们知道如何使用原子操作的一些花哨的附加功能。下面是前言：
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Familiar enough. We pull in our `SwapMutex`, and define `Bridge`. Fairly unsurprising
    material. To continue:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 足够熟悉了。我们引入了 `SwapMutex`，并定义了 `Bridge`。相当不出所料的内容。继续：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This is new, though. Statics in Rust are compile-time globals. We''ve seen
    their lifetime static floating around from time to time but have never remarked
    on it. A static will be part of the binary itself and any initialization code
    needed for the static will have to be evaluated at compile-time as well as being
    threadsafe. We''re creating two static `AtomicUsizes` here at the top of our program.
    Why? One of the major issues with the previous rope bridge implementation was
    its silence. You could watch it in a debugger, sure, but that''s slow and won''t
    impress your friends. Now that we have atomics at hand, what we''re going to do
    is get each side of the bridge to tally up how many baboons cross. `LHS_TRANSFERS`
    are the number of baboons that go from right to left, `RHS_TRANSFERS` is the other
    direction. Our left and right sides of the bridge are broken out into functions
    this time around, too:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是新的。Rust中的静态是编译时全局变量。我们偶尔看到它们的静态生命周期在周围飘荡，但从未对此发表评论。静态将成为二进制文件的一部分，并且为静态所需的任何初始化代码都必须在编译时评估，并且是线程安全的。我们在程序顶部创建了两个静态`AtomicUsizes`。为什么？前一个绳桥实现的主要问题之一是它的沉默。你当然可以在调试器中观察它，但这很慢，而且不会给你的朋友留下深刻印象。现在，我们有了原子操作，我们将要做的就是让桥的每一侧都统计有多少狒狒通过。`LHS_TRANSFERS`是向左移动的狒狒的数量，`RHS_TRANSFERS`是相反的方向。这次我们将桥的左右两侧分解成函数：
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This ought to be familiar enough from the previous chapter, but do note that
    the transfer counters are now being updated. We''ve used `Relaxed` ordering—the
    increments are guaranteed to land but it''s not especially necessary for them
    to occur strictly before or after the modification to the guard. Finally, the
    `main` function:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该足够熟悉了，来自上一章，但请注意，传输计数器现在正在更新。我们使用了`Relaxed`排序——增量保证会到达，但它们严格发生在修改守卫之前或之后并不是特别必要。最后，是`main`函数：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can see here that the mutex is set up, the threads have been spawned, and
    that the main thread spends its time in an infinite loop printing out information
    on transfer rates. This is done by sleeping the thread in one-second intervals,
    swapping the transfer counters with zero and printing the previous value. The
    output looks something like this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这里已经设置了互斥锁，创建了线程，并且主线程花费时间在一个无限循环中打印传输率的信息。这是通过在一秒钟间隔中暂停线程，交换传输计数器与零并打印前一个值来完成的。输出看起来像这样：
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The exact numbers will vary depending on your system. Also note that the numbers
    are not very close in some instances, either. The `SwapMutex` is not *fair*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 具体的数字将根据您的系统而变化。请注意，在某些情况下，这些数字也不是非常接近。`SwapMutex`不是**公平的**。
- en: An incorrect atomic queue
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个错误的原子队列
- en: Before we build anything else, we're going to need a key data structure—a unbounded
    queue. In [Chapter 5](e2de1ba7-c291-494e-82da-37fee7323c1d.xhtml), *Locks – Mutex,
    Condvar, Barriers, and RWLock*, we discussed a bounded deque protected at either
    end by mutexes. We're in the business, now, of building synchronization and can't
    make use of the mutex approach. Our ambition is going to be to produce an unbounded
    first-in-first-out data structure that has no locks, never leaves an enqueuer
    or dequeuer deadlocked, and is linearizable to a sequential queue. It turns out
    there's a pretty straightforward data structure that achieves this aim; the Michael
    and Scott Queue, introduced in their 1995 paper *Simple, Fast, and Practical Non-Blocking
    and Blocking Concurrent Queue Algorithms*. The reader is encouraged to breeze
    through that paper before continuing on with our discussion here, but it's not
    strictly necessary.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建任何其他东西之前，我们需要一个关键的数据结构——一个无界队列。在[第五章](e2de1ba7-c291-494e-82da-37fee7323c1d.xhtml)中，*锁——互斥锁、条件变量、屏障和读写锁*，我们讨论了由互斥锁保护在两端的有界双端队列。现在，我们正在从事构建同步的工作，不能使用互斥锁的方法。我们的目标是创建一个无界先进先出数据结构，它没有锁，永远不会让入队或出队操作死锁，并且可以线性化为顺序队列。结果证明，有一个相当直接的数据结构可以实现这个目标；迈克尔和斯科特队列，在他们的1995年论文*简单、快速和实用的非阻塞和阻塞并发队列算法*中介绍。鼓励读者在继续我们这里的讨论之前快速浏览一下那篇论文，但这不是严格必要的。
- en: A word of warning. Our implementation will be *wrong*. The careful reader will
    note that we're following the paper closely. There are two, and maybe more, issues
    with the implementation we'll present, one major and unresolvable and the other
    addressable with some care. Both are fairly subtle. Let's dig in.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个警告。我们的实现将是**错误的**。仔细的读者会注意到我们正在紧密遵循论文。我们将展示的实现有两个，可能还有更多，问题，一个是主要且无法解决的，另一个可以通过一些小心处理来解决。这两个问题都相当微妙。让我们深入探讨。
- en: 'Queue is defined in `src/queue.rs`, the preamble of which is:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`Queue` 定义在 `src/queue.rs` 中，其前导部分如下：'
- en: '[PRE24]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Right off you can see from `null_mut` that we're going to be dealing with raw
    pointers. The `AtomicPtr` is new, though we did mention it in passing earlier
    in the chapter. An atomic pointer adapts a raw pointer—`*mut T`—to be suitable
    for atomic operations. There's no runtime overhead associated with AtomicPtr;
    the Rust documentation notes that the type has the same in-memory representation
    as a `*mut T`. Modern machines expose instructions, giving the programmer the
    ability to fiddle with memory atomically. Capabilities vary by processor, as you'd
    expect. LLVM and therefore Rust expose these atomic memory fiddling capabilities
    through `AtomicPtr`, allowing the range of pointer fiddling in the sequential
    language but atomically. What this means, in practice, is that we can start setting
    up happens-before/synchronizes-with causality relationships for pointer manipulation,
    which is essential for building data structures.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 立刻从 `null_mut` 可以看出，我们将要处理原始指针。`AtomicPtr` 是新的，尽管我们在本章前面提到了它。原子指针将原始指针——`*mut
    T`——适配为适合原子操作。与 `AtomicPtr` 相关的运行时开销为零；Rust 文档指出，该类型与 `*mut T` 具有相同的内存表示。现代机器暴露出指令，使程序员能够原子地操作内存。正如预期的那样，能力因处理器而异。LLVM
    和因此 Rust 通过 `AtomicPtr` 暴露这些原子内存操作能力，允许在顺序语言中原子地执行指针操作。在实践中，这意味着我们可以开始设置指针操作的前置/同步因果关系，这对于构建数据结构至关重要。
- en: 'Here''s the next part:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是下一部分：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The interior of deque from the previous chapter was a single, contiguous block.
    That''s not the approach we''re taking here. Instead, every element inserted into
    the queue will get a `Node` and that `Node` will point to the next `Node`, which
    may or may not exist yet. It''s a linked list. The contiguous block approach is
    a bit harder to pull off in an atomic context—though it is entirely possible and
    there are discussions in the *Further reading* section papers—and would come down
    to a linked list of contiguous blocks. It''s more trouble than it''s worth for
    our purposes here:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章中的双端队列的内部是一个单一的、连续的块。我们这里不是采取这种方法。相反，每个插入队列的元素都将获得一个 `Node`，该 `Node` 将指向下一个
    `Node`，而这个 `Node` 可能存在也可能不存在。它是一个链表。在原子上下文中实现连续块的方法有点困难——尽管这是完全可能的，并且在 *进一步阅读*
    部分的论文中有所讨论——并且将归结为连续块的链表。对于我们这里的用途来说，这比它值得的麻烦要多：
- en: '[PRE26]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'One key thing to note is that `Node` holds a pointer to a heap allocated `T`,
    not the `T` directly. In the preceding code, we have the `InnerQueue<T>` of `Queue<T>`,
    pulling the usual inner/outer structure detailed elsewhere in this book and in
    `rustc`. Why is it important to note that `Node` doesn''t hold its `T` directly?
    The value inside of the head of the `Queue<T>` is never inspected. The head of
    the `Queue` is a sentinel. When the `InnerQueue` is created, we''ll see the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一个需要注意的关键点是 `Node` 持有一个指向堆分配的 `T` 的指针，而不是直接持有 `T`。在先前的代码中，我们有 `Queue<T>` 的 `InnerQueue<T>`，这是这本书和其他地方详细描述的常规内/外结构。为什么要注意
    `Node` 不直接持有其 `T` 呢？`Queue<T>` 的头部的值永远不会被检查。`Queue` 的头部是一个哨兵。当创建 `InnerQueue`
    时，我们将看到以下内容：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Both the `head` and `tail` of the `InnerQueue` point to the same nonsense-valued
    `Node`, as expected. The value at the outset is, in fact, null. Atomic data structures
    have issues with memory reclamation in that coordinating drops is problematic
    and must be done only once. It''s possible to alleviate this issue somewhat by
    relying on Rust''s type system, but it''s still a non-trivial project and is an
    active area of research, generally. Here, we note that we''re careful to hand
    out the ownership of the element only once. Being a raw pointer, it can be given
    away more than once at a time, but that path leads to double-frees. `InnerQueue`
    converts `*const T` into `T`—an unsafe operation—and just never dereferences the
    `*const T` again, allowing the caller to do the drop in its own sweet time:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，`InnerQueue` 的 `head` 和 `tail` 都指向同一个无意义的 `Node`。实际上，初始值是 null。原子数据结构在内存回收方面存在问题，因为协调释放是困难的，并且必须只执行一次。通过依赖
    Rust 的类型系统，可以稍微缓解这个问题，但这仍然是一个非平凡的工程，并且是研究的一个活跃领域。在这里，我们注意到我们非常小心地只分配一次元素的拥有权。作为一个原始指针，它可以一次分配多次，但这条路径会导致双重释放。`InnerQueue`
    将 `*const T` 转换为 `T`——这是一个不安全的操作——并且永远不会再次解引用 `*const T`，允许调用者在合适的时间执行释放操作：
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This is the `enq` operation, marked unsafe because of the raw pointer manipulation
    going on. That''s an important point to consider—`AtomicPtr` is necessarily going
    to be done with raw pointers. There''s a lot going on here, so let''s break it
    up into smaller chunks:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`enq`操作，由于涉及原始指针操作，因此标记为不安全。这是一个需要考虑的重要点——`AtomicPtr`必然需要使用原始指针。这里有很多事情在进行，所以让我们将其分解成更小的部分：
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here, we''re constructing the `Node` for `val`. Notice we''re using the same
    boxing, the `into_raw` approach used so often in previous chapters. This node
    doesn''t have a place in the queue yet and the calling thread does not hold an
    exclusive lock over the queue. Insertion will have to take place in the midst
    of other insertions:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们正在为`val`构造`Node`。注意我们正在使用与之前章节中经常使用的相同的装箱，即`into_raw`方法。这个节点在队列中还没有位置，调用线程也没有对队列持有独占锁。插入将不得不在其他插入过程中进行：
- en: '[PRE30]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: With that in mind, it's entirely possible that an insertion attempt can fail.
    The enqueing of an element in a queue takes place at the `tail`, the pointer to
    which we load up and call last. The next node after `tail` is called `next`. In
    a sequential queue, we'd be guaranteed that the next of `tail` is null, but that's
    not so here. Consider that between the load of the `tail` pointer and the load
    of the `next` pointer an `enq` run by another thread may have already completed.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，插入尝试可能会失败。在队列中入队一个元素发生在`尾`部分，我们加载并调用`last`的指针。`tail`之后的下一个节点称为`next`。在顺序队列中，我们可以保证`tail`的下一个是null，但在这里并非如此。考虑在加载`tail`指针和加载`next`指针之间，另一个线程可能已经完成了`enq`操作。
- en: 'Enqueing is, then, an operation that might take several attempts before we
    hit just the right conditions for it to succeed. Those conditions are last still
    being the `tail` of the structure and next being `null`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 入队操作可能需要尝试多次，才能达到成功的条件。这些条件是结构体的`尾`部分，接下来是`null`：
- en: '[PRE31]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Note that the first load of `tail` is `Acquire` and each of the possible stores
    of it, in either branch, are `Release`. This satisfies our `Acquire`/`Release`
    needs, with regard to locking primitives. All other stores and loads here are
    conspicuously `Relaxed`. How can we be sure we''re not accidentally stomping writes
    or, since this is a linked list, cutting them loose in memory? That''s where the
    `AtomicPtr` comes in:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一次加载`tail`是`Acquire`，而可能的存储操作，无论是哪个分支，都是`Release`。这满足了我们的`Acquire`/`Release`需求，关于锁定原语。这里的所有其他存储和加载操作都是明显的`Relaxed`。我们如何确保我们没有意外地覆盖写入，或者由于这是一个链表，在内存中切断它们？这就是`AtomicPtr`发挥作用的地方：
- en: '[PRE32]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'It *is* entirely possible that by the time we''ve detected the proper conditions
    for enqueing another thread will have been scheduled in, have detected the proper
    conditions for enqueing, and have been enqueued. We attempt to slot our new node
    in with `(*last).next.compare_and_swap(next, node, Ordering::Relaxed)`, that is,
    we compare the current next of last and if and only if that succeeds—that''s the
    `== next` bit—do we attempt to set `tail` to the node pointer, again with a compare
    and swap. If both of those succeed then the new element has been fully enqueued.
    It''s possible that the swap of `tail` will fail, however, in which case the linked
    list is correctly set up but the `tail` pointer is off. Both the `enq` and `deq`
    operations must be aware they could stumble into a situation where the `tail`
    pointer needs to be adjusted. That is in fact how the `enq` function finishes
    off:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 完全有可能，当我们检测到入队正确条件的时候，另一个线程已经被调度，已经检测到入队正确条件，并且已经被入队。我们尝试使用`(*last).next.compare_and_swap(next,
    node, Ordering::Relaxed)`将新节点插入，也就是说，我们比较当前`last`的`next`，如果并且只有如果这成功了——那就是`==
    next`部分——我们尝试将`tail`设置为节点指针，再次使用比较和交换。如果这两个都成功了，那么新元素已经完全入队。然而，`tail`的交换可能会失败，在这种情况下，链表被正确设置，但`tail`指针偏移了。`enq`和`deq`操作都必须意识到它们可能会遇到需要调整`tail`指针的情况。实际上，这就是`enq`函数结束的方式：
- en: '[PRE33]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'On an x86, all of these `Relaxed` operations are more strict but on ARMv8 there
    will be all sorts of reordering. It''s very important, and difficult, to establish
    a causal relationship between all modifications. If, for example, we swapped the
    `tail` pointer and then the `next` of the `tail` pointer, we''d open ourselves
    up to breaking the linked list, or making whole isolated chains depending on the
    threads'' view of memory. The `deq` operation is similar:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在 x86 上，所有这些 `Relaxed` 操作都更为严格，但在 ARMv8 上会有各种重排序。在所有修改之间建立因果关系非常重要且困难。例如，如果我们交换了
    `tail` 指针然后是 `tail` 指针的 `next`，我们就会使自己面临破坏链表或根据线程对内存的视图创建整个孤立链的风险。`deq` 操作类似：
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The function is a loop, like `enq`, in which we search for the correct conditions
    and circumstance to dequeue an element. The first outer if clause checks that
    head hasn't shifted on us, while the inner first branch is to do with a queue
    that has no elements, where first and last are pointers to the same storage. Note
    here that if next is not null we try and patch up a partially completed linked
    list of nodes before looping back around again for another pass at dequeing.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数是一个循环，就像 `enq` 一样，我们在其中寻找正确的条件和情况来出队一个元素。第一个外部 if 子句检查 `head` 是否在我们身上移动，而内部第一个分支是关于一个没有元素的队列，其中第一个和最后一个都是指向相同存储的指针。注意，如果
    `next` 不是空，我们会在再次循环回来进行另一次出队尝试之前尝试修复一个部分完成的节点链表。
- en: This is because, as discussed previously, `enq` may not fully succeed. The second
    inner loop is hit when `head` and `tail` are not equal, meaning there's an element
    to be pulled. As the inline comment explains, we give out the ownership of the
    element `T` when the first hasn't shifted on us but are careful not to dereference
    the pointer until we can be sure we're the only thread that will ever manage that.
    We can be on account of only one thread that will ever manage to swap the particular
    first and next pair the calling thread currently holds.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为，正如之前讨论的那样，`enq` 可能不会完全成功。当 `head` 和 `tail` 不相等时，会触发第二个内部循环，这意味着有一个元素需要被取出。正如内联注释所解释的，当第一个元素没有在我们身上移动时，我们就会给出元素
    `T` 的所有权，但我们非常小心，直到我们可以确定我们将是唯一一个将管理该元素的线程，我们才不会解引用指针。我们可以因为只有一个线程能够交换调用线程当前持有的特定第一个和下一个对。
- en: 'After all of that, the actual outer `Queue<T>` is a touch anti-climactic:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些之后，实际的 `Queue<T>` 外部函数有点令人失望：
- en: '[PRE35]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We''ve already reasoned our way through the implementation and, hopefully,
    you, dear reader, are convinced that the idea should function. Where the rubber
    meets the road is in testing:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经通过推理完成了实现，并且，希望亲爱的读者，你已经被说服这个想法应该能够工作。真正考验的地方在于测试：
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This is the usual `test` preamble that we''ve seen elsewhere in the book. We
    define an `Op` enumeration to drive an interpreter style `quickcheck test`, which
    we call here `sequential`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在这本书的其他地方看到过的通常的 `test` 前置代码。我们定义一个 `Op` 枚举来驱动解释器风格的 `quickcheck test`，我们在这里称之为
    `sequential`：
- en: '[PRE37]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We have a `VecDeque` as the model; we know it''s a proper queue. Then, without
    dipping into any kind of real concurrency, we confirm that `Queue` behaves similarly
    to a `VecDeque`. At least in a sequential setting, `Queue` will work. Now, for
    a parallel `test`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个 `VecDeque` 作为模型；我们知道它是一个合适的队列。然后，在不涉及任何真实并发的情况下，我们确认 `Queue` 的行为类似于 `VecDeque`。至少在顺序设置中，`Queue`
    将会工作。现在，对于并行 `test`：
- en: '[PRE38]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We set up two groups of threads, one responsible for enqueing and the other
    for dequeing. The enqueuing threads push a `total` number of items through the
    `Queue` and the dequeuers pull until a counter—shared between each of the dequeuers—hits
    bingo. Finally, back in the main `test` thread, we confirm that the total number
    of retrieved items is the same as the expected number of items. It''s possible
    that our dequeing threads will read *past the end* of the queue because of a race
    between the check on the while loop and the call of `q.deq`, which works in our
    favor because confirming the queue allows the deque of no more elements than were
    enqueued. That, and there are no double-free crashes when the `test` is run. This
    inner `test` function is used twice, once in repeated runs and then again in a
    `quickcheck` setup:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了两组线程，一组负责入队，另一组负责出队。入队线程将`total`数量的项目推入`Queue`，而出队线程则拉取，直到一个共享在每个出队线程之间的计数器达到bingo。最后，回到主`test`线程，我们确认检索到的总项目数与预期的项目数相同。我们的出队线程可能会因为while循环的检查和`q.deq`调用的竞争而读取队列的末尾，这反而对我们有利，因为确认队列允许出队不超过入队元素的数量。此外，当运行`test`时，没有出现双重释放崩溃。这个内部的`test`函数被使用了两次，一次是在重复运行中，然后又在`quickcheck`设置中：
- en: '[PRE39]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: What's wrong here? If you run the `test` suite, you may or may not hit one of
    the issues. They are fairly improbable, though we'll shortly see a way to reliably
    trigger the worst. The first issue our implementation runs into is the ABA problem.
    In a compare-and-swap operation, pointer `A` is to be swapped by some thread with
    `B`. Before the check can be completed in the first thread, another thread swaps
    `A` with `C` and then `C` back again to `A`. The first thread is then rescheduled
    and performs its compare-and-swap of `A` to `B`, none the wiser that `A` is not
    really the `A` it had at the start of the swap. This will cause chunks of the
    queue's linked list to point incorrectly, possibly into the memory that the queue
    does not rightly own. That's bad enough. What could be worse?
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有什么问题？如果你运行`test`套件，你可能会或可能不会遇到一个问题。它们相当不可能，尽管我们很快就会看到一种可靠触发最坏情况的方法。我们实现遇到的第一问题是ABA问题。在比较和交换操作中，指针`A`将被某个线程与`B`交换。在第一个线程完成检查之前，另一个线程将`A`与`C`交换，然后`C`又回到`A`。然后第一个线程被重新调度，并执行其将`A`比较和交换为`B`的操作，而不知道`A`实际上并不是交换开始时的那个`A`。这将导致队列的链表块指向错误，可能指向队列不应拥有的内存。这已经足够糟糕了。还能更糟吗？
- en: 'Let''s cause a use-after-free violation with this structure. Our demonstration
    program is short and lives at `src/bin/queue_spin.rs`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用这个结构引起使用后释放违规。我们的演示程序很短，位于`src/bin/queue_spin.rs`：
- en: '[PRE40]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The program creates four threads, each of which enqueue and dequeue in sequence
    as rapidly as possible with no coordination between them. It''s important to have
    at least two threads, else the queue is used sequentially and the issue does not
    exist:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 程序创建了四个线程，每个线程尽可能快地按顺序入队和出队，它们之间没有任何协调。至少需要有两个线程，否则队列是顺序使用的，问题就不会存在：
- en: '[PRE41]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Ouch. That took no time at all. Let''s have a look at the program in a debugger.
    We''ll use `lldb`, but if you''re using `gdb`, the results will be the same:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ouch。这根本没花什么时间。让我们用调试器看看程序。我们将使用`lldb`，但如果你使用`gdb`，结果将是相同的：
- en: '[PRE42]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Well look at that! And, just to confirm:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，看看这个！只是为了确认：
- en: '[PRE43]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Can we turn up another? Yes:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能再增加一个吗？是的：
- en: '[PRE44]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In the first example, the head is pointing to null, which will happen when
    the queue is empty, but this particular branch is only hit when the queue is not
    empty. What''s going on here? It turns out there''s a nasty race and its down
    to deallocation. Let''s look at `deq` again, this time with line numbers:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，头指针指向null，这将在队列为空时发生，但这个特定的分支只有在队列为空时才会被击中。这里发生了什么？实际上，这里有一个讨厌的竞争，它与释放有关。让我们再次看看`deq`，这次带有行号：
- en: '[PRE45]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Say we have four threads, `A` through `D`. Thread `A` gets to line 78 and is
    stopped. Thread `A` is now in possession of a `head`, a `tail`, and a `next`,
    which point to a sensible linked-list in memory. Now, threads `B`, `C`, and `D`
    each perform multiple `enq` and `deq` operations such that when `A` wakes up the
    linked list pointed to by the head of `A` is long gone. In fact, head itself is
    actually deallocated, but `A` gets lucky and the OS hasn't overwritten its memory
    yet.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有四个线程，`A` 到 `D`。线程 `A` 到达第78行并停止。此时，线程 `A` 拥有一个 `head`、一个 `tail` 和一个 `next`，它们指向内存中的一个合理的链表。现在，线程
    `B`、`C` 和 `D` 各自行使多次 `enq` 和 `deq` 操作，使得当 `A` 唤醒时，`A` 的头指针指向的链表已经不复存在。实际上，头节点本身已经被释放，但
    `A` 很幸运，操作系统还没有覆盖它的内存。
- en: 'Thread `A` wakes up and performs line 78 but next now points off to nonsense
    and the whole thing crashes. Alternatively, say we have two threads, `A` and `B`.
    Thread `A` wakes up and advances through to line 70 and is stopped. Thread `B`
    wakes up and is very fortunate and advances all the way through to line 88, deallocating
    head. The OS is feeling its oats and overwrites the memory that `A` is pointing
    at. Thread `A` then wakes up, fires `(*head).next.load(Ordering::Relaxed)`, and
    subsequently crashes. There are many alternatives here. What''s common between
    them all is deallocation happening while there''s still outstanding references
    to one or more nodes. In fact, Michael and Scott''s paper does mention this as
    a problem, but briefly in a way that''s easy to overlook:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 线程 `A` 唤醒并执行第78行，但接下来却指向了无意义的内容，整个程序崩溃。或者，假设我们有两个线程，`A` 和 `B`。线程 `A` 唤醒并通过到第70行并停止。线程
    `B` 唤醒并且非常幸运，一直前进到第88行，释放头节点。操作系统感觉到了自己的力量，覆盖了 `A` 指向的内存。然后线程 `A` 唤醒，触发 `(*head).next.load(Ordering::Relaxed)`，随后崩溃。这里有很多不同的可能性。它们之间的共同点是，在仍有未解决的对一个或多个节点的引用时发生释放。实际上，Michael
    和 Scott 的论文确实提到了这个问题，但只是简要地提了一下，很容易被忽略：
- en: '"To obtain consistent values of various pointers we rely on sequences of reads
    that re-check earlier values to be sure they haven''t changed. These sequences
    of reads are similar to, but simpler than, the snapshots of Prakash et al. (we
    need to check only one shared variable rather than two). A similar technique can
    be used to prevent the race condition in Stone''s blocking algorithm. We use Treiber''s
    simple and efficient non-blocking stack algorithm to implement a non-blocking
    free list."'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: “为了获得各种指针的一致值，我们依赖于一系列的读取操作，这些操作重新检查早期值以确保它们没有改变。这些读取序列与Prakash等人（我们需要检查一个共享变量而不是两个）的快照类似，但更简单。可以使用类似的技术来防止Stone的阻塞算法中的竞争条件。我们使用Treiber的简单高效的非阻塞栈算法来实现非阻塞的空闲列表。”
- en: The key ideas here are *sequences of reads that re-check earlier values* and
    *free list*. What we've seen by inspection is that it's entirely possible to compare-and-swap
    a value that has changed—the ABA problem—which leaves our implementation pointing
    off into space. Also, immediately deallocating nodes will leave us open to crashes
    even absent a compare-and-swap. What Michael and Scott have done is create a minimal
    kind of memory management; rather than delete nodes, they move them into a *free
    list* to be reused or deleted at a later time. Free lists can be thread-local,
    in which case you avoid expensive synchronization, but it's still kind of tricky
    to be sure your thread-local free list doesn't have the same pointer as another
    thread.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键思想是*重新检查早期值的读取序列*和*空闲列表*。通过检查我们发现，比较并交换一个已更改的值——ABA问题——会导致我们的实现指向空间。此外，立即释放节点将使我们即使在没有比较并交换的情况下也容易崩溃。Michael
    和 Scott 做的是创建一种最小化的内存管理；而不是删除节点，他们把它们移动到*空闲列表*中，以便稍后重用或删除。空闲列表可以是线程本地的，在这种情况下，你可以避免昂贵的同步，但仍然有点难以确保你的线程本地空闲列表不与另一个线程的指针相同。
- en: Options to correct the incorrect queue
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 纠正错误队列的选项
- en: 'What should we do? Well, it turns out there''s lots of options in the literature.
    Hart, McKenney, Brown and Walpole''s 2007 *Performance of Memory Reclamation for
    Lockless Synchronization* discusses four, along with references to their originating
    papers:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该怎么做？实际上，文献中有很多选项。Hart、McKenney、Brown 和 Walpole 在2007年的论文《无锁同步的内存回收性能》中讨论了四种，并附上了它们原始论文的引用：
- en: '**Quiescent-state-based reclamation** (**QSRB**): The application''s signal
    quiescent''periods in which reclamation is allowed'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于静默状态的回收**（**QSRB**）：应用允许的信号静默''时间段进行回收'
- en: '**Epoch-based reclamation** (**EBR**): The applications make use of structures
    that determine when reclamation is allowed by moving memory through epochs'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于时代的回收**（**EBR**）：应用程序通过在时代中移动内存来使用结构，以确定何时允许回收'
- en: '**Hazard-pointer-based reclamation** (**HPBR**): Threads cooperate and signal
    pointers as being hazardous for reclamation'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于危险指针的回收**（**HPBR**）：线程协同并标记指针为回收时的危险状态'
- en: '**Lock-free reference counting (LFRC**): Pointers carry an atomic counter of
    uses alongside them, which threads use to determine safe reclamation periods'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无锁引用计数（LFRC**）：指针携带一个原子计数器，用于跟踪其使用情况，线程使用这个计数器来确定安全的回收周期'
- en: Quiescent state reclamation works by having threads that operate on a shared
    concurrent structure signal that have entered a *quiescent state* for a period
    of time, meaning that for the signaled state the thread does not claim any stake
    over a reference from inside the shared structure. A separate reclamation actor,
    which may be the structure itself or a thread in a quiescent state, searched for
    *grace periods* in which it's safe to reclaim storage, either because all threads
    have gone quiescent or because some relevant subsets have. It's quite tricky and
    involves peppering your code with notifications of entering and exiting quiescent
    states. Hart et al note that this reclamation method was commonly used in the
    Linux kernel at the time of writing, which remains true at the time of *this*
    writing. Interested readers are referred to the available literature on the Linux
    kernel's read-copy update mechanism. The key difficulty of QSRB as a technique
    is that it's not always clear in user-space when a natural quiescent period exists;
    if you signal one incorrectly you're going to get data-races. Context switches
    in an operating system are a natural quiescent period—the relevant contexts won't
    be manipulating anything until they get called back in—but it's less clear to
    me in our queue where such a period would comfortably exist. The benefit of QSRB
    is that it's a very *fast* technique, requiring less thread synchronization than
    any of the other methods below.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 静止状态回收通过让在共享并发结构上操作的线程在一段时间内发出已进入*静止状态*的信号来实现，这意味着对于已标记的状态，线程不会对共享结构内的引用提出任何要求。一个单独的回收操作者，可能是结构本身或处于静止状态的线程，寻找*宽限期*，在这个期限内回收存储是安全的，要么因为所有线程都已静止，要么因为一些相关子集已静止。这相当棘手，需要在代码中添加进入和退出静止状态的通告。Hart等人指出，在撰写本文时，这种方法在Linux内核中很常见，在撰写*本文*时仍然如此。感兴趣的读者可以参考有关Linux内核读-拷贝-更新机制的现有文献。QSRB作为一项技术的主要困难在于，在用户空间中并不总是清楚是否存在自然静止期；如果你错误地发出信号，你可能会遇到数据竞争。操作系统的上下文切换是一个自然静止期——相关的上下文在它们被调用回来之前不会操作任何东西——但在我看来，在我们的队列中，这样一个时期的存在并不那么清晰。QSRB的好处是它是一种非常*快速*的技术，比以下任何其他方法都需要更少的线程同步。
- en: Epoch-based reclamation is similar to QSRB, except that it's application independent.
    That is, the application interacts with storage through an intermediary layer
    and this layer stores enough information to decide when a grace period has come.
    In particular, every thread ticks a flag, indicating that it means to interact
    with shared data, interacts, and then ticks the flag the other direction on its
    way out. The number of flag cycles is referred to as an 'epoch'. Once shared data
    has gone through enough epochs, the thread will attempt to reclaim the storage.
    This method has the benefit of being more automatic than QSRB—the application
    developer uses specially designed intermediary types—but has the downside of requiring
    some synchronization on either side of the data access—those flag ticks. (The
    paper actually introduces a fifth, new epoch-based reclamation, which improves
    on epoch-based replication but does require hints from the application developer.
    In some sense, it is an EBR/QSRB hybrid approach.)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 基于时代的回收与QSRB类似，但它是与应用程序无关的。也就是说，应用程序通过一个中介层与存储交互，而这个层存储了足够的信息来决定何时到来宽限期。特别是，每个线程都会切换一个标志，表示它打算与共享数据交互，交互后，在离开时将标志切换到另一个方向。标志周期数被称为一个“时代”。一旦共享数据通过了足够的时代，线程将尝试回收存储。这种方法的好处是比QSRB更自动——应用程序开发者使用专门设计的中间类型——但缺点是在数据访问的两侧都需要一些同步——那些标志切换。（论文实际上介绍了一种第五种新的基于时代的回收方法，它改进了基于时代的复制，但需要应用程序开发者的提示。在某种程度上，它是一种EBR/QSRB混合方法。）
- en: The hazard pointer reclamation scheme is entirely data structure dependent.
    The idea is that every involved thread will keep a collection of hazard pointers,
    one for each lockless operation it intended to perform. This thread-local collection
    of hazard pointers is mirrored into some reclamation system—perhaps a special-purpose
    garbage collector or plug-in allocator—and this system is able to check the status
    of the hazard pointers. When a thread is intended to perform a lockless operation
    on some shared data, it signals the hazard pointer as protected, disallowing its
    reclamation. When the thread is done, it signals that the shared data is no longer
    protected. Depending on context, the shared data may now be free for reclamation
    or may be subject to later reclamation. Like EBR, HPBR requires access to shared
    data through special structures and has overheads in terms of synchronization.
    Also, there maybe be a higher burden of implementation compared to EBR, but less
    than QSRB. Harder yet, if your data structure passes references to its internal
    storage, that also requires a hazard pointer. The more threads, the more hazard
    pointer lists you require. Memory overheads can get non-trivial real quick. Compared
    to grace period approaches, hazard pointers can be much faster at reclaiming memory,
    due to there being no need to search for a time when a reference is not hazardous.
    It is or it isn't. HPBR has higher overheads per operation and higher memory requirements
    overall, but is potentially better at reclamation. That's a non-trivial detail.
    If QSBR or EBR are run on systems where allocation blocks the CPU, it's possible
    for these methods to *never* find a grace period, eventually exhausting system
    memory.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 危险指针回收方案完全依赖于数据结构。其理念是每个涉及的线程都将保留一个危险指针的集合，每个指针对应于它打算执行的无锁操作。这个线程本地的危险指针集合被映射到某个回收系统中——可能是一个专用垃圾收集器或插件分配器——并且这个系统能够检查危险指针的状态。当一个线程打算对某些共享数据进行无锁操作时，它会将危险指针标记为受保护，不允许其回收。当线程完成操作后，它会标记共享数据不再受保护。根据上下文，共享数据现在可能可以回收，或者可能需要后续回收。就像EBR一样，HPBR需要通过特殊结构访问共享数据，并且在同步方面有开销。此外，与EBR相比，实现可能需要更高的负担，但低于QSRB。更难的是，如果你的数据结构传递对其内部存储的引用，这也需要危险指针。线程越多，你需要更多的危险指针列表。内存开销可能会迅速变得相当大。与宽限期方法相比，由于无需搜索引用不是危险的时间，危险指针在回收内存方面可以更快。HPBR每个操作的开销更高，总体内存需求也更高，但在回收方面可能更有效。这是一个非同小可的细节。如果QSBR或EBR在分配操作阻塞CPU的系统上运行，这些方法可能*永远*找不到宽限期，最终耗尽系统内存。
- en: The last of the bunch is atomic reference counting, akin to standard library's
    `Arc` but for atomics. This method is not especially fast, incurring a cost per
    access to the reference counter as well as the branch on that counter, but it
    is simple and reclamation occurs promptly.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个是原子引用计数，类似于标准库中的`Arc`，但用于原子操作。这种方法并不特别快，每次访问引用计数器以及该计数器的分支都会产生开销，但它很简单，回收发生得很快。
- en: 'Aside from reference counting, each of these methods are tricky to implement.
    But we''re in luck; the crate ecosystem has an implementation of epoch-based reclamation
    *and* hazard pointer reclamation. The relevant libraries are crossbeam ([https://crates.io/crates/crossbeam](https://crates.io/crates/crossbeam))
    and conc ([https://crates.io/crates/conc](https://crates.io/crates/conc)). We''ll
    discuss them in great detail in the next chapter. Crossbeam aims to be the `libcds`
    ([https://github.com/khizmax/libcds](https://github.com/khizmax/libcds)) of Rust
    and has a Michael and Scott queue already implemented for us. Let''s give it a
    spin. The relevant file is `src/bin/crossbeam_queue_spin.rs`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 除了引用计数之外，这些方法中的每一个都很难实现。但我们很幸运；crate生态系统提供了一个基于纪元的回收实现和危险指针回收。相关的库是crossbeam
    ([https://crates.io/crates/crossbeam](https://crates.io/crates/crossbeam)) 和 conc
    ([https://crates.io/crates/conc](https://crates.io/crates/conc))。我们将在下一章中详细讨论它们。Crossbeam旨在成为Rust的`libcds`
    ([https://github.com/khizmax/libcds](https://github.com/khizmax/libcds))，并且已经为我们实现了一个Michael和Scott队列。让我们试一试。相关文件是`src/bin/crossbeam_queue_spin.rs`：
- en: '[PRE46]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This is fairly similar to `queue_spin`, save that `enq` is now push and `deq`
    is `pop`. It's worth noting as well that `MsQueue::pop` is blocking. The mechanism
    for that is very neat; we'll discuss that in the next chapter.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这与`queue_spin`相当相似，只是`enq`现在是推送操作，而`deq`是`pop`操作。同样值得注意的是，`MsQueue::pop`是阻塞的。这种机制的实现非常巧妙；我们将在下一章中讨论这一点。
- en: Semaphore
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信号量
- en: 'We discussed semaphores in passing in [Chapter 5](e2de1ba7-c291-494e-82da-37fee7323c1d.xhtml),
     *Locks – Mutex, Condvar, Barriers, and RWLock*, especially with regard to the
    concurrency puzzles from *The Little Semaphore*. It''s now, as promised, time
    to implement a semaphore. What *exactly* is a semaphore? Similar to our analysis
    of mutex as an *atomic object*, let''s consider it:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第5章](e2de1ba7-c291-494e-82da-37fee7323c1d.xhtml)中简要讨论了信号量，*锁 – Mutex, Condvar,
    Barriers, 和 RWLock*，特别是关于来自*小信号量*的并发难题。现在，正如承诺的那样，是时候实现一个信号量了。信号量究竟是什么？类似于我们将互斥锁分析为*原子对象*，让我们考虑它：
- en: Semaphore supports two operations, `wait` and `signal`.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号量支持两种操作，`wait`和`signal`。
- en: A semaphore has an isize `value` that is used to track the available resource
    capacity of the semaphore. This value is only manipulable by `wait` and `signal`.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号量有一个`isize`类型的`value`，用于跟踪信号量的可用资源容量。这个值只能由`wait`和`signal`操作修改。
- en: The operation `wait` decrements `value`. If the value is less than zero, 'wait'
    blocks the calling thread until such time as a value becomes available. If the
    value is not less than zero, the thread does not block.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作`wait`会减少`value`的值。如果值小于零，则`wait`会阻塞调用线程，直到有值可用。如果值不小于零，则线程不会阻塞。
- en: The operation `signal` increments `value`. After increment, if the value is
    less than or equal to zero then there are one or more waiting threads. One of
    these threads is woken up. If the value is greater than zero after increment,
    there are no waiting threads.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作`signal`会增加`value`的值。在增加之后，如果值小于或等于零，则有一个或多个等待线程。其中一个线程会被唤醒。如果增加后的值大于零，则没有等待线程。
- en: All loads and stores that occur after and before a wait in program order must
    not be moved prior to or after the wait.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在程序顺序中发生在等待之前和之后的所有加载和存储操作不得移动到等待之前或之后。
- en: All loads and stores that occur before an signal in program order must not be
    moved to after the signal.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在程序顺序中发生在信号之前的所有加载和存储操作不得移动到信号之后。
- en: If this seems familiar, there's a good reason for that. Viewed a certain way,
    a mutex is a semaphore with only one resource available; a locking mutex maps
    to wait and signaling maps to unwait. We have specified the behaviour of loads
    and stores around the waiting and signaling of the semaphore to avoid the same
    deadlock behaviour identified previously in the mutex analysis.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这看起来很熟悉，那是有很好的原因的。从某种角度看，互斥锁是一个只有一个可用资源的信号量；锁定互斥锁映射到`wait`，而信号映射到`unwait`。我们已指定了在信号量等待和信号过程中的加载和存储行为，以避免在互斥锁分析中先前确定的相同死锁行为。
- en: There are some subtleties not captured in the preceding breakdown that don't
    affect the analysis of the semaphore but do affect the programming model. We'll
    be building a semaphore with fixed resources. That is, when the semaphore is created,
    the programmer is responsible for setting the maximum total resources available
    in the semaphore *and* ensuring that the semaphore starts with these resources
    available. Some semaphore implementations allow for the resource capacity to shift
    over time. These are commonly called *counting semaphores*. Our variant is called
    a *bounded semaphore;* the subvariant of this sort with only a single resource
    is called a *binary semaphore*. Behavior around the signaling of waiting threads
    may vary. We will signal our threads on a first-come, first-serve basis.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的分解没有捕捉到的一些细微之处不会影响信号量的分析，但会影响编程模型。我们将构建一个具有固定资源的信号量。也就是说，当创建信号量时，程序员负责设置信号量中可用的最大总资源，并确保信号量以这些资源可用开始。某些信号量实现允许资源容量随时间变化。这些通常被称为*计数信号量*。我们的变体被称为*有界信号量*；这种类型中只有一个资源的子变体称为*二进制信号量*。等待线程的信号行为可能有所不同。我们将根据先来先服务的原则唤醒线程。
- en: 'Let''s dig in. Our semaphore implementation is in `src/semaphore.rs` and it''s
    very short:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨。我们的信号量实现位于`src/semaphore.rs`中，它非常简短：
- en: '[PRE47]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Well! Crossbeam''s `MsQueue` has the correct ordering semantics when `MsQueue::push`
    and then `MsQueue::pop` are done in that sequence by the same thread, where pop
    has the added bonus of blocking until such a time where the queue is empty. So,
    our semaphore is an `MsQueue` filled with the capacity total `()`. The operation
    `wait` decreases the `value`—the total number of `()` in the queue—and does so
    with `Acquire`/`Release` ordering. The operation signal increases the *value* of
    the semaphore by pushing an additional `()` onto the queue with `Release` semantics.
    It is possible for a programming error to result in `wait`/`signal` invocations
    that are not one-to-one, and we can resolve this with the same `Guard` approach
    taken by Mutex and `SwapMutex`. The underlying queue linearizes `Guard`—see the
    next chapter for that discussion—and so our semaphore does so also. Let''s try
    this thing out. We''ve got a program in-project to demonstrate the use of `Semaphore`,
    `src/bin/status_demo.rs`:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！Crossbeam的`MsQueue`在`MsQueue::push`和`MsQueue::pop`按该顺序由同一线程完成时具有正确的排序语义，其中`pop`有额外的优势，即阻塞直到队列变为空。因此，我们的信号量是一个容量总计为`()`的`MsQueue`。`wait`操作通过`Acquire`/`Release`排序减少`value`（队列中的`()`总数）。`signal`操作通过将额外的`()`推送到队列中并带有`Release`语义来增加信号量的`value`。编程错误可能导致`wait`/`signal`调用不是一对一的，我们可以通过Mutex和`SwapMutex`采取的相同`Guard`方法来解决此问题。底层队列线性化`Guard`——请参阅下一章以了解该讨论——因此我们的信号量也是如此。让我们试试这个。我们有一个项目内的程序来演示`Semaphore`的使用，`src/bin/status_demo.rs`：
- en: '[PRE48]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We make `THRS` total worker threads, whose responsibilities are to wait on
    the semaphore, flip their `STATUS` to true, add one to their `COUNT`, and flip
    their `STATUS` to false before signaling the semaphore. Mutable static arrays
    is kind of a goofy setup for any program, but it''s a neat trick and causes no
    harm here, except for interacting oddly with the optimizer. If you compile this
    program under release mode, you may find that the optimizer determines worker
    to be a no-op. The `main` function creates a semaphore with a capacity of two,
    carefully offsets the workers, and then spins, forever printing out the contents
    of `STATUS` and `COUNT`. A run on my x86 test article looks like the following:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置`THRS`为总工作线程数，其职责是等待信号量，将它们的`STATUS`设置为true，将`COUNT`加一，然后在发出信号量之前将`STATUS`设置为false。可变静态数组对于任何程序来说都有些愚蠢的设置，但这是一个巧妙的技巧，并且在这里不会造成伤害，除了与优化器交互异常。如果你以发布模式编译此程序，你可能会发现优化器确定工作线程是一个无操作。`main`函数创建了一个容量为二的信号量，仔细地偏移了工作线程，然后无限期地打印出`STATUS`和`COUNT`的内容。在我的x86测试文章上的运行结果如下：
- en: '[PRE49]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'And from my ARMv7 machine:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以及从我的ARMv7机器上：
- en: '[PRE50]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Binary semaphore, or, a less wasteful mutex
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二进制信号量，或者，一个更节省的互斥锁
- en: 'How does our semaphore stack up against standard library''s Mutex? How about
    our spinlock `Mutex`? Apply this `diff` to `status_demo`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的信号量与标准库的Mutex相比如何？我们的自旋锁`Mutex`呢？将此`diff`应用到`status_demo`：
- en: '[PRE51]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You''ll create a mutex variant of the semaphore status demo. The numbers for
    that on my x86 test article look like this:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你将创建一个信号量状态演示的互斥锁变体。在我的x86测试文章上的数字如下：
- en: '[PRE52]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The numbers at capacity two were 170,000 per second, so there''s quite a dip.
    Let''s compare that against standard library mutex first. The adaptation is in
    `src/bin/mutex_status_demo.rs`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在容量为二的情况下，每秒是170,000次，所以有一个相当大的下降。让我们先与标准库互斥锁进行比较。这个适配在`src/bin/mutex_status_demo.rs`中：
- en: '[PRE53]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Straightforward, yeah? The numbers from that on the same x86 test article are
    as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 很直接，对吧？在相同的x86测试文章上的数字如下：
- en: '[PRE54]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let''s be a little generous and say that the standard library mutex is clocking
    in at 2,000,000 per second, while our binary semaphore is getting 170,000 per
    second. How about the spinlock? At this point, I will avoid the source listing.
    Just be sure to import `SwapMutex` and adjust it from the standard library `Mutex`
    accordingly. The numbers for that are as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微慷慨一点，说标准库互斥锁每秒达到2,000,000次，而我们的二进制信号量每秒达到170,000次。那么自旋锁呢？在这个时候，我将避免列出源代码。只需确保导入`SwapMutex`并将其从标准库的`Mutex`相应地调整即可。这些数字如下：
- en: '[PRE55]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Which, you know what, makes sense. The spinlock is doing the least amount of
    work and every thread is burning up CPU time to be right there when it''s time
    to enter their critical section. Here is a summary of our results:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，这很有道理。自旋锁所做的最少工作量，每个线程都在燃烧CPU时间，以便在需要进入它们的临界区时立即到达。以下是我们的结果总结：
- en: 'Spinlock mutex: 11,000,000/sec'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自旋锁互斥锁：每秒11,000,000次
- en: 'Standard library mutex: 2,000,000/sec'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准库互斥锁：每秒2,000,000次
- en: 'Binary semaphore: 170,000/sec'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制信号量：每秒170,000次
- en: The reader is warmly encouraged to investigate each of these implementations
    in Linux perf. The key thing to understand, from these results, is not that any
    of these implementations is better or worse than the other. Rather, that they
    are suitable for different purposes.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励读者在 Linux perf 中调查这些实现中的每一个。从这些结果中理解的关键不是这些实现中有哪一个比另一个更好或更差。而是它们适用于不同的目的。
- en: We could, for instance, use techniques from the next chapter to reduce the CPU
    consumption of the spinlock mutex. This would slow it down some, likely bringing
    it within the range of standard library Mutex. In this case, use the standard
    library Mutex.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用下一章中的技术来减少自旋锁互斥锁的CPU消耗。这可能会使其速度减慢一些，可能使其接近标准库Mutex的范围。在这种情况下，使用标准库Mutex。
- en: Atomics programming is not a thing to take lightly. It's difficult to get right,
    and an implementation that may *seem* right might well not obey the causal properties
    of the memory model. Furthermore, just because a thing is lock-free does not mean
    that it is *faster* to execute.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 原子编程不是一件可以轻视的事情。正确实现它很困难，一个看似正确的实现可能并不遵守内存模型的因果属性。此外，仅仅因为一个事物是无锁的，并不意味着它在执行时会更*快*。
- en: Writing software is about recognizing and adapting to trade-offs. Atomics demonstrates
    that in abundance.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 编写软件是关于识别和适应权衡。原子编程在这方面表现得尤为明显。
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the atomic primitives available on modern CPU
    architectures and their reflection in Rust. We built higher-level synchronization
    primitives of the sort discussed in [Chapter 4](5a332d94-37e4-4748-8920-1679b07e2880.xhtml),
    *Sync and Send – the Foundation of Rust Concurrency*, and [Chapter 5](e2de1ba7-c291-494e-82da-37fee7323c1d.xhtml),
    *Locks – Mutex, Condvar, Barriers, and RWLock*, as well as our own semaphore,
    which does not exist in Rust. The semaphore implementation could be improved,
    depending on your needs, and I warmly encourage the readers to give that a shot.
    We also ran into a common problem of atomic programming, memory reclamation, which
    we discussed in terms of a Michael Scott queue. We'll discuss approaches to this
    problem in-depth in the next chapter.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了现代CPU架构上可用的原子原语及其在Rust中的反映。我们构建了类似于[第4章](5a332d94-37e4-4748-8920-1679b07e2880.xhtml)中讨论的*同步和发送
    – Rust并发的基石*、[第5章](e2de1ba7-c291-494e-82da-37fee7323c1d.xhtml)中讨论的*锁 – Mutex、Condvar、屏障和RWLock*以及我们自己的信号量（在Rust中不存在）。根据您的需求，信号量的实现可以改进，我鼓励读者尝试一下。我们还遇到了原子编程的常见问题——内存回收，我们用迈克尔·斯科特队列来讨论这个问题。我们将在下一章深入讨论解决这个问题的方法。
- en: Further reading
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Axioms for Concurrent Objects*, Maurice Herlihy and Jeannette Wing. This paper
    by Herlihy and Wing introduced the formal definition of linearizability and the
    formal analysis framework. Subsequent work has expanded or simplified on this
    paper, but the reader is warmly encouraged to digest the first half of the paper,
    at least.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*并发对象的公理*，莫里斯·赫尔利希和珍妮特·温。赫尔利希和温的这篇论文介绍了线性化的形式定义和形式分析框架。后续工作在此基础上进行了扩展或简化，但鼓励读者至少消化这篇论文的前半部分。'
- en: '*Distributed Algorithms*, Nancy Lynch. To my knowledge, Lynch''s work was the
    first general overview of distributed algorithms in textbook form. It is incredibly
    accessible and Chapter 13 of Lynch''s book is especially relevant to this discussion.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分布式算法*，南希·林奇。据我所知，林奇的这项工作是首次以教科书形式对分布式算法进行的一般概述。它非常易于理解，林奇的书中的第13章特别与这次讨论相关。'
- en: '*In C++, are acquire-release memory order semantics transitive?*, available
    at [https://softwareengineering.stackexchange.com/questions/253537/in-c-are-acquire-release-memory-order-semantics-transitive](https://softwareengineering.stackexchange.com/questions/253537/in-c-are-acquire-release-memory-order-semantics-transitive). The
    consequences of memory orderings are not always clear. This question on `StackExchange`,
    which influenced the writing of this chapter, is about the consequence of splitting `Acquire`
    and `Release `across threads. The reader is encouraged to ponder the question
    before reading the answer.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在C++中，acquire-release内存顺序语义是传递的吗？*，可在[https://softwareengineering.stackexchange.com/questions/253537/in-c-are-acquire-release-memory-order-semantics-transitive](https://softwareengineering.stackexchange.com/questions/253537/in-c-are-acquire-release-memory-order-semantics-transitive)找到。内存顺序的影响并不总是清晰的。这个问题在`StackExchange`上，它影响了本章的写作，是关于在多线程中分割`Acquire`和`Release`的后果。鼓励读者在阅读答案之前思考这个问题。'
- en: '*std::memory_order, CppReference*, available at [http://en.cppreference.com/w/cpp/atomic/memory_order](http://en.cppreference.com/w/cpp/atomic/memory_order).
    Rust''s memory model is LLVM''s, which is, in turn, influenced by the C++ memory
    model. This discussion of memory ordering is particularly excellent, as it has
    example code and a less language-lawyer approach to explaining orders.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*std::memory_order, CppReference*，可在[http://en.cppreference.com/w/cpp/atomic/memory_order](http://en.cppreference.com/w/cpp/atomic/memory_order)找到。Rust的内存模型是LLVM的，而LLVM的内存模型又受到C++内存模型的影响。这篇关于内存排序的讨论特别出色，因为它有示例代码，并且采用了更少语言律师风格的方法来解释排序。'
- en: '*Peterson''s lock with C++0x atomics*, available at [https://www.justsoftwaresolutions.co.uk/threading/petersons_lock_with_C++0x_atomics.html](https://www.justsoftwaresolutions.co.uk/threading/petersons_lock_with_C++0x_atomics.html). This
    post discusses Bartosz Milewski''s implementation of Peterson''s algorithm for
    Mutex, demonstrates how and why it is incorrect, and describes a functional alternative.
    Milewski is a well-known C++ expert. It just goes to show, atomic programming
    is difficult and easy to get subtly wrong.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用C++0x原子操作实现Peterson锁*，可在[https://www.justsoftwaresolutions.co.uk/threading/petersons_lock_with_C++0x_atomics.html](https://www.justsoftwaresolutions.co.uk/threading/petersons_lock_with_C++0x_atomics.html)找到。这篇文章讨论了Bartosz
    Milewski对Peterson互斥锁算法的实现，演示了为什么它是错误的，并描述了一个功能性的替代方案。Milewski是一位知名的C++专家。这正好说明了原子编程既困难又容易犯细微的错误。'
- en: '*Algorithms for Mutual Exclusion*, Michael Raynal. This book by Raynal was
    written in 1986, well before the x86_64 architecture we''ve discussed was introduced
    and a mere year after ARMv1 was introduced. Raynal''s book remains useful, both
    as a historical overview of mutual exclusion algorithms and for environments where
    synchronization primitives are not available, such as on file systems.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*互斥锁算法*，迈克尔·雷纳尔。雷纳尔的这本书是在1986年写的，远在我们讨论的x86_64架构之前，并且是在ARMv1发布后的第二年。雷纳尔的书仍然很有用，既作为互斥锁算法的历史概述，也适用于同步原语不可用的环境，例如在文件系统中。'
- en: '*Review of many Mutex implementations*, available at [http://cbloomrants.blogspot.com/2011/07/07-15-11-review-of-many-mutex.html](http://cbloomrants.blogspot.com/2011/07/07-15-11-review-of-many-mutex.html).
    As it says on the tin, this post is a review of many mutex implementations that
    are given a more modern context than Raynal''s book. Some explanations rely on
    Microsoft Windows features, which may be welcome to the reader as this book is
    heavily invested in a Unix-like environment.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多种互斥锁实现综述*，可在[http://cbloomrants.blogspot.com/2011/07/07-15-11-review-of-many-mutex.html](http://cbloomrants.blogspot.com/2011/07/07-15-11-review-of-many-mutex.html)找到。正如标题所示，这篇文章是对许多互斥锁实现的综述，这些实现比Raynal的书提供了更现代的背景。一些解释依赖于微软Windows的功能，对于这本书主要投资于类Unix环境的读者来说可能很受欢迎。'
- en: '*Encheapening Cernan Metrics*, available at [http://blog.troutwine.us/2017/08/31/encheapening-cernan-internal-metrics/](http://blog.troutwine.us/2017/08/31/encheapening-cernan-internal-metrics/).
    In this chapter we have discussed atomics largely in terms of synchronization.
    There are many other use cases. This post discusses the application of atomics
    to providing cheap self-telemetry to a complicated software project.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*降低Cernan度量成本*，可在[http://blog.troutwine.us/2017/08/31/encheapening-cernan-internal-metrics/](http://blog.troutwine.us/2017/08/31/encheapening-cernan-internal-metrics/)找到。在这一章中，我们主要从同步的角度讨论了原子操作。还有许多其他用例。这篇文章讨论了将原子操作应用于为复杂的软件项目提供低成本的自监控。'
- en: '*Roll Your Own Lightweight Mutex*, available at [http://preshing.com/20120226/roll-your-own-lightweight-mutex/](http://preshing.com/20120226/roll-your-own-lightweight-mutex/).
    Preshing On Programming has a run of excellent atomics material, focused on C++
    and Microsoft environments. This particular post is to do with implementing mutexes—a
    topic dear to this chapter—and has an excellent follow-up conversation in the
    comments. The post discusses a variant of semaphores called benaphores.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自己动手实现轻量级互斥锁*，可在[http://preshing.com/20120226/roll-your-own-lightweight-mutex/](http://preshing.com/20120226/roll-your-own-lightweight-mutex/)找到。Preshing
    On Programming网站有一系列优秀的原子操作材料，专注于C++和微软环境。这篇特定的文章是关于实现互斥锁的，这个主题与本章内容紧密相关，并在评论中有很好的后续讨论。文章讨论了信号量的一种变体，称为benaphores。'
- en: '*Simple, Fast, and Practical Non-Blocking and Blocking Concurrent Queue Algorithms*,
    Maged Michael and Michael Scott. This paper introduces the Michael and Scott Queue,
    discussed in this chapter, and that''s what it''s best known for. You may also
    recognize their queue with two locks from the previous chapter, adapted through
    the Erlang VM, of course.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*简单、快速、实用的非阻塞和阻塞并发队列算法*，作者：Maged Michael 和 Michael Scott。本文介绍了本章讨论的 Michael
    和 Scott 队列，这也是它最著名的部分。你也许会从上一章中认出他们的带有两个锁的队列，当然是通过 Erlang 虚拟机进行适配的。'
- en: '*Lock-Free Data Structures. The Evolution of a Stack*, available at [https://kukuruku.co/post/lock-free-data-structures-the-evolution-of-a-stack/](https://kukuruku.co/post/lock-free-data-structures-the-evolution-of-a-stack/).
    This post discusses the `libcds` implementation of Trieber stacks, as well as
    making reference to other areas of the literature. Readers will note that previous
    *Further reading* have introduced some of the same literature; it''s always good
    to seek alternate takes. Readers are especially encouraged to investigate the
    Hendler, Shavit, and Yerushalmi paper referenced in the post.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无锁数据结构：栈的演变*，可在[https://kukuruku.co/post/lock-free-data-structures-the-evolution-of-a-stack/](https://kukuruku.co/post/lock-free-data-structures-the-evolution-of-a-stack/)找到。这篇帖子讨论了
    `libcds` 对 Trieber 栈的实现，并参考了其他文献领域。读者会注意到之前的 *进一步阅读* 中介绍了一些相同的文献；寻求不同的观点总是好的。特别鼓励读者调查帖子中引用的
    Hendler、Shavit 和 Yerushalmi 的论文。'

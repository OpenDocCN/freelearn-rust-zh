<html><head></head><body>
        

                            
                    <h1 class="header-title">Performance, Debugging, and Metaprogramming</h1>
                
            
            
                
<p class="mce-root">Writing fast efficient code can be something to be proud of. It also might be a waste of your employer's resources. In the performance section, we will explore how to tell the difference between the two and give best-practices, processes, and guidelines to keep your application slim.</p>
<p>In the debugging section, we offer tips to help find and resolve bugs faster. We also introduce the concept of defensive coding, which describes techniques and habits to prevent or isolate potential issues.</p>
<p>In the metaprogramming section, we explain macros and other features that are similar to macros. Rust has a fairly sophisticated metaprogramming system that allows the user or libraries to extend the language with automatic code generation or custom syntax forms.</p>
<p>In this chapter, we will learn the following:</p>
<ul>
<li>Recognizing and applying good performant code practices</li>
<li>Diagnosing and improving performance bottlenecks</li>
<li>Recognizing and applying good defensive coding practices</li>
<li>Diagnosing and resolving software bugs</li>
<li>Recognizing and applying metaprogramming techniques</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>A recent version of Rust is necessary to run the examples provided:</p>
<p><a href="https://www.rust-lang.org/en-US/install.html">https://www.rust-lang.org/en-US/install.html</a></p>
<p>This chapter's code is available on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Hands-On-Functional-Programming-in-RUST">https://github.com/PacktPublishing/Hands-On-Functional-Programming-in-RUST</a></p>
<p>Specific installation and build instructions are also included in each chapter's <kbd>README.md</kbd> file.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Writing faster code</h1>
                
            
            
                
<p>Premature optimization is the root of all evil </p>
<p>– Donald Knuth</p>
<p>A good software design tends to create faster programs, while a bad software design tends to create slower programs. If you find yourself asking, "W<em>hy is my program slow?, then first ask yourself, Is my program disorderly?</em>"</p>
<p>In this section, we describe some performance tips. These are generally good habits when programming in Rust that will coincidentally lead to improved performance. If your program is slow, then first check to see whether you are violating one of these principles.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Compiling with release mode</h1>
                
            
            
                
<p>This is a really simple suggestion that you should know about if you are at all concerned about performance.</p>
<ul>
<li>Rust normally compiles in debug mode, which is slow:</li>
</ul>
<pre class="p1" style="padding-left: 60px"><strong>cargo build</strong></pre>
<ul>
<li>Rust optionally compiles in release mode, which is fast:</li>
</ul>
<pre class="p1" style="padding-left: 60px"><strong>cargo build --release</strong></pre>
<ul>
<li>Here is a comparison using debug mode for a toy program:</li>
</ul>
<pre class="p1" style="padding-left: 60px"><strong>$ time performance_release_mode<br/>real 0m13.424s<br/>user 0m13.406s<br/>sys 0m0.010s</strong></pre>
<ul>
<li>The following is the release mode:</li>
</ul>
<pre class="p1" style="padding-left: 60px"><strong>$ time ./performance_release_mode<br/>real 0m0.316s<br/>user 0m0.309s<br/>sys 0m0.005s</strong></pre>
<p>Release mode is 98% more efficient with regard to CPU usage for this example.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Doing less work</h1>
                
            
            
                
<p>Faster programs do less. All optimization is a process of searching for work that doesn't need to be done, and then not doing it.</p>
<p>Similarly, the smallest programs fewer resources less. All space optimization is a process of searching for resources that don't need to be used, and then not using them.</p>
<p>For example, don't collect an iterator when you don't need the result, consider the following example:</p>
<pre class="p1" style="padding-left: 30px">extern crate flame;<br/>use std::fs::File;<br/><br/>fn main() {<br/>   let v: Vec&lt;u64&gt; = vec![2; 1000000];<br/><br/>   flame::start("Iterator .collect");<br/>   let mut _z = vec![];<br/>   for _ in 0..1000 {<br/>      _z = v.iter().map(|x| x*x).collect::&lt;Vec&lt;u64&gt;&gt;();<br/>   }<br/>   flame::end("Iterator .collect");<br/><br/>   flame::start("Iterator iterate");<br/>   for _ in 0..1000 {<br/>      v.iter().map(|x| x * x).for_each(drop);<br/>   }<br/>   flame::end("Iterator iterate");<br/><br/>   flame::dump_html(&amp;mut File::create("flame-graph.html").unwrap()).unwrap();<br/>}</pre>
<p>Needlessly collecting the result of the iterator makes the code 27% slower compared to code that just drops the result.</p>
<p>Memory allocation is similar. Well-designed code preferring pure functions and avoiding side-effects will tend to minimize memory usage. In contrast, messy code can lead to old data hanging around. Rust memory safety does not extend to preventing memory leaks. Leaks are considered safe code:</p>
<pre class="p1">use std::mem::forget;<br/><br/>fn main() {<br/>   for _ in 0..10000 {<br/>      let mut a = vec![2; 10000000];<br/>      a[2] = 2;<br/>      forget(a);<br/>   }<br/>}<br/></pre>
<p>The <kbd>forget</kbd> function is seldom used. Similarly, memory leaks are permitted but sufficiently discouraged that they are somewhat uncommon. Rust memory management tends to be such that by the time you cause a memory leak you are probably waist-deep in other poor design decisions.</p>
<p>However, unsused memory is not uncommon. If you don't keep track of what variables you are actively using, then old variables will likely remain in scope. This is not the typical definition of a memory leak; however, unused data is a similar waste of resources.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Optimizing the code that needs it – profiling</h1>
                
            
            
                
<p>Don't optimize code that doesn't need to be optimized. It's a waste of your time and probably poor software engineering. Save yourself the trouble and identify performance problems accurately before attempting to optimize the program.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">For a code rarely executed, performance is not affected</h1>
                
            
            
                
<p>It is very common that you will initialize some resource and use it multiple times. Optimizing <kbd>initialization</kbd> of resources may be misdirected. You should consider focusing on improving the work efficiency. This is done as follows:</p>
<pre class="p1" style="padding-left: 30px">use std::{thread,time};<br/><br/>fn initialization() {<br/>   let t = time::Duration::from_millis(15000);<br/>   thread::sleep(t);<br/>}<br/><br/>fn work() {<br/>   let t = time::Duration::from_millis(15000);<br/>   loop {<br/>      thread::sleep(t);<br/>      println!("Work.");<br/>   }<br/>}<br/><br/>fn main() {<br/>   initialization();<br/>   println!("Done initializing, start work.");<br/>   work();<br/>}</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Multiples of small numbers are also small numbers</h1>
                
            
            
                
<p>The reverse may also be true. Sometimes the low frequency of <kbd>work</kbd> is overwhelmed by frequent and expensive <kbd>initialization</kbd>. Knowing which problem you have will let you know where to start looking to improve:</p>
<pre class="p1" style="padding-left: 30px">use std::{thread,time};<br/><br/>fn initialization() -&gt; Vec&lt;i32&gt; {<br/>   let t = time::Duration::from_millis(15000);<br/>   thread::sleep(t);<br/>   println!("Initialize data.");<br/>   vec![1, 2, 3];<br/>}<br/><br/>fn work(x: i32) -&gt; i32 {<br/>   let t = time::Duration::from_millis(150);<br/>   thread::sleep(t);<br/>   println!("Work.");<br/>   x * x<br/>}<br/><br/>fn main() {<br/>   for _ in 0..10 {<br/>      let data = initialization();<br/>      data.iter().map(|x| work(*x)).for_each(drop);<br/>   }<br/>}</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Measuring first, to optimize it</h1>
                
            
            
                
<p>There are a lot of options for profiling. Here are some that we recommend.</p>
<p>The <kbd>flame</kbd> crate is one option to manually profile an application. Here we create the nested procedures <kbd>a</kbd>, <kbd>b</kbd>, and <kbd>c</kbd>. Each function creates a profiling context corresponding do that method. After running the profiler we will see proportionally how much time was spent for each call to each function.</p>
<p>Starting with function <kbd>a</kbd>, this procedure creates a new profiling context, sleeps for one second, then calls <kbd>b</kbd> three times:</p>
<pre class="p1">extern crate flame;<br/>use std::fs::File;<br/>use std::{thread,time};<br/><br/>fn a() {<br/>   flame::start("fn a");<br/>   let t = time::Duration::from_millis(1000);<br/>   thread::sleep(t);<br/>   b();<br/>   b();<br/>   b();<br/>   flame::end("fn a");<br/>}</pre>
<p>Function <kbd>b</kbd> is nearly identical to <kbd>a</kbd>, and further calls into function <kbd>c</kbd>:</p>
<pre class="p1">fn b() {<br/>   flame::start("fn b");<br/>   let t = time::Duration::from_millis(1000);<br/>   thread::sleep(t);<br/>   c();<br/>   c();<br/>   c();<br/>   flame::end("fn b");<br/>}</pre>
<p>Function <kbd>c</kbd> profiles itself and sleeps, but does not call any further nested function:</p>
<pre class="p1">fn c() {<br/>   flame::start("fn c");<br/>   let t = time::Duration::from_millis(1000);<br/>   thread::sleep(t);<br/>   flame::end("fn c");<br/>}</pre>
<p>The <kbd>main</kbd> entrypoint sets up the flame graph library and calls a three times, then saves the flamegraph to a file:</p>
<pre class="p1">fn main() {<br/>   flame::start("fn main");<br/>   let t = time::Duration::from_millis(1000);<br/>   thread::sleep(t);<br/>   a();<br/>   a();<br/>   a();<br/>   flame::end("fn main");<br/>   flame::dump_html(&amp;mut File::create("flame-graph.html").unwrap()).unwrap();<br/>}</pre>
<p>After running this program, the <kbd>flame-graph.html</kbd> file will contain a visualization of what program sections took what percentage of resources. The <kbd>flame</kbd> crate is easy to install, requires some manual code manipulation, but produces a cool-looking graph.</p>
<p><kbd>cargo profiler</kbd> is a tool that extends <kbd>cargo</kbd> to do performance profiling without any code changes. Here is a random program that we will profile:</p>
<pre class="p1">fn a(n: u64) -&gt; u64 {<br/>   if n&gt;0 {<br/>      b(n);<br/>      b(n);<br/>   }<br/>   n * n<br/>}<br/><br/>fn b(n: u64) -&gt; u64 {<br/>   c(n);<br/>   c(n);<br/>   n + 2 / 3<br/>}<br/><br/>fn c(n: u64) -&gt; u64 {<br/>   a(n-1);<br/>   a(n-1);<br/>   vec![1, 2, 3].into_iter().map(|x| x+2).sum()<br/>}<br/><br/>fn main() {<br/>   a(6);<br/>}</pre>
<p>To profile the application we run the following command:</p>
<pre class="p1"><strong>$ cargo profiler callgrind --bin ./target/debug/performance_profiling4 -n 10</strong><br/></pre>
<p>This will run the program and collect information regarding which functions were most used. This profiler also has another option to profile memory usage. The output will look like the following:</p>
<pre class="p1"><strong>Profiling performance_profiling4 with callgrind...<br/><br/>Total Instructions...344,529,557<br/><br/>27,262,872 (7.9%) ???:core::iter::iterator::Iterator<br/>----------------------------------------------------------<br/>22,319,604 (6.5%) ???:&lt;alloc::vec<br/>----------------------------------------------------------<br/>16,627,356 (4.8%) ???:&lt;core::iter<br/>----------------------------------------------------------<br/>13,182,048 (3.8%) ???:&lt;alloc::vec<br/>----------------------------------------------------------<br/>10,785,312 (3.1%) ???:core::iter::iterator::Iterator::fold<br/>----------------------------------------------------------<br/>10,485,720 (3.0%) ???:core::mem<br/>----------------------------------------------------------<br/>8,088,984 (2.3%) ???:alloc::slice::hack<br/>----------------------------------------------------------<br/>7,639,596 (2.2%) ???:core::ptr<br/>----------------------------------------------------------<br/>7,190,208 (2.1%) ???:core::ptr<br/>----------------------------------------------------------<br/>7,190,016 (2.1%) ???:performance_profiling4</strong></pre>
<p>This clearly shows us that the most time is spent in iterator and vector creation. Running this command may make the program execute much more slowly than normal, but it also saves writing any code before profiling.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Putting the fridge next to the computer</h1>
                
            
            
                
<p>If you take a snack break while coding, then it would be convenient to have a fridge and microwave next to the computer. If you travel to the kitchen for a snack, then it will take a little longer to satisfy your appetite. If your kitchen is empty and you need to make a grocery run, then the break is even further extended. If your grocery store is empty and you need to drive to a farm to harvest vegetables, then your work environment is clearly not designed for snacking purposes.</p>
<p>This strange analogy illustrates the necessary trade-off between time and space. This relation is not quite a physical law for our purposes, but almost. The rule is that traveling, or communicating, over longer distances is directly proportional to time spent. More distance (d) in one direction also means an increase in available space of quadratic (d<sup>2</sup>) or cubic (d<sup>3</sup>) scale. In other words building the fridge farther away provides more space for a larger fridge.</p>
<p>Bringing this story back to a technical context, here are some latency numbers that every programmer should know (~2012: <a href="https://gist.github.com/jboner/2841832">https://gist.github.com/jboner/2841832</a>):</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 585px"><strong>Request</strong></td>
<td style="width: 146px"><strong>Time</strong></td>
</tr>
<tr>
<td style="width: 585px">L1 cache reference</td>
<td style="width: 146px">0.5 ns</td>
</tr>
<tr>
<td style="width: 585px">Branch mispredict</td>
<td style="width: 146px">5 ns</td>
</tr>
<tr>
<td style="width: 585px">L2 cache reference</td>
<td style="width: 146px">7 ns</td>
</tr>
<tr>
<td style="width: 585px">Mutex lock/unlock</td>
<td style="width: 146px">25 ns</td>
</tr>
<tr>
<td style="width: 585px">Main memory reference</td>
<td style="width: 146px">100 ns</td>
</tr>
<tr>
<td style="width: 585px">Compress 1 Kb with Zippy</td>
<td style="width: 146px">3000 ns</td>
</tr>
<tr>
<td style="width: 585px">Send 1 Kb over 1 Gbps network</td>
<td style="width: 146px">10000 ns</td>
</tr>
<tr>
<td style="width: 585px">Read 4 Kb randomly from SSD</td>
<td style="width: 146px">150000 ns</td>
</tr>
<tr>
<td style="width: 585px">Read 1 Mb sequentially from memory</td>
<td style="width: 146px">250000 ns</td>
</tr>
<tr>
<td style="width: 585px">Round trip within same datacenter</td>
<td style="width: 146px">500000 ns</td>
</tr>
<tr>
<td style="width: 585px">Send packet CA | Netherlands | CA</td>
<td style="width: 146px">150000000 ns</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Here, we can see in specific numbers that if you want a donut and some coffee then you could eat 300,000,000 donuts from the fridge next to your computer before taking your first bite from a Danish.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Capping the Big O</h1>
                
            
            
                
<p>Big <em>O</em> notation is a computer science term used to group functions with respect to how fast they grow as the input value gets larger. This term is most often used with respect to algorithm runtime or space requirement.</p>
<p>When using this term in software engineering, we are usually concerned with one of these four cases:</p>
<ul>
<li>Constant</li>
<li>Logarithmic growth</li>
<li>Polynomial growth</li>
<li>Exponential growth</li>
</ul>
<p>When we are concerned with application performance, it is good to consider the Big <em>O</em> efficiency of the logic you are using. Depending on which of the preceding four cases you are dealing with, the appropriate response to optimization strategies may change.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Constanting no growth</h1>
                
            
            
                
<p>Constant time operations are the indivisible units of runtime performance. In the previous section, we provided a table of common operations and how long each one takes. These are, for our purposes as programmers, basically physical constants. You can't optimize the speed of light to make it go faster.</p>
<p>Not all constant time operations are irreducible, however. If you have a procedure that does a fixed number of operations on fixed-size data, then it will be constant time. That does not mean that the procedure is automatically efficient. When trying to optimize constant time procedures, ask yourself these two questions:</p>
<ul>
<li>Can any of the work be avoided?</li>
<li>Is the fridge too far from the computer?</li>
</ul>
<p>Here is a program consisting of emphasizing constant time operations:</p>
<pre class="p1">fn allocate() -&gt; [u64; 1000] {<br/>   [22; 1000]<br/>}<br/><br/>fn flop(x: f64, y: f64) -&gt; f64 {<br/>   x * y<br/>}<br/><br/>fn lookup(x: &amp;[u64; 1000]) -&gt; u64 {<br/>   x[234] * x[345]<br/>}<br/><br/>fn main() {<br/>   let mut data = allocate();<br/>   for _ in 0..1000 {<br/>      //constant size memory allocation<br/>      data = allocate();<br/>   }<br/><br/>   for _ in 0..1000000 {<br/>      //reference data<br/>      lookup(&amp;data);<br/>   }<br/><br/>   for _ in 0..1000000 {<br/>      //floating point operation<br/>      flop(2.0, 3.0);<br/>   }<br/>}</pre>
<p>Then, let's profile this program:</p>
<pre class="p1"><strong>Profiling performance_constant with callgrind...<br/><br/>Total Instructions...896,049,080<br/><br/>217,133,740 (24.2%) ???:_platform_memmove$VARIANT$Haswell<br/>-----------------------------------------------------------<br/>108,054,000 (12.1%) ???:core::ptr<br/>-----------------------------------------------------------<br/>102,051,069 (11.4%) ???:core::iter::range<br/>-----------------------------------------------------------<br/>76,038,000 (8.5%) ???:&lt;i32<br/>-----------------------------------------------------------<br/>56,028,000 (6.3%) ???:core::ptr<br/>-----------------------------------------------------------<br/>46,023,000 (5.1%) ???:core::iter::range::ptr_try_from_impls<br/>-----------------------------------------------------------<br/>45,027,072 (5.0%) ???:performance_constant<br/>-----------------------------------------------------------<br/>44,022,000 (4.9%) ???:core::ptr<br/>-----------------------------------------------------------<br/>40,020,000 (4.5%) ???:core::mem<br/>-----------------------------------------------------------<br/>30,015,045 (3.3%) ???:core::cmp::impls</strong></pre>
<p>We see that the heavy memory allocation is fairly expensive. As for the memory access and floating point calculation, it is seemingly overwhelmed by the expense of the loop that executes them multiple times. Unless there is a clear culprit for poor performance in a constant time procedure, then optimizing this code may not be straightforward.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Logarithmic growth</h1>
                
            
            
                
<p>Logarithmic algorithms are the pride of computer science. If your <em>O</em>(<em>n</em>) for <em>n</em>=5 code could have been written with an <em>O</em>(<em>log n</em>) algorithm, then surely at least one person will point this out.</p>
<p>A binary search is O(<em>log n</em>). A sort is typically <em>O</em>(<em>n log n</em>). Everything with a log in it is better. This fondness is not misplaced. Logarithmic growth has an amazing property—growth slows down as the input value increases.</p>
<p>Here is a program emphasizing logarithmic growth. We initialize a vector with random numbers having size of 1000 or 10000. Then we use the builtin library to sort and perform 100 binary search operations. First let's capture the time for sort and search for the 1000 case:</p>
<pre class="p1">extern crate rand;<br/>extern crate flame;<br/>use std::fs::File;<br/><br/>fn main() {<br/>   let mut data = vec![0; 1000];<br/>   for di in 0..data.len() {<br/>      data[di] = rand::random::&lt;u64&gt;();<br/>   }<br/><br/>   flame::start("sort n=1000");<br/>   data.sort();<br/>   flame::end("sort n=1000");<br/><br/>   flame::start("binary search n=1000 100 times");<br/>   for _ in 0..100 {<br/>      let c = rand::random::&lt;u64&gt;();<br/>      data.binary_search(&amp;c).ok();<br/>   }<br/>   flame::end("binary search n=1000 100 times");</pre>
<p>Now we profile the 10000 case:</p>
<pre class="p1">   let mut data = vec![0; 10000];<br/>   for di in 0..data.len() {<br/>      data[di] = rand::random::&lt;u64&gt;();<br/>   }<br/><br/>   flame::start("sort n=10000");<br/>   data.sort();<br/>   flame::end("sort n=10000");<br/><br/>   flame::start("binary search n=10000 100 times");<br/>   for _ in 0..100 {<br/>      let c = rand::random::&lt;u64&gt;();<br/>      data.binary_search(&amp;c).ok();<br/>   }<br/>   flame::end("binary search n=10000 100 times");<br/><br/>   flame::dump_html(&amp;mut File::create("flame-graph.html").unwrap()).unwrap();<br/>}</pre>
<p>After running this and examining the flamegraphs, we can see that sorting for a vector that is 10 times larger takes barely 10 times as much time—<kbd>O(n log n)</kbd>. Search performance is hardly affected at all—<kbd>O(log n)</kbd>. So for practical uses, logarithmic growth is almost negligible.</p>
<p>When trying to optimize logarithmic code, follow the same approach as for constant time optimization. Logarithmic complexity is usually not a good target for optimization, particularly considering that logarithmic complexity is a strong indicator of good algorithm design.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Polynomial growth</h1>
                
            
            
                
<p>Most algorithms are polynomial.</p>
<p>If you have one <kbd>for</kbd> loop, then your complexity is <em>O</em>(<em>n</em>). This is shown in the following code:</p>
<pre class="p1">fn main() {<br/>   for _ in 0..1000 {<br/>      //O(n)<br/>      //n = 1000<br/>   }<br/>}</pre>
<p>If you have two <kbd>for</kbd> loops, then your complexity is <em>O</em>(<em>n</em><sup>2</sup>):</p>
<pre class="p1">fn main() {<br/>   for _ in 0..1000 {<br/>      for _ in 0..1000 {<br/>         //O(n^2)<br/>         //n = 1000<br/>      }<br/>   }<br/>}</pre>
<p>Higher polynomials are somewhat less common. Sometimes code accidentally becomes a higher polynomial, which you should be careful about; otherwise, let's just consider both the previous cases.</p>
<p>Linear complexity is very common. Any time you process the entirety of data in a collection, the complexity will be linear. The running time of a linear algorithm will be approximately the number of items (<em>n</em>) processed, multiplied by the time to process individual items (<em>c</em>). If you want to make a linear algorithm go faster, you need to:</p>
<ul>
<li>Reduce the number of items processed (n)</li>
<li>Reduce the constant time associated with processing an item (<em>c</em>)</li>
</ul>
<p>If the time to process an item is not constant or approximately constant, then your overall time complexity is now recursively dependent on that processing time. This is shown with the following code:</p>
<pre class="p1">fn a(n: u64) {<br/>   //Is this O(n)?<br/>   for _ in 0..n {<br/>      b(n)<br/>   }<br/>}<br/><br/>fn b(n: u64) {<br/>   //Is this O(n)?<br/>   for _ in 0..n {<br/>      c(n)<br/>   }<br/>}<br/><br/>fn c(n: u64) {<br/>   //This is O(n)<br/>   for _ in 0..n {<br/>      let _ = 1 + 1;<br/>   }<br/>}<br/><br/>fn main() {<br/>   //What time complexity is this?<br/>   a(1000)<br/>}</pre>
<p>Higher polynomial complexity is also common but may indicate that your algorithm is poorly designed. In the preceding description, we mentioned that the linear processing time can become dependent on the time to process individual items. If your program is designed carelessly, then it is very easy to string together three or four linear algorithms and unintentionally create an <em>O</em>(<em>n</em><sup>4</sup>) monster.</p>
<p>Higher polynomials are proportionally slower. In the case of algorithms that naively require high polynomial calculations, it is often the case that the algorithm can be pruned to remove calculations that are redundant or entirely unnecessary. Consider the following code:</p>
<pre class="p1">extern crate rusty_machine;<br/>use rusty_machine::linalg::{Matrix,Vector};<br/>use rusty_machine::learning::gp::{GaussianProcess,ConstMean};<br/>use rusty_machine::learning::toolkit::kernel;<br/>use rusty_machine::learning::SupModel;<br/><br/>fn main() {<br/>   let inputs = Matrix::new(3,3,vec![1.1,1.2,1.3,2.1,2.2,2.3,3.1,3.2,3.3]);<br/>   let targets = Vector::new(vec![0.1,0.8,0.3]);<br/>   let test_inputs = Matrix::new(2,3, vec![1.2,1.3,1.4,2.2,2.3,2.4]);<br/>   let ker = kernel::SquaredExp::new(2., 1.);<br/>   let zero_mean = ConstMean::default();<br/>   let mut gp = GaussianProcess::new(ker, zero_mean, 0.5);<br/><br/>   gp.train(&amp;inputs, &amp;targets).unwrap();<br/>   let _ = gp.predict(&amp;test_inputs).unwrap();<br/>}</pre>
<p>When you need to use higher polynomial algorithms, use a library! This stuff gets complicated fast and improving these algorithms is the main job of academic Computer Scientists. If you are performance-tuning a common algorithm and not expecting to publish your results, then you may likely be duplicating work.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exponential growth</h1>
                
            
            
                
<p>Exponential performance in engineering is almost always a bug or a dead end. This is the wall that separates algorithms that we use from algorithms that we would like to use but can't due to performance reasons.</p>
<p>Exponential growth in programs is often accompanied by the term <kbd>bomb</kbd>:</p>
<pre class="p1">fn bomb(n: u64) -&gt; u64 {<br/>   if n &gt; 0 {<br/>      bomb(n-1);<br/>      bomb(n-1);<br/>   }<br/>   n<br/>}<br/><br/>fn main() {<br/>   bomb(1000);<br/>}</pre>
<p>This program is only <em>O</em>(2<sup>n</sup>) and therefore barely even exponential!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Referencing data is faster</h1>
                
            
            
                
<p>There is a rule of thumb that referencing data is faster than copying data. Similarly, copying data is faster than cloning. This is not always true, but it is a good rule to consider when trying to improve program performance.</p>
<p>Here is a function that alternatively uses data by reference, copied, intrinsic cloned, or custom cloned:</p>
<pre class="p1">extern crate flame;<br/>use std::fs::File;<br/><br/>fn byref(n: u64, data: &amp;[u64; 1024]) {<br/>   if n&gt;0 {<br/>      byref(n-1, data);<br/>      byref(n-1, data);<br/>   }<br/>}<br/><br/>fn bycopy(n: u64, data: [u64; 1024]) {<br/>   if n&gt;0 {<br/>      bycopy(n-1, data);<br/>      bycopy(n-1, data);<br/>   }<br/>}<br/><br/>struct DataClonable([u64; 1024]);<br/>impl Clone for DataClonable {<br/>   fn clone(&amp;self) -&gt; Self {<br/>      let mut newdata = [0; 1024];<br/>      for i in 0..1024 {<br/>         newdata[i] = self.0[i];<br/>      }<br/>      DataClonable(newdata)<br/>   }<br/>}<br/><br/>fn byclone&lt;T: Clone&gt;(n: u64, data: T) {<br/>   if n&gt;0 {<br/>      byclone(n-1, data.clone());<br/>      byclone(n-1, data.clone());<br/>   }<br/>}</pre>
<p>Here we declare array of <kbd>1024</kbd> elements. Then using the flamegraph profiling library we apply the above functions to measure the differences between reference, copy and clone performance:</p>
<pre class="p1">fn main() {<br/>   let data = [0; 1024];<br/>   flame::start("by reference");<br/>   byref(15, &amp;data);<br/>   flame::end("by reference");<br/><br/>   let data = [0; 1024];<br/>   flame::start("by copy");<br/>   bycopy(15, data);<br/>   flame::end("by copy");<br/><br/>   let data = [0; 1024];<br/>   flame::start("by clone");<br/>   byclone(15, data);<br/>   flame::end("by clone");<br/><br/>   let data = DataClonable([0; 1024]);<br/>   flame::start("by clone (with extras)");<br/>   //2^4 instead of 2^15!!!!<br/>   byclone(4, data);<br/>   flame::end("by clone (with extras)");<br/><br/>   flame::dump_html(&amp;mut File::create("flame-graph.html").unwrap()).unwrap();<br/>}</pre>
<p>Looking at the runtime of this application, we see that the referenced data uses only a small sliver of the resources compared to copying or cloning this data. The default clone and copy traits unsurprisingly give a similar performance. The custom clone is really slow. It does semantically the same thing as all the others, but it is not as optimized at a low level.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Preventing bugs with defensive coding</h1>
                
            
            
                
<p class="p1">You don’t need to fix bugs that never happen. Preventative medicine is good software engineering that will save you time in the long run.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using Option and Result instead of panic!</h1>
                
            
            
                
<p class="p1">In many other languages, exception handling is performed through <kbd>try…catch</kbd> blocks. Rust does not automatically provide this functionality, instead it encourages the programmer to explicitly localize all error handling.</p>
<p class="p1">In many Rust contexts, if you don’t want to deal with error handling, you always have the option to use <kbd>panic!</kbd>. This will immediately end the program and provide a short error message. Don't do this. Panicking is usually just a way of avoiding the responsibility of handling errors.</p>
<p class="p1">Instead, use either the <kbd>Option</kbd> or <kbd>Result</kbd> types to communicate error or exceptional conditions. <kbd>Option</kbd> indicates that no value is available. The <kbd>None</kbd> value of <kbd>Option</kbd> should indicate that there is no value but that everything is okay and expected.</p>
<p class="p1">The <kbd>Result</kbd> type is used to communicate whether or not there was an error in processing. <kbd>Result</kbd> types can be used in combination with the <kbd>?</kbd> syntax to propagate errors while avoiding introducing too much extra syntax. The <kbd>?</kbd> operation will return errors from the function, if any, and therefore the function must have a <kbd>Result</kbd> return type.</p>
<p class="p1">Here we create two functions that return <kbd>Option</kbd> or <kbd>Result</kbd> to handle exceptional circumstances. Note the use of the try <kbd>?</kbd> syntax when handling <kbd>Result</kbd> return values. This syntax will pass through <kbd>Ok</kbd> values or immediately return any <kbd>Err</kbd> from that function. For this reason, any function using ? must also return a compatible <kbd>Result</kbd> type:</p>
<pre class="p1">//This function returns an Option if the value is not expected<br/>fn expect_1or2or_other(n: u64) -&gt; Option&lt;u64&gt; {<br/>   match n {<br/>      1|2 =&gt; Some(n),<br/>      _ =&gt; None<br/>   }<br/>}<br/><br/>//This function returns an Err if the value is not expected<br/>fn expect_1or2or_error(n: u64) -&gt; Result&lt;u64,()&gt; {<br/>   match n {<br/>      1|2 =&gt; Ok(n),<br/>      _ =&gt; Err(())<br/>   }<br/>}<br/><br/>//This function uses functions that return Option and Return types<br/>fn mixed_1or2() -&gt; Result&lt;(),()&gt; {<br/>   expect_1or2or_other(1);<br/>   expect_1or2or_other(2);<br/>   expect_1or2or_other(3);<br/><br/>   expect_1or2or_error(1)?;<br/>   expect_1or2or_error(2)?;<br/>   expect_1or2or_error(3).unwrap_or(222);<br/>   Ok(())<br/>}<br/><br/>fn main() {<br/>   mixed_1or2().expect("mixed 1 or 2 is OK.");<br/>}</pre>
<p><kbd>Result</kbd> types are very common when interacting with external resources such as files:</p>
<pre class="p1">use std::fs::File;<br/>use std::io::prelude::*;<br/>use std::io;<br/><br/>fn lots_of_io() -&gt; io::Result&lt;()&gt; {<br/>   {<br/>      let mut file = File::create("data.txt")?;<br/>      file.write_all(b"data\ndata\ndata")?;<br/>   }<br/><br/>   {<br/>      let mut file = File::open("data.txt")?;<br/>      let mut data = String::new();<br/>      file.read_to_string(&amp;mut data)?;<br/>      println!("{}", data);<br/>   }<br/>   Ok(())<br/>}<br/><br/>fn main() {<br/>   lots_of_io().expect("lots of io is OK.");<br/>}</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Using typesafe interfaces instead of stringly typed interfaces</h1>
                
            
            
                
<p class="p1">Enumerations in Rust are less error-prone than using numbers or strings. Whenever possible, write the following code:</p>
<pre class="p1">const MyEnum_A: u32 = 1;<br/>const MyEnum_B: u32 = 2;<br/>const MyEnum_C: u32 = 3;</pre>
<p class="p1">Similarly, you can write a stringly enumeration:</p>
<pre class="p1">"a"<br/>"b"<br/>"c"</pre>
<p class="p1">It is better to use the following enum type:</p>
<pre class="p1">enum MyEnum {<br/>   A,<br/>   B,<br/>   C,<br/>}</pre>
<p class="p1">This way, functions accepting the enumeration will be typesafe:</p>
<pre class="p1">fn foo(n: u64) {} //not all u64 are valid inputs<br/>fn bar(n: &amp;str) {} //not all &amp;str are valid inputs<br/>fn baz(n: MyEnum) {} //all MyEnum are valid</pre>
<p class="p1">Enums also fit naturally with pattern matching for the same reason. Pattern matching against an enumeration does not require a final error case like the integer or string typed case would:</p>
<pre class="p1">match a {<br/>   1 =&gt; println!(“1 is ok”),<br/>   2 =&gt; println!(“2 is ok”),<br/>   3 =&gt; println!(“3 is ok”),<br/>   n =&gt; println!(“{} was unexpected”, n)<br/>}</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Using the heartbeat pattern for long running processes</h1>
                
            
            
                
<p class="p1">When you want to create a long running process, it is nice to be able to recover from program errors that crash or terminate the process. Perhaps the process runs out of stack space or encounters a <kbd>panic!</kbd> from some code path. For any number of reasons, a process might get terminated and will need to be restarted.</p>
<p class="p1">To accommodate this desire, there are many tools that will watch a program for you and restart it if it dies or stops responding to health checks. Here, we recommend a completely self-contained version of this pattern that is based on Rust concurrency.</p>
<p class="p1">The goal is to create a parent process that acts as a monitor and oversees one or more workers. The process tree should look something like this:</p>
<pre class="p1">parent<br/> —- child 1<br/> —- child 2<br/> —- child 3</pre>
<p class="p1">When a child dies or stops responding to health checks, the parent should kill or otherwise clean up the process resources, then start a new process to replace it. Here is an example of this behavior, starting with a subprocess that sometimes dies:</p>
<pre class="p1">use std::{thread,time,process};<br/><br/>fn main() {<br/>   let life_expectancy = process::id() % 8;<br/>   let t = time::Duration::from_millis(1000);<br/>   for _ in 0..life_expectancy {<br/>      thread::sleep(t);<br/>   }<br/>   println!("process {} dies unexpectedly.", process::id());<br/>}</pre>
<p>This worker process is highly unreliable and lives no longer than eight seconds. However, if we wrap it with a heartbeat monitor, then we can make it more reliable:</p>
<pre class="p1">use std::process::Command;<br/>use std::env::current_exe;<br/>use std::{thread,time};<br/><br/>fn main() {<br/>   //There is an executable called debugging_buggy_worker<br/>   //it crashes a lot but we still want to run it<br/>   let path = current_exe()<br/>             .expect("could not find current executable");<br/>   let path = path.with_file_name("debugging_buggy_worker");<br/>   let mut children = Vec::new();<br/><br/>   //we start 3 workers<br/>   for _ in 0..3 {<br/>      children.push(<br/>         Command::new(path.as_os_str())<br/>                 .spawn()<br/>                 .expect("failed to spawn child")<br/>      );<br/>   }<br/><br/>   //those workers will randomly die because they are buggy<br/>   //so after they die, we restart a new process to replace them<br/>   let t = time::Duration::from_millis(1000);<br/>   loop {<br/>      thread::sleep(t);<br/>      for ci in 0..children.len() {<br/>         let is_dead = children[ci].try_wait().expect("failed to try_wait");<br/>         if let Some(_exit_code) = is_dead {<br/>            children[ci] = Command::new(path.as_os_str())<br/>                                   .spawn()<br/>                                   .expect("failed to spawn child");<br/>            println!("starting a new process from parent.");<br/>         }<br/>      }<br/>   }<br/>}</pre>
<p>Now, the running processes will get restarted if they die unexpectedly. Optionally, the parent can check the health status of each child process and restart unresponsive workers.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Validating input and output</h1>
                
            
            
                
<p class="p1">Preconditions and postconditions are a great way to lock down program behavior and find bugs or invalid states before they get out of hand.</p>
<p class="p1">If you use macros to do this, then the preconditions and postconditions can optionally be run only in debug mode, and removed from production code. The built-in <kbd>debug_assert!</kbd> macro does this. However, using assertions for return values is not particularly elegant and, if you forget to check a branch with a return statement, then your postcondition won't be checked.</p>
<p class="p1"><kbd>debug_assert!</kbd> is not a good choice for the validation of anything dependent on external data or otherwise nondeterministic behavior. When you want to check preconditions or postconditions in production code, you should instead use <kbd>Result</kbd> or <kbd>Option</kbd> values to handle exceptional behavior.</p>
<p class="p1">Here are some examples of preconditions and postconditions in Rust:</p>
<pre class="p1">use std::io;<br/><br/>//This function checks the precondition that [n &lt; 100]<br/>fn debug_precondition(n: u64) -&gt; u64 {<br/>   debug_assert!(n &lt; 100);<br/>   n * n<br/>}<br/><br/>//This function checks the postcondition that [return &gt; 10]<br/>fn debug_postcondition(n: u64) -&gt; u64 {<br/>   let r = n * n;<br/>   debug_assert!(r &gt; 10);<br/>   r<br/>}<br/><br/>//this function dynamically checks the precondition [n &lt; 100]<br/>fn runtime_precondition(n: u64) -&gt; Result&lt;u64,()&gt; {<br/>   if !(n&lt;100) { return Err(()) };<br/>   Ok(n * n)<br/>}<br/><br/>//this function dynamically checks the postcondition [return &gt; 10]<br/>fn runtime_postcondition(n: u64) -&gt; Result&lt;u64,()&gt; {<br/>   let r = n * n;<br/>   if !(r&gt;10) { return Err(()) };<br/>   Ok(r)<br/>}<br/><br/>//This main function uses all of the functions<br/>//The dynamically validated functions are subjected to user input<br/>fn main() {<br/>   //inward facing code should assert expectations<br/>   debug_precondition(5);<br/>   debug_postcondition(5);<br/><br/>   //outward facing code should handle errors<br/>   let mut s = String::new();<br/>   println!("Please input a positive integer greater or equal to 4:");<br/>   io::stdin().read_line(&amp;mut s).expect("error reading input");<br/>   let i = s.trim().parse::&lt;u64&gt;().expect("error parsing input as integer");<br/>   runtime_precondition(i).expect("runtime precondition violated");<br/>   runtime_postcondition(i).expect("runtime postcondition violated");<br/>}</pre>
<p>Notice that the user input is out of our control. The best option for validating user input is to return an <kbd>Error</kbd> condition if the input is invalid.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Finding and fixing bugs</h1>
                
            
            
                
<p>Debugging tools are quite platform dependent. Here we will explain <kbd>lldb</kbd>, which is available, and macOS and other Unix-like systems.</p>
<p>To start debugging, you will need to compile the program with debugging symbols turned on. The normal <kbd>cargo debug build</kbd> is usually sufficient:</p>
<pre><strong>cargo build</strong></pre>
<p>After the program has been compiled, start the debugger:</p>
<pre class="p1"><strong>$ sudo rust-lldb target/debug/deps/performance_polynomial3-8048e39c94dd7157</strong></pre>
<p>Here we reference the <kbd>debugs/deps/program_name-GITHASH</kbd> copy of the program. This is necessary for now just because of how lldb works.</p>
<p>After running <kbd>lldb</kbd>, you will see some information scroll past on startup. Then, you should be dropped into a LLDB Command Prompt:</p>
<pre class="p1">(lldb) command source -s 0 '/tmp/rust-lldb-commands.YnRBkV'<br/>Executing commands in '/tmp/rust-lldb-commands.YnRBkV'.<br/>(lldb) command script import "/Users/andrewjohnson/.rustup/toolchains/nightly-x86_64-apple-darwin/lib/rustlib/etc/lldb_rust_formatters.py"<br/>(lldb) type summary add --no-value --python-function lldb_rust_formatters.print_val -x ".*" --category Rust<br/>(lldb) type category enable Rust<br/>(lldb) target create "target/debug/deps/performance_polynomial3-8048e39c94dd7157"<br/>Current executable set to 'target/debug/deps/performance_polynomial3-8048e39c94dd7157' (x86_64).<br/>(lldb)<br/></pre>
<p>Now, set a breakpoint. We will set a breakpoint to stop at function <kbd>a</kbd>:</p>
<pre class="p1">(lldb) b a<br/>Breakpoint 1: where = performance_polynomial3-8048e39c94dd7157`performance_polynomial3::a::h0b267f360bbf8caa + 12 at performance_polynomial3.rs:3, address = 0x000000010000191c</pre>
<p>Now that our breakpoint is set, run the <kbd>r</kbd> command:</p>
<pre class="p1">(lldb) r<br/>Process 99468 launched: '/Users/andrewjohnson/subarctic.org/subarctic.org/Hands-On-Functional-Programming-in-RUST/Chapter09/target/debug/deps/performance_polynomial3-8048e39c94dd7157' (x86_64)<br/>Process 99468 stopped<br/>* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1<br/>  frame #0: 0x000000010000191c performance_polynomial3-8048e39c94dd7157`performance_polynomial3::a::h0b267f360bbf8caa(n=1000) at performance_polynomial3.rs:3<br/>   1    fn a(n: u64) {<br/>   2       //Is this O(n);<br/>-&gt; 3       for _ in 0..n {<br/>   4          b(n);<br/>   5       }<br/>   6    }<br/>   7    <br/>Target 0: (performance_polynomial3-8048e39c94dd7157) stopped.</pre>
<p>After stopping at the breakpoint, LLDB will print some context for where the code is stopped at. Now we can inspect the program. Let's print what variables are defined in this function:</p>
<pre class="p2">(lldb) frame variable<br/>(unsigned long) n = 1000</pre>
<p>We can similarly print any variable in scope:</p>
<pre class="p1">(lldb) p n<br/>(unsigned long) $0 = 1000</pre>
<p>When we want to continue the program, type <kbd>c</kbd> to continue:</p>
<pre class="p1">(lldb) c<br/>Process 99468 resuming<br/>Process 99468 exited with status = 0 (0x00000000)</pre>
<p>The program exits here because we did not set any more breakpoints. This method of debugging is nice because it allows you to inspect a running program without constantly adding <kbd>println!</kbd> statements and recompiling. If nothing else works, that is still an option though.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Metaprogramming</h1>
                
            
            
                
<p>Metaprogramming in Rust has two forms—macros and procedural macros. Both of these utilities accept an abstract syntax tree as new input and output symbols to be compiled. Procedural macros are very similar to normal macros but with fewer restrictions on how they work and how they are defined.</p>
<p>Macros defined with the <kbd>macro_rules!</kbd> syntax are defined recursively by matching the input syntax to produce output. It is crucial to understand that macro matching happens <em>after</em> parsing. This means the following:</p>
<ul>
<li>Macros must follow certain rules when creating new syntax forms</li>
<li>The AST is decorated with information regarding each node's grammar category</li>
</ul>
<p>Macros can match individual tokens, or a macro can match (and capture) an entire grammar category. The Rust grammar categories are as follows:</p>
<ul>
<li><kbd>tt</kbd>: This is a token tree (which is a token output from the lexer before parsing)</li>
<li><kbd>ident</kbd>: This is an identifier</li>
<li><kbd>expr</kbd>: This is an expression</li>
<li><kbd>ty</kbd>: This is a type</li>
<li><kbd>stmt</kbd>: This is a statement</li>
<li><kbd>block</kbd>: These are the braces containing a block of statements</li>
<li><kbd>item</kbd>: This is a top-level definition such as a function or a struct</li>
<li><kbd>pat</kbd>: This is the match part of a pattern match expression, also called the <strong>left hand side</strong></li>
<li><kbd>path</kbd>: This is a path such as <kbd>std::fs::File</kbd></li>
<li><kbd>meta</kbd>: This is a meta item that goes inside either <kbd>#[...]</kbd> or <kbd>#![...]</kbd> syntax forms</li>
</ul>
<p>Using these patterns we can create macros to match various groups of syntax expressions:</p>
<pre class="p1">//This macro rule matches one token tree "tt"<br/>macro_rules! match_tt {<br/>   ($e: tt) =&gt; { println!("match_tt: {}", stringify!($e)) }<br/>}<br/><br/>//This macro rule matches one identifier "ident"<br/>macro_rules! match_ident {<br/>   ($e: ident) =&gt; { println!("match_ident: {}", stringify!($e)) }<br/>}<br/><br/>//This macro rule matches one expression "expr"<br/>macro_rules! match_expr {<br/>   ($e: expr) =&gt; { println!("match_expr: {}", stringify!($e)) }<br/>}<br/><br/>//This macro rule matches one type "ty" <br/>macro_rules! match_ty {<br/>   ($e: ty) =&gt; { println!("match_ty: {}", stringify!($e)) }<br/>}<br/><br/>//This macro rule matches one statement "stmt"<br/>macro_rules! match_stmt {<br/>   ($e: stmt) =&gt; { println!("match_stmt: {}", stringify!($e)) }<br/>}<br/><br/>//This macro rule matches one block "block"<br/>macro_rules! match_block {<br/>   ($e: block) =&gt; { println!("match_block: {}", stringify!($e)) }<br/>}<br/><br/>//This macro rule matches one item "item"<br/>//items are things like function definitions, struct definitions, ...<br/>macro_rules! match_item {<br/>   ($e: item) =&gt; { println!("match_item: {}", stringify!($e)) }<br/>}<br/><br/>//This macro rule matches one pattern "pat"<br/>macro_rules! match_pat {<br/>   ($e: pat) =&gt; { println!("match_pat: {}", stringify!($e)) }<br/>}<br/><br/>//This macro rule matches one path "path"<br/>//A path is a canonical named path like std::fs::File<br/>macro_rules! match_path {<br/>   ($e: path) =&gt; { println!("match_path: {}", stringify!($e)) }<br/>}<br/><br/>//This macro rule matches one meta "meta"<br/>//A meta is anything inside of the #[...] or #![...] syntax<br/>macro_rules! match_meta {<br/>   ($e: meta) =&gt; { println!("match_meta: {}", stringify!($e)) }<br/>}</pre>
<p>Then, let's apply the macros to some different input:</p>
<pre class="p1">fn main() {<br/>   match_tt!(a);<br/>   match_tt!(let);<br/>   match_tt!(+);<br/><br/>   match_ident!(a);<br/>   match_ident!(bcd);<br/>   match_ident!(_def);<br/><br/>   match_expr!(1.2);<br/>   match_expr!(bcd);<br/>   match_expr!(1.2 + bcd / "b" - [1, 3, 4] .. vec![1, 2, 3]);<br/><br/>   match_ty!(A);<br/>   match_ty!(B + 'static);<br/>   match_ty!(A&lt;&amp;(B + 'b),&amp;mut (C + 'c)&gt; + 'static);<br/><br/>   match_stmt!(let x = y);<br/>   match_stmt!(());<br/>   match_stmt!(fn f(){});<br/>   <br/>   match_block!({});<br/>   match_block!({1; 2});<br/>   match_block!({1; 2 + 3});<br/><br/>   match_item!(struct A(u64););<br/>   match_item!(enum B { C, D });<br/>   match_item!(fn C(n: NotAType) -&gt; F&lt;F&lt;F&lt;F&lt;F&gt;&gt;&gt;&gt; { a + b });<br/><br/>   match_pat!(_);<br/>   match_pat!(1);<br/>   match_pat!(A {b, c:D( d@3 )} );<br/><br/>   match_path!(A);<br/>   match_path!(::A);<br/>   match_path!(std::A);<br/>   match_path!(a::&lt;A,_&gt;);<br/><br/>   match_meta!(A);<br/>   match_meta!(Property(B,C));<br/>}</pre>
<p>As we can see from the example, token trees are, for the most part, not restricted to normal Rust grammar, only to the Rust lexer. The lexer is aware of opening and closing <kbd>() [] {}</kbd> bracketed forms. This is why tokens are structured in a token tree rather than a token list. This also means that all tokens inside macro calls will be stored as token trees and not processed any further until the macro is invoked; as long as we create a syntax compatible with Rust token trees, then other syntax innovations should usually be permitted. This rule applies also to the other grammar categories: grammar categories are just a short hand to match certain pattern of tokens that happen to correspond to Rust syntax forms.</p>
<p>Just matching single tokens or grammar categories probably won't be very useful for a macro. To make use of macros in a practical context, we will need to make use of macro grammar sequences and grammar alternative<em>s</em>. A grammar sequence is a request to match more than one token or grammar category in the same rule. A grammar alternative is a separate rule within the same macro that matches a different syntax. Grammar sequences and alternatives can also be combined in the same macro. Additionally, there is a special syntax form to match <em>many</em> tokens or grammar categories.</p>
<p>Here are corresponding examples to illustrate these patterns:</p>
<pre class="p1">//this is a grammar sequence<br/>macro_rules! abc {<br/>   (a b c) =&gt; { println!("'a b c' is the only correct syntax.") };<br/>}<br/><br/>//this is a grammar alternative<br/>macro_rules! a_or_b {<br/>   (a) =&gt; { println!("'a' is one correct syntax.") };<br/>   (b) =&gt; { println!("'b' is also correct syntax.") };<br/>}<br/><br/>//this is a grammar of alternative sequences<br/>macro_rules! abc_or_aaa {<br/>   (a b c) =&gt; { println!("'a b c' is one correct syntax.") };<br/>   (a a a) =&gt; { println!("'a a a' is also correct syntax.") };<br/>}<br/><br/>//this is a grammar sequence matching many of one token<br/>macro_rules! many_a {<br/>   ( $($a:ident)* ) =&gt; {{ $( print!("one {} ", stringify!($a)); )* println!(""); }};<br/>   ( $($a:ident),* ) =&gt; {{ $( print!("one {} comma ", stringify!($a)); )* println!(""); }};<br/>}<br/><br/>fn main() {<br/>   abc!(a b c);<br/><br/>   a_or_b!(a);<br/>   a_or_b!(b);<br/><br/>   abc_or_aaa!(a b c);<br/>   abc_or_aaa!(a a a);<br/><br/>   many_a!(a a a);<br/>   many_a!(a, a, a);<br/>}</pre>
<p>If you've paid attention to the generated code for all of these macros, you might have noticed that all production rules have created expressions. Macro input can be tokens, but output must be a contextually well-formed Rust syntax. For this reason, you cannot write <kbd>macro_rules!</kbd> as shown here:</p>
<pre class="p1">macro_rules! f {<br/>   () =&gt; { f!(1) f!(2) f!(3) };<br/>   (1) =&gt; { 1 };<br/>   (2) =&gt; { + };<br/>   (3) =&gt; { 2 };<br/>}<br/><br/>fn main() {<br/>   f!()<br/>}</pre>
<p>The specific error from the compiler is as follows:</p>
<pre class="p1">error: macro expansion ignores token `f` and any following<br/>--&gt; t.rs:2:19<br/>  |<br/>2 |    () =&gt; { f!(1); f!(2); f!(3) };<br/>  |                   ^<br/>  |<br/>note: caused by the macro expansion here; the usage of `f!` is likely invalid in expression context<br/>--&gt; t.rs:9:4<br/>  |<br/>9 |    f!()<br/>  |    ^^^^<br/><br/>error: aborting due to previous error</pre>
<p>The key phrase here is <kbd>f!</kbd>, which is likely invalid in an expression context. Each pattern of <kbd>macro_rules!</kbd> output must be a well-formed expression. The preceding example will create well-formed Rust syntax in the end, but its intermediate results are fragmented expressions. This awkwardness is one of the several reasons to use procedural macros, which are much like <kbd>macro_rules!</kbd> but programmed directly in Rust rather than through the special <kbd>macro_rules!</kbd> syntax.</p>
<p>Procedural macros are programmed in Rust, but are also used to compile Rust programs. How does that work? Procedural macros must be isolated into their own modules and compiled separately; they are basically a compiler plugin.</p>
<p>To start our procedural macro, let's create a new subproject:</p>
<ol>
<li>Make a <kbd>procmacro</kbd> directory inside the project root</li>
<li>Inside the <kbd>procmacro</kbd> directory, create a <kbd>Cargo.toml</kbd> file with the following contents:</li>
</ol>
<pre class="p1" style="padding-left: 30px">[package]<br/>name = "procmacro"<br/>version = "1.0.0"<br/><br/>[dependencies]<br/>syn = "0.12"<br/>quote = "0.4"<br/><br/>[lib]<br/>proc-macro = true</pre>
<ol start="3">
<li>Inside the <kbd>procmacro</kbd> directory, create a <kbd>src/lib.rs</kbd> file with the following contents:</li>
</ol>
<pre class="p1" style="padding-left: 30px">#![feature(proc_macro)]<br/>#![crate_type = "proc-macro"]<br/>extern crate proc_macro;<br/>extern crate syn;<br/>#[macro_use] extern crate quote;<br/>use proc_macro::TokenStream;<br/>#[proc_macro]<br/><br/>pub fn f(input: TokenStream) -&gt; TokenStream {<br/>   assert!(input.is_empty());<br/><br/>   (quote! {<br/>      1 + 2<br/>   }).into()<br/>}</pre>
<p style="padding-left: 30px">This <kbd>f!</kbd> macro now implements the preceding semantics without any of the complaints. Using the macro looks like the following:</p>
<pre class="p1" style="padding-left: 30px">#![feature(proc_macro_non_items)]<br/>#![feature(use_extern_macros)]<br/>extern crate procmacro;<br/><br/>fn main() {<br/>   let _ = procmacro::f!();<br/>}</pre>
<p>The interface of a procedural macro is really simple. There is a <kbd>TokenStream</kbd> as input and a <kbd>TokenStream</kbd> as output. The <kbd>proc_macro</kbd> and <kbd>syn</kbd> crates also provide utilities to parse tokens or to easily create token streams using the <kbd>quote!</kbd> macro. To use procedural macros, there is some additional setup and boilerplate, but after getting past these hurdles the interface is fairly straightforward now.</p>
<p>Additionally, there are many more detailed grammar categories available to procedural macros through the <kbd>syn</kbd> crate. There are 163 categories (<a href="https://dtolnay.github.io/syn/syn/#macros">https://dtolnay.github.io/syn/syn/#macros</a>) right now! These include the same vague syntax trees from recursive macros, but also very specific syntax forms. These categories correspond to the full Rust grammar, therefore permitting very expressive macro syntax without having to create your own parser.</p>
<p>Let's make a procedural macro that uses some of these syntax categories. First we make a new procedural macro folder, just like preceding <kbd>procmacro</kbd>; this one we will name <kbd>procmacro2</kbd>. Now we define the AST that will hold all of the program information if the user input is valid:</p>
<pre class="p1">#![feature(proc_macro)]<br/>#![crate_type = "proc-macro"]<br/>extern crate proc_macro;<br/>#[macro_use] extern crate syn;<br/>#[macro_use] extern crate quote;<br/>use proc_macro::TokenStream;<br/>use syn::{Ident, Type, Expr, WhereClause, TypeSlice, Path};<br/>use syn::synom::Synom;<br/><br/>struct MiscSyntax {<br/>   id: Ident,<br/>   ty: Type,<br/>   expr: Expr,<br/>   where_clause: WhereClause,<br/>   type_slice: TypeSlice,<br/>   path: Path<br/>}</pre>
<p>The <kbd>MiscSyntax</kbd> struct will contain all information gathered from our macro. That macro and its syntax is what we should define now:</p>
<pre class="p1">impl Synom for MiscSyntax {<br/>   named!(parse -&gt; Self, do_parse!(<br/>      keyword!(where) &gt;&gt;<br/>      keyword!(while) &gt;&gt;<br/>      id: syn!(Ident) &gt;&gt;<br/>      punct!(:) &gt;&gt;<br/>      ty: syn!(Type) &gt;&gt;<br/>      punct!(&gt;&gt;) &gt;&gt;<br/>      expr: syn!(Expr) &gt;&gt;<br/>      punct!(;) &gt;&gt;<br/>      where_clause: syn!(WhereClause) &gt;&gt;<br/>      punct!(;) &gt;&gt;<br/>      type_slice: syn!(TypeSlice) &gt;&gt;<br/>      punct!(;) &gt;&gt;<br/>      path: syn!(Path) &gt;&gt;<br/>      (MiscSyntax { id, ty, expr, where_clause, type_slice, path })<br/>   ));<br/>}</pre>
<p>The <kbd>do_parse!</kbd> macro helps simplify the use of the parser combinators from the <kbd>syn</kbd> crate. The <kbd>id: expr &gt;&gt;</kbd> syntax corresponds to the monadic bind operation, and <kbd>expr &gt;&gt;</kbd> syntax is also a form of a monadic bind.</p>
<p>Now we utilize these definitions to parse input, generate output, and expose the macro:</p>
<pre class="p1">#[proc_macro]<br/>pub fn misc_syntax(input: TokenStream) -&gt; TokenStream {<br/>   let m: MiscSyntax = syn::parse(input).expect("expected Miscellaneous Syntax");<br/>   let MiscSyntax { id, ty, expr, where_clause, type_slice, path } = m;<br/><br/>   (quote! {<br/>      let #id: #ty = #expr;<br/>      println!("variable = {}", #id);<br/>    }).into()<br/>}</pre>
<p>When using this macro, it really is a bunch of random syntax. This emphasizes how macros are not limited to valid Rust syntax, which looks like the following:</p>
<pre class="p1">#![feature(proc_macro_non_items)]<br/>#![feature(use_extern_macros)]<br/>extern crate procmacro2;<br/><br/>fn main() {<br/>   procmacro2::misc_syntax!(<br/>      where while abcd : u64 &gt;&gt; 1 + 2 * 3;<br/>      where T: 'x + A&lt;B='y+C+D&gt;;<br/>      [M];A::f<br/>   );<br/>}</pre>
<p>Procedural macros are very powerful and helpful if Rust syntax becomes annoying for your purposes. For specific contexts it is possible to create very semantically dense code using macros that would otherwise require lots of boilerplate and copy-paste coding.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we introduced many applied and practical considerations for Rust programming. Performance and debugging are certainly not problems that are exclusive to Functional Programming. Here we tried to introduce tips that are generally applicable but also highly compatible with functional programming.</p>
<p>Metaprogramming in Rust may be considered a functional feature by itself. Logic programming and thereby derived functionality are closely associated with functional programming principles. The recursive, context-free nature of macros also lends itself to a functional perspective.</p>
<p>This is also the last chapter in the book. We hope you have enjoyed the book and we welcome any feedback. If you are looking for further reading, you might want to research some of the topics presented in the final three chapters of the book. There is an enormous amount of material available on these subjects and any path taken will surely further improve your understanding of Rust and functional programming.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Questions</h1>
                
            
            
                
<ol>
<li>How is release mode different from debug mode?</li>
<li>How long will an empty loop take to run?</li>
<li>What is linear time in <em>Big O</em> notation?</li>
<li>Name a function that grows faster than exponential growth.</li>
<li>What is faster, a disk read or a network read?</li>
<li>How would you return a <kbd>Result</kbd> with multiple error conditions?</li>
<li>What is a token tree?</li>
<li>What is an abstract syntax tree?</li>
<li>Why do procedural macros need to be compiled separately?</li>
</ol>


            

            
        
    </body></html>
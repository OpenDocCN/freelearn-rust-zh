<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Performance, Debugging, and Metaprogramming</h1>
                </header>
            
            <article>
                
<p class="mce-root">Writing fast efficient code can be something to be proud of. It also might be a waste of your employer's resources. In the performance section, we will explore how to tell the difference between the two and give best-practices, processes, and guidelines to keep your application slim.</p>
<p>In the debugging section, we offer tips to help find and resolve bugs faster. We also introduce the concept of defensive coding, which describes techniques and habits to prevent or isolate potential issues.</p>
<p>In the metaprogramming section, we explain macros and other features that are similar to macros. Rust has a fairly sophisticated metaprogramming system that allows the user or libraries to extend the language with automatic code generation or custom syntax forms.</p>
<p>In this chapter, we will learn the following:</p>
<ul>
<li>Recognizing and applying good performant code practices</li>
<li>Diagnosing and improving performance bottlenecks</li>
<li>Recognizing and applying good defensive coding practices</li>
<li>Diagnosing and resolving software bugs</li>
<li>Recognizing and applying metaprogramming techniques</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>A recent version of Rust is necessary to run the examples provided:</p>
<p><a href="https://www.rust-lang.org/en-US/install.html">https://www.rust-lang.org/en-US/install.html</a></p>
<p>This chapter's code is available on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Hands-On-Functional-Programming-in-RUST">https://github.com/PacktPublishing/Hands-On-Functional-Programming-in-RUST</a></p>
<p>Specific installation and build instructions are also included in each chapter's <kbd>README.md</kbd> file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing faster code</h1>
                </header>
            
            <article>
                
<div class="packt_quote">Premature optimization is the root of all evil </div>
<div class="packt_quote"><span>                                                                                                                  –</span> Donald Knuth</div>
<p>A good software design tends to create faster programs, while a bad software design tends to create slower programs. If you find yourself asking, "W<em>hy is my program slow?, then first ask yourself, Is my program disorderly?</em>"</p>
<p>In this section, we describe some performance tips. These are generally good habits when programming in Rust that will coincidentally lead to improved performance. If your program is slow, then first check to see whether you are violating one of these principles.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compiling with release mode</h1>
                </header>
            
            <article>
                
<p>This is a really simple suggestion that you should know about if you are at all concerned about performance.</p>
<ul>
<li>Rust normally compiles in debug mode, which is slow:</li>
</ul>
<pre class="p1" style="padding-left: 60px"><strong><span class="s1">cargo build</span></strong></pre>
<ul>
<li>Rust optionally compiles in release mode, which is fast:</li>
</ul>
<pre class="p1" style="padding-left: 60px"><strong><span class="s1">cargo build --release</span></strong></pre>
<ul>
<li>Here is a comparison using debug mode for a toy program:</li>
</ul>
<pre class="p1" style="padding-left: 60px"><strong><span class="s1">$ time performance_release_mode<br/></span><span class="s1">real<span class="Apple-tab-span"> </span>0m13.424s<br/></span><span class="s1">user<span class="Apple-tab-span"> </span>0m13.406s<br/></span><span class="s1">sys<span class="Apple-tab-span"> </span>0m0.010s</span></strong></pre>
<ul>
<li>The following is the release mode:</li>
</ul>
<pre class="p1" style="padding-left: 60px"><strong><span class="s1">$ time ./performance_release_mode<br/></span><span class="s1">real<span class="Apple-tab-span"> </span>0m0.316s<br/></span><span class="s1">user<span class="Apple-tab-span"> </span>0m0.309s<br/></span><span class="s1">sys<span class="Apple-tab-span"> </span>0m0.005s</span></strong></pre>
<p>Release mode is 98% more efficient with regard to CPU usage for this example.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Doing less work</h1>
                </header>
            
            <article>
                
<p>Faster programs do less. All optimization is a process of searching for work that doesn't need to be done, and then not doing it.</p>
<p>Similarly, the smallest programs fewer resources less. All space optimization is a process of searching for resources that don't need to be used, and then not using them.</p>
<p>For example, don't collect an iterator when you don't need the result, consider the following example:</p>
<pre class="p1" style="padding-left: 30px"><span class="s1">extern crate flame;<br/></span><span class="s1">use std::fs::File;<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   let v: Vec&lt;u64&gt; = vec![2; 1000000];<br/><br/></span><span class="s1">   flame::start("Iterator .collect");<br/></span><span class="s1">   let mut _z = vec![];<br/></span><span class="s1">   for _ in 0..1000 {<br/></span><span class="s1">      _z = v.iter().map(|x| x*x).collect::&lt;Vec&lt;u64&gt;&gt;();<br/></span><span class="s1">   }<br/></span><span class="s1">   flame::end("Iterator .collect");<br/><br/></span><span class="s1">   flame::start("Iterator iterate");<br/></span><span class="s1">   for _ in 0..1000 {<br/></span><span class="s1">      v.iter().map(|x| x * x).for_each(drop);<br/></span><span class="s1">   }<br/></span><span class="s1">   flame::end("Iterator iterate");<br/><br/></span><span class="s1">   flame::dump_html(&amp;mut File::create("flame-graph.html").unwrap()).unwrap();<br/></span><span class="s1">}</span></pre>
<p>Needlessly collecting the result of the iterator makes the code 27% slower compared to code that just drops the result.</p>
<p>Memory allocation is similar. Well-designed code preferring pure functions and avoiding side-effects will tend to minimize memory usage. In contrast, messy code can lead to old data hanging around. Rust memory safety does not extend to preventing memory leaks. Leaks are considered safe code:</p>
<pre class="p1"><span class="s1">use std::mem::forget;<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   for _ in 0..10000 {<br/></span><span class="s1">      let mut a = vec![2; 10000000];<br/></span><span class="s1">      a[2] = 2;<br/></span><span class="s1">      forget(a);<br/></span><span class="s1">   }<br/></span><span class="s1">}<br/></span></pre>
<p><span>The <kbd>forget</kbd> function is seldom used. Similarly, memory leaks are permitted but sufficiently discouraged that they are somewhat uncommon. Rust memory management tends to be such that by the time you cause a memory leak you are probably waist-deep in other poor design decisions.</span></p>
<p><span>However,</span> unsused <span>memory is not uncommon. If you don't keep track of what variables you are actively using, then old variables will likely remain in scope. This is not the typical definition of a memory leak; however, unused data is a similar waste of resources.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing the code that needs it – profiling</h1>
                </header>
            
            <article>
                
<p><span>Don't optimize code that doesn't need to be optimized. It's a waste of your time and probably poor software engineering. Save yourself the trouble and identify performance problems accurately before attempting to optimize the program.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">For a code rarely executed, performance is not affected</h1>
                </header>
            
            <article>
                
<p><span>It is very common that you will initialize some resource and use it multiple times. Optimizing</span> <kbd>initialization</kbd><span> of resources may be misdirected. You should consider focusing on improving the work efficiency. This is done as follows:</span></p>
<pre class="p1" style="padding-left: 30px"><span class="s1">use std::{thread,time};<br/></span><span class="s1"><br/>fn initialization() {<br/></span><span class="s1">   let t = time::Duration::from_millis(15000);<br/></span><span class="s1">   thread::sleep(t);<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn work() {<br/></span><span class="s1">   let t = time::Duration::from_millis(15000);<br/></span><span class="s1">   loop {<br/></span><span class="s1">      thread::sleep(t);<br/></span><span class="s1">      println!("Work.");<br/></span><span class="s1">   }<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   initialization();<br/></span><span class="s1">   println!("Done initializing, start work.");<br/></span><span class="s1">   work();<br/></span><span class="s1">}</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multiples of small numbers are also small numbers</h1>
                </header>
            
            <article>
                
<p><span>The reverse may also be true. Sometimes the low frequency of </span><kbd>work</kbd><span> is overwhelmed by frequent and expensive </span><kbd>initialization</kbd><span>. Knowing which problem you have will let you know where to start looking to improve:</span></p>
<pre class="p1" style="padding-left: 30px"><span class="s1">use std::{thread,time};<br/><br/></span><span class="s1">fn initialization() -&gt; Vec&lt;i32&gt; {<br/></span><span class="s1">   let t = time::Duration::from_millis(15000);<br/></span><span class="s1">   thread::sleep(t);<br/></span><span class="s1">   println!("Initialize data.");<br/></span><span class="s1">   vec![1, 2, 3];<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn work(x: i32) -&gt; i32 {<br/></span><span class="s1">   let t = time::Duration::from_millis(150);<br/></span><span class="s1">   thread::sleep(t);<br/></span><span class="s1">   println!("Work.");<br/></span><span class="s1">   x * x<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   for _ in 0..10 {<br/></span><span class="s1">      let data = initialization();<br/></span><span class="s1">      data.iter().map(|x| work(*x)).for_each(drop);<br/></span><span class="s1">   }<br/></span><span class="s1">}</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measuring first, to optimize it</h1>
                </header>
            
            <article>
                
<p>There are a lot of options for profiling. Here are some that we recommend.</p>
<p>The <kbd>flame</kbd> crate is one option to manually profile an application. Here we create the nested procedures <kbd>a</kbd>, <kbd>b</kbd>, and <kbd>c</kbd>. Each function creates a profiling context corresponding do that method. After running the profiler we will see proportionally how much time was spent for each call to each function.</p>
<p>Starting with function <kbd>a</kbd>, this procedure creates a new profiling context, sleeps for one second, then calls <kbd>b</kbd> three times:</p>
<pre class="p1"><span class="s1">extern crate flame;<br/></span><span class="s1">use std::fs::File;<br/></span><span class="s1">use std::{thread,time};<br/><br/></span><span class="s1">fn a() {<br/></span><span class="s1">   flame::start("fn a");<br/></span><span class="s1">   let t = time::Duration::from_millis(1000);<br/></span><span class="s1">   thread::sleep(t);<br/></span><span class="s1">   b();<br/></span><span class="s1">   b();<br/></span><span class="s1">   b();<br/></span><span class="s1">   flame::end("fn a");<br/></span><span class="s1">}</span></pre>
<p>Function <kbd>b</kbd> is nearly identical to <kbd>a</kbd>, and further calls into function <kbd>c</kbd>:</p>
<pre class="p1"><span class="s1">fn b() {<br/></span><span class="s1">   flame::start("fn b");<br/></span><span class="s1">   let t = time::Duration::from_millis(1000);<br/></span><span class="s1">   thread::sleep(t);<br/></span><span class="s1">   c();<br/></span><span class="s1">   c();<br/></span><span class="s1">   c();<br/></span><span class="s1">   flame::end("fn b");<br/></span><span class="s1">}</span></pre>
<p>Function <kbd>c</kbd> profiles itself and sleeps, but does not call any further nested function:</p>
<pre class="p1"><span class="s1">fn c() {<br/></span><span class="s1">   flame::start("fn c");<br/></span><span class="s1">   let t = time::Duration::from_millis(1000);<br/></span><span class="s1">   thread::sleep(t);<br/></span><span class="s1">   flame::end("fn c");<br/></span><span class="s1">}</span></pre>
<p>The <kbd>main</kbd> entrypoint sets up the flame graph library and calls a three times, then saves the flamegraph to a file:</p>
<pre class="p1"><span class="s1">fn main() {<br/></span><span class="s1">   flame::start("fn main");<br/></span><span class="s1">   let t = time::Duration::from_millis(1000);<br/></span><span class="s1">   thread::sleep(t);<br/></span><span class="s1">   a();<br/></span><span class="s1">   a();<br/></span><span class="s1">   a();<br/></span><span class="s1">   flame::end("fn main");<br/></span><span class="s1">   flame::dump_html(&amp;mut File::create("flame-graph.html").unwrap()).unwrap();<br/></span><span class="s1">}</span></pre>
<p>After running this program, the <kbd>flame-graph.html</kbd> file will contain a visualization of what program sections took what percentage of resources. The <kbd>flame</kbd> crate is easy to install, requires some manual code manipulation, but produces a cool-looking graph.</p>
<p><kbd>cargo profiler</kbd> is a tool that extends <kbd>cargo</kbd> to do performance profiling without any code changes. Here is a random program that we will profile:</p>
<pre class="p1"><span class="s1">fn a(n: u64) -&gt; u64 {<br/></span><span class="s1">   if n&gt;0 {<br/></span><span class="s1">      b(n);<br/></span><span class="s1">      b(n);<br/></span><span class="s1">   }<br/></span><span class="s1">   n * n<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn b(n: u64) -&gt; u64 {<br/></span><span class="s1">   c(n);<br/></span><span class="s1">   c(n);<br/></span><span class="s1">   n + 2 / 3<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn c(n: u64) -&gt; u64 {<br/></span><span class="s1">   a(n-1);<br/></span><span class="s1">   a(n-1);<br/></span><span class="s1">   vec![1, 2, 3].into_iter().map(|x| x+2).sum()<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   a(6);<br/></span><span class="s1">}</span></pre>
<p>To profile the application we run the following command:</p>
<pre class="p1"><span class="s1"><strong>$ cargo profiler callgrind --bin ./target/debug/performance_profiling4 -n 10</strong><br/></span></pre>
<p>This will run the program and collect information regarding which functions were most used. This profiler also has another option to profile memory usage. The output will look like the following:</p>
<pre class="p1"><strong><span class="s1">Profiling </span><span class="s2">performance_profiling4 with callgrind...<br/><br/></span><span class="s2">Total Instructions</span><span class="s3">...344,529,557<br/><br/></span><span class="s2">27,262,872 (</span><span class="s4">7.9%</span><span class="s2">) ???:core::iter::iterator::Iterator<br/></span><span class="s2">----------------------------------------------------------<br/></span><span class="s2">22,319,604 (</span><span class="s4">6.5%</span><span class="s2">) ???:&lt;alloc::vec<br/></span><span class="s2">----------------------------------------------------------<br/></span><span class="s2">16,627,356 (</span><span class="s4">4.8%</span><span class="s2">) ???:&lt;core::iter<br/></span><span class="s2">----------------------------------------------------------<br/></span><span class="s2">13,182,048 (</span><span class="s4">3.8%</span><span class="s2">) ???:&lt;alloc::vec<br/></span><span class="s2">----------------------------------------------------------<br/></span><span class="s2">10,785,312 (</span><span class="s4">3.1%</span><span class="s2">) ???:core::iter::iterator::Iterator::fold<br/>----------------------------------------------------------<br/></span><span class="s2">10,485,720 (</span><span class="s4">3.0%</span><span class="s2">) ???:core::mem<br/>----------------------------------------------------------<br/></span><span class="s2">8,088,984 (</span><span class="s4">2.3%</span><span class="s2">) ???:alloc::slice::hack<br/>----------------------------------------------------------<br/></span><span class="s2">7,639,596 (</span><span class="s4">2.2%</span><span class="s2">) ???:core::ptr<br/>----------------------------------------------------------<br/></span><span class="s2">7,190,208 (</span><span class="s4">2.1%</span><span class="s2">) ???:core::ptr<br/>----------------------------------------------------------<br/></span><span class="s2">7,190,016 (</span><span class="s4">2.1%</span><span class="s2">) ???:performance_profiling4</span></strong></pre>
<p>This clearly<span> shows us </span>that the most time is spent in iterator and vector creation. Running this command may make the program execute much more slowly than normal, but it also saves writing any code before profiling.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting the fridge next to the computer</h1>
                </header>
            
            <article>
                
<p>If you take a snack break while coding, then it would be convenient to have a fridge and microwave next to the computer. If you travel to the kitchen for a snack, then it will take a little longer to satisfy your appetite. If your kitchen is empty and you need to make a grocery run, then the break is even further extended. If your grocery store is empty and you need to drive to a farm to harvest vegetables, then your work environment is clearly not designed for snacking purposes.</p>
<p>This strange analogy illustrates the necessary trade-off between time and space. This relation is not quite a physical law for our purposes, but almost. The rule is that traveling, or communicating, over longer distances is directly proportional to time spent. More distance (d) in one direction also means an increase in available space of quadratic (d<sup>2</sup>) or cubic (d<sup>3</sup>) scale. In other words building the fridge farther away provides more space for a larger fridge.</p>
<p>Bringing this story back to a technical context, here are some latency numbers that every programmer should know (~2012: <a href="https://gist.github.com/jboner/2841832">https://gist.github.com/jboner/2841832</a>):</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 585px"><strong>Request</strong></td>
<td style="width: 146px"><strong>Time</strong></td>
</tr>
<tr>
<td style="width: 585px">L1 cache reference</td>
<td style="width: 146px">0.5 ns</td>
</tr>
<tr>
<td style="width: 585px">Branch mispredict</td>
<td style="width: 146px">5 ns</td>
</tr>
<tr>
<td style="width: 585px">L2 cache reference</td>
<td style="width: 146px">7 ns</td>
</tr>
<tr>
<td style="width: 585px">Mutex lock/unlock</td>
<td style="width: 146px">25 ns</td>
</tr>
<tr>
<td style="width: 585px">Main memory reference</td>
<td style="width: 146px">100 ns</td>
</tr>
<tr>
<td style="width: 585px">Compress 1 Kb with Zippy</td>
<td style="width: 146px">3000 ns</td>
</tr>
<tr>
<td style="width: 585px">Send 1 Kb over 1 Gbps network</td>
<td style="width: 146px">10000 ns</td>
</tr>
<tr>
<td style="width: 585px">Read 4 Kb randomly from SSD</td>
<td style="width: 146px">150000 ns</td>
</tr>
<tr>
<td style="width: 585px">Read 1 Mb sequentially from memory</td>
<td style="width: 146px">250000 ns</td>
</tr>
<tr>
<td style="width: 585px"><span>Round trip within same datacenter</span></td>
<td style="width: 146px">500000 ns</td>
</tr>
<tr>
<td style="width: 585px"><span>Send packet CA | Netherlands | CA</span></td>
<td style="width: 146px">150000000 ns</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Here, we can see in specific numbers that if you want a donut and some coffee then you could eat 30<span>0,000,000 donuts from the fridge next to your computer before taking your first bite from a Danish.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Capping the Big O</h1>
                </header>
            
            <article>
                
<p>Big <em>O</em> notation is a computer science term used to group functions with respect to how fast they grow as the input value gets larger. This term is most often used with respect to algorithm runtime or space requirement.</p>
<p>When using this term in software engineering, we are usually concerned with one of these four cases:</p>
<ul>
<li>Constant</li>
<li>Logarithmic growth</li>
<li>Polynomial growth</li>
<li>Exponential growth</li>
</ul>
<p>When we are concerned with application performance, it is good to consider the Big <em>O</em> efficiency of the logic you are using. Depending on which of the preceding four cases you are dealing with, the appropriate response to optimization strategies may change.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constanting no growth</h1>
                </header>
            
            <article>
                
<p>Constant time operations are the indivisible units of runtime performance. In the previous section, we provided a table of common operations and how long each one takes. These are, for our purposes as programmers, basically physical constants. You can't optimize the speed of light to make it go faster.</p>
<p>Not all constant time operations are irreducible, however. If you have a procedure that does a fixed number of operations on fixed-size data, then it will be constant time. That does not mean that the procedure is automatically efficient. When trying to optimize constant time procedures, ask yourself these two questions:</p>
<ul>
<li>Can any of the work be avoided?</li>
<li>Is the fridge too far from the computer?</li>
</ul>
<p>Here is a program consisting of emphasizing constant time operations:</p>
<pre class="p1"><span class="s1">fn allocate() -&gt; [u64; 1000] {<br/></span><span class="s1">   [22; 1000]<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn flop(x: f64, y: f64) -&gt; f64 {<br/></span><span class="s1">   x * y<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn lookup(x: &amp;[u64; 1000]) -&gt; u64 {<br/></span><span class="s1">   x[234] * x[345]<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   let mut data = allocate();<br/></span><span class="s1">   for _ in 0..1000 {<br/></span><span class="s1">      //constant size memory allocation<br/></span><span class="s1">      data = allocate();<br/></span><span class="s1">   }<br/><br/></span><span class="s1">   for _ in 0..1000000 {<br/></span><span class="s1">      //reference data<br/></span><span class="s1">      lookup(&amp;data);<br/></span><span class="s1">   }<br/><br/></span><span class="s1">   for _ in 0..1000000 {<br/></span><span class="s1">      //floating point operation<br/></span><span class="s1">      flop(2.0, 3.0);<br/></span><span class="s1">   }<br/></span><span class="s1">}</span></pre>
<p>Then, let's profile this program:</p>
<pre class="p1"><strong><span class="s1">Profiling </span><span class="s2">performance_constant with callgrind...<br/><br/></span><span class="s2">Total Instructions</span><span class="s3">...896,049,080<br/><br/></span><span class="s2">217,133,740 (</span><span class="s4">24.2%</span><span class="s2">) ???:_platform_memmove$VARIANT$Haswell<br/></span><span class="s2">-----------------------------------------------------------<br/></span><span class="s2">108,054,000 (</span><span class="s4">12.1%</span><span class="s2">) ???:core::ptr<br/></span><span class="s2">-----------------------------------------------------------<br/></span><span class="s2">102,051,069 (</span><span class="s4">11.4%</span><span class="s2">) ???:core::iter::range<br/></span><span class="s2">-----------------------------------------------------------<br/></span><span class="s2">76,038,000 (</span><span class="s4">8.5%</span><span class="s2">) ???:&lt;i32<br/></span><span class="s2">-----------------------------------------------------------<br/></span><span class="s2">56,028,000 (</span><span class="s4">6.3%</span><span class="s2">) ???:core::ptr<br/></span><span class="s2">-----------------------------------------------------------<br/></span><span class="s2">46,023,000 (</span><span class="s4">5.1%</span><span class="s2">) ???:core::iter::range::ptr_try_from_impls<br/></span><span class="s2">-----------------------------------------------------------<br/></span><span class="s2">45,027,072 (</span><span class="s4">5.0%</span><span class="s2">) ???:performance_constant<br/></span><span class="s2">-----------------------------------------------------------<br/></span><span class="s2">44,022,000 (</span><span class="s4">4.9%</span><span class="s2">) ???:core::ptr<br/></span><span class="s2">-----------------------------------------------------------<br/></span><span class="s2">40,020,000 (</span><span class="s4">4.5%</span><span class="s2">) ???:core::mem<br/></span><span class="s2">-----------------------------------------------------------<br/></span><span class="s2">30,015,045 (</span><span class="s4">3.3%</span><span class="s2">) ???:core::cmp::impls</span></strong></pre>
<p>We see that the heavy memory allocation is fairly expensive. As for the memory access and floating point calculation, it is seemingly overwhelmed by the expense of the loop that executes them multiple times. Unless there is a clear culprit for poor performance in a constant time procedure, then optimizing this code may not be straightforward.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logarithmic growth</h1>
                </header>
            
            <article>
                
<p>Logarithmic algorithms are the pride of computer science. If your <em>O</em>(<em>n</em>) for <em>n</em>=5 code could have been written with an <em>O</em>(<em>log n</em>) algorithm, then surely at least one person will point this out.</p>
<p>A binary search is O(<em>log n</em>). A sort is typically <em>O</em>(<em>n log n</em>). Everything with a log in it is better. This fondness is not misplaced. Logarithmic growth has an amazing property—growth slows down as the input value increases.</p>
<p>Here is a program emphasizing logarithmic growth. We initialize a vector with random numbers having size of 1000 or 10000. Then we use the builtin library to sort and perform 100 binary search operations. First let's capture the time for sort and search for the 1000 case:</p>
<pre class="p1"><span class="s1">extern crate rand;<br/></span><span class="s1">extern crate flame;<br/></span><span class="s1">use std::fs::File;<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   let mut data = vec![0; 1000];<br/></span><span class="s1">   for di in 0..data.len() {<br/></span><span class="s1">      data[di] = rand::random::&lt;u64&gt;();<br/></span><span class="s1">   }<br/><br/></span><span class="s1">   flame::start("sort n=1000");<br/></span><span class="s1">   data.sort();<br/></span><span class="s1">   flame::end("sort n=1000");<br/><br/></span><span class="s1">   flame::start("binary search n=1000 100 times");<br/></span><span class="s1">   for _ in 0..100 {<br/></span><span class="s1">      let c = rand::random::&lt;u64&gt;();<br/></span><span class="s1">      data.binary_search(&amp;c).ok();<br/></span><span class="s1">   }<br/></span><span class="s1">   flame::end("binary search n=1000 100 times");</span></pre>
<p>Now we profile the 10000 case:</p>
<pre class="p1"><span class="s1">   let mut data = vec![0; 10000];<br/></span><span class="s1">   for di in 0..data.len() {<br/></span><span class="s1">      data[di] = rand::random::&lt;u64&gt;();<br/></span><span class="s1">   }<br/><br/></span><span class="s1">   flame::start("sort n=10000");<br/></span><span class="s1">   data.sort();<br/></span><span class="s1">   flame::end("sort n=10000");<br/><br/></span><span class="s1">   flame::start("binary search n=10000 100 times");<br/></span><span class="s1">   for _ in 0..100 {<br/></span><span class="s1">      let c = rand::random::&lt;u64&gt;();<br/></span><span class="s1">      data.binary_search(&amp;c).ok();<br/></span><span class="s1">   }<br/></span><span class="s1">   flame::end("binary search n=10000 100 times");<br/></span><span class="s1"><br/>   flame::dump_html(&amp;mut File::create("flame-graph.html").unwrap()).unwrap();<br/></span><span class="s1">}</span></pre>
<p>After running this and examining the flamegraphs, we can see that sorting for a vector that is 10 times larger takes barely 10 times as much time—<kbd>O(n log n)</kbd>. Search performance is hardly affected at all—<kbd>O(log n)</kbd>. So for practical uses, logarithmic growth is almost negligible.</p>
<p>When trying to optimize logarithmic code, follow the same approach as for constant time optimization. Logarithmic complexity is usually not a good target for optimization, particularly considering that logarithmic complexity is a strong indicator of good algorithm design.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Polynomial growth</h1>
                </header>
            
            <article>
                
<p>Most algorithms are polynomial.</p>
<p>If you have one <kbd>for</kbd> loop, then your complexity is <em>O</em>(<em>n</em>). This is shown in the following code:</p>
<pre class="p1"><span class="s1">fn main() {<br/></span><span class="s1">   for _ in 0..1000 {<br/></span><span class="s1">      //O(n)<br/></span><span class="s1">      //n = 1000<br/></span><span class="s1">   }<br/></span><span class="s1">}</span></pre>
<p>If you have two <kbd>for</kbd> loops, then your complexity is <em>O</em>(<em>n</em><sup>2</sup>):</p>
<pre class="p1"><span class="s1">fn main() {<br/></span><span class="s1">   for _ in 0..1000 {<br/></span><span class="s1">      for _ in 0..1000 {<br/></span><span class="s1">         //O(n^2)<br/></span><span class="s1">         //n = 1000<br/></span><span class="s1">      }<br/></span><span class="s1">   }<br/></span><span class="s1">}</span></pre>
<p>Higher polynomials are somewhat less common. Sometimes code accidentally becomes a higher polynomial, which you should be careful about; otherwise, let's just consider both the previous cases.</p>
<p>Linear complexity is very common. Any time you process the entirety of data in a collection, the complexity will be linear. The running time of a linear algorithm will be approximately the number of items (<em>n</em>) processed, multiplied by the time to process individual items (<em>c</em>). If you want to make a linear algorithm go faster, you need to:</p>
<ul>
<li>Reduce the number of items processed (n)</li>
<li>Reduce the constant time associated with processing an item (<em>c</em>)</li>
</ul>
<p>If the time to process an item is not constant or approximately constant, then your overall time complexity is now recursively dependent on that processing time. This is shown with the following code:</p>
<pre class="p1"><span class="s1">fn a(n: u64) {<br/></span><span class="s1">   //Is this O(n)?<br/></span><span class="s1">   for _ in 0..n {<br/></span><span class="s1">      b(n)<br/></span><span class="s1">   }<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn b(n: u64) {<br/></span><span class="s1">   //Is this O(n)?<br/></span><span class="s1">   for _ in 0..n {<br/></span><span class="s1">      c(n)<br/></span><span class="s1">   }<br/></span><span class="s1">}<br/><br/>fn c(n: u64) {<br/></span><span class="s1">   //This is O(n)<br/></span><span class="s1">   for _ in 0..n {<br/></span><span class="s1">      let _ = 1 + 1;<br/></span><span class="s1">   }<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   //What time complexity is this?<br/></span><span class="s1">   a(1000)<br/></span><span class="s1">}</span></pre>
<p>Higher polynomial complexity is also common but may indicate that your algorithm is poorly designed. In the preceding description, we mentioned that the linear processing time can become dependent on the time to process individual items. If your program is designed carelessly, then it is very easy to string together three or four linear algorithms and unintentionally create an <em>O</em>(<em>n</em><sup>4</sup>) monster.</p>
<p>Higher polynomials are proportionally slower. In the case of algorithms that naively require high polynomial calculations, it is often the case that the algorithm can be pruned to remove calculations that are redundant or entirely unnecessary. Consider the following code:</p>
<pre class="p1"><span class="s1">extern crate rusty_machine;<br/></span><span class="s1">use rusty_machine::linalg::{Matrix,Vector};<br/></span><span class="s1">use rusty_machine::learning::gp::{GaussianProcess,ConstMean};<br/></span><span class="s1">use rusty_machine::learning::toolkit::kernel;<br/></span><span class="s1">use rusty_machine::learning::SupModel;<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   let inputs = Matrix::new(3,3,vec![1.1,1.2,1.3,2.1,2.2,2.3,3.1,3.2,3.3]);<br/></span><span class="s1">   let targets = Vector::new(vec![0.1,0.8,0.3]);<br/></span><span class="s1">   let test_inputs = Matrix::new(2,3, vec![1.2,1.3,1.4,2.2,2.3,2.4]);<br/></span><span class="s1">   let ker = kernel::SquaredExp::new(2., 1.);<br/></span><span class="s1">   let zero_mean = ConstMean::default();<br/></span><span class="s1">   let mut gp = GaussianProcess::new(ker, zero_mean, 0.5);<br/><br/></span><span class="s1">   gp.train(&amp;inputs, &amp;targets).unwrap();<br/></span><span class="s1">   let _ = gp.predict(&amp;test_inputs).unwrap();<br/></span><span class="s1">}</span></pre>
<p>When you need to use higher polynomial algorithms, use a library! This stuff gets complicated fast and improving these algorithms is the main job of academic Computer Scientists. If you are performance-tuning a common algorithm and not expecting to publish your results, then you may likely be duplicating work.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exponential growth</h1>
                </header>
            
            <article>
                
<p>Exponential performance in engineering is almost always a bug or a dead end. This is the wall that separates algorithms that we use from algorithms that we would like to use but can't due to performance reasons.</p>
<p>Exponential growth in programs is often accompanied by the term <kbd>bomb</kbd>:</p>
<pre class="p1"><span class="s1">fn bomb(n: u64) -&gt; u64 {<br/></span><span class="s1">   if n &gt; 0 {<br/></span><span class="s1">      bomb(n-1);<br/></span><span class="s1">      bomb(n-1);<br/></span><span class="s1">   }<br/></span><span class="s1">   n<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   bomb(1000);<br/></span><span class="s1">}</span></pre>
<p>This program is only <em>O</em>(2<sup>n</sup>) and therefore barely even exponential!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Referencing data is faster</h1>
                </header>
            
            <article>
                
<p>There is a rule of thumb that referencing data is faster than copying data. Similarly, copying data is faster than cloning. This is not always true, but it is a good rule to consider when trying to improve program performance.</p>
<p>Here is a function that alternatively uses data by reference, copied, intrinsic cloned, or custom cloned:</p>
<pre class="p1"><span class="s1">extern crate flame;<br/></span><span class="s1">use std::fs::File;<br/><br/></span><span class="s1">fn byref(n: u64, data: &amp;[u64; 1024]) {<br/></span><span class="s1">   if n&gt;0 {<br/></span><span class="s1">      byref(n-1, data);<br/></span><span class="s1">      byref(n-1, data);<br/></span><span class="s1">   }<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn bycopy(n: u64, data: [u64; 1024]) {<br/></span><span class="s1">   if n&gt;0 {<br/></span><span class="s1"><span class="Apple-converted-space">      </span>bycopy(n-1, data);<br/></span><span class="s1">      bycopy(n-1, data);<br/></span><span class="s1">   }<br/></span><span class="s1">}<br/><br/></span><span class="s1">struct DataClonable([u64; 1024]);<br/></span><span class="s1">impl Clone for DataClonable {<br/></span><span class="s1">   fn clone(&amp;self) -&gt; Self {<br/></span><span class="s1">      let mut newdata = [0; 1024];<br/></span><span class="s1">      for i in 0..1024 {<br/></span><span class="s1">         newdata[i] = self.0[i];<br/></span><span class="s1">      }<br/></span><span class="s1">      DataClonable(newdata)<br/></span><span class="s1">   }<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn byclone&lt;T: Clone&gt;(n: u64, data: T) {<br/></span><span class="s1">   if n&gt;0 {<br/></span><span class="s1">      byclone(n-1, data.clone());<br/></span><span class="s1">      byclone(n-1, data.clone());<br/></span><span class="s1">   }<br/></span><span class="s1">}</span></pre>
<p>Here we declare array of <kbd>1024</kbd> elements. Then using the flamegraph profiling library we apply the above functions to measure the differences between reference, copy and clone performance:</p>
<pre class="p1"><span class="s1">fn main() {<br/></span><span class="s1">   let data = [0; 1024];<br/></span><span class="s1">   flame::start("by reference");<br/></span><span class="s1">   byref(15, &amp;data);<br/></span><span class="s1">   flame::end("by reference");<br/><br/></span><span class="s1">   let data = [0; 1024];<br/></span><span class="s1">   flame::start("by copy");<br/></span><span class="s1">   bycopy(15, data);<br/></span><span class="s1">   flame::end("by copy");<br/><br/></span><span class="s1">   let data = [0; 1024];<br/></span><span class="s1">   flame::start("by clone");<br/></span><span class="s1">   byclone(15, data);<br/></span><span class="s1">   flame::end("by clone");<br/><br/></span><span class="s1">   let data = DataClonable([0; 1024]);<br/></span><span class="s1">   flame::start("by clone (with extras)");<br/></span><span class="s1">   //2^4 instead of 2^15!!!!<br/></span><span class="s1">   byclone(4, data);<br/></span><span class="s1">   flame::end("by clone (with extras)");<br/><br/></span><span class="s1">   flame::dump_html(&amp;mut File::create("flame-graph.html").unwrap()).unwrap();<br/></span><span class="s1">}</span></pre>
<p>Looking at the runtime of this application, we see that the referenced data uses only a small sliver of the resources compared to copying or cloning this data. The default clone and copy traits unsurprisingly give a similar performance. The custom clone is really slow. It does semantically the same thing as all the others, but it is not as optimized at a low level.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preventing bugs with defensive coding</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">You don’t need to fix bugs that never happen. Preventative medicine is good software engineering that will save you time in the long run.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Option and Result instead of panic!</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">In many other languages, exception handling is performed through <kbd>try…catch</kbd> blocks. Rust does not automatically provide this functionality, instead it encourages the programmer to explicitly localize all error handling.</span></p>
<p class="p1"><span class="s1">In many Rust contexts, if you don’t want to deal with error handling, you always have the option to use <kbd>panic!</kbd>. This will immediately end the program and provide a short error message. Don't do this. Panicking is usually just a way of avoiding the responsibility of handling errors.</span></p>
<p class="p1"><span class="s1">Instead, use either the <kbd>Option</kbd> or <kbd>Result</kbd> types to communicate error or exceptional conditions. <kbd>Option</kbd> indicates that no value is available. The <kbd>None</kbd> value of <kbd>Option</kbd> should indicate that there is no value but that everything is okay and expected.</span></p>
<p class="p1"><span class="s1">The <kbd>Result</kbd> type is used to communicate whether or not there was an error in processing. <kbd>Result</kbd> types can be used in combination with the <kbd>?</kbd> syntax to propagate errors while avoiding introducing too much extra syntax. The <kbd>?</kbd> operation will return errors from the function, if any, and therefore the function must have a <kbd>Result</kbd> return type.</span></p>
<p class="p1"><span class="s1">Here we create two functions that return <kbd>Option</kbd> or <kbd>Result</kbd> to handle exceptional circumstances. Note the use of the try <kbd>?</kbd> syntax when handling <kbd>Result</kbd> return values. This syntax will pass through <kbd>Ok</kbd> values or immediately return any <kbd>Err</kbd> from that function. For this reason, any function using ? must also return a compatible <kbd>Result</kbd> type:</span></p>
<pre class="p1"><span class="s1">//This function returns an Option if the value is not expected<br/>fn expect_1or2or_other(n: u64) -&gt; Option&lt;u64&gt; {<br/></span><span class="s1">   match n {<br/></span><span class="s1">      1|2 =&gt; Some(n),<br/></span><span class="s1">      _ =&gt; None<br/></span><span class="s1">   }<br/></span><span class="s1">}<br/><br/>//This function returns an Err if the value is not expected<br/></span><span class="s1">fn expect_1or2or_error(n: u64) -&gt; Result&lt;u64,()&gt; {<br/></span><span class="s1">   match n {<br/></span><span class="s1">      1|2 =&gt; Ok(n),<br/></span><span class="s1">      _ =&gt; Err(())<br/></span><span class="s1">   }<br/></span><span class="s1">}<br/><br/>//This function uses functions that return Option and Return types<br/></span><span class="s1">fn mixed_1or2() -&gt; Result&lt;(),()&gt; {<br/></span><span class="s1">   expect_1or2or_other(1);<br/></span><span class="s1">   expect_1or2or_other(2);<br/></span><span class="s1">   expect_1or2or_other(3);<br/><br/></span><span class="s1">   expect_1or2or_error(1)?;<br/></span><span class="s1">   expect_1or2or_error(2)?;<br/></span><span class="s1">   expect_1or2or_error(3).unwrap_or(222);<br/></span><span class="s1">   Ok(())<br/></span><span class="s1">}<br/><br/>fn main() {<br/></span><span>   mixed_1or2().expect("mixed 1 or 2 is OK.");<br/>}</span></pre>
<p><kbd>Result</kbd> types are very common when interacting with external resources such as files:</p>
<pre class="p1"><span class="s1">use std::fs::File;<br/></span><span class="s1">use std::io::prelude::*;<br/></span><span class="s1">use std::io;<br/><br/></span><span class="s1">fn lots_of_io() -&gt; io::Result&lt;()&gt; {<br/></span><span class="s1">   {<br/></span><span class="s1">      let mut file = File::create("data.txt")?;<br/></span><span class="s1">      file.write_all(b"data\ndata\ndata")?;<br/></span><span class="s1">   }<br/><br/></span><span class="s1">   {<br/></span><span class="s1">      let mut file = File::open("data.txt")?;<br/></span><span class="s1">      let mut data = String::new();<br/></span><span class="s1">      file.read_to_string(&amp;mut data)?;<br/></span><span class="s1">      println!("{}", data);<br/></span><span class="s1">   }<br/></span><span class="s1">   Ok(())<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   lots_of_io().expect("lots of io is OK.");<br/></span><span class="s1">}</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using typesafe interfaces instead of stringly typed interfaces</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">Enumerations in Rust are less error-prone than using numbers or strings. Whenever possible, write the following code:</span></p>
<pre class="p1"><span class="s1">const MyEnum_A: u32 = 1;<br/></span><span class="s1">const MyEnum_B: u32 = 2;<br/></span><span class="s1">const MyEnum_C: u32 = 3;</span></pre>
<p class="p1"><span class="s1">Similarly, you can write a stringly enumeration:</span></p>
<pre class="p1"><span class="s1">"a"<br/>"b"<br/>"c"</span></pre>
<p class="p1"><span class="s1">It is better to use the following enum type:</span></p>
<pre class="p1"><span class="s1">enum MyEnum {<br/></span><span class="s1">   A,<br/></span><span class="s1">   B,<br/></span><span class="s1">   C,<br/></span><span class="s1">}</span></pre>
<p class="p1"><span class="s1">This way, functions accepting the enumeration will be typesafe:</span></p>
<pre class="p1"><span class="s1">fn foo(n: u64) {} //not all u64 are valid inputs<br/></span><span class="s1">fn bar(n: &amp;str) {} //not all &amp;str are valid inputs<br/></span><span class="s1">fn baz(n: MyEnum) {} //all MyEnum are valid</span></pre>
<p class="p1"><span class="s1">Enums also fit naturally with pattern matching for the same reason. Pattern matching against an enumeration does not require a final error case like the integer or string typed case would:</span></p>
<pre class="p1"><span class="s1">match a {<br/></span><span class="s1">   1 =&gt; println!(“1 is ok”),<br/></span><span class="s1">   2 =&gt; println!(“2 is ok”),<br/></span><span class="s1">   3 =&gt; println!(“3 is ok”),<br/></span><span class="s1">   n =&gt; println!(“{} was unexpected”, n)<br/></span><span class="s1">}</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the heartbeat pattern for long running processes</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">When you want to create a long running process, it is nice to be able to recover from program errors that crash or terminate the process. Perhaps the process runs out of stack space or encounters a <kbd>panic!</kbd> from some code path. For any number of reasons, a process might get terminated and will need to be restarted.</span></p>
<p class="p1"><span class="s1">To accommodate this desire, there are many tools that will watch a program for you and restart it if it dies or stops responding to health checks. Here, we recommend a completely self-contained version of this pattern that is based on Rust concurrency.</span></p>
<p class="p1"><span class="s1">The goal is to create a parent process that acts as a monitor and oversees one or more workers. The process tree should look something like this:</span></p>
<pre class="p1"><span class="s1">parent<br/></span><span class="s1"> —- child 1<br/></span><span class="s1"> —- child 2<br/></span><span class="s1"> —- child 3</span></pre>
<p class="p1"><span class="s1">When a child dies or stops responding to health checks, the parent should kill or otherwise clean up the process resources, then start a new process to replace it. Here is an example of this behavior, starting with a subprocess that sometimes dies:</span></p>
<pre class="p1"><span class="s1">use std::{thread,time,process};<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   let life_expectancy = process::id() % 8;<br/></span><span class="s1">   let t = time::Duration::from_millis(1000);<br/></span><span class="s1">   for _ in 0..life_expectancy {<br/></span><span class="s1">      thread::sleep(t);<br/></span><span class="s1">   }<br/></span><span class="s1">   println!("process {} dies unexpectedly.", process::id());<br/></span><span class="s1">}</span></pre>
<p>This worker process is highly unreliable and lives no longer than eight seconds. However, if we wrap it with a heartbeat monitor, then we can make it more reliable:</p>
<pre class="p1"><span class="s1">use std::process::Command;<br/></span><span class="s1">use std::env::current_exe;<br/></span><span class="s1">use std::{thread,time};<br/><br/></span><span class="s1">fn main() {<br/>   //There is an executable called debugging_buggy_worker<br/>   //it crashes a lot but we still want to run it<br/></span><span class="s1">   let path = current_exe()<br/></span><span class="s1">             .expect("could not find current executable");<br/></span><span class="s1">   let path = path.with_file_name("debugging_buggy_worker");<br/></span><span class="s1">   let mut children = Vec::new();<br/><br/>   //we start 3 workers<br/></span><span class="s1">   for _ in 0..3 {<br/></span><span class="s1">      children.push(<br/></span><span class="s1">         Command::new(path.as_os_str())<br/></span><span class="s1">                 .spawn()<br/></span><span class="s1"><span class="Apple-converted-space">                 </span>.expect("failed to spawn child")<br/></span><span class="s1">      );<br/></span><span class="s1">   }<br/><br/>   //those workers will randomly die because they are buggy<br/>   //so after they die, we restart a new process to replace them<br/></span><span class="s1"><span class="Apple-converted-space">   </span>let t = time::Duration::from_millis(1000);<br/></span><span class="s1">   loop {<br/></span><span class="s1">      thread::sleep(t);<br/></span><span class="s1">      for ci in 0..children.len() {<br/></span><span class="s1">         let is_dead = children[ci].try_wait().expect("failed to try_wait");<br/></span><span class="s1">         if let Some(_exit_code) = is_dead {<br/></span><span class="s1">            children[ci] = Command::new(path.as_os_str())<br/></span><span class="s1">                                   .spawn()<br/></span><span class="s1">                                   .expect("failed to spawn child");<br/></span><span class="s1">            println!("starting a new process from parent.");<br/></span><span class="s1">         }<br/></span><span class="s1">      }<br/></span><span class="s1">   }<br/></span><span class="s1">}</span></pre>
<p>Now, the running processes will get restarted if they die unexpectedly. Optionally, the parent can check the health status of each child process and restart unresponsive workers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Validating input and output</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">Preconditions and postconditions are a great way to lock down program behavior and find bugs or invalid states before they get out of hand.</span></p>
<p class="p1"><span class="s1">If you use macros to do this, then the preconditions and postconditions can optionally be run only in debug mode, and removed from production code. The built-in <kbd>debug_assert!</kbd> macro does this. However, using assertions for return values is not particularly elegant and, if you forget to check a branch with a return statement, then your postcondition won't be checked.</span></p>
<p class="p1"><span class="s1"><kbd>debug_assert!</kbd> is not a good choice for the validation of anything dependent on external data or otherwise nondeterministic behavior. When you want to check preconditions or postconditions in production code, you should instead use <kbd>Result</kbd> or <kbd>Option</kbd> values to handle exceptional behavior.</span></p>
<p class="p1"><span class="s1">Here are some examples of preconditions and postconditions in Rust:</span></p>
<pre class="p1"><span class="s1">use std::io;<br/><br/>//This function checks the precondition that [n &lt; 100]<br/></span><span class="s1">fn debug_precondition(n: u64) -&gt; u64 {<br/></span><span class="s1">   debug_assert!(n &lt; 100);<br/></span><span class="s1">   n * n<br/></span><span class="s1">}<br/><br/>//This function checks the postcondition that [return &gt; 10]<br/></span><span class="s1">fn debug_postcondition(n: u64) -&gt; u64 {<br/></span><span class="s1">   let r = n * n;<br/></span><span class="s1">   debug_assert!(r &gt; 10);<br/></span><span class="s1">   r<br/></span><span class="s1">}<br/><br/>//this function dynamically checks the precondition [n &lt; 100]<br/></span><span class="s1">fn runtime_precondition(n: u64) -&gt; Result&lt;u64,()&gt; {<br/></span><span class="s1">   if !(n&lt;100) { return Err(()) };<br/></span><span class="s1">   Ok(n * n)<br/></span><span class="s1">}<br/><br/>//this function dynamically checks the postcondition [return &gt; 10]<br/></span><span class="s1">fn runtime_postcondition(n: u64) -&gt; Result&lt;u64,()&gt; {<br/></span><span class="s1">   let r = n * n;<br/></span><span class="s1">   if !(r&gt;10) { return Err(()) };<br/></span><span class="s1">   Ok(r)<br/></span><span class="s1">}<br/><br/>//This main function uses all of the functions<br/>//The dynamically validated functions are subjected to user input<br/></span><span class="s1">fn main() {<br/></span><span class="s1">   //inward facing code should assert expectations<br/></span><span class="s1">   debug_precondition(5);<br/></span><span class="s1">   debug_postcondition(5);<br/><br/></span><span class="s1">   //outward facing code should handle errors<br/></span><span class="s1">   let mut s = String::new();<br/></span><span class="s1">   println!("Please input a positive integer greater or equal to 4:");<br/></span><span class="s1">   io::stdin().read_line(&amp;mut s).expect("error reading input");<br/></span><span class="s1">   let i = s.trim().parse::&lt;u64&gt;().expect("error parsing input as integer");<br/></span><span class="s1">   runtime_precondition(i).expect("runtime precondition violated");<br/></span><span class="s1">   runtime_postcondition(i).expect("runtime postcondition violated");<br/></span><span class="s1">}</span></pre>
<p>Notice that the user input is out of our control. The best option for validating user input is to return an <kbd>Error</kbd> condition if the input is invalid.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding and fixing bugs</h1>
                </header>
            
            <article>
                
<p>Debugging tools are quite platform dependent. Here we will explain <kbd>lldb</kbd>, which is available, and macOS and other Unix-like systems.</p>
<p>To start debugging, you will need to compile the program with debugging symbols turned on. The normal <kbd>cargo debug build</kbd> is usually sufficient:</p>
<pre><strong>cargo build</strong></pre>
<p>After the program has been compiled, start the debugger:</p>
<pre class="p1"><strong><span class="s1">$ sudo rust-lldb target/debug/deps/performance_polynomial3-8048e39c94dd7157</span></strong></pre>
<p>Here we reference the <kbd>debugs/deps/program_name-GITHASH</kbd> copy of the program. This is necessary for now just because of how lldb works.</p>
<p>After running <kbd>lldb</kbd>, you will see some information scroll past on startup. Then, you should be dropped into a LLDB Command Prompt:</p>
<pre class="p1"><span class="s1">(lldb) command source -s 0 '/tmp/rust-lldb-commands.YnRBkV'<br/></span><span class="s1">Executing commands in '/tmp/rust-lldb-commands.YnRBkV'.<br/></span><span class="s1">(lldb) command script import "/Users/andrewjohnson/.rustup/toolchains/nightly-x86_64-apple-darwin/lib/rustlib/etc/lldb_rust_formatters.py"<br/></span><span class="s1">(lldb) type summary add --no-value --python-function lldb_rust_formatters.print_val -x ".*" --category Rust<br/></span><span class="s1">(lldb) type category enable Rust<br/></span><span class="s1">(lldb) target create "target/debug/deps/performance_polynomial3-8048e39c94dd7157"<br/></span><span class="s1">Current executable set to 'target/debug/deps/performance_polynomial3-8048e39c94dd7157' (x86_64).<br/>(lldb)<br/></span></pre>
<p>Now, set a breakpoint. We will set a breakpoint to stop at function <kbd>a</kbd>:</p>
<pre class="p1"><span class="s1">(lldb) </span><span class="s2">b a<br/></span><span class="s1">Breakpoint 1: where = performance_polynomial3-8048e39c94dd7157`performance_polynomial3::a::h0b267f360bbf8caa + 12 at performance_polynomial3.rs:3, address = 0x000000010000191c</span></pre>
<p>Now that our breakpoint is set, run the <kbd>r</kbd> command:</p>
<pre class="p1"><span class="s1">(lldb) </span><span class="s2">r<br/></span><span class="s1">Process 99468 launched: '/Users/andrewjohnson/subarctic.org/subarctic.org/Hands-On-Functional-Programming-in-RUST/Chapter09/target/debug/deps/performance_polynomial3-8048e39c94dd7157' (x86_64)<br/></span><span class="s1">Process 99468 stopped<br/></span><span class="s1">* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1<br/></span><span class="s1">  frame #0: 0x000000010000191c performance_polynomial3-8048e39c94dd7157`performance_polynomial3::a::h0b267f360bbf8caa(n=1000) at performance_polynomial3.rs:3<br/></span><span class="s1">   1 <span class="Apple-converted-space">  <span class="Apple-tab-span"> </span></span>fn a(n: u64) {<br/></span><span class="s1">   2<span class="Apple-converted-space">   <span class="Apple-tab-span"> </span>   </span>//Is this O(n);<br/></span><span class="s1">-&gt; 3<span class="Apple-converted-space">   <span class="Apple-tab-span"> </span>   </span>for _ in 0..n {<br/></span><span class="s1">   4 <span class="Apple-converted-space">  <span class="Apple-tab-span"> </span>      </span>b(n);<br/></span><span class="s1">   5<span class="Apple-converted-space">   <span class="Apple-tab-span"> </span>   </span>}<br/></span><span class="s1"><span class="Apple-converted-space">   </span>6 <span class="Apple-converted-space">  <span class="Apple-tab-span"> </span></span>}<br/></span><span class="s1"><span class="Apple-converted-space">   </span>7<span class="Apple-converted-space">   <span class="Apple-tab-span"> <br/></span></span></span><span class="s1">Target 0: (performance_polynomial3-8048e39c94dd7157) stopped.</span></pre>
<p>After stopping at the breakpoint, LLDB will print some context for where the code is stopped at. Now we can inspect the program. Let's print what variables are defined in this function:</p>
<pre class="p2"><span class="s3">(lldb) </span><span class="s1">frame variable<br/></span><span class="s1">(unsigned long) n = 1000</span></pre>
<p>We can similarly print any variable in scope:</p>
<pre class="p1"><span class="s1">(lldb) </span><span class="s2">p n<br/></span><span class="s1">(unsigned long) $0 = 1000</span></pre>
<p>When we want to continue the program, type <kbd>c</kbd> to continue:</p>
<pre class="p1"><span class="s1">(lldb) </span><span class="s2">c<br/></span><span class="s1">Process 99468 resuming<br/></span><span class="s1">Process 99468 exited with status = 0 (0x00000000)</span></pre>
<p>The program exits here because we did not set any more breakpoints. This method of debugging is nice because it allows you to inspect a running program without constantly adding <kbd>println!</kbd> statements and recompiling. If nothing else works, that is still an option though.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Metaprogramming</h1>
                </header>
            
            <article>
                
<p>Metaprogramming in Rust has two forms—macros and procedural macros. Both of these utilities accept an abstract syntax tree as new input and output symbols to be compiled. Procedural macros are very similar to normal macros but with fewer restrictions on how they work and how they are defined.</p>
<p>Macros defined with the <kbd>macro_rules!</kbd> syntax are defined recursively by matching the input syntax to produce output. It is crucial to understand that macro matching happens <em>after</em> parsing. This means the following:</p>
<ul>
<li>Macros must follow certain rules when creating new syntax forms</li>
<li>The AST is decorated with information regarding each node's grammar category</li>
</ul>
<p>Macros can match individual tokens, or a macro can match (and capture) an entire grammar category. The Rust grammar categories are as follows:</p>
<ul>
<li><kbd>tt</kbd>: This is a token tree (which is a token output from the lexer before parsing)</li>
<li><kbd>ident</kbd>: This is an identifier</li>
<li><kbd>expr</kbd>: This is an expression</li>
<li><kbd>ty</kbd>: This is a type</li>
<li><kbd>stmt</kbd>: This is a statement</li>
<li><kbd>block</kbd>: These are the braces containing a block of statements</li>
<li><kbd>item</kbd>: This is a top-level definition such as a function or a struct</li>
<li><kbd>pat</kbd>: This is the match part of a pattern match expression, also called the <strong>left hand side</strong></li>
<li><kbd>path</kbd>: This is a path such as <kbd>std::fs::File</kbd></li>
<li><kbd>meta</kbd>: This is a meta item that goes inside either <kbd>#[...]</kbd> or <kbd>#![...]</kbd> syntax forms</li>
</ul>
<p>Using these patterns we can create macros to match various groups of syntax expressions:</p>
<pre class="p1"><span class="s1">//This macro rule matches one token tree "tt"<br/>macro_rules! match_tt {<br/></span><span class="s1">   ($e: tt) =&gt; { println!("match_tt: {}", stringify!($e)) }<br/></span><span class="s1">}<br/><br/>//This macro rule matches one identifier "ident"<br/></span><span class="s1">macro_rules! match_ident {<br/></span><span class="s1">   ($e: ident) =&gt; { println!("match_ident: {}", stringify!($e)) }<br/></span><span class="s1">}<br/><br/>//This macro rule matches one expression "expr"<br/></span><span class="s1">macro_rules! match_expr {<br/></span><span class="s1">   ($e: expr) =&gt; { println!("match_expr: {}", stringify!($e)) }<br/></span><span class="s1">}<br/><br/>//This macro rule matches one type "ty" <br/></span><span class="s1">macro_rules! match_ty {<br/></span><span class="s1">   ($e: ty) =&gt; { println!("match_ty: {}", stringify!($e)) }<br/></span><span class="s1">}<br/><br/>//This macro rule matches one statement "stmt"<br/></span><span class="s1">macro_rules! match_stmt {<br/></span><span class="s1">   ($e: stmt) =&gt; { println!("match_stmt: {}", stringify!($e)) }<br/></span><span class="s1">}<br/><br/>//This macro rule matches one block "block"<br/></span><span class="s1">macro_rules! match_block {<br/></span><span class="s1">   ($e: block) =&gt; { println!("match_block: {}", stringify!($e)) }<br/></span><span class="s1">}<br/><br/>//This macro rule matches one item "item"<br/>//items are things like function definitions, struct definitions, ...<br/></span><span class="s1">macro_rules! match_item {<br/></span><span class="s1">   ($e: item) =&gt; { println!("match_item: {}", stringify!($e)) }<br/></span><span class="s1">}<br/><br/>//This macro rule matches one pattern "pat"<br/></span><span class="s1">macro_rules! match_pat {<br/></span><span class="s1">   ($e: pat) =&gt; { println!("match_pat: {}", stringify!($e)) }<br/></span><span class="s1">}<br/><br/>//This macro rule matches one path "path"<br/>//A path is a canonical named path like std::fs::File<br/></span><span class="s1">macro_rules! match_path {<br/></span><span class="s1">   ($e: path) =&gt; { println!("match_path: {}", stringify!($e)) }<br/></span><span class="s1">}<br/><br/>//This macro rule matches one meta "meta"<br/>//A meta is anything inside of the #[...] or #![...] syntax<br/></span><span class="s1">macro_rules! match_meta {<br/></span><span class="s1">   ($e: meta) =&gt; { println!("match_meta: {}", stringify!($e)) }<br/></span><span class="s1">}</span></pre>
<p>Then, let's apply the macros to some different input:</p>
<pre class="p1"><span class="s1">fn main() {<br/></span><span class="s1">   match_tt!(a);<br/></span><span class="s1">   match_tt!(let);<br/></span><span class="s1">   match_tt!(+);<br/><br/></span><span class="s1">   match_ident!(a);<br/></span><span class="s1">   match_ident!(bcd);<br/></span><span class="s1">   match_ident!(_def);<br/><br/></span><span class="s1">   match_expr!(1.2);<br/></span><span class="s1">   match_expr!(bcd);<br/></span><span class="s1">   match_expr!(1.2 + bcd / "b" - [1, 3, 4] .. vec![1, 2, 3]);<br/><br/></span><span class="s1">   match_ty!(A);<br/></span><span class="s1">   match_ty!(B + 'static);<br/></span><span class="s1">   match_ty!(A&lt;&amp;(B + 'b),&amp;mut (C + 'c)&gt; + 'static);<br/><br/></span><span class="s1">   match_stmt!(let x = y);<br/></span><span class="s1">   match_stmt!(());<br/></span><span class="s1">   match_stmt!(fn f(){});<br/></span><span class="s1">   <br/>   match_block!({});<br/></span><span class="s1">   match_block!({1; 2});<br/></span><span class="s1">   match_block!({1; 2 + 3});<br/><br/></span><span class="s1">   match_item!(struct A(u64););<br/></span><span class="s1">   match_item!(enum B { C, D });<br/></span><span class="s1">   match_item!(fn C(n: NotAType) -&gt; F&lt;F&lt;F&lt;F&lt;F&gt;&gt;&gt;&gt; { a + b });<br/><br/></span><span class="s1">   match_pat!(_);<br/></span><span class="s1">   match_pat!(1);<br/></span><span class="s1">   match_pat!(A {b, c:D( d@3 )} );<br/><br/></span><span class="s1">   match_path!(A);<br/></span><span class="s1">   match_path!(::A);<br/></span><span class="s1">   match_path!(std::A);<br/></span><span class="s1">   match_path!(a::&lt;A,_&gt;);<br/><br/></span><span class="s1">   match_meta!(A);<br/></span><span class="s1">   match_meta!(Property(B,C));<br/></span><span class="s1">}</span></pre>
<p>As we can see from the example, token trees are, for the most part, not restricted to normal Rust grammar, only to the Rust lexer. The lexer is aware of opening and closing <kbd>() [] {}</kbd> bracketed forms. This is why tokens are structured in a token tree rather than a token list. This also means that all tokens inside macro calls will be stored as token trees and not processed any further until the macro is invoked; as long as we create a syntax compatible with Rust token trees, then other syntax innovations should usually be permitted. This rule applies also to the other grammar categories: grammar categories are just a short hand to match certain pattern of tokens that happen to correspond to Rust syntax forms.</p>
<p>Just matching single tokens or grammar categories probably won't be very useful for a macro. To make use of macros in a practical context, we will need to make use of macro grammar sequences and grammar alternative<em>s</em>. A grammar sequence is a request to match more than one token or grammar category in the same rule. A grammar alternative is a separate rule within the same macro that matches a different syntax. Grammar sequences and alternatives can also be combined in the same macro. Additionally, there is a special syntax form to match <em>many</em> tokens or grammar categories.</p>
<p>Here are corresponding examples to illustrate these patterns:</p>
<pre class="p1"><span class="s1">//this is a grammar sequence<br/></span><span class="s1">macro_rules! abc {<br/></span><span class="s1">   (a b c) =&gt; { println!("'a b c' is the only correct syntax.") };<br/></span><span class="s1">}<br/><br/></span><span class="s1">//this is a grammar alternative<br/></span><span class="s1">macro_rules! a_or_b {<br/></span><span class="s1">   (a) =&gt; { println!("'a' is one correct syntax.") };<br/></span><span class="s1">   (b) =&gt; { println!("'b' is also correct syntax.") };<br/></span><span class="s1">}<br/><br/></span><span class="s1">//this is a grammar of alternative sequences<br/></span><span class="s1">macro_rules! abc_or_aaa {<br/></span><span class="s1">   (a b c) =&gt; { println!("'a b c' is one correct syntax.") };<br/></span><span class="s1">   (a a a) =&gt; { println!("'a a a' is also correct syntax.") };<br/></span><span class="s1">}<br/><br/></span><span class="s1">//this is a grammar sequence matching many of one token<br/></span><span class="s1">macro_rules! many_a {<br/></span><span class="s1">   ( $($a:ident)* ) =&gt; {{ $( print!("one {} ", stringify!($a)); )* println!(""); }};<br/></span><span class="s1">   ( $($a:ident),* ) =&gt; {{ $( print!("one {} comma ", stringify!($a)); )* println!(""); }};<br/></span><span class="s1">}</span><span class="s1"><br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   abc!(a b c);<br/><br/></span><span class="s1">   a_or_b!(a);<br/></span><span class="s1">   a_or_b!(b);<br/></span><span class="s1"><br/>   abc_or_aaa!(a b c);<br/></span><span class="s1">   abc_or_aaa!(a a a);<br/><br/></span><span class="s1"><span class="Apple-converted-space">   </span>many_a!(a a a);<br/></span><span class="s1"><span class="Apple-converted-space">   </span>many_a!(a, a, a);</span><span class="s1"><br/></span><span class="s1">}</span></pre>
<p>If you've paid attention to the generated code for all of these macros, you might have noticed that all production rules have created expressions. Macro input can be tokens, but output must be a contextually well-formed Rust syntax. For this reason, you cannot write <kbd>macro_rules!</kbd> as shown here:</p>
<pre class="p1"><span class="s1">macro_rules! f {<br/></span><span class="s1">   () =&gt; { f!(1) f!(2) f!(3) };<br/></span><span class="s1">   (1) =&gt; { 1 };<br/></span><span class="s1">   (2) =&gt; { + };<br/></span><span class="s1">   (3) =&gt; { 2 };<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   f!()<br/></span><span class="s1">}</span></pre>
<p>The specific error from the compiler is as follows:</p>
<pre class="p1"><span class="s1">error</span><span class="s2">: macro expansion ignores token `f` and any following<br/></span>--&gt; <span class="s2">t.rs:2:19<br/></span><span class="s3">  |<br/></span><span class="s3">2</span><span class="s2"> </span><span class="s3">| </span><span class="s2"> <span class="Apple-converted-space">  </span>() =&gt; { f!(1); f!(2); f!(3) };<br/></span><span class="s3">  | </span><span class="s2"><span class="Apple-converted-space">                  </span></span><span class="s1">^<br/></span><span class="s3">  |<br/></span><span class="s4">note</span><span class="s2">: caused by the macro expansion here; the usage of `f!` is likely invalid in expression context<br/></span>--&gt; <span class="s2">t.rs:9:4<br/></span><span class="s3">  |<br/></span><span class="s3">9</span><span class="s2"> </span><span class="s3">| </span><span class="s2"> <span class="Apple-converted-space">  </span>f!()<br/></span><span class="s3">  | </span><span class="s2"> <span class="Apple-converted-space">  </span></span><span class="s4">^^^^<br/><br/></span><span class="s1">error</span><span class="s2">: aborting due to previous error</span></pre>
<p>The key phrase here is <kbd>f!</kbd>, which is likely invalid in an expression context. Each pattern of <kbd>macro_rules!</kbd> output must be a well-formed expression. The preceding example will create well-formed Rust syntax in the end, but its intermediate results are fragmented expressions. This awkwardness is one of the several reasons to use procedural macros, which are much like <kbd>macro_rules!</kbd> but programmed directly in Rust rather than through the special <kbd>macro_rules!</kbd> syntax.</p>
<p>Procedural macros are programmed in Rust, but are also used to compile Rust programs. How does that work? Procedural macros must be isolated into their own modules and compiled separately; they are basically a compiler plugin.</p>
<p>To start our procedural macro, let's create a new subproject:</p>
<ol>
<li>Make a <kbd>procmacro</kbd> <span>directory </span>inside the project root</li>
<li>Inside the <kbd>procmacro</kbd> directory, create a <kbd>Cargo.toml</kbd> file with the following contents:</li>
</ol>
<pre class="p1" style="padding-left: 30px"><span class="s1">[package]<br/></span><span class="s1">name = "procmacro"<br/></span><span class="s1">version = "1.0.0"<br/></span><span class="s1"><br/>[dependencies]<br/></span><span class="s1">syn = "0.12"<br/></span><span class="s1">quote = "0.4"<br/><br/></span><span class="s1">[lib]<br/></span><span class="s1">proc-macro = true</span></pre>
<ol start="3">
<li>Inside the <kbd>procmacro</kbd> directory, create a <kbd>src/lib.rs</kbd> file with the following contents:</li>
</ol>
<pre class="p1" style="padding-left: 30px"><span class="s1">#![feature(proc_macro)]<br/></span><span class="s1">#![crate_type = "proc-macro"]<br/></span><span class="s1">extern crate proc_macro;<br/></span><span class="s1">extern crate syn;<br/></span><span class="s1">#[macro_use] extern crate quote;<br/></span><span class="s1">use proc_macro::TokenStream;<br/></span><span class="s1">#[proc_macro]<br/><br/></span><span class="s1">pub fn f(input: TokenStream) -&gt; TokenStream {<br/></span><span class="s1">   assert!(input.is_empty());<br/><br/></span><span class="s1">   (quote! {<br/></span><span class="s1">      1 + 2<br/></span><span class="s1">   }).into()<br/></span><span class="s1">}</span></pre>
<p style="padding-left: 30px">This <kbd>f!</kbd> macro now implements the preceding semantics without any of the complaints. Using the macro looks like the following:</p>
<pre class="p1" style="padding-left: 30px"><span class="s1">#![feature(proc_macro_non_items)]<br/></span><span class="s1">#![feature(use_extern_macros)]<br/></span><span class="s1">extern crate procmacro;<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   let _ = procmacro::f!();<br/></span><span class="s1">}</span></pre>
<p>The interface of a procedural macro is really simple. There is a <kbd>TokenStream</kbd> as input and a <kbd>TokenStream</kbd> as output. The <kbd>proc_macro</kbd> and <kbd>syn</kbd> crates also provide utilities to parse tokens or to easily create token streams using the <kbd>quote!</kbd> macro. To use procedural macros, there is some additional setup and boilerplate, but after getting past these hurdles the interface is fairly straightforward now.</p>
<p>Additionally, there are many more detailed grammar categories available to procedural macros through the <kbd>syn</kbd> crate. There are 163 categories (<a href="https://dtolnay.github.io/syn/syn/#macros">https://dtolnay.github.io/syn/syn/#macros</a>) right now! These include the same vague syntax trees from recursive macros, but also very specific syntax forms. These categories correspond to the full Rust grammar, therefore permitting very expressive macro syntax without having to create your own parser.</p>
<p>Let's make a procedural macro that uses some of these syntax categories. First we make a new procedural macro folder, just like preceding <kbd>procmacro</kbd>; this one we will name <kbd>procmacro2</kbd>. Now we define the AST that will hold all of the program information if the user input is valid:</p>
<pre class="p1"><span class="s1">#![feature(proc_macro)]<br/></span><span class="s1">#![crate_type = "proc-macro"]<br/></span><span class="s1">extern crate proc_macro;<br/></span><span class="s1">#[macro_use] extern crate syn;<br/></span><span class="s1">#[macro_use] extern crate quote;<br/></span><span class="s1">use proc_macro::TokenStream;<br/></span><span class="s1">use syn::{Ident, Type, Expr, WhereClause, TypeSlice, Path};<br/></span><span class="s1">use syn::synom::Synom;<br/><br/></span><span class="s1">struct MiscSyntax {<br/></span><span class="s1">   id: Ident,<br/></span><span class="s1">   ty: Type,<br/></span><span class="s1">   expr: Expr,<br/></span><span class="s1">   where_clause: WhereClause,<br/></span><span class="s1">   type_slice: TypeSlice,<br/></span><span class="s1">   path: Path<br/></span><span class="s1">}</span></pre>
<p>The <kbd>MiscSyntax</kbd> struct will contain all information gathered from our macro. That macro and its syntax is what we should define now:</p>
<pre class="p1"><span class="s1">impl Synom for MiscSyntax {<br/></span><span class="s1">   named!(parse -&gt; Self, do_parse!(<br/></span><span class="s1">      keyword!(where) &gt;&gt;<br/></span><span class="s1">      keyword!(while) &gt;&gt;<br/></span><span class="s1">      id: syn!(Ident) &gt;&gt;<br/></span><span class="s1">      punct!(:) &gt;&gt;<br/></span><span class="s1">      ty: syn!(Type) &gt;&gt;<br/></span><span class="s1">      punct!(&gt;&gt;) &gt;&gt;<br/></span><span class="s1">      expr: syn!(Expr) &gt;&gt;<br/></span><span class="s1">      punct!(;) &gt;&gt;<br/></span><span class="s1">      where_clause: syn!(WhereClause) &gt;&gt;<br/></span><span class="s1">      punct!(;) &gt;&gt;<br/></span><span class="s1">      type_slice: syn!(TypeSlice) &gt;&gt;<br/></span><span class="s1">      punct!(;) &gt;&gt;<br/></span><span class="s1">      path: syn!(Path) &gt;&gt;<br/></span><span class="s1">      (MiscSyntax { id, ty, expr, where_clause, type_slice, path })<br/></span><span class="s1">   ));<br/></span><span class="s1">}</span></pre>
<p>The <kbd>do_parse!</kbd> macro helps simplify the use of the parser combinators from the <kbd>syn</kbd> crate. The <kbd>id: expr &gt;&gt;</kbd> syntax corresponds to the monadic bind operation, and <kbd>expr &gt;&gt;</kbd> syntax is also a form of a monadic bind.</p>
<p>Now we utilize these definitions to parse input, generate output, <span>and expose the macro</span>:</p>
<pre class="p1"><span class="s1">#[proc_macro]<br/></span><span class="s1">pub fn misc_syntax(input: TokenStream) -&gt; TokenStream {<br/></span><span class="s1">   let m: MiscSyntax = syn::parse(input).expect("expected Miscellaneous Syntax");<br/></span><span class="s1">   let MiscSyntax { id, ty, expr, where_clause, type_slice, path } = m;<br/></span><span class="s1"><br/>   (quote! {<br/></span><span class="s1">      let #id: #ty = #expr;<br/></span><span class="s1">      println!("variable = {}", #id);<br/></span><span class="s1">    }).into()<br/></span><span class="s1">}</span></pre>
<p>When using this macro, it really is a bunch of random syntax. This emphasizes how macros are not limited to valid Rust syntax, which looks like the following:</p>
<pre class="p1"><span class="s1">#![feature(proc_macro_non_items)]<br/></span><span class="s1">#![feature(use_extern_macros)]<br/></span><span class="s1">extern crate procmacro2;<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   procmacro2::misc_syntax!(<br/></span><span class="s1">      where while abcd : u64 &gt;&gt; 1 + 2 * 3;<br/>      where T: 'x + A&lt;B='y+C+D&gt;;<br/>      [M];A::f<br/></span><span class="s1">   );<br/></span><span class="s1">}</span></pre>
<p>Procedural macros are very powerful and helpful if Rust syntax becomes annoying for your purposes. For specific contexts it is possible to create very semantically dense code using macros that would otherwise require lots of boilerplate and copy-paste coding.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced many applied and practical considerations for Rust programming. Performance and debugging are certainly not problems that are exclusive to Functional Programming. Here we tried to introduce tips that are generally applicable but also highly compatible with functional programming.</p>
<p>Metaprogramming in Rust may be considered a functional feature by itself. Logic programming and thereby derived functionality are closely associated with functional programming principles. The recursive, context-free nature of macros also lends itself to a functional perspective.</p>
<p>This is also the last chapter in the book. We hope you have enjoyed the book and we welcome any feedback. If you are looking for further reading, you might want to research some of the topics presented in the final three chapters of the book. There is an enormous amount of material available on these subjects and any path taken will surely further improve your understanding of Rust and functional programming.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>How is release mode different from debug mode?</li>
<li>How long will an empty loop take to run?</li>
<li>What is linear time in <em>Big O</em> notation?</li>
<li>Name a function that grows faster than exponential growth.</li>
<li>What is faster, a disk read or a network read?</li>
<li>How would you return a <kbd>Result</kbd> with multiple error conditions?</li>
<li>What is a token tree?</li>
<li>What is an abstract syntax tree?</li>
<li>Why do procedural macros need to be compiled separately?</li>
</ol>


            </article>

            
        </section>
    </body></html>
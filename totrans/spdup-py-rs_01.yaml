- en: '*Chapter 3*: Understanding Concurrency'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Speeding up our code with Rust is useful. However, understanding concurrency
    and utilizing threads and processes can take our ability to speed up our code
    to the next level. In this chapter, we will go through what processes and threads
    are. We then go through the practical steps of spinning up threads and processes
    in Python and Rust. However, while this can be exciting, we also must acknowledge
    that reaching for threads and processes without thinking about our approach can
    end up tripping us up. To avoid this, we also explore algorithm complexity and
    how this affects our computation time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic asynchronous programming with threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running multiple processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing threads and processes safely
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for this chapter can be accessed via the following GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Speed-up-your-Python-with-Rust/tree/main/chapter_three](https://github.com/PacktPublishing/Speed-up-your-Python-with-Rust/tree/main/chapter_three)'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we explored in the introduction of [*Chapter 1*](B17720_01_Final_SK_ePub.xhtml#_idTextAnchor014),
    *An Introduction to Rust from a Python Perspective*, Moore's law is now failing,
    and therefore we have to consider other ways in which we can speed up our processing.
    This is where concurrency comes in. Concurrency is essentially running multiple
    computations at the same time. Concurrency is everywhere, and to give the concept
    full justice, we would have to write a whole book on it.
  prefs: []
  type: TYPE_NORMAL
- en: However, for the scope of this book, understanding the basics of concurrency
    (and when to use it) can add an extra tool to our belt that enables us to speed
    up computations. Furthermore, threads and processes are how we can break up our
    program into computations that run at the same time. To start our concurrency
    tour, we will cover threads.
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Threads** are the smallest unit of computation that we can process and manage
    independently. Threads are used to break a program into computational parts that
    can be run at the same time. It also has to be noted that threads can be run out
    of sequence. This brings forward an important distinction between concurrency
    and parallelism. **Concurrency** is the task of running and managing multiple
    computations at the same time, while **parallelism** is the task of running multiple
    computations at the same time. Concurrency has a non-deterministic control flow,
    while parallelism has a deterministic control flow. Threads share resources such
    as memory and processing power; however, they also block each other. For instance,
    if we spin off a thread that requires constant processing power, we will merely
    block the other thread, as seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Two threads over time'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.01_B17720.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Two threads over time
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that **Thread A** stops running when **Thread B** is running.
    This is demonstrated in Pan Wu''s 2020 article on understanding multithreading
    through simulations where different types of tasks were timed. The results in
    the article are summed up in the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Times of different tasks](img/Figure_3.02_B17720.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2 – Times of different tasks [Source: Pan Wu, https://towardsdatascience.com/understanding-python-multithreading-and-multiprocessing-via-simulation-3f600dbbfe31]'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the times decrease as the number of workers decreases,
    *apart from* the **central processing unit** (**CPU**)-heavy multithreaded tasks.
    This is because, as demonstrated in *Figure 3.1*, the CPU-intensive threads are
    blocking, so only one worker can process at a time. It does not matter how many
    more workers you add. It must be noted that this is because of Python's **global
    interpreter lock** (**GIL**), which is covered in [*Chapter 6*](B17720_06_Final_SK_ePub.xhtml#_idTextAnchor100),
    *Working with Python Objects in Rust*. In other contexts, such as Rust, they can
    be executed on different CPU cores and generally will not block each other.
  prefs: []
  type: TYPE_NORMAL
- en: We can also see in *Figure 3.2* that the **input/output** (**I/O**)-heavy tasks
    do reduce in time taken when the workers increase. This is because there is idle
    time in I/O-heavy tasks. This is where we can really utilize threads. Let's say
    our task is making a call to a server. There is some idle time when waiting for
    a response, therefore utilizing threads to make multiple calls to servers will
    speed up the time. We also must note that processes work for CPU- and I/O-heavy
    tasks. Because of this, it is beneficial for us to explore what processes are.
  prefs: []
  type: TYPE_NORMAL
- en: Processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Processes** are more expensive to produce compared to threads. In fact, a
    process can host multiple threads. This is usually depicted in the following classic
    multithreading diagram, as seen everywhere (including the multiprocessing *Wikimedia*
    page):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Relationship between threads and processes](img/Figure_3.03_B17720.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3 – Relationship between threads and processes [Source: Cburnett (2007)
    (https://commons.wikimedia.org/wiki/File:Multithreaded_process.svg), CC BY-SA
    3.0]'
  prefs: []
  type: TYPE_NORMAL
- en: This is a classic diagram because it encapsulates the relationship between processes
    and threads so well. Here, we can see that threads are a subset of a process.
    We can also see why threads share memory, and as a result, we must note that processes
    are typically independent and do not share memory. We also must note that context
    switches are more expensive when using processes. A context switch refers to when
    the state of a process (or thread) is stored so that it can be restored and resumed
    at a later state. An example of this would be waiting for an **application programming
    interface** (**API**) response. The state can be saved, and another process/thread
    can run while we wait for the API response.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the basic concepts behind threads and processes, we need
    to learn how to practically use threads in our programs.
  prefs: []
  type: TYPE_NORMAL
- en: Basic asynchronous programming with threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To utilize threading, we need to be able to start threads, allow them to run,
    and then join them. We can see the stages of practically managing our threads
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Stages of threads'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.04_B17720.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Stages of threads
  prefs: []
  type: TYPE_NORMAL
- en: 'We start the threads, we then let them run, and once they have run, we join
    them. If we did not join them, the program would continue to run before the threads
    had finished. In Python, we create a thread by inheriting the `Thread` object,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that we have overwritten the `run` function in the `Thread`
    class. This function runs when the thread is running. We then overwrite the `join`
    method. However, we must note that in the `join` function, there is extra functionality
    going on under the hood; therefore, we must call the `Thread` class''s `join`
    method, and then return whatever we want at the end. We do not have to return
    anything if we do not want to. If this is the case, then there is no point overwriting
    the `join` function. We can then implement the threads by running the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We then have to time the process of starting, running, and joining the outcomes,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this code, we get the following console printout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Straight away, we can see that it took just over 5 seconds to execute the whole
    process. If we were running our program sequentially, it would take 15 seconds.
    This shows that our threads are working!
  prefs: []
  type: TYPE_NORMAL
- en: It also must be noted that thread `three` finished before thread `two`, even
    though thread `two` started before. Don't worry if you get a finishing sequence
    of `one`, `two`, `three`; this is because threads finish in an indeterminate order.
    Even though the scheduling is deterministic, there are thousands of events and
    processes running under the hood of the CPU when the program is running. As a
    result, the exact time slices that each thread gets are never the same. These
    tiny changes add up over time, and as a result, we cannot guarantee that the threads
    will finish in a determinate order if the executions are close and the durations
    are roughly the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have the basics of Python threads, we can move on to spinning off threads
    in Rust. However, before we start doing this, we must understand the concept of
    `main` function or inside other scopes including other functions. A simple example
    of a building closure is to print an input, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'With this approach, we can exploit scopes. It also must be noted that as closures
    are scope-sensitive, we can also utilize the existing variables around a closure.
    To demonstrate this, we can create a closure that calculates the amount of interest
    we have to pay on a loan due to the external base rate. We will also define it
    in an inner scope, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code would give us the following printout in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that closures can return values, but we have not defined the
    type for the closure. This is the case even though it is returning a float. In
    fact, if we set `calculate_interest` to `f32`, the compiler would complain, stating
    that the types were mismatched. This is because the closure is a unique anonymous
    type that cannot be written out. A closure is a struct generated by the compiler
    that houses captured variables. If we try to call the closure outside the inner
    scope, our application will fail to compile as the closure cannot be accessed
    outside the scope.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have covered Rust closures, we can replicate the Python threading
    example that we covered earlier in the section. Initially, we must import the
    standard module crates that are required by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We are using `thread` to spawn off threads, `time` to keep track of how long
    our processes take, and the `JoinHandle` struct to join the threads. With these
    imports, we can build our own thread by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that we create a `Duration` struct denoted as `total_seconds`.
    We then use the thread and `total_seconds` to put the function to sleep, returning
    the number of seconds when the whole process is finished. Right now, this is just
    a function, and running it by itself will not spin off different threads. Inside
    our `main` function, we start our timer and spawn off our three threads by running
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we spawn threads and pass our function in with the right parameters in
    the closure. Nothing is stopping us from putting any code in the closure. The
    final line in the closure would be what is returned to the `JoinHandle` struct
    to unwrap. Once this is done, we join all the threads to hold the program until
    all the threads have finished before moving on with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: The `join` function returns a result with the `Result<i8, Box<dyn Any + Send>>`
    type.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some new concepts here, but we can break them down, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We remember that a `Result` struct in Rust either returns an `Ok` or an `Err`
    response. If the thread runs without any problems, then we will return the `i8`
    value that we are expecting. If not, then we have this rather ugly `Result<i8,
    Box<dyn Any + Send>>` output as the error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first thing we must address here is the `Box` struct. This is one of the
    most basic forms of a pointer and allows us to store data on the heap rather than
    the stack. What remains on the stack is the pointer to the data in the heap. We
    are using this because we do not know how big the data is when coming out of the
    thread.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next expression that we must explain is `dyn`. This keyword is used to indicate
    that the type is a trait object. For instance, we might want to store a range
    of `Box` structs in an array. These `Box` structs might point to different structs.
    However, we can still ensure that they can be grouped together if they have a
    certain trait in common. For instance, if all the structs had to have `TraitA`
    implemented, we would denote this with `Box<dyn TraitA>`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Any` keyword is a trait for dynamic typing. This means that the data type
    can be anything. The `Any` trait is combined with `Send` by using the `Any + Send`
    expression. This means that both traits must be implemented.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Send` trait is for types that can be transferred across thread boundaries.
    `Send` is implemented automatically by the compiler if it is deemed appropriate.
    With all this, we can confidently state that the join of a thread in Rust returns
    a result that can either be the integer that we desire or a pointer to anything
    else that can be transferred across threads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To process the results of the thread, we could just directly unwrap them. However,
    this would not be very useful when the demands of our multithreaded programs increase.
    We must be able to handle what potentially comes out of a thread, and to do this,
    we are going to have to downcast the outcome. Downcasting is Rust''s method of
    converting a trait into a concrete type. In this context, we will be converting
    `PyO3` structs that denote Python types into concrete Rust data types such as
    strings or integers. To demonstrate this, let''s build a function that handles
    the outcome of our thread, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we are going to have to import everything we need, as seen in the following
    code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With these imports, we can create a function that unpacks the result and prints
    it using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we merely print out the result if it is a success. However, if it is an
    error, as pointed out earlier, we do not know what data type the error is. However,
    we would still like to handle this. This is where we downcast. Downcasting returns
    an option, which is why we have the `if let Some(string) = result.downcast_ref::<String>()`
    condition. If the downcast is successful, we can move the string into the scope
    and print out the error string. If it is not successful, we can move on and state
    that although there was an error, an error string was not provided. We can use
    multiple conditional statements to account for a range of data types if we want.
    We can write a lot of Rust code without having to rely on downcasting, as Rust
    has a strict typing section. However, when interfacing with Python this can be
    useful, as we know that Python objects are dynamic and could essentially be anything.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we can process our threads when they have finished, we can stop the
    clock and process the outcomes by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following printout:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And here we have it: we can run and process threads in Python and Rust. However,
    remember that if we try to run CPU-intensive tasks with the code that we have
    written, we will not get the speed up. However, it must be noted that in the Rust
    context of the code, there could be a speedup depending on the environment. For
    instance, if multiple CPU cores are available, the **operating system** (**OS**)
    scheduler can put those threads onto those cores to be executed in parallel. To
    write code that will speed up our code in this context, we will have to learn
    how to practically spin up multiple processes, which we cover in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Running multiple processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Technically with Python, we can simply switch the inheritance of our thread
    from `Thread` to `Process` by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: However, there are some compilations. If we refer to *Figure 3.3*, we can see
    that processes have their own memory. This is where things can get complicated.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, there is nothing wrong with the process defined previously if
    the process is not returning anything directly but writing to a database or file.
    On the other hand, the `join` function will not return anything directly and will
    just have `None` instead. This is because `Process` is not sharing the same memory
    space as the main process. We also must remember that spinning off processes is
    more expensive, so we must be more careful with this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are getting more complex with the memory and the resources are getting
    more expensive, it makes sense to rein it in and keep it simple. This is where
    we utilize a **pool**. A pool is where we have several workers processing inputs
    simultaneously and then packaging them as an array, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Pool of processes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.05_B17720.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – Pool of processes
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage here is that we keep the expensive multiprocessing context to
    a small part of the program. We can also easily control the number of workers
    that we are willing to support. For Python, this means that we keep the interaction
    as lightweight as possible. As seen in the next diagram, we package an individual
    isolated function in a tuple with an array of inputs. This tuple gets processed
    in the pool by a worker, and then the result of the outcome is returned from the
    pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Pool data flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.06_B17720.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 – Pool data flow
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate multiprocessing via a pool, we can utilize the **Fibonacci sequence**.
    This is where the next number of the sequence is the sum of the previous number
    in the sequence and the number before that, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17720_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To calculate a number in the sequence, we will have to use **recursion**. There
    is a closed form of the Fibonacci sequence; however, this will not let us explore
    multiprocessing as the closed sequence by its very nature doesn''t scale in computation
    as *n* increases. To calculate a Fibonacci number in Python, we can write an isolated
    function, as seen in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'This function keeps going back until it hits the bottom of the tree at either
    1 or 0\. This function is terrible at scaling. To demonstrate this, let''s look
    at the recursion tree shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Fibonacci recursion tree'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.07_B17720.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 – Fibonacci recursion tree
  prefs: []
  type: TYPE_NORMAL
- en: We can see that these are not perfect trees, and if you go online and search
    for *big O notation of the Fibonacci sequence*, there are debates, and some equations
    will equate the scaling factor to the golden ratio. While this is interesting,
    it is outside the scope of this book as we are focusing on computational complexity.
    As a result, we will simplify the math and treat this as a perfectly symmetrical
    tree. Recursion trees scale at the rate of ![](img/B17720_03_002.png), where *n*
    is the depth of the tree. Referring to *Figure 3.7*, we can see that if we treat
    the tree as perfectly symmetrical, a *n* value of 3 has a depth of 3, and a *n*
    value of 4 has a depth of 4\. As *n* increases, the computation increases exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: We have taken a slight detour of complexity to highlight the importance of taking
    this into account before reaching for multiprocessing. The reason why you bought
    this book as opposed to searching online for multiprocessing code snippets to
    copy and paste into your code is that you want to be guided through these concepts
    with pointers for further reading and an appreciation of their context. In the
    case of this sequence, reaching for a closed form or caching answers would reduce
    the computation time greatly. If we have an ordered list of numbers, getting the
    highest number in the list and then creating a full sequence up to the highest
    number would be a lot quicker than repeatedly calculating the sequence again and
    again for each number we want to calculate. Avoiding recursion altogether is a
    better option than reaching for multiprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement and test our multiprocessing pool, we first need to time how long
    it would take to calculate a range of numbers sequentially. This can be done like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: We have introduced a pretty long list; however, this is essential to see the
    difference. If we just had two Fibonacci numbers to compute, then the cost of
    spinning up processes could eclipse the gain in multiprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our multiple processing pool can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that we have nested this code under `if __name__ ==` "`__main__`"`:`.
    This is because the whole script gets run again when spinning up another process,
    which can result in infinite loops. If the code is nested under `if __name__ ==`
    "`__main__`"`:` then it will not run again as there is only one main process.
    It also must be noted that we defined a pool of four workers. This can be changed
    to whatever we feel fit but there are diminishing returns when increasing this,
    as we will explore later. The tuples in the list are the arguments for each computation.
    Running the whole script gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the speed is not a quarter of the sequential calculations.
    However, the multiprocessing pool is slightly faster. If you run this multiple
    times, you will get some variance in the difference in times. However, the multiprocessing
    approach will always be faster. Now that we have run a multiprocessing tool in
    Python, we can implement our Fibonacci multithreading in the different context
    of a multiprocessing pool in Rust. Here''s how we''ll do this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our new Cargo project, we can code the following function in our `main.rs`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that our Rust function is not more complex than our Python version.
    The extra lines of code are just to account for unexpected inputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To run this and time it, we must import the time crate at the top of the `main.rs`
    file by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must compute the exact same Fibonacci numbers as we did in the Python
    implementation, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To run this, we are going to use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are going to use the release version as that is what we will be using in
    production. Running it gives us the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running this several times will give us an average roundabout of 40 milliseconds.
    Considering that our multiprocessing Python code ran at roughly 3.1 seconds, our
    Rust single-threaded implementation runs 77 times faster than our Python multiprocessing
    code. Just let that sink in! The code was not more complex, and it is memory-safe.
    Therefore, fusing Rust with Python is such a quick win! Combining the aggressive
    typing with the compiler forces us to account for every input and outcome, and
    we are on the way to turbocharging our Python systems with safer, faster code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to see what happens to the speed when we run our numbers
    through a multithreading tool. Here''s how we''ll go about this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we are going to use the `rayon` crate. We define this dependency
    in our `Cargo.toml` file by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once this is done, we import it into our `main.rs` file, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then run our multithreading pool in our `main` function below our sequential
    calculations by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we define the number of threads that our pool builder has. We then execute
    the `into_par_iter` function on the vector. This is achieved by implementing the
    `IntoParallelIterator` trait onto the vector, which is done when the `rayon` crate
    is imported. If it were not imported, then the compiler would complain, stating
    that a vector does not have the `into_par_iter` function associated with it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then map our Fibonacci function over the integers in the vector utilizing
    a closure and collect them. The calculated Fibonacci numbers are associated with
    the `outcomes` variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then print them and print the time elapsed. Running this via a release gives
    us the following printout in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running this several times will give you roughly the times stated in the preceding
    console printout. Calculating this gives us a 20% increase in speed. Considering
    that the Python multiprocessing only gave us a 5% increase, we can deduce that
    Rust is also more efficient at multithreading when the right context is applied.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can go a little further to really see the advantages of these pools. Remember
    that our sequence increases exponentially. In our Rust program, we can add three
    computations for *n* being 46 to our sequential calculations and pooled calculations,
    and we get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: First, we must acknowledge that the time went from milliseconds to double-digit
    seconds. Exponential scaling algorithms are painful, and just adding 10 to your
    calculation pushes it up greatly. We can also see that our savings have increased.
    Our pooled calculations are now 3.11 times faster as opposed to 1.2 times faster
    in the previous test!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If we add three extra computations for *n* being 46 for our Python implementation,
    we get the following console printout:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see that our Python pooled processing is 2.85 times faster than
    our Python sequential processing. We also must note here that our Rust sequential
    processing is roughly 95 times faster than our Python sequential processing, and
    our Rust pool multithreading is roughly 96 times faster than our Python pool processing.
    As the number of points that need processing increases, so will the difference.
    This highlights even more motivation for plugging Rust into Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'It must be noted that we got our speed increase in our Rust program through
    multithreading as opposed to multiprocessing. Multiprocessing in Rust is not as
    straightforward as in Python—this is mainly down to Rust being a newer language.
    For instance, there is a crate called `mitosis` that will enable us to run functions
    in a separate process; however, this crate only has four contributors, and the
    last contribution at the time of writing this book was 13 months ago. Considering
    this, we should approach multiprocessing in Rust without any third-party crates.
    To achieve this, we need to code a Fibonacci calculation program and a multiprocessing
    program that will call this in different processes, as seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Multiprocessing in Rust'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.08_B17720.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 – Multiprocessing in Rust
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to pass our data into these processes and parse the outputs handling
    them in our `multiprocessing.rs` file. To carry this out in the simplest way,
    we code both files in the same directory. First, we build our `fib_process.rs`
    file. We must import what we are going to do by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: 'We want our processes to accept a list of integers to calculate, so we define
    Fibonacci `number` and `numbers` functions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: 'We have seen these functions before as they have become the standard way to
    calculate Fibonacci numbers in this book. We now must take a list of integers
    from arguments, parse them into integers, pass them into our calculation function,
    and return the results, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that we collect the input from the environment. Once the input
    integers have been parsed into `i32` integers and used to calculate the Fibonacci
    numbers, we merely print them out. Printing out to the console generally acts
    as `stdout`. Our process file is fully coded, so we can compile it with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates a binary of our file. Now that this is done, we can move on to
    our `multiprocessing.rs` file that will spawn multiple processes. We import what
    we need by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Command` struct is going to be used to spawn off a new process, the `Stdio`
    struct is going to be used to define the piping of data back from the process,
    and the `Child` struct is returned when the process is spawned. We will use them
    to access the output data and get the process to wait to finish. The `BufReader`
    struct is used to read the data from the child process. Now that we have imported
    everything we need, we can define a function that accepts an array of integers
    as strings and spins off the process, returning the `Child` struct, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that we just must call our binary and pass in our array of
    strings in the `args` function. We then define the `stdout` and spawn the process,
    returning the `Child` struct. Now that this is done, we can fire off three processes
    in our `main` function and wait for them to complete by running the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE195]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now start extracting the data from these processes inside our `main`
    function by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE199]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE202]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that we have accessed the data using the `stdout` field, and
    then processed it using the `BufReader` struct. We can then loop through our extracted
    data, appending it to an empty vector and then printing it out by running the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE209]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE212]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE213]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE214]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE215]'
  prefs: []
  type: TYPE_PRE
- en: 'This code is a little repetitive, but it illustrates how to spawn and manage
    multiple processes in Rust. We then compile the file with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE216]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then run our multiprocessing code with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE217]'
  prefs: []
  type: TYPE_PRE
- en: 'We then get the output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE218]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE219]'
  prefs: []
  type: TYPE_PRE
- en: We have now covered all we need to know about running processes and threads
    to speed up our computations. However, we need to be mindful and investigate how
    to customize our threads and processes safely to avoid pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing threads and processes safely
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover some of the pitfalls that we have to avoid when
    being creative with threads and processes. We will not cover the concepts in depth,
    as advanced multiprocessing and concurrency is a big topic and there are books
    completely dedicated to this. However, it is important to understand what to look
    out for and which topics to read if you want to increase your knowledge of multiprocessing/threading.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at our Fibonacci sequences, it might be tempting to spin off extra
    threads inside our thread to speed up the individual computations in the thread
    pool. However, to truly understand if this is a good idea, we need to understand
    **Amdahl's law**.
  prefs: []
  type: TYPE_NORMAL
- en: Amdahl's law
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Amdahl''s law lets us describe the trade-off on adding more threads. If we
    spin off threads inside the threads, we will have exponential growth of threads.
    You may be forgiven for thinking this to be a good idea; however, Amdahl''s law
    states that there are diminishing returns when increasing the cores. Have a look
    at the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17720_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Speed*: This is the theoretical speedup of the execution of the whole task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s*: This is the speedup of the part of the task that benefits from improved
    system resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p*: This is the proportion of execution time that the part benefiting from
    improved resources originally occupied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general, increasing the cores does have an impact; however, the diminishing
    returns can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Diminishing returns through Amdahl''s law](img/Figure_3.09_B17720.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9 – Diminishing returns through Amdahl''s law [Source: Daniels220
    (https://commons.wikimedia.org/w/index.php?curid=6678551), CC BY-SA 3.0]'
  prefs: []
  type: TYPE_NORMAL
- en: Considering this, we might want to investigate using a broker to manage our
    multiprocessing. However, this can lead to using **clogging** up the broker, resulting
    in **deadlock**. To understand the gravity of this situation, we will explore
    deadlocks in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deadlocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deadlocks can arise when it comes to bigger applications, where it is common
    to manage the multiprocessing through a task broker. This is usually managed via
    a database or caching mechanism such as Redis. This consists of a queue where
    the tasks are added, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – The flow of tasks when multiprocessing with a broker or queue'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.10_B17720.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.10 – The flow of tasks when multiprocessing with a broker or queue
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that new tasks can be added to the queue. As time goes on,
    the oldest tasks get taken off the queue and passed into the pool. Throughout
    the application, our code can send functions and parameters to the queue anywhere
    in the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, the library that does this is called **Celery**. There is also a
    Celery crate for Rust. This approach is also utilized for multiple server setups.
    Considering this, we could be tempted to send tasks to the queue inside another
    task. However, we can see here that this approach can lock up our queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Deadlock with task broker'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.11_B17720.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.11 – Deadlock with task broker
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 3.11*, we can see that the tasks in the pool have sent tasks to
    the queue. However, they cannot complete until their dependencies have been executed.
    The thing is, they will never execute because the pool is full of tasks waiting
    for their dependency to complete and the pool is full, so they cannot be processed.
    The issue with this problem is that there are no errors raised with this—the pool
    will just hang. Deadlock is not the only problem that will arise without helpful
    warnings. Considering this, we must cover our last concept that we should be aware
    of before being creative: **race conditions**.'
  prefs: []
  type: TYPE_NORMAL
- en: Race conditions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Race conditions occur when two or more threads access shared data that they
    both try to change. As we have noted when we were building and running our threads,
    they sometimes ran out of order. We can demonstrate this with a simple concept,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If we were to have *thread one* calculate a price and write to a file and *thread
    two* to also calculate a price, read the price calculated from the *thread one*
    file, and add them together, there could be a chance that the price will not be
    written to the file before *thread two* reads it. What is even worse is that there
    could be an old price in the file. If this is the case, we will never know that
    the error occurred. The term *race conditions* is built upon the fact that both
    threads are racing to the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a solution to race conditions, we can introduce **locks**. Locks can be utilized
    for stopping other threads from accessing certain things such as a file until
    your thread has finished with it. However, it has to be noted that these locks
    only work inside the process; therefore, other processes can access the file.
    Caching solutions such as Redis and general databases have already implemented
    these safeguards, and locks will not protect against the race condition described
    in this section. In my experience, when we get creative with thread concepts such
    as locks, it is usually a sign that we must take a step back and rethink our design.
  prefs: []
  type: TYPE_NORMAL
- en: Even an SQLite database file will manage our data race issues when reading and
    writing to a file, and if the data race condition described at the start of this
    section looks like it might happen, it is best to just not have them running at
    the same time at all. Sequential programming is safer and more useful.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through the basics of multiprocessing and multithreading.
    We then went through practical ways to utilize threads and processes. We then
    explored the Fibonacci sequence to explore how processes can speed up our computations.
    We also saw through the Fibonacci sequence that how we compute our problems is
    the biggest factor over threads and processes. Algorithms that scale exponentially
    should be avoided before reaching for multiprocessing for speed gains. We must
    remember that while it might be tempting to reach for more complex approaches
    to multiprocessing, this can lead to problems such as deadlock and data races.
    We kept our multiprocessing tight by keeping it contained within a processing
    pool. If we keep these principles in mind and keep all our multiprocessing contained
    to a pool, we will keep our hard-to-diagnose problems to a minimum. This does
    not mean that we should never be creative with multiprocessing but it is advised
    to do further reading on this field, as there are books entirely dedicated to
    concurrency (as noted in the *Further reading* section, with particular chapters
    to focus on). This is just an introduction to enable us to use concurrency in
    our Python packages if needed. In the next chapter, we will be building our own
    Python packages so that we can distribute our Python code across multiple projects
    and reuse code.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the difference between a process and a thread?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why wouldn't multithreading speed up our Python Fibonacci sequence calculations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is a multiprocessing pool used?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our threads in Rust return `Result<i8, Box<dyn Any + Send>>`. What does this
    mean?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why should we avoid using a recursion tree if we can?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Should you just spin up more processes when you need a faster runtime?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why should you avoid complex multiprocessing if you can?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does `join` do for our program in multithreading?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does `join` not return anything in a process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threads are lightweight and enable multithreading, where we can run multiple
    tasks that could have idle time. A process is more expensive, enabling us to run
    multiple CPU-heavy tasks at the same time. Processes do not share memory, while
    threads do.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multithreading would not speed up our Fibonacci sequence calculations because
    calculating Fibonacci numbers is a CPU-heavy task that does not have any idle
    time; therefore, the threads would run sequentially in Python. However, we did
    demonstrate that Rust can run multiple threads at the same time, getting a significant
    speed increase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiprocessing is expensive and the processes do not share memory, making the
    implementation potentially more complex. A processing pool keeps the multiprocessing
    part of a program to a minimum. This approach also enables us to easily control
    the different numbers of workers we need as they're all in one place, and we can
    also return all the outcomes in the same sequence as they are returned from the
    multiprocessing pool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our Rust thread could fail. If it doesn't, then it will return an integer. If
    it fails, it could return anything of any size, which is why it's on the heap.
    It also has the `Send` trait, which means that it can be sent across threads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recursion trees scale exponentially. Even if we are multithreading, our computation
    time will quickly scale, pushing our milliseconds into seconds once we've crossed
    a boundary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No—as demonstrated in Amdahl's law, increasing the workers will give us some
    speedup, but we will have diminishing returns as the number of workers increases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complex multiprocessing/multithreading can introduce a range of silent errors
    such as deadlock and data races that can be hard to diagnose and solve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`join` blocks the program until the thread has completed. It can also return
    the result of the thread if we overwrite Python''s `join` function.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Processes do not share the same memory space, therefore they cannot be accessed.
    We can, however, access data from other processes by saving data to files for
    our main process to access or pipe data via `stdin` and `stdout`, as we did in
    our Rust multiprocessing example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Pan Wu* (2020). *Understanding Python Multithreading and Multiprocessing via
    Simulation:* https://towardsdatascience.com/understanding-python-multithreading-and-multiprocessing-via-simulation-3f600dbbfe31'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Brian Troutwine* (2018). *Hands-On Concurrency with Rust*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gabriele Lanaro* and *Quan Nguyen* (2019). *Learning Path Advanced Python
    Programming*: [*Chapter 8*](B17720_08_Final_SK_ePub.xhtml#_idTextAnchor142) (*Advanced
    Introduction to Concurrent and Parallel Programming*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Andrew Johnson* (2018). *Hands-On Functional Programming in Rust*: [*Chapter
    8*](B17720_08_Final_SK_ePub.xhtml#_idTextAnchor142) (*Implementing Concurrency*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rahul Sharma and Vesa Kaihlavirta* (2018). *Mastering Rust*: [*Chapter 8*](B17720_08_Final_SK_ePub.xhtml#_idTextAnchor142)
    (*Concurrency*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Locks – Mutex, Condvar, Barriers and RWLock</h1>
                </header>
            
            <article>
                
<p>In this chapter, we're going to do a deep-dive on hopper, the grown-up version of Ring from <a href="5a332d94-37e4-4748-8920-1679b07e2880.xhtml" target="_blank">Chapter 4</a>, <em>Sync and Send – the Foundation of Rust Concurren</em><em>cy</em>. Hopper's approach to back-pressure<span>—</span>the weakness we identified in telem<em>—</em>is to block when filled to capacity, as <kbd>SyncSender</kbd> does. Hopper's special trick is that it pages out to disk. The hopper user defines how many bytes of in-memory space hopper is allowed to consume, like <kbd>SyncSender</kbd>, except in terms of bytes rather than total elements of <kbd>T</kbd>. Furthermore, the user is able to configure the number of on-disk bytes that are consumed when hopper's in-memory capacity is filled and it has to page out to disk. The other properties of MSPC are held, in-order delivery, retention of data once stored, and so on.</p>
<p>Before we can dig through hopper, however, we need to introduce more of Rust's concurrency primitives. We'll work on some puzzles from <em>The Little Book of Semaphores</em> to explain them, which will get a touch hairy in some places on account of how Rust does not have a semaphore available.</p>
<p><span>By the close of this chapter, we will have:</span></p>
<ul>
<li>Discussed the purpose and use of Mutex, Condvar, Barriers, and RWLock</li>
<li>Investigated a disk-backed specialization of the standard library's MPSC called hopper </li>
<li>Seen how to apply QuickCheck, AFL, and comprehensive benchmarks in a production setting</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="mce-root">This chapter requires a working Rust installation. The details of verifying your installation are covered in <a href="5f3aec9d-fd53-48ff-9ba8-43ce13e91cff.xhtml" target="_blank">Chapter 1</a>, <em>Preliminaries – Machine Architecture and Getting Started with Rust</em>. No additional software tools are required.</p>
<p class="mce-root">You can find the source code for this book's projects on GitHub at: <a href="https://github.com/PacktPublishing/Rust-Concurrency/">https://github.com/PacktPublishing/Rust-Concurrency/</a>. This chapter has its source code under <kbd>Chapter05</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Read many, write exclusive locks – RwLock</h1>
                </header>
            
            <article>
                
<p>Consider a situation where you have a resource that must be manipulated only a single thread at a time, but is safe to be queried by many—that is, you have many readers and only one writer. While we could protect this resource with a <kbd>Mutex</kbd>, the trouble is that the mutex makes no distinction between its lockers; every thread will be forced to wait, no matter what their intentions. <kbd>RwLock&lt;T&gt;</kbd> is an alternative to the mutex concept, allowing for two kinds of locks—read and write. Analogously to Rust's references, there can only be one write lock taken at a time but multiple reader locks, exclusive of a write lock. Let's look at an example:</p>
<pre style="padding-left: 30px">use std::thread;
use std::sync::{Arc, RwLock};

fn main() {
    let resource: Arc&lt;RwLock&lt;u16&gt;&gt; = Arc::new(RwLock::new(0));

    let total_readers = 5;

    let mut reader_jhs = Vec::with_capacity(total_readers);
    for _ in 0..total_readers {
        let resource = Arc::clone(&amp;resource);
        reader_jhs.push(thread::spawn(move || {
            let mut total_lock_success = 0;
            let mut total_lock_failure = 0;
            let mut total_zeros = 0;
            while total_zeros &lt; 100 {
                match resource.try_read() {
                    Ok(guard) =&gt; {
                        total_lock_success += 1;
                        if *guard == 0 {
                            total_zeros += 1;
                        }
                    }
                    Err(_) =&gt; {
                        total_lock_failure += 1;
                    }
                }
            }
            (total_lock_failure, total_lock_success)
        }));
    }

    {
        let mut loops = 0;
        while loops &lt; 100 {
            let mut guard = resource.write().unwrap();
            *guard = guard.wrapping_add(1);
            if *guard == 0 {
                loops += 1;
            }
        }
    }

    for jh in reader_jhs {
        println!("{:?}", jh.join().unwrap());
    }
}</pre>
<p>The idea here is that we'll have one writer thread spinning and incrementing, in a wrapping fashion, a shared resource—a <kbd>u16.</kbd> Once the <kbd>u16</kbd> has been wrapped 100 times, the writer thread will exit. Meanwhile, a <kbd>total_readers</kbd> number of read threads will attempt to take a read lock on the shared resource—a <kbd>u16—until</kbd> it hits zero <kbd>100</kbd> times. We're gambling here, essentially, on thread ordering. Quite often, the program will exit with this result:</p>
<pre style="padding-left: 30px"><strong>(0, 100)
(0, 100)
(0, 100)
(0, 100)
(0, 100)</strong></pre>
<p>This means that each reader thread never failed to get its read lock—there were no write locks present. That is, the reader threads were scheduled before the writer. Our main function only joins on reader handlers and so the writer is left writing as we exit. Sometimes, we'll hit just the right scheduling order, and get the following result:</p>
<pre style="padding-left: 30px"><strong>(0, 100)
(126143752, 2630308)
(0, 100)
(0, 100)
(126463166, 2736405)</strong></pre>
<p>In this particular instance, the second and final reader threads were scheduled just after the writer and managed to catch a time when the guard was not zero. Recall that the first element of the pair is the total number of times the reader thread was not able to get a read lock and was forced to retry. The second is the number of times that the lock was acquired. In total, the writer thread did <kbd>(2^18 * 100) ~= 2^24</kbd> writes, whereas the second reader thread did <kbd>log_2 2630308 ~= 2^21</kbd> reads. That's a lot of lost writes, which, maybe, is okay. Of more concern, that's a lot of useless loops, approximately <kbd>2^26</kbd>. Ocean levels are rising and we're here burning up electricity like nobody had to die for it.</p>
<p>How do we avoid all this wasted effort? Well, like most things, it depends on what we're trying to do. If we need every reader to get every write, then an MPSC is a reasonable choice. It would look like this:</p>
<pre style="padding-left: 30px">use std::thread;
use std::sync::mpsc;

fn main() {
    let total_readers = 5;
    let mut sends = Vec::with_capacity(total_readers);

    let mut reader_jhs = Vec::with_capacity(total_readers);
    for _ in 0..total_readers {
        let (snd, rcv) = mpsc::sync_channel(64);
        sends.push(snd);
        reader_jhs.push(thread::spawn(move || {
            let mut total_zeros = 0;
            let mut seen_values = 0;
            for v in rcv {
                seen_values += 1;
                if v == 0 {
                    total_zeros += 1;
                }
                if total_zeros &gt;= 100 {
                    break;
                }
            }
            seen_values
        }));
    }

    {
        let mut loops = 0;
        let mut cur: u16 = 0;
        while loops &lt; 100 {
            cur = cur.wrapping_add(1);
            for snd in &amp;sends {
                snd.send(cur).expect("failed to send");
            }
            if cur == 0 {
                loops += 1;
            }
        }
    }

    for jh in reader_jhs {
        println!("{:?}", jh.join().unwrap());
    }
}</pre>
<p>It will run—for a while—and print out the following:</p>
<pre><strong>6553600
6553600
6553600
6553600
6553600</strong></pre>
<p>But what if every reader does not need to see every write, meaning that it's acceptable for a reader to miss writes so long as it does not miss all of the writes? We have options. Let's look at one.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Blocking until conditions change – condvar</h1>
                </header>
            
            <article>
                
<p>One option is a condvar, or CONDition VARiable. Condvars are a nifty way to block a thread, pending a change in some Boolean condition. One difficulty is that condvars are associated exclusively with mutexes, but in this example, we don't mind all that much.</p>
<p>The way a condvar works is that, after taking a lock on a mutex, you pass the <kbd>MutexGuard</kbd> into <kbd>Condvar::wait</kbd>, which blocks the thread. Other threads may go through this same process, blocking on the same condition. Some other thread will take the same exclusive lock and eventually call either <kbd>notify_one</kbd> or <kbd>notify_all</kbd> on the condvar. The first wakes up a single thread, the second wakes up <em>all</em> threads. Condvars are subject to spurious wakeup, meaning the thread may leave its block without a notification being sent to it. For this reason condvars check their conditions in a loop. But, once the condvar wakes, you <em>are</em> guaranteed to hold the mutex, which prevents deadlocks on spurious wakeup.</p>
<p>Let's adapt our example to use a condvar. There's actually a fair bit going on in this example, so we'll break it down into pieces:</p>
<pre style="padding-left: 30px">use std::thread;
use std::sync::{Arc, Condvar, Mutex};

fn main() {
    let total_readers = 5;
    let mutcond: Arc&lt;(Mutex&lt;(bool, u16)&gt;, Condvar)&gt; =
        Arc::new((Mutex::new((false, 0)), Condvar::new()));</pre>
<p>Our preamble is similar to the previous examples, but the setup is already quite strange. We're synchronizing threads on <kbd>mutcond</kbd>, which is an <kbd>Arc&lt;(Mutex&lt;(bool, u16)&gt;, Condvar)&gt;</kbd>. Rust's condvar is a touch awkward. It's undefined behavior to associate a condvar with more than one mutex, but there's really nothing in the type of <kbd>Condvar</kbd> that makes that an invariant. We just have to remember to keep them associated. To that end, it's not uncommon to see a <kbd>Mutex</kbd> and <kbd>Condvar</kbd> paired up in a tuple, as here. Now, why <kbd>Mutex&lt;(bool, u16)&gt;</kbd>? The second element of the tuple is our <em>resource</em>, which is common to other examples. The first element is a Boolean flag, which we use as a signal to mean that there are writes available. Here are our reader threads:</p>
<pre>    let mut reader_jhs = Vec::with_capacity(total_readers);
    for _ in 0..total_readers {
        let mutcond = Arc::clone(&amp;mutcond);
        reader_jhs.push(thread::spawn(move || {
            let mut total_zeros = 0;
            let mut total_wakes = 0;
            let &amp;(ref mtx, ref cnd) = &amp;*mutcond;

            while total_zeros &lt; 100 {
                let mut guard = mtx.lock().unwrap();
                while !guard.0 {
                    guard = cnd.wait(guard).unwrap();
                }
                guard.0 = false;

                total_wakes += 1;
                if guard.1 == 0 {
                    total_zeros += 1;
                }
            }
            total_wakes
        }));
    }</pre>
<p>Until <kbd>total_zeros</kbd> hits 100, the reader thread locks the mutex, checks the guard inside the mutex for write availability, and, if there are no writes, does a wait on the condvar, which gives up the lock. The reader thread is then blocked until a <kbd>notify_all</kbd> is called—as we'll see shortly. Every reader thread then races to be the first to reacquire the lock. The lucky winner notes that there are no more writes to be read and then does the normal flow we've seen in previous examples. It bears repeating that every thread that wakes up from a condition wait is racing to be the first to reacquire the mutex. Our reader is uncooperative in that it immediately prevents the chance of any other reader threads finding a resource available. However, they will still wake up spuriously and be forced to wait again. Maybe. The reader threads are also competing with the writer thread to acquire the lock. Let's look at the writer thread:</p>
<pre>    let _ = thread::spawn(move || {
        let &amp;(ref mtx, ref cnd) = &amp;*mutcond;
        loop {
            let mut guard = mtx.lock().unwrap();
            guard.1 = guard.1.wrapping_add(1);
            guard.0 = true;
            cnd.notify_all();
        }
    });</pre>
<p>The writer thread is an infinite loop, which we orphan in an unjoined thread. Now, it's entirely possible that the writer thread will acquire the lock, bump the resource, notify waiting reader threads, give up the lock, and then immediately re-acquire the lock to begin the while process before any reader threads can get scheduled in. This means it's entirely possible that the resource being zero will happen several times before a reader thread is lucky enough to notice. Let's close out this program:</p>
<pre>    for jh in reader_jhs {
        println!("{:?}", jh.join().unwrap());
    }
}</pre>
<p>Ideally, what we'd like is some manner of bi-directionality—we'd like the writer to signal that there are reads and the reader to signal that there is capacity. This is suspiciously like how Ring in the previous chapter worked through its size variable, when we were careful to not race on that variable, that is. We might, for instance, layer another condition variable into the mix, this one for the writer, but that's not what we have here and the program suffers for it. Here's one run:</p>
<pre><strong>7243473
6890156
6018468
6775609
6192116</strong></pre>
<p>Phew! That's significantly more loops than previous examples. None of this is to say that condition variables are hard to use—they're not—it's just that they need to be used in conjunction with other primitives. We'll see an excellent example of this later in the chapter.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Blocking until the gang's all here - barrier</h1>
                </header>
            
            <article>
                
<p>A barrier is a synchronization device that blocks threads until such time that a predefined number of threads have waited on the same barrier.  When a barrier's waiting threads wake up, one is declared leader—discoverable by inspecting the <kbd>BarrierWaitResult</kbd>—but this confers no scheduling advantage. A barrier becomes useful when you wish to delay threads behind an unsafe initialization of some resource—say a C library's internals that have no thread-safety at startup, or have a need to force participating threads to start a critical section at roughly the same time. The latter is the broader category, in your author's experience. When programming with atomic variables, you'll run into situations where a barrier will be useful. Also, consider for a second writings multi-threaded code for low-power devices. There are two strategies possible these days for power management: scaling the CPU to meet requirements, adjusting the runtime of your program live, or burning through a section of your program as quickly as possible and then shutting down the chip. In the latter approach, a barrier is just the primitive you need.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">More mutexes, condvars, and friends in action</h1>
                </header>
            
            <article>
                
<p>Admittedly, the examples in the preceding sections got a little convoluted. There's a reason to that madness, I promise, but before we move on I'd like to show you some working examples from the charming <em>The Little Book of Semaphores</em>. This book, in case you skipped previous bibliographic notes, is a collection of concurrency puzzles suitable for self-learning, on account of the puzzles being amusing and coming with good hints. As the title implies, the book does make use of the semaphore primitive, which Rust does not have. Though, as mentioned in the previous chapter, we will build a semaphore in the next chapter.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The rocket preparation problem</h1>
                </header>
            
            <article>
                
<p>This puzzle does not actually appear in <em>The Little Book of Semaphores</em> but it is based on one of the puzzles there - the cigarette smoker's problem from section 4.5. Personally, I think cigarettes are gross so we're going to reword things a touch. The idea is the same.</p>
<p>We have four threads in total. One thread, the <kbd>producer</kbd>, randomly publishes one of <kbd>fuel</kbd>, <kbd>oxidizer</kbd>, or <kbd>astronauts</kbd>. The remaining three threads are the consumers, or <em>rockets</em>, which must take their resources in the order listed previously. If a rocket doesn't get its resources in that order, it's not safe to prepare the rocket, and if it doesn't have all three, the rocket can't lift-off. Moreover, once all the rockets are prepped, we want to start a 10 second count-down, only after which may the rockets lift-off.</p>
<p>The preamble to our solution is a little longer than usual, in the interest of keeping the solution as a standalone:</p>
<pre style="padding-left: 30px">use std::{thread, time};
use std::sync::{Arc, Barrier, Condvar, Mutex};

// NOTE if this were a crate, rather than a stand-alone<br/>// program, we could and _should_ use the XorShiftRng<br/>// from the 'rand' crate.
pub struct XorShiftRng {
    state: u32,
}

impl XorShiftRng {
    fn with_seed(seed: u32) -&gt; XorShiftRng {
        XorShiftRng { state: seed }
    }

    fn next_u32(&amp;mut self) -&gt; u32 {
        self.state ^= self.state &lt;&lt; 13;
        self.state ^= self.state &gt;&gt; 17;
        self.state ^= self.state &lt;&lt; 5;
        return self.state;
    }
}</pre>
<p>We don't really need excellent randomness for this solution—the OS scheduler injects enough already—but just something small-ish. <kbd>XorShift</kbd> fits the bill. Now, for our resources:</p>
<pre style="padding-left: 30px">struct Resources {
    lock: Mutex&lt;(bool, bool, bool)&gt;,
    fuel: Condvar,
    oxidizer: Condvar,
    astronauts: Condvar,
}</pre>
<p>The struct is protected by a single <kbd>Mutex&lt;(bool, bool, bool)&gt;</kbd>, the Boolean being a flag to indicate that there's a resource available. We hold the first flag to mean <kbd>fuel</kbd>, the second <kbd>oxidizer</kbd>, and the third <kbd>astronauts</kbd>. The remainder of the struct are condvars to match each of these resource concerns. The <kbd>producer</kbd> is a straightforward infinite loop:</p>
<pre>fn producer(resources: Arc&lt;Resources&gt;) {
    let mut rng = XorShiftRng::with_seed(2005);
    loop {
        let mut guard = resources.lock.lock().unwrap();
        (*guard).0 = false;
        (*guard).1 = false;
        (*guard).2 = false;
        match rng.next_u32() % 3 {
            0 =&gt; {
                (*guard).0 = true;
                resources.fuel.notify_all()
            }
            1 =&gt; {
                (*guard).1 = true;
                resources.oxidizer.notify_all()
            }
            2 =&gt; {
                (*guard).2 = true;
                resources.astronauts.notify_all()
            }</pre>
<pre>            _ =&gt; unreachable!(),
        }
    }
}</pre>
<p>On each iteration, the producer chooses a new resource—<kbd>rng.next_u32() % 3</kbd>—and sets the Boolean flag for that resource before notifying all threads waiting on the <kbd>fuel</kbd> condvar. Meanwhile, the compiler and CPU are free to re-order instructions and the memory <kbd>notify_all</kbd> acts like a causality gate; everything before in the code is before in causality, and likewise, afterwards. If the resource bool flip was after the notification, the wake-up would be spurious from the point of view of the waiting threads and lost from the point of view of the producer. The <kbd>rocket</kbd> is straightforward:</p>
<pre style="padding-left: 30px">fn rocket(name: String, resources: Arc&lt;Resources&gt;, <br/>          all_go: Arc&lt;Barrier&gt;, lift_off: Arc&lt;Barrier&gt;) {
    {
        let mut guard = resources.lock.lock().unwrap();
        while !(*guard).0 {
            guard = resources.fuel.wait(guard).unwrap();
        }
        (*guard).0 = false;
        println!("{:&lt;6} ACQUIRE FUEL", name);
    }
    {
        let mut guard = resources.lock.lock().unwrap();
        while !(*guard).1 {
            guard = resources.oxidizer.wait(guard).unwrap();
        }
        (*guard).1 = false;
        println!("{:&lt;6} ACQUIRE OXIDIZER", name);
    }
    {
        let mut guard = resources.lock.lock().unwrap();
        while !(*guard).2 {
            guard = resources.astronauts.wait(guard).unwrap();
        }
        (*guard).2 = false;
        println!("{:&lt;6} ACQUIRE ASTRONAUTS", name);
    }

    all_go.wait();
    lift_off.wait();
    println!("{:&lt;6} LIFT OFF", name);
}</pre>
<p>Each thread, regarding the resource requirements, waits on the condvar for the producer to make it available. A race occurs to re-acquire the mutex, as discussed, and only a single thread gets the resource. Finally, once all the resources are acquired, the <kbd>all_go</kbd> barrier is hit to delay any threads ahead of the count-down. Here we need the <kbd>main</kbd> function:</p>
<pre style="padding-left: 30px">fn main() {
    let all_go = Arc::new(Barrier::new(4));
    let lift_off = Arc::new(Barrier::new(4));
    let resources = Arc::new(Resources {
        lock: Mutex::new((false, false, false)),
        fuel: Condvar::new(),
        oxidizer: Condvar::new(),
        astronauts: Condvar::new(),
    });

    let mut rockets = Vec::new();
    for name in &amp;["KSC", "VAB", "WSMR"] {
        let all_go = Arc::clone(&amp;all_go);
        let lift_off = Arc::clone(&amp;lift_off);
        let resources = Arc::clone(&amp;resources);
        rockets.push(thread::spawn(move || {
            rocket(name.to_string(), resources, <br/>                   all_go, lift_off)
        }));
    }

    thread::spawn(move || {
        producer(resources);
    });

    all_go.wait();
    let one_second = time::Duration::from_millis(1_000);
    println!("T-11");
    for i in 0..10 {
        println!("{:&gt;4}", 10 - i);
        thread::sleep(one_second);
    }
    lift_off.wait();

    for jh in rockets {
        jh.join().unwrap();
    }
}</pre>
<p>Note that, roughly, the first half of the function is made up of the barriers and resources or rocket threads. <kbd>all_go.wait()</kbd> is where it gets interesting. This main thread has spawned all the children and is now blocked on the all-go signal from the rocket threads, meaning they've collected their resources and are also blocked on the same barrier. That done, the count-down happens, to add a little panache to the solution; meanwhile, the rocket threads have started to wait on the <kbd>lift_off</kbd> barrier. It is, incidentally, worth noting that the producer is still producing, drawing CPU and power. Once the count-down is complete, the rocket threads are released, the main thread joins on them to allow them to print their goodbyes, and the program is finished. Outputs will vary, but here's one representative example:</p>
<pre style="padding-left: 30px"><strong>KSC    ACQUIRE FUEL
WSMR   ACQUIRE FUEL
WSMR   ACQUIRE OXIDIZER
VAB    ACQUIRE FUEL
WSMR   ACQUIRE ASTRONAUTS
KSC    ACQUIRE OXIDIZER
KSC    ACQUIRE ASTRONAUTS
VAB    ACQUIRE OXIDIZER
VAB    ACQUIRE ASTRONAUTS
T-11
  10
   9
   8
   7
   6
   5
   4
   3
   2
   1
VAB    LIFT OFF
WSMR   LIFT OFF
KSC    LIFT OFF</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The rope bridge problem</h1>
                </header>
            
            <article>
                
<p>This puzzle appears in <em>The Little Book of Semaphores</em> as the <em>Baboon crossing problem</em>, section 6.3. Downey notes that it is adapted from Tanenbaum's <em>Operating Systems: Design and Implementation</em>, so you're getting it third-hand here. The problem description is thus:</p>
<div class="packt_quote">"There is a deep canyon somewhere in Kruger National Park, South Africa, and a single rope that spans the canyon. Baboons can cross the canyon by swinging hand-over-hand on the rope, but if two baboons going in opposite directions meet in the middle, they will fight and drop to their deaths. Furthermore, the rope is only strong enough to hold 5 baboons. If there are more baboons on the rope at the same time, it will break.Assuming that we can teach the baboons to use semaphores, we would like to design a synchronization scheme with the following properties:</div>
<div class="packt_quote">
<ul>
<li><em>Once a baboon has begun to cross, it is guaranteed to get to the other side without running into a baboon going the other way.</em></li>
<li><em>There are never more than 5 baboons on the rope.</em>"</li>
</ul>
</div>
<p>Our solution does not use semaphores. Instead, we lean on the type system to provide guarantees, leaning on it to ensure that no left-traveler will ever meet a right-traveler. Let's dig in:</p>
<pre style="padding-left: 30px">use std::thread;
use std::sync::{Arc, Mutex};

#[derive(Debug)]
enum Bridge {
    Empty,
    Left(u8),
    Right(u8),
}</pre>
<p>We see here the usual preamble, and then a type, <kbd>Bridge</kbd>. <kbd>Bridge</kbd> is the model of the <kbd>rope</kbd> bridge of the problem statement and is either empty, has left-traveling baboons on it, or has right-traveling baboons on it; there's no reason to fiddle with flags and infer state when we can just encode it into a type. In fact, leaning on the type system, our synchronization is very simple:</p>
<pre style="padding-left: 30px">fn main() {
    let rope = Arc::new(Mutex::new(Bridge::Empty));</pre>
<p>Just a single mutex. We represent either side of the bridge as a thread, each side trying to get its own baboons across but co-operatively allowing baboons to reach its side. Here's the left side of the bridge:</p>
<pre style="padding-left: 30px">    let lhs_rope = Arc::clone(&amp;rope);
    let lhs = thread::spawn(move || {
        let rope = lhs_rope;
        loop {
            let mut guard = rope.lock().unwrap();
            match *guard {
                Bridge::Empty =&gt; {
                    *guard = Bridge::Right(1);
                }
                Bridge::Right(i) =&gt; {
                    if i &lt; 5 {
                        *guard = Bridge::Right(i + 1);
                    }
                }
                Bridge::Left(0) =&gt; {
                    *guard = Bridge::Empty;
                }
                Bridge::Left(i) =&gt; {
                    *guard = Bridge::Left(i - 1);
                }
            }
        }
    });</pre>
<p>When the left side of the bridge finds the bridge itself empty, one right-traveling baboon is sent on its way. Further, when the left side finds that there are already right-traveling baboons on the bridge and the rope's capacity of five has not been reached, another baboon is sent on its way. Left-traveling baboons are received from the rope, decrementing the total baboons on the rope. The special case here is the clause for <kbd>Bridge::Left(0)</kbd>. While there are no baboons on the bridge still, technically, if the right side of the bridge were to be scheduled before the left side, it would send a baboon on its way, as we'll see shortly. We could make the removal of a baboon more aggressive and set the bridge to <kbd>Bridge::Empty</kbd> as soon as there are no travelers, of course. Here's the right side of the bridge, which is similar to the left:</p>
<pre>    let rhs = thread::spawn(move || loop {
        let mut guard = rope.lock().unwrap();
        match *guard {
            Bridge::Empty =&gt; {
                *guard = Bridge::Left(1);
            }
            Bridge::Left(i) =&gt; {
                if i &lt; 5 {
                    *guard = Bridge::Left(i + 1);
                }
            }
            Bridge::Right(0) =&gt; {
                *guard = Bridge::Empty;
            }
            Bridge::Right(i) =&gt; {
                *guard = Bridge::Right(i - 1);
            }
        }
    });

    rhs.join().unwrap();
    lhs.join().unwrap();
}</pre>
<p>Now, is this fair? It depends on your system mutex. Some systems provide mutexes that are fair in that, if one thread has acquired the lock repeatedly and starved other threads attempting to lock the same primitive, the greedy thread will be de-prioritized under the others. Such fairness may or may not incur additional overheads when locking, depending on the implementation. Whether you want fairness, in fact, depends strongly on the problem you're trying to solve. If we were only interested in shuffling baboons across the rope bridge as quickly as possible—maximizing throughput—then it doesn't truly matter which direction they're coming from. The original problem statement makes no mention of fairness, which it kind of shuffles around by allowing the stream of baboons to be infinite. Consider what would happen if the baboons were finite on either side of the bridge and we wanted to reduce the time it takes for any individual baboon to cross to the other side, to minimize latency. Our present solution, adapted to cope with finite streams, is pretty poor, then, in that regard. The situation could be improved by occasional yielding, layering in more aggressive rope packing, intentional back off, or a host of other strategies. Some platforms allow you to dynamically shift the priority of threads with regard to locks, but Rust does not offer that in the standard library.</p>
<p>Or, you know, we could use two bounded MPSC channels. That's an option. It all depends on what you're trying to get done.</p>
<p>Ultimately, the tools in safe Rust are very useful for performing computations on existing data structures without having to dip into any funny business. If that's your game, you're very unlikely to need to head much past <kbd>Mutex</kbd> and <kbd>Condvar</kbd>, and possibly into <kbd>RwLock</kbd> and <kbd>Barrier</kbd>. But, if you're building structures that are made of pointers, you'll have to dip into some of the funny business we saw in Ring, with all the dangers that brings.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hopper—an MPSC specialization</h1>
                </header>
            
            <article>
                
<p>As mentioned at the tail end of the last chapter, you'd need a fairly specialized use-case to consider not using stdlib's MPSC. In the rest of this chapter, we'll discuss such a use-case and the implementation of a library meant to fill it.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The problem</h1>
                </header>
            
            <article>
                
<p>Recall back to the last chapter, where the role-threads in telem communicated with one another over MPSC channels. Recall also that telem was a quick version of the cernan (<a href="https://crates.io/crates/cernan">https://crates.io/crates/cernan</a>) project, which fulfills basically the same role but over many more ingress protocols, egress protocols, and with the sharp edges worn down. One of the key design goals of cernan is that if it receives your data, it will deliver it downstream at least once. This implies that, for supporting ingress protocols, cernan must know, along the full length of the configured routing topology, that there is sufficient space to accept a new event, whether it's a piece of telemetry, a raw byte buffer, or a log line. Now, that's possible by using <kbd>SyncSender</kbd>, as we've seen. The trick comes in combination with a second design goal of cernan—very low, configurable resource consumption. Your author has seen cernan deployed to high-availability clusters with a single machine dedicated to running a cernan, as well as cernan running shotgun with an application server or as a part of a daemonset in a k8s cluster. In the first case, cernan can be configured to consume all of the machine's resources. In the later two, some thought has to be taken to giving cernan just enough resources to do its duties, relative to the expected input of the telemetered systems.</p>
<p>On modern systems, there's often an abundance of disk space and limited—relatively speaking—RAM. The use of <kbd>SyncSender</kbd> would require either a relatively low number of ingestible events or a high possible consumption of memory by cernan. These cases usually hit when an egress protocol is failing, possibly because the far-system is down. If cernan were to exacerbate a partial system failure by crowding out an application server because of a fault in a loosely-coupled system, well, that'd be pretty crummy.</p>
<p>For many users, it's also not acceptable to go totally blind during such a partial outage. Tricky constraints for <kbd>SyncSender</kbd>. Tricky enough, in fact, that we decided to write our own MPSC, called hopper (<a href="https://crates.io/crates/hopper">https://crates.io/crates/hopper</a>). Hopper allows endusers to configure the total in-memory consumption of the queue, in bytes. That's not far off <kbd>SyncSender</kbd>, with a bit of calculation. What's special about hopper is that it can page to disk.</p>
<p>Hopper shines where memory constraints are very tight but you do not want to shed events. Or, you want to push shedding events off as far as possible. Similarly to the in-memory queue, hopper allows endusers to configure the maximum number of bytes to be consumed ondisk. A single hopper queue will hold <kbd>in_memory_bytes + on_disk_bytes</kbd> maximally before it's forced to shed events; we'll see the exact mechanism here directly. All of this is programmed with an eye toward maximal throughput and blocking threads that have no work, to save CPU time.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hopper in use</h1>
                </header>
            
            <article>
                
<p>Before we dig into the implementation of hopper, it'll be instructive to see it in practice. Let's adapt the last iteration of our ring program series. For reference, here's what that looked like:</p>
<pre style="padding-left: 30px">use std::thread;
use std::sync::mpsc;

fn writer(chan: mpsc::SyncSender&lt;u32&gt;) -&gt; () {
    let mut cur: u32 = 0;
    while let Ok(()) = chan.send(cur) {
        cur = cur.wrapping_add(1);
    }
}

fn reader(read_limit: usize, chan: mpsc::Receiver&lt;u32&gt;) -&gt; () {
    let mut cur: u32 = 0;
    while (cur as usize) &lt; read_limit {
        let num = chan.recv().unwrap();
        assert_eq!(num, cur);
        cur = cur.wrapping_add(1);
    }
}

fn main() {
    let capacity = 10;
    let read_limit = 1_000_000;
    let (snd, rcv) = mpsc::sync_channel(capacity);

    let reader_jh = thread::spawn(move || {
        reader(read_limit, rcv);
    });
    let _writer_jh = thread::spawn(move || {
        writer(snd);
    });

    reader_jh.join().unwrap();
}</pre>
<p>The hopper version is a touch different. This is the preamble:</p>
<pre>extern crate hopper;
extern crate tempdir;

use std::{mem, thread};</pre>
<p>No surprise there. The function that will serve as the <kbd>writer</kbd> thread is:</p>
<pre>fn writer(mut chan: hopper::Sender&lt;u32&gt;) -&gt; () {
    let mut cur: u32 = 0;
    while let Ok(()) = chan.send(cur) {
        cur = cur.wrapping_add(1);
    }
}</pre>
<p>Other than the switch from <kbd>mpsc::SyncSender&lt;u32&gt;</kbd> to <kbd>hopper::Sender&lt;u32&gt;</kbd>—all hopper senders are impliclty bounded—the only other difference is that <kbd>chan</kbd> must be mutable. Now, for the <kbd>reader</kbd> thread:</p>
<pre>fn reader(read_limit: usize, <br/>          mut chan: hopper::Receiver&lt;u32&gt;) -&gt; () {
    let mut cur: u32 = 0;
    let mut iter = chan.iter();
    while (cur as usize) &lt; read_limit {
        let num = iter.next().unwrap();
        assert_eq!(num, cur);
        cur = cur.wrapping_add(1);
    }
}</pre>
<p>Here there's a little more to do. In hopper, the receiver is intended to be used as an iterator, which <em>is</em> a little awkward here as we want to limit the total number of received values. The iterator will block on calls to <kbd>next</kbd>, never returning a <kbd>None</kbd>. However, the <kbd>send</kbd> of the sender in  hopper is very different to that of MPSC's—<kbd>hopper::Sender&lt;T&gt;::send(&amp;mut self, event: T) -&gt; Result&lt;(), (T, Error)&gt;</kbd>. Ignore <kbd>Error</kbd> for a second; why return a tuple in the error condition that contains a <kbd>T</kbd>? To be clear, it's the same <kbd>T</kbd> that is passed in. When the caller sends a <kbd>T</kbd> into hopper, its ownership is passed into hopper as well, which is a problem in the case of an error. The caller might well want that <kbd>T</kbd> back to avoid its loss. Hopper wants to avoid doing a clone of every <kbd>T</kbd> that comes through and, so, hopper smuggles the ownership of the <kbd>T</kbd> back to the caller. What of <kbd>Error</kbd>? It's a simple enumeration:</p>
<pre style="padding-left: 30px">pub enum Error {
    NoSuchDirectory,
    IoError(Error),
    NoFlush,
    Full,
}</pre>
<p>The important one is <kbd>Error::Full</kbd>. This is the condition that occurs when both the in-memory and disk-backed buffers are full at the time of sending. This error is recoverable, but in a way that only the caller can determine. Now, finally, the <kbd>main</kbd> function of our hopper example:</p>
<pre>fn main() {<br/>    let read_limit = 1_000_000;<br/>    let in_memory_capacity = mem::size_of::&lt;u32&gt;() * 10;<br/>    let on_disk_capacity = mem::size_of::&lt;u32&gt;() * 100_000;<br/><br/>    let dir = tempdir::TempDir::new("queue_root").unwrap();<br/>    let (snd, rcv) = hopper::channel_with_explicit_capacity::&lt;u32&gt;(<br/>        "example",<br/>        dir.path(),<br/>        in_memory_capacity,<br/>        on_disk_capacity,<br/>        1,<br/>    ).unwrap();

    let reader_jh = thread::spawn(move || {
        reader(read_limit, rcv);
    });
    let _writer_jh = thread::spawn(move || {</pre>
<pre>        writer(snd);
    });

    reader_jh.join().unwrap();
}</pre>
<p>The <kbd>read_limit</kbd> is in place as before, but the big difference is the creation of the channel. First, hopper has to have some place to put its disk storage. Here we're deferring to some temporary directory—<kbd>let dir = tempdir::TempDir::new("queue_root").unwrap();</kbd>—to avoid cleaning up after running the example. In real use, the disk location is chosen carefully. <kbd>hopper::channel_with_explicit_capacity</kbd> creates the same sender as <kbd>hopper::channel</kbd> except that all the configuration knobs are open to the caller. The first argument is the <em>name</em> of the channel. This value is important as it will be used to create a directory under <kbd>dir</kbd> for disk storage. It is important for the channel name to be unique. <kbd>in_memory_capacity</kbd> is in bytes, as well as <kbd>on_disk_capacity</kbd>, which is why we have the use of our old friend <kbd>mem::size_of</kbd>. Now, what's that last configuration option there, set to <kbd>1</kbd>? That's the maximum number of <em>queue files</em>.</p>
<p>Hopper's disk storage is broken into multiple <em>queue files</em>, each of <kbd>on_disk_capacity</kbd> size. Senders carefully coordinate their writes to avoid over-filling the queue files, and the receiver is responsible for destroying them once it's sure there are no more writes coming—we'll talk about the signalling mechanism later in this chapter. The use of queue files allows hopper to potentially reclaim disk space that it may not otherwise have been able to, were one large file to be used in a circular fashion. This does incur some complexity in the sender and receiver code, as we'll see, but is worth it to provide a less resource-intensive library.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">A conceptual view of hopper</h1>
                </header>
            
            <article>
                
<p>Before we dig into the implementation, let's walk through how the previous example works at a conceptual level. We have enough in-memory space for 10 u32s. If the writer thread is much faster than the reader thread—or gets more scheduled time—we could end up with a situation like this:</p>
<pre style="padding-left: 30px">write: 99

in-memory-buffer: [90, 91, 92, 93, 94, 95, 96, 97, 98]
disk-buffer: [87, 88, 89]</pre>
<p>That is, a write of <kbd>99</kbd> is entering the system when the in-memory buffer is totally full and there are three queued writes in the diskbuffer. While it is possible for the state of the world to shift between the time a write enters the system for queuing and between the time it is queued, let's assume that no receivers pull items between queuing. The result will then be a system that looks like this:</p>
<pre style="padding-left: 30px">in-memory-buffer: [90, 91, 92, 93, 94, 95, 96, 97, 98]
disk-buffer: [87, 88, 89, 99]</pre>
<p>The receivers, together, must read only three elements from the diskbuffer, then all the in-memory elements, and then a single element from the diskbuffer, to maintain the queue's ordering. This is further complicated considering that a write may be split by one or more reads from the receivers. We saw something analogous in the previous chapter with regard to guarding writes by doing loads and checks - the conditions that satisfy a check may change between the load, check, and operation. There's a further complication as well; the unified disk buffer displayed previously does not actually exist. Instead, there are potentially many individual queue files.</p>
<p>Let's say that hopper has been configured to allow for 10 <kbd>u32</kbd> in-memory, as mentioned, and 10 on-disk but split across five possible queue files. Our revised after-write system is as follows:</p>
<pre style="padding-left: 30px">in-memory-buffer: [90, 91, 92, 93, 94, 95, 96, 97, 98]
disk-buffer: [ [87, 88], [89, 99] ]</pre>
<p>The senders are responsible for creating queue files and filling them to their max. The <kbd>Receiver</kbd> is responsible for reading from the queue file and deleting the said file when it's exhausted. The mechanism for determining that a queue file is exhausted is simple; when the <kbd>Sender</kbd> exits a queue file, it moves on to create a new queue file, and marks the old path as read-only in the filesystem. When the <kbd>Receiver</kbd> attempts to read bytes from disk and finds there are none, it checks the write status of the file. If the file is still read-write, more bytes will come eventually. If the file is read-only, the file is exhausted. There's a little trick to it yet, and further unexplained cooperation between <kbd>Sender</kbd> and <kbd>Receiver</kbd>, but that should be enough abstract detail to let us dig in effectively.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The deque</h1>
                </header>
            
            <article>
                
<p>Roughly speaking, hopper is a concurrent deque with two cooperating finite state machines layered on top. We'll start in with the deque, defined in <kbd>src/deque.rs</kbd>.</p>
<div class="packt_infobox"><span>The discussion of hopper that follows lists almost all of its source code. We'll call out the few instances where the reader will need to refer to the listing in the book's source repository. </span></div>
<p>To be totally clear, a deque is a data structure that allows for queuing and dequeuing at either end of the queue. Rust's <kbd>stdlib</kbd> has <kbd>VecDeque&lt;T&gt;</kbd>, which is very useful. Hopper is unable to use it, however, as one of its design goals is to allow for parallel sending and receiving against the hopper queue and <kbd>VecDeque</kbd> is not thread-safe. Also, while there are concurrent deque implementations in the crate ecosystem, the hopper deque is closely tied to the finite state machines it supports and to hopper's internal ownership model. That is, you probably can't use hopper's deque in your own project without some changes. Anyhow, here's the preamble:</p>
<pre style="padding-left: 30px">use std::sync::{Condvar, Mutex, MutexGuard};
use std::sync::atomic::{AtomicUsize, Ordering};
use std::{mem, sync};

unsafe impl&lt;T, S&gt; Send for Queue&lt;T, S&gt; {}
unsafe impl&lt;T, S&gt; Sync for Queue&lt;T, S&gt; {}</pre>
<p>The only unfamiliar piece here are the things imported from <kbd>std::sync::atomic</kbd>. We'll be covering atomics in more detail in the next chapter, but we're going to breeze over them at a high-level as needed in the explanation to follow. Note as well the unsafe declarations of <kbd>send</kbd> and <kbd>sync</kbd> for some as yet unknown type <kbd>Queue&lt;T, S&gt;</kbd>. We're about to go off-road:</p>
<pre>pub struct Queue&lt;T, S&gt; {
    inner: sync::Arc&lt;InnerQueue&lt;T, S&gt;&gt;,
}</pre>
<p>The <kbd>Queue&lt;T, S&gt;</kbd> definition is similar to what we saw in previous chapters: a simple structure wrapping an inner structure, here called <kbd>InnerQueue&lt;T, S&gt;</kbd>. The <kbd>InnerQueue</kbd> is wrapped in an <kbd>Arc,</kbd> meaning there's only one allocated <kbd>InnerQueue</kbd> on the heap. As you might expect, the clone of <kbd>Queue</kbd> is a copy of the <kbd>Arc</kbd> into a new struct:</p>
<pre style="padding-left: 30px">impl&lt;T, S&gt; Clone for Queue&lt;T, S&gt; {
    fn clone(&amp;self) -&gt; Queue&lt;T, S&gt; {
        Queue {
            inner: sync::Arc::clone(&amp;self.inner),
        }
    }
}</pre>
<p>It's important that every thread that interacts with <kbd>Queue</kbd> sees the same <kbd>InnerQueue</kbd>. Otherwise, the threads are dealing with distinct areas of memory and have no relationship with one another. Let's look at <kbd>InnerQueue</kbd>:</p>
<pre style="padding-left: 30px">struct InnerQueue&lt;T, S&gt; {
    capacity: usize,
    data: *mut Option&lt;T&gt;,
    size: AtomicUsize,
    back_lock: Mutex&lt;BackGuardInner&lt;S&gt;&gt;,
    front_lock: Mutex&lt;FrontGuardInner&gt;,
    not_empty: Condvar,
}</pre>
<p>Okay, this is much more akin to the internals we saw in Rust itself, and there's a lot going on. The field <kbd>capacity</kbd> is the maximum number of elements that the <kbd>InnerQueue</kbd> will hold in data. Like the first <kbd>Ring</kbd> in the previous chapter, <kbd>InnerQueue</kbd> uses a contiguous block of memory to store its <kbd>T</kbd> elements, exploding a <kbd>Vec</kbd> to get that contiguous block. Also, like the first <kbd>Ring</kbd>, we store <kbd>Option&lt;T&gt;</kbd> elements in the contiguous block of memory. Technically, we could deal with a contiguous block of raw pointers or copy memory blocks in and out. But the use of <kbd>Option&lt;T&gt;</kbd> simplifies the code, both for insertion and removal, at the cost of a single byte of memory per element. The added complication just isn't worth it for the performance goals hopper is trying to hit.</p>
<p><span>The <kbd>size</kbd> field is an atomic <kbd>usize.</kbd> Atomics will be covered in more detail in the next chapter, but the behavior of <kbd>size</kbd> is going to be important. For now, think of it as a very small piece of synchronization between threads; a little hook that will allow us to order memory accesses that happens also to act like a <kbd>usize.</kbd> The condvar</span> <kbd>not_empty</kbd> <span>is used to signal to any potential readers waiting for new elements to pop that there are, in fact, elements to pop. The use of condvar greatly reduces the CPU load of</span> hopper <span>without sacrificing latency to busy loops with sleeps. Now,</span> <kbd>back_lock</kbd> <span>and</span> <kbd>front_lock</kbd><span>. What's going on here? Either side of the deque is protected by a mutex, meaning there can be only one enqueuer and one dequeuer at a time, but these can be running in parallel to each other. Here are the definitions of the two inner values of the mutexes:</span></p>
<pre style="padding-left: 30px">#[derive(Debug, Clone, Copy)]
pub struct FrontGuardInner {
    offset: isize,
}

#[derive(Debug)]
pub struct BackGuardInner&lt;S&gt; {
    offset: isize,
    pub inner: S,
}</pre>
<p> <kbd>FrontGuardInner</kbd>   is the easier to explain of the two. The only field is <kbd>offset</kbd>, which defines the offset from the first pointer of <kbd>InnerGuard</kbd>'s data of the thread manipulating the front of the queue. This contiguous store is also used in a ring buffer fashion. In <kbd>BackGuardInner</kbd>, we see the same offset, but an additional <kbd>inner</kbd>, <kbd>S</kbd>. What is this? As we'll see, the threads manipulating the back of the buffer need extra coordination between them. Exactly what that is, the queue does not care. Therefore, we make it a type parameter and allow the caller to sort everything out, being careful to pass the data around as needed. In this fashion, the queue smuggles state through itself but does not have to inspect it.</p>
<p>Let's start on the implementation of <kbd>InnerQueue</kbd>:</p>
<pre>impl&lt;T, S&gt; InnerQueue&lt;T, S&gt;
where
    S: ::std::default::Default,
{
    pub fn with_capacity(capacity: usize) -&gt; InnerQueue&lt;T, S&gt; {
        assert!(capacity &gt; 0);
        let mut data: Vec&lt;Option&lt;T&gt;&gt; = Vec::with_capacity(capacity);
        for _ in 0..capacity {
            data.push(None);
        }
        let raw_data = (&amp;mut data).as_mut_ptr();
        mem::forget(data);
        InnerQueue {
            capacity: capacity,
            data: raw_data,
            size: AtomicUsize::new(0),
            back_lock: Mutex::new(BackGuardInner {
                offset: 0,
                inner: S::default(),
            }),
            front_lock: Mutex::new(FrontGuardInner { offset: 0 }),
            not_empty: Condvar::new(),
        }
    }</pre>
<p>The type lacks a <kbd>new() -&gt; InnerQueue</kbd>, as there was no call for it to be made. Instead, there's only <kbd>with_capacity</kbd> and that's quite similar to what we saw of <kbd>Ring</kbd>'s <kbd>with_capacity</kbd>—a vector is allocated, and exploded into a raw pointer, and the original reference is forgotten before the pointer is loaded into a newly minted struct.</p>
<p>The type <kbd>S</kbd> has to implement a default for initialization that is sufficient, as the caller's smuggled state will always be the same value, which is more than adequately definable as a default. If this deque were intended for general use, we'd probably need to offer a <kbd>with_capacity</kbd> that also took an <kbd>S</kbd> directly. Now, a few further functions in the implementation that we'll just breeze right past:</p>
<pre>    pub fn capacity(&amp;self) -&gt; usize {
        self.capacity
    }

    pub fn size(&amp;self) -&gt; usize {
        self.size.load(Ordering::Relaxed)
    }

    pub fn lock_back(&amp;self) -&gt; MutexGuard&lt;BackGuardInner&lt;S&gt;&gt; {
        self.back_lock.lock().expect("back lock poisoned")
    }

    pub fn lock_front(&amp;self) -&gt; MutexGuard&lt;FrontGuardInner&gt; {
        self.front_lock.lock().expect("front lock poisoned")
    }</pre>
<p>The next function, <kbd>push_back</kbd>, is very important and subtle:</p>
<pre>    pub unsafe fn push_back(
        &amp;self,
        elem: T,
        guard: &amp;mut MutexGuard&lt;BackGuardInner&lt;S&gt;&gt;,
    ) -&gt; Result&lt;bool, Error&lt;T&gt;&gt; {
        let mut must_wake_dequeuers = false;
        if self.size.load(Ordering::Acquire) == self.capacity {
            return Err(Error::Full(elem));
        } else {
            assert!((*self.data.offset((*guard).offset)).is_none());
            *self.data.offset((*guard).offset) = Some(elem);
            (*guard).offset += 1;
            (*guard).offset %= self.capacity as isize;
            if self.size.fetch_add(1, Ordering::Release) == 0 {
                must_wake_dequeuers = true;
            }
        }
        Ok(must_wake_dequeuers)
    }</pre>
<p><kbd>InnerQueue::push_back</kbd> is responsible for placing a <kbd>T</kbd> onto the current back of the ring buffer, or failing capacity to signal that the buffer is full. When we discussed Ring, we noted that the <kbd>size == capacity</kbd> check was a race. Not so in <kbd>InnerQueue</kbd>, thanks to the atomic nature of the size. <kbd>self.size.load(Ordering::Acquire)</kbd> performs a memory load of the <kbd>self.size</kbd> but does so with certainty that it's the only thread with <kbd>self.size</kbd> as a manipulable value. Subsequent memory operations in the thread will be ordered after <kbd>Acquire</kbd>, at least until a store of <kbd>Ordering::Release</kbd> happens. A store of that nature does happen just a handful of lines down—<kbd>self.size.fetch_add(1, Ordering::Release)</kbd>. Between these two points, we see the element <kbd>T</kbd> loaded into the buffer—with a prior check to ensure that we're not stomping a <kbd>Some</kbd> value—and a wrapping bump of the <kbd>BackGuardInner</kbd>'s offset. Just like in the last chapter. Where this implementation differs is the return of <kbd>Ok(must_wake_dequeuers)</kbd>. Because the inner <kbd>S</kbd> is being guarded, it's not possible for the queue to know if there will be any further work that needs to be done before the mutex can be given up. As a result, the queue cannot itself signal that there's a value to read, even though it's already been written to memory by the time the function returns. The caller has to run the notification. That's a sharp edge. If the caller forgets to notify a thread blocked on the condvar, the blocked thread will stay that way forever.</p>
<p>The <kbd>InnerQueue::push_front</kbd> is a little simpler and not a radical departure from <kbd>push_back</kbd>:</p>
<pre>    pub unsafe fn pop_front(&amp;self) -&gt; T {
        let mut guard = self.front_lock.lock()<br/>                            .expect("front lock poisoned");
        while self.size.load(Ordering::Acquire) == 0 {
            guard = self.not_empty
                .wait(guard)
                .expect("oops could not wait pop_front");
        }
        let elem: Option&lt;T&gt; = mem::replace(&amp;mut <br/>        *self.data.offset((*guard).offset), None);
        assert!(elem.is_some());
        *self.data.offset((*guard).offset) = None;
        (*guard).offset += 1;
        (*guard).offset %= self.capacity as isize;
        self.size.fetch_sub(1, Ordering::Release);
        elem.unwrap()
    }
}</pre>
<p>The thread popping front, because it does not have to coordinate, is able to take the front lock itself, as there's no state that needs to be threaded through. After receiving the lock, the thread then enters a condition check loop to guard against spurious wake-ups on <kbd>not_empty</kbd>, replacing the item at offset with <kbd>None</kbd> when the thread is able to wake up. The usual offset maintenance occurs.</p>
<p>The implementation of <kbd>Queue&lt;T, S&gt;</kbd> is pretty minimal in comparison to the inner structure. Here's <kbd>push_back</kbd>:</p>
<pre style="padding-left: 30px">pub fn push_back(
    &amp;self,
    elem: T,
    mut guard: &amp;mut MutexGuard&lt;BackGuardInner&lt;S&gt;&gt;,
) -&gt; Result&lt;bool, Error&lt;T&gt;&gt; {
    unsafe { (*self.inner).push_back(elem, &amp;mut guard) }
}</pre>
<p>The only function that's substantially new to <kbd>Queue</kbd> is <kbd>notify_not_empty(&amp;self, _guard: &amp;MutexGuard&lt;FrontGuardInner&gt;) -&gt; ()</kbd>. The caller is responsible for calling this whenever <kbd>push_back</kbd> signals that the dequeuer must be notified and, while the guard is not used, one rough edge of the library is smoothed down by requiring that it be passed in, proving that it's held.</p>
<p>That's the deque at the core of hopper. This structure was very difficult to get right. Any slight re-ordering of the load and store on the atomic size with respect to other memory operations will introduce bugs, such as parallel access to the same memory of a region without coordination. These bugs are <em>very</em> subtle and don't manifest immediately. Liberal use of helgrind plus quickcheck testing across x86 and ARM processors—more on that later—will help drive up confidence in the implementation. Test runs of hours were not uncommon to find bugs that were not deterministic but could be reasoned about, given enough time and repeat examples. Building concurrent data structures out of very primitive pieces is <em>hard</em>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The Receiver</h1>
                </header>
            
            <article>
                
<p>Now that we've covered the deque, let's jump on to the <kbd>Receiver</kbd>, defined in <kbd>src/receiver.rs</kbd>. As mentioned previously, the receiver is responsible for either pulling a value out of memory or from disk in the style of an iterator. The <kbd>Receiver</kbd> declares the usual machinery for transforming itself into an iterator, and we won't cover that in this book, mostly just because it's a touch on the tedious side. That said, there are two important functions to cover in the <kbd>Receiver</kbd>, and neither of them are in the public interface. The first is <kbd>Receiver::next_value</kbd>, called by the iterator version of the receiver. This function is defined as follows:</p>
<pre>fn next_value(&amp;mut self) -&gt; Option&lt;T&gt; {
    loop {
        if self.disk_writes_to_read == 0 {
            match self.mem_buffer.pop_front() {
                private::Placement::Memory(ev) =&gt; {
                    return Some(ev);
                }
                private::Placement::Disk(sz) =&gt; {
                    self.disk_writes_to_read = sz;
                    continue;
                }
            }
        } else {
            match self.read_disk_value() {
                Ok(ev) =&gt; return Some(ev),
                Err(_) =&gt; return None,
            }
        }
    }
}</pre>
<p>A hopper defined to move <kbd>T</kbd> values does not define a deque—as discussed in the previous section—over <kbd>T</kbd>s. Instead, the deque actually holds <kbd>Placement&lt;T&gt;</kbd>. Placement, defined in <kbd>src/private.rs</kbd>, is a small enumeration:</p>
<pre style="padding-left: 30px">pub enum Placement&lt;T&gt; {
    Memory(T),
    Disk(usize),
}</pre>
<p>This is the trick that makes hopper work. The primary challenge with a concurrent data structure is providing sufficient synchronization between threads that your results can remain coherent despite the chaotic nature of scheduling, but not require so much synchronization that you're underwater compared to a sequential solution.</p>
<p>That does happen.</p>
<p>Now, recall that maintaining order is a key design need for any channel-style queue. The Rust standard library MPSC achieves this with atomic flags, aided by the fact that, ultimately, there's only one place for inserted elements to be stored and one place for them to be removed from. Not so in hopper. But, that one-stop-shop is a very useful, low synchronization approach. That's where <kbd>Placement</kbd> comes in. When the <kbd>Memory</kbd> variant is hit, the <kbd>T</kbd> is present already and the receiver simply returns it. When <kbd>Disk(usize)</kbd> is returned, that sends a signal to the receiver to flip itself into <em>disk mode</em>. In disk mode, when <kbd>self.disk_writes_to_read</kbd> is not zero, the receiver preferentially reads a value from disk. Only when there are no more disk values to be read does the receiver attempt to read from memory again. This mode-flipping approach maintains ordering but also has the added benefit of requiring no synchronization when in disk mode, saving critical time when reading from a slow disk.</p>
<p>The second important function to examine is <kbd>read_disk_value</kbd>, referenced in <kbd>next_value</kbd>. It's long and mostly book-keeping, but I did want to call out the first part of that function here:</p>
<pre>    fn read_disk_value(&amp;mut self) -&gt; Result&lt;T, super::Error&gt; {
        loop {
            match self.fp.read_u32::&lt;BigEndian&gt;() {
                Ok(payload_size_in_bytes) =&gt; {
                    let mut payload_buf = vec![0; payload_size_in_bytes <br/>                     as usize];
                    match self.fp.read_exact(&amp;mut payload_buf[..]) {
                        Ok(()) =&gt; {
                            let mut dec = <br/>                            DeflateDecoder::new(&amp;payload_buf[..]);
                            match deserialize_from(&amp;mut dec) {
                                Ok(event) =&gt; {
                                    self.disk_writes_to_read -= 1;
                                    return Ok(event);
                                }
                                Err(e) =&gt; panic!("Failed decoding. <br/>                                Skipping {:?}", e),
                            }
                        }</pre>
<p>This small chunk of code uses two very useful libraries. Hopper stores its disk-slop elements to disk in bincode format. Bincode (<a href="https://crates.io/crates/bincode">https://crates.io/crates/bincode</a>) was invented for the servo project and is a serialization library intended for IPC - more or less the exact use hopper has for it. The advantage of bincode is that it's fast to serialize and deserialize but with the disadvantage of not being a standard and not having a guaranteed binary format from version to version. The second library to be called out is almost invisible in this example; byteorder. You can see it here: <kbd>self.fp.read_u32::&lt;BigEndian&gt;</kbd>. Byteorder extends <kbd>std::io::Read</kbd> to allow for the deserialization of primitive types from byte-buffers. It is possible to do this yourself by hand but it's error-prone and tedious to repeat. Use byteorder. So, what we're seeing here is hopper reading a 32-bit length big-ending length prefix from <kbd>self.fp</kbd>—a <kbd>std::io::BufReader</kbd> pointed to the current on-disk queue file—and using the said prefix to read exactly that many bytes from disk, before passing those on into the deserializer. That's it. All hopper on-disk slop elements are a 32 bit length prefix chased by that many bytes.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The Sender</h1>
                </header>
            
            <article>
                
<p>Now that we've covered the <kbd>Receiver</kbd>, all that remains is the <kbd>Sender.</kbd> It's defined in <kbd>src/sender.rs</kbd>. The most important function in the <kbd>Sender</kbd> is <kbd>send.</kbd> The <kbd>Sender</kbd> follows the same disk/memory mode idea that <kbd>Receiver</kbd> uses but is more complicated in its operation. Let's dig in:</p>
<pre>        let mut back_guard = self.mem_buffer.lock_back();
        if (*back_guard).inner.total_disk_writes == 0 {
            // in-memory mode
            let placed_event = private::Placement::Memory(event);
            match self.mem_buffer.push_back(placed_event, &amp;mut <br/>            back_guard) {
                Ok(must_wake_receiver) =&gt; {
                    if must_wake_receiver {
                        let front_guard = self.mem_buffer.lock_front();
                        self.mem_buffer.notify_not_empty(&amp;front_guard);
                        drop(front_guard);
                    }
                }
                Err(deque::Error::Full(placed_event)) =&gt; {
                    self.write_to_disk(placed_event.extract().unwrap(), <br/>                    &amp;mut back_guard)?;
                    (*back_guard).inner.total_disk_writes += 1;
                }
            }</pre>
<p>Recall that the deque allows the holder of the back guard to smuggle state for coordination through it. We're seeing that pay off here. The <kbd>Sender</kbd>'s internal state is called <kbd>SenderSync</kbd> and is defined as follows:</p>
<pre style="padding-left: 30px">pub struct SenderSync {
    pub sender_fp: Option&lt;BufWriter&lt;fs::File&gt;&gt;,
    pub bytes_written: usize,
    pub sender_seq_num: usize,
    pub total_disk_writes: usize,
    pub path: PathBuf, // active fp filename
}</pre>
<p>Every sender thread has to be able to write to the current disk queue file, which <kbd>sender_fp</kbd> points to. Likewise, <kbd>bytes_written</kbd> tracks how many bytes have been, well, written to disk. The <kbd>Sender</kbd> must keep track of this value in order to correctly roll queue files over when they grow too large. <kbd>sender_seq_num</kbd> defines the name of the current writable queue file, being as they are named sequentially from zero on up. The key field for us is <kbd>total_disk_writes</kbd>. Notice that a memory write—<kbd>self.mem_buffer.push_back(placed_event, &amp;mut back_guard)</kbd>—might fail with a <kbd>Full</kbd> error. In that case, <kbd>self.write_to_disk</kbd> is called to write the <kbd>T</kbd> to disk, increasing the total number of disk writes. This write mode was prefixed with a check into the cross-thread <kbd>SenderSync</kbd> to determine if there were outstanding disk writes. Remember, at this point, the <kbd>Receiver</kbd> has no way to determine that there has been an additional write go to disk; the sole communication channel with the <kbd>Receiver</kbd> is through the in-memory deque. To that end, the next <kbd>Sender</kbd> thread will flip into a different write mode:</p>
<pre>        } else {
            // disk mode
            self.write_to_disk(event, &amp;mut back_guard)?;
            (*back_guard).inner.total_disk_writes += 1;
            assert!((*back_guard).inner.sender_fp.is_some());
            if let Some(ref mut fp) = (*back_guard).inner.sender_fp {
                fp.flush().expect("unable to flush");
            } else {
                unreachable!()
            }
            if let Ok(must_wake_receiver) = self.mem_buffer.push_back(
                private::Placement::Disk(<br/>                    (*back_guard).inner.total_disk_writes<br/>                ),
                &amp;mut back_guard,
            ) {
                (*back_guard).inner.total_disk_writes = 0;
                if must_wake_receiver {
                    let front_guard = self.mem_buffer.lock_front();
                    self.mem_buffer.notify_not_empty(&amp;front_guard);
                    drop(front_guard);
                }
            }
        }</pre>
<p>Once there's a single write to disk, the <kbd>Sender</kbd> flips into disk preferential write mode. At the start of this branch the <kbd>T</kbd> goes to disk, is flushed. Then, the <kbd>Sender</kbd> attempts to push a <kbd>Placement::Disk</kbd> onto the in-memory deque, which may fail if the <kbd>Receiver</kbd> is slow or unlucky in its scheduling assignment. Should it succeed, however, the <kbd>total_disk_writes</kbd> is set to zero—there are no longer any outstanding disk writes—and the <kbd>Receiver</kbd> is woken if need be to read its new events. The next time a <kbd>Sender</kbd> thread rolls through it may or may not have space in the in-memory deque to perform a memory placement but that's the next thread's concern.</p>
<p>That's the heart of <kbd>Sender</kbd>. While there is another large function in-module, <kbd>write_to_disk</kbd>, we won't list it here. The implementation is primarily book-keeping inside a mutex, a topic that has been covered in detail in this and the previous chapter, plus filesystem manipulation. That said, the curious reader is warmly encouraged to read through the code.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Testing concurrent data structures</h1>
                </header>
            
            <article>
                
<p>Hopper is a subtle beast and proves to be quite tricky to get correct, owing to the difficulty of manipulating the same memory across multiple threads. Slight synchronization bugs were common in the writing of the implementation, bugs that highlighted fundamental mistakes but were rare to trigger and even harder to reproduce. Traditional unit testing is not sufficient here. There are no clean units to be found, being that the computer's non-deterministic behavior is fundamental to the end result of the program's run. With that in mind, there are three key testing strategies used on hopper: randomized testing over random inputs searching for logic bugs (QuickCheck), randomized testing over random inputs searching for crashes (fuzz testing), and multiple-million runs of the same program searching for consistency failures. In the rest of the chapter, we'll discuss each of these in turn.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">QuickCheck and loops</h1>
                </header>
            
            <article>
                
<p>Previous chapters discussed QuickCheck at length and we'll not duplicate that here. Instead, let's dig into a test from <kbd>hopper</kbd>, defined in <kbd>src/lib.rs</kbd>:</p>
<pre>    #[test]
    fn multi_thread_concurrent_snd_and_rcv_round_trip() {
        fn inner(
            total_senders: usize,
            in_memory_bytes: usize,
            disk_bytes: usize,
            max_disk_files: usize,
            vals: Vec&lt;u64&gt;,
        ) -&gt; TestResult {
            let sz = mem::size_of::&lt;u64&gt;();
            if total_senders == 0 || <br/>               total_senders &gt; 10 || <br/>               vals.len() == 0 ||
               (vals.len() &lt; total_senders) || <br/>               (in_memory_bytes / sz) == 0 ||
               (disk_bytes / sz) == 0
            {
                return TestResult::discard();
            }
            TestResult::from_bool(<br/>              multi_thread_concurrent_snd_and_rcv_round_trip_exp(
                total_senders,
                in_memory_bytes,
                disk_bytes,
                max_disk_files,
                vals,
            ))
        }
        QuickCheck::new()
            .quickcheck(inner as fn(usize, usize, usize, usize, <br/>             Vec&lt;u64&gt;) -&gt; TestResult);
    }</pre>
<p>This test sets up a multiple sender, single receiver round trip environment, being careful to reject inputs that allow less space in the in-memory buffer than 64 bits, no senders, or the like. It defers to another function, <kbd>multi_thread_concurrent_snd_and_rcv_round_trip_exp</kbd>, to actually run the test.</p>
<p>This setup is awkward, admittedly, but has the benefit of allowing <kbd>multi_thread_concurrent_snd_and_rcv_round_trip_exp</kbd> to be run over explicit inputs. That is, when a bug is found you can easily re-play that test by creating a manual—or <em>explicit</em>, in <kbd>hopper</kbd> testing terms—test. The inner test function is complicated and we'll consider it in parts:</p>
<pre>    fn multi_thread_concurrent_snd_and_rcv_round_trip_exp(
        total_senders: usize,
        in_memory_bytes: usize,
        disk_bytes: usize,
        max_disk_files: usize,
        vals: Vec&lt;u64&gt;,
    ) -&gt; bool {
        if let Ok(dir) = tempdir::TempDir::new("hopper") {
            if let Ok((snd, mut rcv)) = <br/>              channel_with_explicit_capacity(
                "tst",
                dir.path(),
                in_memory_bytes,
                disk_bytes,
                max_disk_files,
            ) {</pre>
<p>Much like our example usage of hopper at the start of this section, the inner test function uses tempdir (<a href="https://crates.io/crates/tempdir">https://crates.io/crates/tempdir</a>) to create a temporary path for passing into <kbd>channel_with_explicit_capacity</kbd>. Except, we're careful not to unwrap here. Because hopper makes use of the filesystem and because Rust QuickCheck is aggressively multi-threaded, it's possible that any individual test run will hit a temporary case of file-handler exhaustion. This throws QuickCheck off, with the test failure being totally unrelated to the inputs of this particular execution.</p>
<p>The next piece is as follows:</p>
<pre>                let chunk_size = vals.len() / total_senders;

                let mut snd_jh = Vec::new();
                let snd_vals = vals.clone();
                for chunk in snd_vals.chunks(chunk_size) {
                    let mut thr_snd = snd.clone();
                    let chunk = chunk.to_vec();
                    snd_jh.push(thread::spawn(move || {
                        let mut queued = Vec::new();
                        for mut ev in chunk {
                            loop {
                                match thr_snd.send(ev) {
                                    Err(res) =&gt; {
                                        ev = res.0;
                                    }
                                    Ok(()) =&gt; {
                                        break;
                                    }
                                }
                            }
                            queued.push(ev);
                        }
                        let mut attempts = 0;
                        loop {
                            if thr_snd.flush().is_ok() {
                                break;
                            }
                            thread::sleep(<br/>                              ::std::time::Duration::from_millis(100)<br/>                            );
                            attempts += 1;
                            assert!(attempts &lt; 10);
                        }
                        queued
                    }))
                }</pre>
<p>Here we have the creation of the sender threads, each of which grabs an equal sized chunk of <kbd>vals</kbd>. Now, because of indeterminacy in thread scheduling, it's not possible for us to model the order in which elements of <kbd>vals</kbd> will be pushed into hopper. All we can do is confirm that there are no lost elements after transmission through hopper. They may, in fact, be garbled in terms of order:</p>
<pre>                let expected_total_vals = vals.len();
                let rcv_jh = thread::spawn(move || {
                    let mut collected = Vec::new();
                    let mut rcv_iter = rcv.iter();
                    while collected.len() &lt; expected_total_vals {
                        let mut attempts = 0;
                        loop {
                            match rcv_iter.next() {
                                None =&gt; {
                                    attempts += 1;
                                    assert!(attempts &lt; 10_000);
                                }
                                Some(res) =&gt; {
                                    collected.push(res);
                                    break;
                                }
                            }
                        }
                    }
                    collected
                });

                let mut snd_vals: Vec&lt;u64&gt; = Vec::new();
                for jh in snd_jh {
                    snd_vals.append(&amp;mut jh.join().expect("snd join <br/>                    failed"));
                }
                let mut rcv_vals = rcv_jh.join().expect("rcv join <br/>                failed");

                rcv_vals.sort();
                snd_vals.sort();

                assert_eq!(rcv_vals, snd_vals);</pre>
<p>Another test with a single sender, <kbd>single_sender_single_rcv_round_trip</kbd>, is able to check for correct ordering as well as no data loss:</p>
<pre>    #[test]
    fn single_sender_single_rcv_round_trip() {
        // Similar to the multi sender test except now with a single <br/>        // sender we can guarantee order.
        fn inner(
            in_memory_bytes: usize,
            disk_bytes: usize,
            max_disk_files: usize,
            total_vals: usize,
        ) -&gt; TestResult {
            let sz = mem::size_of::&lt;u64&gt;();
            if total_vals == 0 || (in_memory_bytes / sz) == 0 || <br/>            (disk_bytes / sz) == 0 {
                return TestResult::discard();
            }
            TestResult::from_bool(<br/>              single_sender_single_rcv_round_trip_exp(
                in_memory_bytes,
                disk_bytes,
                max_disk_files,
                total_vals,
            ))
        }
        QuickCheck::new().quickcheck(inner as fn(usize, usize, <br/>                                     usize, usize) -&gt; TestResult);
    }</pre>
<p>Like it's multi-cousin, this QuickCheck test uses an inner function to perform the actual test:</p>
<pre>    fn single_sender_single_rcv_round_trip_exp(
        in_memory_bytes: usize,
        disk_bytes: usize,
        max_disk_files: usize,
        total_vals: usize,
    ) -&gt; bool {
        if let Ok(dir) = tempdir::TempDir::new("hopper") {
            if let Ok((mut snd, mut rcv)) = <br/>            channel_with_explicit_capacity(
                "tst",
                dir.path(),
                in_memory_bytes,
                disk_bytes,
                max_disk_files,
            ) {
                let builder = thread::Builder::new();
                if let Ok(snd_jh) = builder.spawn(move || {
                    for i in 0..total_vals {
                        loop {
                            if snd.send(i).is_ok() {
                                break;
                            }
                        }
                    }
                    let mut attempts = 0;
                    loop {
                        if snd.flush().is_ok() {
                            break;
                        }
                        thread::sleep(<br/>                          ::std::time::Duration::from_millis(100)<br/>                        );
                        attempts += 1;
                        assert!(attempts &lt; 10);
                    }
                }) {
                    let builder = thread::Builder::new();
                    if let Ok(rcv_jh) = builder.spawn(move || {
                        let mut rcv_iter = rcv.iter();
                        let mut cur = 0;
                        while cur != total_vals {
                            let mut attempts = 0;
                            loop {
                                if let Some(rcvd) = rcv_iter.next() {
                                    debug_assert_eq!(
                                        cur, rcvd,
                                    "FAILED TO GET ALL IN ORDER: {:?}",
                                        rcvd,
                                    );
                                    cur += 1;
                                    break;
                                } else {
                                    attempts += 1;
                                    assert!(attempts &lt; 10_000);
                                }
                            }
                        }
                    }) {
                        snd_jh.join().expect("snd join failed");
                        rcv_jh.join().expect("rcv join failed");
                    }
                }
            }
        }
        true
    }</pre>
<p>This, too, should appear familiar, except that the Receiver thread is now able to check order. Where previously a <kbd>Vec&lt;u64&gt;</kbd> was fed into the function, we now stream <kbd>0, 1, 2, .. total_vals</kbd> through the hopper queue, asserting that order and there are no gaps on the other side. Single runs on a single input will fail to trigger low-probability race issues reliably, but that's not the goal. We're searching for logic goofs. For instance, an earlier version of this library would happily allow an in-memory maximum amount of bytes less than the total bytes of an element <kbd>T</kbd>. Another could fit multiple instances of <kbd>T</kbd> into the buffer but if the <kbd>total_vals</kbd> were odd <em>and</em> the in-memory size were small enough to require disk-paging then the last element of the stream would never be kicked out. In fact, that's still an issue. It's a consequence of the lazy flip to disk mode in the sender; without another element to potentially trigger a disk placement to the in-memory buffer, the write will be flushed to disk but the receiver will never be aware of it. To that end, the sender does expose a <kbd>flush</kbd> function, which you see in use in the tests. In practice, in cernan, flushing is unnecessary. But, it's a corner of the design that the authors did not expect and may well have had a hard time noticing had this gone out into the wild.</p>
<p>The inner test of our single-sender variant is also used for the repeat-loop variant of hopper testing:</p>
<pre>    #[test]
    fn explicit_single_sender_single_rcv_round_trip() {
        let mut loops = 0;
        loop {
         assert!(single_sender_single_rcv_round_trip_exp(8, 8, 5, 10));
            loops += 1;
            if loops &gt; 2_500 {
                break;
            }
            thread::sleep(::std::time::Duration::from_millis(1));
        }
    }</pre>
<p>Notice that here the inner loop is only for 2,500 iterations. This is done in deference to the needs of the CI servers which, don't care to run high CPU load code for hours at a time. In development, that 2,500 will be adjusted up. But the core idea is apparent; check that a stream of ordered inputs returns through the hopper queue in order and intact over and over and over again. QuickCheck searches the dark corners of the state space and more traditional manual testing hammers the same spot to dig in to computer indeterminism.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Searching for crashes with AFL</h1>
                </header>
            
            <article>
                
<p>The repeat-loop variant of testing in the previous section is an interesting one. It's not, like QuickCheck, a test wholly focused on finding logic bugs. We know that the test will work for most iterations. What's being sought out there is a failure to properly control the indeterminism of the underlying machine. In some sense that variant of test is using a logical check to search for crashes. It is a kind of fuzz test.</p>
<p>Hopper interacts with the filesystem, spawns threads, and manipulates bytes up from files. Each of these activities is subject to failure for want of resources, offsetting errors, or fundamental misunderstandings of the medium. An early version of hopper, for instance, assumed that small atomic writes to the disk would not interleave, which is true for XFS but not true for most other filesystems. Or, another version of hopper always created threads in-test by use of the more familiar <kbd>thread::spawn</kbd>. It turns out, however, that this function will panic if no thread can be created, which is why the tests use the <kbd>thread::Builder</kbd> pattern instead, allowing for recovery.</p>
<p>Crashes are important to figure out in their own right. To this end, hopper has a fuzzing setup, based on AFL. To avoid producing an unnecessary binary on users' systems, the hopper project does not host its own fuzzing code as of this writing. Instead, it lives in <kbd>blt/hopper-fuzz</kbd> (<a href="https://github.com/blt/hopper-fuzz">https://github.com/blt/hopper-fuzz</a>). This is, admittedly, awkward, but fuzz testing, being uncommon, often is. Fuzz rounds easily run for multiples of days and do not fit well into modern CI systems. AFL itself is not a tool that admits easy automation, either, compounding the problem. Fuzz runs tend to be done in small batches on users' private machines, at least for open-source projects with small communities. Inside hopper-fuzz, there's a single program for fuzzing, the preamble of which is:</p>
<pre style="padding-left: 30px">extern crate hopper;
extern crate rand;
#[macro_use]
extern crate serde_derive;
extern crate bincode;
extern crate tempdir;

use bincode::{deserialize_from, Bounded};
use self::hopper::channel_with_explicit_capacity;
use std::{io, process};
use rand::{Rng, SeedableRng, XorShiftRng};</pre>
<p>The fairly straightforward, based on what we've seen so far. Getting input into a fuzz program is sometimes non-trivial, especially when the input is not much more than a set of inputs rather than, say in the case of a parser, an actual unit of work for the program. Your author's approach is to rely on serde to deserialize the byte buffer that AFL will fling into the program, being aware that most payloads will fail to decode but not minding all that much. To that end, the top of your author's fuzz programs usually have an input struct filled with control data, and this program is no different:</p>
<pre style="padding-left: 30px">#[derive(Debug, PartialEq, Deserialize)]
struct Input { // 22 bytes
    seed_a: u32, // 4
    seed_b: u32, // 4
    seed_c: u32, // 4
    seed_d: u32, // 4
    max_in_memory_bytes: u32, // 4
    max_disk_bytes: u16, // 2
}</pre>
<p>The seeds are for <kbd>XorShiftRng</kbd>, whereas <kbd>max_in_memory_bytes</kbd> and <kbd>max_disk_bytes</kbd> are for hopper. A careful tally of the bytesize of input is made to avoid fuzz testing the bincode deserializer's ability to reject abnormally large inputs. While AFL is not blind—it has instrumented the branches of the program, after all—it is also not very smart. It's entirely possible that what you, the programmer, intends to fuzz is not what gets fuzzed to start. It's not unheard of to shake out bugs in any additional libraries brought into the fuzz project. It's to keep the libraries drawn in to a minimum and their use minimal.</p>
<p>The <kbd>main</kbd> function of the fuzz program starts off with a setup similar to other hopper tests:</p>
<pre style="padding-left: 30px">fn main() {
    let stdin = io::stdin();
    let mut stdin = stdin.lock();
    if let Ok(input) = deserialize_from(&amp;mut stdin, Bounded(22)) {
        let mut input: Input = input;
        let mut rng: XorShiftRng =
            SeedableRng::from_seed([input.seed_a, input.seed_b, <br/>                                    input.seed_c, input.seed_d]);

        input.max_in_memory_bytes = <br/>          if input.max_in_memory_bytes &gt; 2 &lt;&lt; 25 {
            2 &lt;&lt; 25
        } else {
            input.max_in_memory_bytes
        };

        // We need to be absolutely sure we don't run into another <br/>        // running process. Which, this isn't a guarantee but <br/>        // it's _pretty_ unlikely to hit jackpot.
        let prefix = format!(
            "hopper-{}-{}-{}",
            rng.gen::&lt;u64&gt;(),
            rng.gen::&lt;u64&gt;(),
            rng.gen::&lt;u64&gt;()
        );</pre>
<p>The only oddity is the production of prefix. The entropy in the <kbd>tempdir</kbd> name is relatively low, compared to the many, many millions of tests that AFL will run. We want to be especially sure that no two hopper runs are given the same data directory, as that is undefined behavior:</p>
<pre>        if let Ok(dir) = tempdir::TempDir::new(&amp;prefix) {
            if let Ok((mut snd, mut rcv)) = <br/>            channel_with_explicit_capacity(
                "afl",
                dir.path(),
                input.max_in_memory_bytes as usize,
                input.max_disk_bytes as usize,
            ) {</pre>
<p>The meat of the AFL test is surprising:</p>
<pre>                let mut writes = 0;
                let mut rcv_iter = rcv.iter();
                loop {
                    match rng.gen_range(0, 102) {
                        0...50 =&gt; {
                            if writes != 0 {
                                let _ = rcv_iter.next();
                            }
                        }
                        50...100 =&gt; {
                            snd.send(rng.gen::&lt;u64&gt;());
                            writes += 1;
                        }
                        _ =&gt; {
                            process::exit(0);
                        }
                    }
                }
            }
        }
    }
}</pre>
<p>This program pulled in a random number generator to switch off between performing a read and performing a write into the hopper channel. Why no threads? Well, it turns out, AFL struggles to accommodate threading in its model. AFL has a stability notion, which is its ability to re-run a program and achieve the same results in terms of instruction execution and the like. This is not going to fly with a multi-threaded use of hopper. Still, despite missing out on probing the potential races between sender and receiver threads, this fuzz test found:</p>
<ul>
<li>File-descriptor exhaustion crashes</li>
<li>Incorrect offset computations in deque</li>
<li>Incorrect offset computations at the Sender/Receiver level</li>
<li>Deserialization of elements into a non-cleared buffer, resulting in phantom elements</li>
<li>Arithmetic overflow/underflow crashes</li>
<li>A failure to allocate enough space for serialization.</li>
</ul>
<p>Issues with the interior deque that happen only in a multi-threaded context will be missed, of course, but the preceding list is nothing to sneeze at.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Benchmarking</h1>
                </header>
            
            <article>
                
<p>Okay, by now we can be reasonably certain that hopper is fit for purpose, at least in terms of not crashing and producing the correct results. But, it also needs to be <em>fast</em>. To that end, hopper ships with the criterion (<a href="https://crates.io/crates/criterion">https://crates.io/crates/criterion</a>) benchmarks. As the time of writing, criterion is a rapidly evolving library that performs statistical analysis on bench run results that Rust's built-in, nightly-only benchmarking library does not. Also, criterion is available for use on the stable channel. The target to match is standard library's MPSC, and that sets the baseline for hopper. To that end, the benchmark suite performs a comparison, living in <kbd>benches/stdlib_comparison.rs</kbd> in the hopper repository.</p>
<p>The preamble is typical:</p>
<pre style="padding-left: 30px">#[macro_use]
extern crate criterion;
extern crate hopper;
extern crate tempdir;

use std::{mem, thread};
use criterion::{Bencher, Criterion};
use hopper::channel_with_explicit_capacity;
use std::sync::mpsc::channel;</pre>
<p>Note that we've pulled in both MPSC and hopper. The function for MPSC that we'll be benching is:</p>
<pre style="padding-left: 30px">fn mpsc_tst(input: MpscInput) -&gt; () {
    let (tx, rx) = channel();

    let chunk_size = input.ith / input.total_senders;

    let mut snd_jh = Vec::new();
    for _ in 0..input.total_senders {
        let tx = tx.clone();
        let builder = thread::Builder::new();
        if let Ok(handler) = builder.spawn(move || {
            for i in 0..chunk_size {
                tx.send(i).unwrap();
            }
        }) {
            snd_jh.push(handler);
        }
    }

    let total_senders = snd_jh.len();
    let builder = thread::Builder::new();
    match builder.spawn(move || {
        let mut collected = 0;
        while collected &lt; (chunk_size * total_senders) {
            let _ = rx.recv().unwrap();
            collected += 1;
        }
    }) {
        Ok(rcv_jh) =&gt; {
            for jh in snd_jh {
                jh.join().expect("snd join failed");
            }
            rcv_jh.join().expect("rcv join failed");
        }
        _ =&gt; {
            return;
        }
    }
}</pre>
<p>Some sender threads get made, and a receiver exists and pulls the values from MPSC as rapidly as possible. This is not a logic check in any sense and the collected materials are immediately discarded. Like with the fuzz testing, the input to the function is structured data. <kbd>MpscInput</kbd> is defined as follows:</p>
<pre style="padding-left: 30px">#[derive(Debug, Clone, Copy)]
struct MpscInput {
    total_senders: usize,
    ith: usize,
}</pre>
<p>The hopper version of this function is a little longer, as there are more error states to cope with, but it's nothing we haven't seen before:</p>
<pre style="padding-left: 30px">fn hopper_tst(input: HopperInput) -&gt; () {
    let sz = mem::size_of::&lt;u64&gt;();
    let in_memory_bytes = sz * input.in_memory_total;
    let max_disk_bytes = sz * input.max_disk_total;
    if let Ok(dir) = tempdir::TempDir::new("hopper") {
        if let Ok((snd, mut rcv)) = channel_with_explicit_capacity(
            "tst",
            dir.path(),
            in_memory_bytes,
            max_disk_bytes,
            usize::max_value(),
        ) {
            let chunk_size = input.ith / input.total_senders;

            let mut snd_jh = Vec::new();
            for _ in 0..input.total_senders {
                let mut thr_snd = snd.clone();
                let builder = thread::Builder::new();
                if let Ok(handler) = builder.spawn(move || {
                    for i in 0..chunk_size {
                        let _ = thr_snd.send(i);
                    }
                }) {
                    snd_jh.push(handler);
                }
            }

            let total_senders = snd_jh.len();
            let builder = thread::Builder::new();
            match builder.spawn(move || {
                let mut collected = 0;
                let mut rcv_iter = rcv.iter();
                while collected &lt; (chunk_size * total_senders) {
                    if rcv_iter.next().is_some() {
                        collected += 1;
                    }
                }
            }) {
                Ok(rcv_jh) =&gt; {
                    for jh in snd_jh {
                        jh.join().expect("snd join failed");
                    }
                    rcv_jh.join().expect("rcv join failed");
                }
                _ =&gt; {
                    return;
                }
            }
        }
    }
}</pre>
<p>The same is true of <kbd>HopperInput</kbd>:</p>
<pre style="padding-left: 30px">#[derive(Debug, Clone, Copy)]
struct HopperInput {
    in_memory_total: usize,
    max_disk_total: usize,
    total_senders: usize,
    ith: usize,
}</pre>
<p>Criterion has many options for running benchmarks, but we've chosen here to run over inputs. Here's the setup for MPSC:</p>
<pre style="padding-left: 30px">fn mpsc_benchmark(c: &amp;mut Criterion) {
    c.bench_function_over_inputs(
        "mpsc_tst",
        |b: &amp;mut Bencher, input: &amp;MpscInput| b.iter(|| <br/>         mpsc_tst(*input)),
        vec![
            MpscInput {
                total_senders: 2 &lt;&lt; 1,
                ith: 2 &lt;&lt; 12,
            },
        ],
    );
}</pre>
<p>To explain, we've got a function, <kbd>mpsc_benchmark</kbd>, that takes the mutable criterion structure, which is opaque to use but in which criterion will store run data. This structure exposes <kbd>bench_function_over_inputs</kbd>, which consumes a closure that we can thread our <kbd>mpsc_test</kbd> through. The sole input is listed in a vector. The following is a setup that does the same thing, but for hopper:</p>
<pre style="padding-left: 30px">fn hopper_benchmark(c: &amp;mut Criterion) {
    c.bench_function_over_inputs(
        "hopper_tst",
        |b: &amp;mut Bencher, input: &amp;HopperInput| b.iter(|| <br/>         hopper_tst(*input)),
        vec![
            // all in-memory
            HopperInput {
                in_memory_total: 2 &lt;&lt; 11,
                max_disk_total: 2 &lt;&lt; 14,
                total_senders: 2 &lt;&lt; 1,
                ith: 2 &lt;&lt; 11,
            },
            // swap to disk
            HopperInput {
                in_memory_total: 2 &lt;&lt; 11,
                max_disk_total: 2 &lt;&lt; 14,
                total_senders: 2 &lt;&lt; 1,
                ith: 2 &lt;&lt; 12,
            },
        ],
    );
}</pre>
<p>Notice now that we have two inputs, one guaranteed to be all in-memory and the other guaranteed to require disk paging. The disk paging input is sized appropriately to match the MSPC run. There'd be no harm in doing an in-memory comparison for both hopper and MPSC, but your author has a preference for pessimistic benchmarks, being an optimistic sort. The final bits needed by criterion are more or less stable across all the benchmarks we'll see in the rest of this book:</p>
<pre style="padding-left: 30px">criterion_group!{
    name = benches;
    config = Criterion::default().without_plots();
    targets = hopper_benchmark, mpsc_benchmark
}
criterion_main!(benches);</pre>
<p>I encourage you to run the benchmarks yourself. We see times for hopper that are approximately three times faster for the systems we intended hopper for. That's more than fast enough.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered the remainder of the essential non-atomic Rust synchronization primitives, doing a deep-dive on the postmates/hopper libraries to explore their use in a production code base. After having digested <a href="5a332d94-37e4-4748-8920-1679b07e2880.xhtml" target="_blank">Chapter 4</a>, <em>Sync and Send – the Foundation of Rust Concurrency</em>, and this chapter, the reader should be in a fine position to build lock-based, concurrent data structures in Rust. For readers that need even more performance, we'll explore the topic of atomic programming in <a href="d42acb0b-a05e-4068-894f-81365d147bf4.xhtml" target="_blank">Chapter 6</a>, <em>Atomics – the Primitives of Synchronization</em>, and in <a href="2099e79d-45cd-46cb-bf58-fc27b27b84ec.xhtml" target="_blank">Chapter 7</a>, <em>Atomics<span> </span>–<span> </span>Safely Reclaiming Memory</em>.</p>
<p>If you thought lock-based concurrency was hard, wait  until you see atomics.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>Building concurrent data structure is a broad field of wide concern. These notes cover much the same space as the notes from <a href="5a332d94-37e4-4748-8920-1679b07e2880.xhtml" target="_blank">Chapter 4</a><span>, </span><em>Sync and Send – the Foundation of Rust Concurrency</em>. Please do refer back to those notes.</p>
<ul>
<li><em>The Little Book of Semaphores</em>, Allen Downey, available at <a href="http://greenteapress.com/wp/semaphores/">http://greenteapress.com/wp/semaphores/</a>.This is a charming book of concurrency puzzles, suitable for undergraduates but challenging enough in Rust for the absence of semaphores. We'll revisit this book in the next chapter when we build concurrency primitives out of atomics.</li>
<li><em>The Computability of Relaxed Data Structures: Queues and Stacks as Examples</em>, Nir Shavit and Gadi Taubenfeld. This chapter discussed the implementation of a concurrent queue based on the presentation of <em>The Art of Multiprocessor Programming</em> and the author's knowledge of Erlang's process queue. Queues are a common concurrent data structure and there are a great many possible approaches. This paper discusses an interesting notion. Namely, if we relax the ordering constraint of the queue, can we squeeze out more performance from a modern machine?</li>
<li><em>Is Parallel Programming Hard, and, If So, What Can You Do About It?</em>, Paul McKenney. This book covers roughly the same material as <a href="5a332d94-37e4-4748-8920-1679b07e2880.xhtml" target="_blank">Chapter 4</a><span>, </span><em>Sync and Send – the Foundation of Rust Concurrency,</em> and <a href="e2de1ba7-c291-494e-82da-37fee7323c1d.xhtml" target="_blank">Chapter 5</a>, <em>Locks – Mutex, Condvar, Barriers, and RWLock</em>, but in significantly more detail. I highly encourage readers to look into getting a copy and reading it, especially the eleventh chapter on validating implementations.</li>
</ul>


            </article>

            
        </section>
    </div>



  </body></html>
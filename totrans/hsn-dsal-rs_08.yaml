- en: Algorithm Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When looking at algorithms as defined entities, what makes one algorithm better
    than the other? Is it the number of steps required to finish? The amount of memory
    that is committed? CPU cycles? How do they compare across machines and operating
    systems with different memory allocators?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of questions here that need answers, since comparing work with
    others is important in order to find the best approach possible to solve a given
    problem. In this chapter, you can look forward to learning about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating algorithms in practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying algorithm and data structure behaviors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the plausibility of a better algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Big O notation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Physics is not a topic in this book, but its influence is far-reaching and
    powerful enough to be obeyed everywhere, even by virtual constructs such as algorithms!
    However great their design, they still are constrained by two important factors:
    time and space.'
  prefs: []
  type: TYPE_NORMAL
- en: Time? Whenever anything needs to be done, a sequence of steps is required. By
    multiplying the number of steps by the time for each step, the total—absolute—time
    is easy to calculate. Or so we think. For computers, this is *mostly* true, but
    many questions make it very hard to really know, since modern CPUs go way beyond
    what previous generations were able to achieve. Is that only thanks to higher
    clock rates? What about the additional cores? SIMD? Simply taking the absolute
    time won't achieve real comparability between algorithms. Maybe the number of
    steps is what we should use.
  prefs: []
  type: TYPE_NORMAL
- en: Space (as in memory) has become a commodity in many domains over the last few
    years, even in the embedded space. While the situation has improved, it still
    pays to be mindful of how many bytes are stored in memory and how much that contributes
    to the goal of the algorithm. Or in other words, is this worth it? Many algorithmic
    tasks face a trade-off between what's stored in memory and what's computed on
    demand. The latter might be just enough to solve the problem, or it might not
    be; this is a decision the developer has to make.
  prefs: []
  type: TYPE_NORMAL
- en: Other people's code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consequently, every algorithm must have a "number of steps required" and "bytes
    of memory required" property, right? Close: since they are ever-changing variables,
    a universal way of describing what other people have achieved is necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, programmers instinctively know how to do that: "is this thing really
    doing everything twice?!" should be a familiar outcry. What has been said here?
    Assuming it''s a function that has an input parameter `x`, it sounds like the
    function is doing something with `x` twice. Mathematically speaking, this would
    be expressed as *f(x) = 2x*.'
  prefs: []
  type: TYPE_NORMAL
- en: What this is really saying is that for every input, the required number of steps
    to fully execute the function is twice the input—isn't this exactly what we have
    been looking for? What would be a better way to write it down?
  prefs: []
  type: TYPE_NORMAL
- en: The Big O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Looking at that issue from a (mathematical) function perspective, this is a
    shared need across mathematics, computer science, physics, and so on: they all
    want to know how expensive a function is. This is why a common notation was invented
    by Edmund Landau: the Big O notation (or Landau notation) consisting of the uppercase
    letter *O,* which declares the *order* of a function. The main growth factor is
    then put into parentheses following the letter *O*.'
  prefs: []
  type: TYPE_NORMAL
- en: There are other, related notations that use small *o*, Omegas, Theta, and others,
    but those are less relevant in practical terms. Check the *Further reading* section
    for an article by Donald Knuth on this.
  prefs: []
  type: TYPE_NORMAL
- en: Asymptotic runtime complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For computer science, the exact, absolute runtime is typically not important
    when implementing algorithms (you can always get a faster computer). Instead,
    the runtime complexity is more important since it directly influences performance
    as an overall measure of work, independent of details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this is not an exact measurement and the actual performance is influenced
    by other factors, sticking with an asymptotic (read: rough) measure is the best
    strategy. In addition to that, algorithms have best and worst cases. Unless you
    are trying to improve on a particular case, the worst case is what''s typically
    compared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Iterating over this, `Vec<T>` has a runtime complexity of *O(n)* where *n* is
    the length of `Vec<T>`, regardless of the fact that the loop will break right
    away. Why? Because of pessimism. In reality, it is often hard to say what the
    input vector looks like and when it will actually exit, so the worst case is that
    it goes over the entire sequence without breaking, that is, *n* times. Now that
    we have seen how to write this down, let's see how to find out the runtime complexity
    of our own algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Making your own
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are only a few aspects that change the complexity of an algorithm, those
    that have been shown to proportionally increase the total time required of an
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: An arithmetic operation (`10 + 30`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An assignment (`let x = 10`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A test (`x == 10`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A read or write of a basic type (`u32`, `bool`, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a piece of code only does one of these operations, it is one step, that is,
    *O(1),* and whenever there is a choice (`if` or `match`), the more complex branch
    has to be picked. Regardless of any input parameters, it will be the same number
    of steps—or constant time. If they are run in a loop, things get more interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Loops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When in a loop, and the number of iterations is not known at compile time, it
    will be a major influence on runtime complexity. If an operation mentioned earlier
    is executed in the loop (for example, a `sum` operation), one could declare the
    complexity as *O(1 * n)* for the arithmetic operation. After adding another operation,
    we could express it as *O(2 * n)* and, while this would be correct, these are
    not the driving forces of the loop. Regardless of the number of operations that
    are executed *n* times, the main growth factor remains *n*. Hence, we simply say
    *O(n),* unless you are trying to compare the same algorithm, where the number
    of iterations actually makes a difference. If there are subsequent loops, the
    most expensive one is picked.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, upon nesting loops, the complexity changes considerably. Consider
    this (really bad) algorithm for comparing two lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For each element in the first collection, the second collection is fully iterated.
    In other words, each element is looked at *n * m* times, resulting in a runtime
    complexity of *O(n*m)*, or, if both collections are the same size, *O(n²)*.
  prefs: []
  type: TYPE_NORMAL
- en: Can it get even worse? Yes!
  prefs: []
  type: TYPE_NORMAL
- en: Recursion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since all recursive algorithms can be unrolled into a loop, they can achieve
    the same results. However, recursion, or more specifically backtracking (which
    will be discussed in more detail in [Chapter 11](0131b10b-0ea4-4663-966a-46d6ecda142b.xhtml),
    *Random and Combinatorial*), makes it easier to create higher runtime complexities.
  prefs: []
  type: TYPE_NORMAL
- en: Typical combinatorial problems result in exponential runtimes, since there are
    a number of variations (such as different colors) that have to be enumerated *n*
    times so that a constraint is satisfied, which is only evaluated at the end. If
    there are two colors, the runtime complexity will therefore be *O(2^n)* for a
    sequence of *n* colors, if no two colors can be adjacent to each other in a graph
    (graph coloring problem).
  prefs: []
  type: TYPE_NORMAL
- en: Recursive algorithms also make it hard to estimate runtime complexity quickly,
    since the branch development is hard to visualize.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, all algorithms fall into one of a few classes. Let's look at these
    classes ordered by their growth speed. Depending on the literature, there might
    be more or fewer classes, but this is a good set to start with since they represent
    the major directions of growth behavior.
  prefs: []
  type: TYPE_NORMAL
- en: O(1)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Constant time, which means everything will take the same amount of time. Since
    this chart would be a horizontal line at the *y* value of *1*, we will skip it
    in favor of sparing a tree.
  prefs: []
  type: TYPE_NORMAL
- en: O(log(n))
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Growth is defined by the logarithmic function (in general, base 2), which is
    better than linear growth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the plot of the mathematical function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad843385-9de2-43d1-a95e-9a7289852ef4.png)'
  prefs: []
  type: TYPE_IMG
- en: O(n)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear time, which means that the solution performance depends on the input
    in a linear way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3db23b99-73e0-4ae4-9a72-52f08be1f8b7.png)'
  prefs: []
  type: TYPE_IMG
- en: O(n log(n))
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is sometimes called quasilinear time and is the best achievable complexity
    for sorting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0084a6fa-de2c-496a-af29-8ad592f8c8af.png)'
  prefs: []
  type: TYPE_IMG
- en: O(n²)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The squared runtime is typical for the naive implementation of search or sorting
    algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6f8dd4b-d78c-4562-844f-758d836ef0f8.png)'
  prefs: []
  type: TYPE_IMG
- en: O(2n)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is among the most expensive classes and can often be found in really hard-to-solve
    problems. This plot has a significantly smaller *x* value (*0 - 10*) and generates
    a higher *y* value (or runtime) than the `O(n log(n))` chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a186b9ab-3fdf-48e7-995a-9ccd28266e92.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having individual charts is great for imagining the projected runtime and estimating
    what a task's performance could look like when its input is increased. If we plot
    all of these lines into a single chart, however, their performance will become
    obvious.
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical comparison is against the linear time complexity (*O(n)*), since
    most naive solutions would be expected to achieve this performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/becff7dd-48c4-4026-a18b-67110922a5f3.png)'
  prefs: []
  type: TYPE_IMG
- en: With this chart in mind, we can look at problems and their expected performance
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: In the wild
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In reality, there are a lot of factors that may influence the choice of space
    and runtime complexity. Typically, these factors are forms of resource constraints,
    such as power consumption on embedded devices, clock cycles in a cloud-hosted
    environment, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Since it is difficult to find out the complexities of a particular algorithm,
    it is helpful to know a few, so the choice comes intuitively. Often, the runtime
    complexity is not the only important aspect, but the absolute execution time counts.
    Under these conditions, a higher runtime complexity can be preferable if *n* is
    sufficiently small.
  prefs: []
  type: TYPE_NORMAL
- en: This is best demonstrated when `Vec<T>` contains only a few elements, where
    a linear search is a lot faster than sorting and then running a binary search.
    The overhead of sorting might just be too much compared to searching right away.
  prefs: []
  type: TYPE_NORMAL
- en: Getting this trade-off and the overall implementation right is hugely beneficial
    for the entire program and will outweigh any other optimizations. Let's take a
    look at a few runtime complexities that can be found in everyday life.
  prefs: []
  type: TYPE_NORMAL
- en: Data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Algorithms on lists of all kinds almost always exhibit *O(n)* behavior, since
    most actions involve shifting or going through other elements. Hence, operations
    such as insert at or remove from a position, as well as finding elements (when
    unsorted), are *O(n)*. This is very visible, particularly in linked lists, with
    only a few exceptions: a dynamic array''s element access (*O(1)*), prepending/appending
    elements or lists, and splitting lists appending elements in a linked list (*O(1)*).'
  prefs: []
  type: TYPE_NORMAL
- en: Special cases of lists, such as **stacks** and **queues**, make use of these
    exceptions and let a user insert to or remove from only the ends of that list.
    **Skip lists** on the other hand employ a tree-like strategy for achieving great
    search performance, which speeds up inserts and removals too. But this comes at
    the expense of memory, since the additional elements are proportional (*log(n)*)
    to the list length.
  prefs: []
  type: TYPE_NORMAL
- en: For search, **trees** are great. Regular trees (that is, anything that can be
    a B-Tree) exhibit *O(log(n))* complexities on many operations, including insert,
    remove, and find. This is particularly great since difference to *O(n)* actually
    increases the more elements there are in the collection.
  prefs: []
  type: TYPE_NORMAL
- en: The only thing potentially better are **maps** and **sets**, if the underlying
    implementation uses an appropriate hashing algorithm. Any operation *s**hould*
    be completed in constant time (*O(1)*), if there are no collisions. Typically,
    there will be some collisions, but the runtime complexity will not exceed *O(n)*
    because, if all else fails, a linear search works. Consequently, real performance
    will be somewhere in between, with the hashing algorithm being the most important
    influence. For most libraries, hash maps (and sets) are faster than their tree-based
    counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: Everyday things
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Whenever something needs sorting, there are a lot of ways to achieve that,
    but the baseline is *O(n²)*. It''s the same way most people order their socks:
    pick one and find the match, then repeat (called **selection sort**). How else
    would one compare all elements to find their order? Better approaches, such as
    heap sort, merge sort, and so on, all exhibit *O(n log(n))* behavior in the worst
    case, which is the best possible (consistent) performance for sorting algorithms.
    Additionally, since the best case for any sorting algorithm is *O(n)*—making sure
    everything was already in order—the average case matters the most. We will get
    into strategies about that later in this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Search (or lookup) is another topic that we will get into in [Chapter 10](32002bad-c2bb-46e9-918d-12d7dabfe579.xhtml),
    *Finding Stuff*, but the associated runtime complexities are great examples. Searching
    on any unsorted data structure will be *O(n)* most of the time, while sorted collections
    can utilize binary search (a tree''s search strategy) and achieve *O(log(n))*.
    In order to save the cost of sorting, ideal hash tables provide the absolute best
    case for search: *O(1)*.'
  prefs: []
  type: TYPE_NORMAL
- en: Exotic things
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One class that was omitted from the earlier list is **polynomial time** (**P**
    in short). This class is quicker to solve than the exponential time class, but
    worse than *O(n²)*. These problems include checking whether a number is a prime
    number, or solving a Sudoku. However, there are other problems in this class as
    well that actually have *no* "quick" (that is, solvable in P) solution, but a
    solution can be verified in P time. These are called **NP** (an abbreviation of
    **non-deterministic polynomial time**) problems and the hardest of them are NP-hard
    (see the information box).
  prefs: []
  type: TYPE_NORMAL
- en: The distinction between P, NP, NP-complete, and NP-hard is not intuitive. NP
    problems are problems that can be solved using a non-deterministic Turing machine
    in P time. **NP-hard** problems are problems without a solution that, if solved,
    would have a polynomial time solution and if it is also an NP problem, it is also
    considered NP-complete. Additionally, finding a solution for one of either class
    (NP-hard or NP-complete) would imply a solution for *all* NP-hard/NP-complete
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: While there are no known algorithms to solve these problems quickly, there typically
    are naive approaches that result in *very* long runtimes. Popular problems in
    this space include the traveling salesman problem (*O(n!)*), the knapsack problem
    (*O(2^n)*, and the subset sum problem (*O(2^(n/2))*), all of which are currently
    solved (or approximated) using heuristics or programming techniques. For those
    interested, check the further reading section for links.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Big O notation is a way to describe the time and space requirements of
    an algorithm (or data structure). This is not an exact science, however; it''s
    about finding the primary growth factor of each of the things mentioned to answer
    this question: what happens when the problem space grows bigger?'
  prefs: []
  type: TYPE_NORMAL
- en: Any algorithm will fall within a few relevant classes that describe that behavior.
    By applying the algorithm to one more element, how many more steps have to be
    taken? One easy way is to visualize the individual charts and think of whether
    it will be linear (*O(n)*), quasilinear (*O(n log(n))*), quadratic (*O(n²)*),
    or even exponential (*O(2^n)*). Whatever the case may be, it is always best to
    do less work than there are elements to be looked at, such as constant (*O(1)*)
    or logarithmic (*O(log(n)*) behaviors!
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the operations is typically done based on the worst-case behavior,
    that is, the upper limit of what is going to happen. In the next chapter, we will
    take a closer look at these behaviors in the cases of popular search algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why estimate runtime complexity over, for example, number of statements?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does runtime complexity relate to math functions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the complexity class that is typically provided the best or worst case?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are loops important in estimating complexity?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is *O(n log(n))* a better or worse runtime complexity than *O(log(n))*?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are some commonly known complexity classes?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following links to get more information on the topics
    covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Wikipedia's list of best-, worst-, and average-case complexities ([https://en.wikipedia.org/wiki/Best,_worst_and_average_case](https://en.wikipedia.org/wiki/Best,_worst_and_average_case))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big O Cheatsheet ([http://bigocheatsheet.com/](http://bigocheatsheet.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heuristic algorithms at Northwestern University ([https://optimization.mccormick.northwestern.edu/index.php/Heuristic_algorithms](https://optimization.mccormick.northwestern.edu/index.php/Heuristic_algorithms))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heuristic design and optimization at MIT ([http://www.mit.edu/~moshref/Heuristics.html](http://www.mit.edu/~moshref/Heuristics.html))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Big Omicron And Big Omega And Big Theta* by Donald Knuth ([http://www.phil.uu.nl/datastructuren/10-11/knuth_big_omicron.pdf](http://www.phil.uu.nl/datastructuren/10-11/knuth_big_omicron.pdf))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

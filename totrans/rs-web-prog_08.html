<html><head></head><body>
		<div id="_idContainer090">
			<h1 id="_idParaDest-167" class="chapter-number"><a id="_idTextAnchor168"/>8</h1>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor169"/>Building RESTful Services</h1>
			<p>Our to-do application, written in Rust, technically works. However, there are some improvements that we need to make. In this chapter, we will apply these improvements as we explore the concepts of the <strong class="bold">RESTful </strong><span class="No-Break"><strong class="bold">API</strong></span><span class="No-Break"> design.</span></p>
			<p>In this chapter, we will finally reject unauthorized users before the request hits the view by assessing the layers of our system and refactoring how we handle requests throughout the request lifetime. We’ll then use this authentication to enable individual users to have their own list of to-do items. Finally, we will log our requests so that we can troubleshoot our application and get a deeper look into how our application runs, caching data in the frontend to reduce API calls. We will also explore nice-to-have concepts such as executing code on command and creating a uniform interface to split the frontend URLs from the <span class="No-Break">backend URLs.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>What are <span class="No-Break">RESTful services?</span></li>
				<li>Mapping our <span class="No-Break">layered system</span></li>
				<li>Building a <span class="No-Break">uniform interface</span></li>
				<li><span class="No-Break">Implementing statelessness</span></li>
				<li>Logging our <span class="No-Break">server traffic</span></li>
				<li><span class="No-Break">Caching</span></li>
				<li>Code <span class="No-Break">on demand</span></li>
			</ul>
			<p>By the end of this chapter, we will have refactored our Rust application to support the principles of RESTful APIs. This means that we are going to map out the layers of our Rust application, create uniform API endpoints, log requests in our application, and cache results in <span class="No-Break">the frontend.</span></p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor170"/>Technical requirements</h1>
			<p>The code for this chapter can be found <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter08"><span class="No-Break">https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter08</span></a><a href="https://github.com/PacktPublishing/Rust-Web-Programming-2nd-Edition/tree/main/chapter08&#13;"/></p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor171"/>What are RESTful services?</h1>
			<p><strong class="bold">REST</strong> stands for <strong class="bold">representational state transfer</strong>. It is an <a id="_idIndexMarker805"/>architectural style for our <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) to read (<strong class="source-inline">GET</strong>), update (<strong class="source-inline">PUT</strong>), create (<strong class="source-inline">POST</strong>), and delete (<strong class="source-inline">DELETE</strong>) our users and to-do<a id="_idIndexMarker806"/> items. The goal of a RESTful approach is to<a id="_idIndexMarker807"/> increase speed/performance, reliability, and the ability to grow by reusing components that can be managed and updated without affecting <span class="No-Break">the system.</span></p>
			<p>You may have noticed that before Rust, slow, high-level languages seemed to be a wise choice for web development. This is because they are quicker and safer to write. This is due to the main bottleneck for the speed of processing data in web development being the network connection speed. The RESTful design aims to improve the speed by economizing the system, such as reducing API calls, as opposed to just focusing on algorithm speed. With that in mind, in this section, we will be covering the following <span class="No-Break">RESTful concepts:</span></p>
			<ul>
				<li><strong class="bold">Layered system</strong>: This <a id="_idIndexMarker808"/>enables us to add extra functionality, such as authorization, without having to change the interface. For instance, if we must check the <strong class="bold">JSON Web Token</strong> (<strong class="bold">JWT</strong>) in <a id="_idIndexMarker809"/>every view, then this is a lot of repetitive code that is hard to maintain and is prone <span class="No-Break">to error.</span></li>
				<li><strong class="bold">Uniform system</strong>: This <a id="_idIndexMarker810"/>simplifies and decouples the architecture, enabling whole parts of the application to evolve independently <span class="No-Break">without clashing.</span></li>
				<li><strong class="bold">Statelessness</strong>: This<a id="_idIndexMarker811"/> ensures that our application does not directly save anything on the server. This has implications for microservices and <span class="No-Break">cloud computing.</span></li>
				<li><strong class="bold">Logging</strong>: This<a id="_idIndexMarker812"/> enables us to peek into our application and see how it runs, exposing undesirable behavior even if there are no <span class="No-Break">errors displayed.</span></li>
				<li><strong class="bold">Caching</strong>: This <a id="_idIndexMarker813"/>enables us to store data in the frontend to reduce the number of API calls to our <span class="No-Break">backend API.</span></li>
				<li><strong class="bold">Code on demand</strong>: This<a id="_idIndexMarker814"/> is where our backend server directly runs code on <span class="No-Break">the frontend.</span></li>
			</ul>
			<p>We’ll look at the layered system concept in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor172"/>Mapping our layered system</h1>
			<p>A layered system consists of layers with<a id="_idIndexMarker815"/> different units of functionality. It could be argued that these layers are different servers. This can be true in microservices and big systems. This can be the case when it comes to different layers of data. In big systems, it makes sense to<a id="_idIndexMarker816"/> have <em class="italic">hot data</em> that gets accessed and updated regularly and <em class="italic">cold data </em>where<a id="_idIndexMarker817"/> it is rarely accessed. However, while it is easy to think of layers as on different servers, they can be on the same server. We can map our layers with the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/Figure_8.1_B18722.jpg" alt="Figure 8.1 – The layers in our app"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – The layers in our app</p>
			<p>As you can see, our <a id="_idIndexMarker818"/>app follows <span class="No-Break">this process:</span></p>
			<ol>
				<li>First, our <em class="italic">HTTP Handler</em> accepts the call by listening to the port that we defined when creating <span class="No-Break">the server.</span></li>
				<li>Then, it goes through the <em class="italic">middleware</em>, which is defined by using the <strong class="source-inline">wrap_fn</strong> function on <span class="No-Break">our app.</span></li>
				<li>Once this is done, the URL of the request is mapped to the right view and the schemas we defined in our <strong class="source-inline">src/json_serialization/</strong> directory. These get passed into the resource (our views) defined in the <span class="No-Break"><strong class="source-inline">src/views</strong></span><span class="No-Break"> directory.</span></li>
			</ol>
			<p>If we then want to update or get data from the database, we use the Diesel ORM to map these requests. At this stage, all our layers have been defined to manage the flow of data effectively, apart from our middleware. As pointed out in the previous chapter, <a href="B18722_07.xhtml#_idTextAnchor149"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Managing User Sessions</em>, we have implemented our middleware for authentication with the <strong class="source-inline">JwToken</strong> struct by implementing the <strong class="source-inline">FromRequest</strong> trait. With this, we can see that we can implement our middleware using either the <strong class="source-inline">wrap_fn</strong> or implementing the <strong class="source-inline">FromRequest</strong> trait. When do you think we should use the <strong class="source-inline">wrap_fn</strong> or <strong class="source-inline">FromRequest</strong> trait? Both have advantages and disadvantages. If we want to implement our middleware for specific individual views, then implementing the <strong class="source-inline">FromRequest</strong> trait is the best option. This is because we can slot a struct implementing the <strong class="source-inline">FromRequest</strong> trait into the view that we want. Authentication is a good use case for implementing the <strong class="source-inline">FromRequest</strong> trait because we want to pick and choose what endpoints require authentication. However, if we want to implement a blanket rule, we would be better off implementing the selection of views for authentication in the <strong class="source-inline">wrap_fn</strong> function. Implementing our middleware in the <strong class="source-inline">wrap_fn</strong> function means that it is implemented for <span class="No-Break">every request.</span></p>
			<p>An example of this <a id="_idIndexMarker819"/>could be that we are no longer supporting version one for all our endpoints. If we were going to do this, we would have to warn third-party users of our decision to no longer support version one of our API. Once our date has passed, we will have to give a helpful message that we are no longer supporting version one. Before we start working on our middleware layer, we must define the following imports at the top of our <span class="No-Break"><strong class="source-inline">main.rs</strong></span><span class="No-Break"> file:</span></p>
			<pre class="source-code">
use actix_web::{App, HttpServer, HttpResponse};
use actix_service::Service;
use futures::future::{ok, Either};
use actix_cors::Cors;</pre>
			<p>To ensure that we know that the incoming request is destined for a <strong class="source-inline">v1</strong> endpoint, we must define a flag that we can check later when deciding whether to process the request or reject it. We can do this by using the following code in our <span class="No-Break"><strong class="source-inline">main.rs</strong></span><span class="No-Break"> file:</span></p>
			<pre class="source-code">
.wrap_fn(|req, srv|{
    let passed: bool;
    if req.path().contains("/v1/") {
        passed = false;
    } else {
        passed = true;
    }
. . .</pre>
			<p>From the preceding code, we can see that we declare that there is a Boolean under the name of <strong class="source-inline">passed</strong>. If <strong class="source-inline">v1</strong> is not in the URL, then it is set to <strong class="source-inline">true</strong>. If <strong class="source-inline">v1</strong> is present in the URL, then <strong class="source-inline">passed</strong> is set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">false</strong></span><span class="No-Break">.</span></p>
			<p>Now that we have defined a flag, we can use it to dictate what happens to the request. Before we do this, we must take note of the last lines of <strong class="source-inline">wrap_fn</strong>, as denoted in this <span class="No-Break">code block:</span></p>
			<pre class="source-code">
let future = srv.call(req);
async {
    let result = fut.await?;
    Ok(result)
}</pre>
			<p>We are waiting for the<a id="_idIndexMarker820"/> call to finish, then returning the result as the variable called <strong class="source-inline">result</strong>. With our blocking of the <strong class="source-inline">v1</strong> API called, we must check to see whether the request passes. If it does, we then run the preceding code. However, if the request fails, we must bypass this and define another future, which is just <span class="No-Break">the response.</span></p>
			<p>At face value, this can seem straightforward. Both will return the same thing, that is, a response. However, Rust will not compile. It will throw an error based on incompatible types. This is because <strong class="source-inline">async</strong> blocks behave like closures. This means that every <strong class="source-inline">async</strong> block is its own type. This can be frustrating, and due to this subtle detail, it can lead to developers burning hours trying to get the two futures to play with <span class="No-Break">each other.</span></p>
			<p>Luckily, there is an enum in the futures crate that solves this problem for us. The <strong class="source-inline">Either</strong> enum combines two different futures, streams, or sinks that have the same associated types into a single type. This enables us to match the <strong class="source-inline">passed</strong> flag, and fire and return the appropriate process with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
    let end_result;
    if passed == true {
        end_result = Either::Left(srv.call(req))
    }
    else {
        let resp = HttpResponse::NotImplemented().body(
            "v1 API is no longer supported"
            );
        end_result = Either::Right(
            ok(req.into_response(resp)
                  .map_into_boxed_body())
        )
    }
    async move {
        let result = end_result.await?;
        Ok(result)
    }
}).configure(views::views_factory).wrap(cors);</pre>
			<p>From the preceding code, we can see that we assign <strong class="source-inline">end_result</strong> to be called as a view, or directly return it to an unauthorized response depending on the <strong class="source-inline">passed</strong> flag. We then return this at the end of <strong class="source-inline">wrap_fn</strong>. Knowing how to use the <strong class="source-inline">Either</strong> enum is a handy trick to have up your sleeve and will save you hours when you need your code to choose between two<a id="_idIndexMarker821"/> <span class="No-Break">different futures.</span></p>
			<p>To check to see whether we are blocking <strong class="source-inline">v1</strong>, we can call a simple <strong class="source-inline">get</strong> request as seen in the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/Figure_8.2_B18722.jpg" alt="Figure 8.2 – The response to blocked v1"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – The response to blocked v1</p>
			<p>We can see that our <a id="_idIndexMarker822"/>API call is blocked with a helpful message. If we do the API call through Postman, we will see that we get a <strong class="source-inline">501 Not Implemented</strong> error as seen in the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/Figure_8.3_B18722.jpg" alt="Figure 8.3 – Postman’s response to blocked v1"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Postman’s response to blocked v1</p>
			<p>We might want to add more resources for getting items in the future. This poses a potential problem because some views might start clashing with the app views. For instance, our to-do item API views only have the prefix <strong class="source-inline">item</strong>. Getting all the items requires the <span class="No-Break"><strong class="source-inline">v1/item/get</strong></span><span class="No-Break"> endpoint.</span></p>
			<p>It could be<a id="_idIndexMarker823"/> reasonable to develop a view for the app that looks at a to-do item in detail for editing with the <strong class="source-inline">v1/item/get/{id}</strong> endpoint later. However, this increases the risk of clashes between the frontend app views and the backend API calls. To prevent this, we are going to have to ensure that our API has a <span class="No-Break">uniform interface.</span></p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor173"/>Building a uniform interface</h1>
			<p>Having a uniform interface means<a id="_idIndexMarker824"/> that our resources can be uniquely identifiable through a URL. This decouples the backend endpoints and frontend views, enabling our app to scale without clashes between the frontend views and backend endpoints. We have decoupled our backend from the frontend using the version tag. When a URL endpoint includes a version tag, such as <strong class="source-inline">v1</strong> or <strong class="source-inline">v2</strong>, we know that call is hitting the backend Rust server. When we are developing our Rust server, we might want to work on a newer version of our API calls. However, we do not want to allow users to access the version in development. To enable live users to access one version while we deploy another version on a test server, we will need to dynamically define the API version for the server. With the knowledge that you have acquired so far in this book, you could simply define the version number in the <strong class="source-inline">config.yml</strong> file and load it. However, we would have to read the <strong class="source-inline">config.yml</strong> config file for every request. Remember that when we set up the database connection pool we read the connection string from the <strong class="source-inline">config.yml</strong> file once, meaning that it is present for the entire lifetime of the program. We would like to define the version once and then refer to it for the lifecycle of the program. Intuitively, you might want to define the version in the <strong class="source-inline">main</strong> function in the <strong class="source-inline">main.rs</strong> file before we define the server and then access the definition of the version inside the <strong class="source-inline">wrap_fn</strong>, as <a id="_idIndexMarker825"/>seen in the <span class="No-Break">following example:</span></p>
			<pre class="source-code">
let outcome = "test".to_owned();
HttpServer::new(|| {
    . . .
    let app = App::new()
        .wrap_fn(|req, srv|{
            println!("{}", outcome);
            . . .
        }
    . . .</pre>
			<p>However, if we try and compile the preceding code, it will fail as the lifetime of the <strong class="source-inline">outcome</strong> variable is not long enough. We can convert our <strong class="source-inline">outcome</strong> variable as a constant with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
const OUTCOME: &amp;str = "test";
HttpServer::new(|| {
    . . .
    let app = App::new()
        .wrap_fn(|req, srv|{
            println!("{}", outcome);
            . . .
        }
    . . .</pre>
			<p>The preceding code will<a id="_idIndexMarker826"/> run without any lifetime issues. However, if we were to load our version, we will have to read it from a file. In Rust, if we read from a file, we do not know what the size of the variable being read from the file is. Therefore, the variable we read from the file is going to be a string. The problem here is that allocating a string is not something that can be computed at compile time. Therefore, we are going to have to write the version directly into our <strong class="source-inline">main.rs</strong> file. We can do this by using a <span class="No-Break"><strong class="source-inline">build</strong></span><span class="No-Break"> file.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">We utilize <strong class="source-inline">build</strong> files in this problem to teach the concept of <strong class="source-inline">build</strong> files so you can use them if needed. There is nothing stopping you from hardcoding the constant in <span class="No-Break">the code.</span></p>
			<p>This is where a single Rust file runs before the Rust application is run. This <strong class="source-inline">build</strong> file will automatically run when we are compiling the main Rust application. We can define the dependencies needed to run our <strong class="source-inline">build</strong> file in the <strong class="source-inline">build dependencies</strong> section in the <strong class="source-inline">Cargo.toml</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
[package]
name = "web_app"
version = "0.1.0"
edition = "2021"
build = "build.rs"
[build-dependencies]
serde_yaml = "0.8.23"
serde = { version = "1.0.136", features = ["derive"] }</pre>
			<p>This means that our <strong class="source-inline">build</strong> Rust file is defined in the root of the application in the <strong class="source-inline">build.rs</strong> file. We will then <a id="_idIndexMarker827"/>define the dependencies needed for the build phase in the <strong class="source-inline">[build-dependencies]</strong> section. Now that our dependencies are defined, our <strong class="source-inline">build.rs</strong> file can take the <span class="No-Break">following form:</span></p>
			<pre class="source-code">
use std::fs::File;
use std::io::Write;
use std::collections::HashMap;
use serde_yaml;
fn main() {
  let file =
      std::fs::File::open("./build_config.yml").unwrap();
  let map: HashMap&lt;String, serde_yaml::Value&gt; =
      serde_yaml::from_reader(file).unwrap();
  let version =
      map.get("ALLOWED_VERSION").unwrap().as_str()
          .unwrap();
  let mut f =
      File::create("./src/output_data.txt").unwrap();
  write!(f, "{}", version).unwrap();
}</pre>
			<p>Here we can see that we need to import what we need to read from a YAML file and write it to a standard text file. Then, we will open a <strong class="source-inline">build_config.yml</strong> file, which is in the root of the web application next to the <strong class="source-inline">config.yml</strong> file. We will then extract the <strong class="source-inline">ALLOWED_VERSION</strong> from the <strong class="source-inline">build_config.yml</strong> file and write it into a text file. Now that we have defined the build process and what is needed from the <strong class="source-inline">build_config.yml</strong> file, our <strong class="source-inline">build_config.yml</strong> file will have to take the <span class="No-Break">following form:</span></p>
			<pre class="source-code">
ALLOWED_VERSION: v1</pre>
			<p>Now that we have everything defined for our build, we can introduce a <strong class="source-inline">const</strong> instance for our version through the file that we wrote to in our <strong class="source-inline">build.rs</strong> file. To do this, our <strong class="source-inline">main.rs</strong> file will <a id="_idIndexMarker828"/>need a few changes. First, we define the <strong class="source-inline">const</strong> with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
const ALLOWED_VERSION: &amp;'static str = include_str!(
    "./output_data.txt");
HttpServer::new(|| {
. . .</pre>
			<p>We then deem the request to pass if the version is allowed with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
HttpServer::new(|| {
. . .
let app = App::new()
.wrap_fn(|req, srv|{
    let passed: bool;
    if *&amp;req.path().contains(&amp;format!("/{}/",
                             ALLOWED_VERSION)) {
        passed = true;
    } else {
        passed = false;
    }</pre>
			<p>Then, we define the<a id="_idIndexMarker829"/> error response and service call with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
. . .
let end_result = match passed {
    true =&gt; {
        Either::Left(srv.call(req))
    },
    false =&gt; {
        let resp = HttpResponse::NotImplemented()
            .body(format!("only {} API is supported",
                ALLOWED_VERSION));
        Either::Right(
            ok(req.into_response(resp).map_into_boxed_body())
        )
    }
};
. . .</pre>
			<p>We are now ready to build and run our application with a specific version supported. If we run our application and make a <strong class="source-inline">v2</strong> request, we get the <span class="No-Break">following response:</span></p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/Figure_8.4_B18722.jpg" alt="Figure 8.4 – Postman response to blocked v2"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Postman response to blocked v2</p>
			<p>We can see that our version guard is now working. This also means that we must use the React application to access the frontend views or you can add a <strong class="source-inline">v1</strong> to the frontend <span class="No-Break">API endpoints.</span></p>
			<p>Now, if we run our <a id="_idIndexMarker830"/>app, we can see that our frontend works with the new endpoints. With this, we are one step closer to developing a RESTful API for our app. However, we still have some glaring shortcomings. Right now, we can create another user and log in under that user. In the next section, we’ll explore how to manage our user state in a <span class="No-Break">stateless fashion.</span></p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor174"/>Implementing statelessness</h1>
			<p>Statelessness is <a id="_idIndexMarker831"/>where the server does not store any information about the client session. The advantages here are straightforward. It enables our application to scale more easily as we free up resources on the server side by storing session information on the client’s <span class="No-Break">side instead.</span></p>
			<p>It also empowers us to be more flexible with our computing approach. For instance, let’s say that our application has exploded in popularity. As a result, we may want to spin our app up on two computing instances or servers and have a load balancer direct traffic to both instances in a balanced manner. If information is stored on the server, the user will have an <span class="No-Break">inconsistent experience.</span></p>
			<p>They may update<a id="_idIndexMarker832"/> the state of their session on one computing instance, but then, when they make another request, they may hit another computing instance that has outdated data. Considering this, statelessness cannot just be achieved by storing everything in the client. If our database is not dependent on a computing instance of our app, we can also store our data on this database, as shown in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/Figure_8.5_B18722.jpg" alt="Figure 8.5 – Our stateless approach"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Our stateless approach</p>
			<p>As you can see, our app is already stateless. It stores the user ID in a JWT in the frontend and we store our user data models and to-do items in our PostgreSQL database. However, we might want to store Rust structs in our application. For instance, we could build a struct that counts the number of requests hitting the server. With reference to <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.5</em>, we cannot just save our structs locally on the server. Instead, we will store our structs in Redis, carrying out the processes in the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/Figure_8.6_B18722.jpg" alt="Figure 8.6 – Steps to saving structs in Redis"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Steps to saving structs in Redis</p>
			<p class="callout-heading">The difference between PostgreSQL and Redis</p>
			<p class="callout">Redis<a id="_idIndexMarker833"/> is a database, but it is different from a PostgreSQL database. Redis is closer to a key-value store. Redis is also fast due to the data being in memory. While Redis is not as complete as PostgreSQL<a id="_idIndexMarker834"/> for managing tables and how they relate to each other, Redis does have advantages. Redis supports useful data structures such as lists, sets, hashes, queues, and channels. You can also set expiry times on the data that you insert into Redis. You also do not need to handle data migrations with Redis. This makes Redis an ideal database for caching data that you need quick access to, but you are not too concerned about persistence. For the channels and queues, Redis is also ideal for facilitating communications between subscribers <span class="No-Break">and publishers.</span></p>
			<p>We can achieve<a id="_idIndexMarker835"/> the process in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.6</em> by carrying out the <span class="No-Break">following steps:</span></p>
			<ol>
				<li value="1">Define Redis service <span class="No-Break">for Docker.</span></li>
				<li>Update <span class="No-Break">Rust dependencies.</span></li>
				<li>Update our config file for a <span class="No-Break">Redis connection.</span></li>
				<li>Build a counter struct that can be saved and loaded in a <span class="No-Break">Redis database.</span></li>
				<li>Implement the counter for <span class="No-Break">each request.</span></li>
			</ol>
			<p>Let’s go over <a id="_idIndexMarker836"/>each step <span class="No-Break">in detail:</span></p>
			<ol>
				<li value="1">When it comes to spinning up a Redis Docker service, we need to use a standard Redis container with standard ports. After we have implemented our Redis service, our <strong class="source-inline">docker-compose.yml</strong> file should have the <span class="No-Break">current state:</span><pre class="source-code">
version: "3.7"</pre><pre class="source-code">
services:</pre><pre class="source-code">
  postgres:</pre><pre class="source-code">
    container_name: 'to-do-postgres'</pre><pre class="source-code">
    image: 'postgres:11.2'</pre><pre class="source-code">
    restart: always</pre><pre class="source-code">
    ports:</pre><pre class="source-code">
      - '5433:5432'</pre><pre class="source-code">
    environment:</pre><pre class="source-code">
      - 'POSTGRES_USER=username'</pre><pre class="source-code">
      - 'POSTGRES_DB=to_do'</pre><pre class="source-code">
      - 'POSTGRES_PASSWORD=password'</pre><pre class="source-code">
  redis:</pre><pre class="source-code">
      container_name: 'to-do-redis'</pre><pre class="source-code">
      image: 'redis:5.0.5'</pre><pre class="source-code">
      ports:</pre><pre class="source-code">
        - '6379:6379'</pre></li>
			</ol>
			<p>We can see that we now have the Redis service and the database service running on the local machine. Now that Redis can be run, we need to update our dependencies in the <span class="No-Break">next step.</span></p>
			<ol>
				<li value="2">Recalling <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.6</em>, we need to serialize the Rust struct into bytes before inserting it into Redis. With these steps in mind, we need the following dependencies in our <span class="No-Break"><strong class="source-inline">Cargo.toml</strong></span><span class="No-Break"> file:</span><pre class="source-code">
[dependencies]</pre><pre class="source-code">
. . .</pre><pre class="source-code">
redis = "0.21.5"</pre></li>
			</ol>
			<p>We are using the <strong class="source-inline">redis</strong> crate to connect to the Redis database. Now that our dependencies are defined, we can start defining our config file in the <span class="No-Break">next step.</span></p>
			<ol>
				<li value="3">When it comes to our <strong class="source-inline">config.yml</strong> file, we must add the URL for the Redis database connection. At this point in time of the book, our <strong class="source-inline">config.yml</strong> file should have <a id="_idIndexMarker837"/>the <span class="No-Break">following form:</span><pre class="source-code">
DB_URL: postgres://username:password@localhost:5433/to_do</pre><pre class="source-code">
SECRET_KEY: secret</pre><pre class="source-code">
EXPIRE_MINUTES: 120</pre><pre class="source-code">
REDIS_URL: redis://127.0.0.1/</pre></li>
			</ol>
			<p>We have not added the port number for our <strong class="source-inline">REDIS_URL</strong> parameter. This is because we are using the standard port in our Redis service, which is <strong class="source-inline">6379</strong>, so we do not have to define the port. We now have all the data ready to define a struct that can connect to Redis, which we will do in the <span class="No-Break">next step.</span></p>
			<ol>
				<li value="4">We will define our <strong class="source-inline">Counter</strong> struct in the <strong class="source-inline">src/counter.rs</strong> file. First, we will have to import <span class="No-Break">the following:</span><pre class="source-code">
use serde::{Deserialize, Serialize};</pre><pre class="source-code">
use crate::config::Config;</pre></li>
				<li>We will be using the <strong class="source-inline">Config</strong> instance to get the Redis URL, and the <strong class="source-inline">Deserialize</strong> and <strong class="source-inline">Serialize</strong> traits to enable conversion to bytes. Our <strong class="source-inline">Counter</strong> struct takes the <span class="No-Break">following form:</span><pre class="source-code">
#[derive(Serialize, Deserialize, Debug)]</pre><pre class="source-code">
pub struct Counter {</pre><pre class="source-code">
    pub count: i32</pre><pre class="source-code">
}</pre></li>
				<li>Now that we have our <strong class="source-inline">Counter</strong> struct defined with all the traits, we need to define the<a id="_idIndexMarker838"/> functions that are required for our operations with the <span class="No-Break">following code:</span><pre class="source-code">
impl Counter {</pre><pre class="source-code">
    fn get_redis_url() -&gt; String {</pre><pre class="source-code">
        . . .</pre><pre class="source-code">
    }</pre><pre class="source-code">
    pub fn save(self) {</pre><pre class="source-code">
        . . .</pre><pre class="source-code">
    }</pre><pre class="source-code">
    pub fn load() -&gt; Counter {</pre><pre class="source-code">
        . . .</pre><pre class="source-code">
    }</pre><pre class="source-code">
}</pre></li>
				<li>With the preceding functions defined, we can load and save our <strong class="source-inline">Counter</strong> struct into the Redis database. When it comes to building our <strong class="source-inline">get_redis_url</strong> function, it should be no surprise that it can take the <span class="No-Break">following form:</span><pre class="source-code">
fn get_redis_url() -&gt; String {</pre><pre class="source-code">
    let config = Config::new();</pre><pre class="source-code">
    config.map.get("REDIS_URL")</pre><pre class="source-code">
              .unwrap().as_str()</pre><pre class="source-code">
              .unwrap().to_owned()</pre><pre class="source-code">
}</pre></li>
				<li>Now that we have the Redis URL, we can save our <strong class="source-inline">Counter</strong> struct with the <span class="No-Break">following </span><span class="No-Break"><a id="_idIndexMarker839"/></span><span class="No-Break">code:</span><pre class="source-code">
pub fn save(self) -&gt; Result&lt;(), redis::RedisError&gt; {</pre><pre class="source-code">
    let serialized = serde_yaml::to_vec(&amp;self).unwrap();</pre><pre class="source-code">
    let client = match redis::Client::open(</pre><pre class="source-code">
                     Counter::get_redis_url()) {</pre><pre class="source-code">
        Ok(client) =&gt; client,</pre><pre class="source-code">
        Err(error) =&gt; return Err(error)</pre><pre class="source-code">
    };</pre><pre class="source-code">
    let mut con = match client.get_connection() {</pre><pre class="source-code">
        Ok(con) =&gt; con,</pre><pre class="source-code">
        Err(error) =&gt; return Err(error)</pre><pre class="source-code">
    };</pre><pre class="source-code">
    match redis::cmd("SET").arg("COUNTER")</pre><pre class="source-code">
                           .arg(serialized)</pre><pre class="source-code">
                           .query::&lt;Vec&lt;u8&gt;&gt;(&amp;mut con) {</pre><pre class="source-code">
        Ok(_) =&gt; Ok(()),</pre><pre class="source-code">
        Err(error) =&gt; Err(error)</pre><pre class="source-code">
    }</pre><pre class="source-code">
}</pre></li>
				<li>Here, we can see that we can serialize our <strong class="source-inline">Counter</strong> struct to a <strong class="source-inline">Vec&lt;u8&gt;</strong>. We will then define the client for the Redis and insert our serialized <strong class="source-inline">Counter</strong> struct under the key <strong class="source-inline">"COUNTER"</strong>. There are more features to Redis, however, you can utilize Redis for this chapter by thinking about Redis as being a big scalable in-memory hashmap. With the hashmap concept in mind, how do you think we could get the <strong class="source-inline">Counter</strong> struct from the Redis database? You probably guessed it; we <a id="_idIndexMarker840"/>use the <strong class="source-inline">GET</strong> command with the <strong class="source-inline">"COUNTER"</strong> key and then deserialize it with the <span class="No-Break">following code:</span><pre class="source-code">
pub fn load() -&gt; Result&lt;Counter, redis::RedisError&gt; {</pre><pre class="source-code">
    let client = match redis::Client::open(</pre><pre class="source-code">
                     Counter::get_redis_url()){</pre><pre class="source-code">
        Ok(client) =&gt; client,</pre><pre class="source-code">
        Err(error) =&gt; return Err(error)</pre><pre class="source-code">
    };</pre><pre class="source-code">
    let mut con = match client.get_connection() {</pre><pre class="source-code">
        Ok(con) =&gt; con,</pre><pre class="source-code">
        Err(error) =&gt; return Err(error)</pre><pre class="source-code">
    };</pre><pre class="source-code">
    let byte_data: Vec&lt;u8&gt; = match redis::cmd("GET")</pre><pre class="source-code">
                                 .arg("COUNTER")</pre><pre class="source-code">
                                 .query(&amp;mut con) {</pre><pre class="source-code">
        Ok(data) =&gt; data,</pre><pre class="source-code">
        Err(error) =&gt; return Err(error)</pre><pre class="source-code">
    };</pre><pre class="source-code">
    Ok(serde_yaml::from_slice(&amp;byte_data).unwrap())</pre><pre class="source-code">
}</pre></li>
			</ol>
			<p>We have now defined our <strong class="source-inline">Counter</strong> struct. We have everything in-line to implement in our <strong class="source-inline">main.rs</strong> file in the <span class="No-Break">next step.</span></p>
			<ol>
				<li value="10">When it comes to increasing the count by one every time a request comes in, we need to<a id="_idIndexMarker841"/> carry out the following code in the <span class="No-Break"><strong class="source-inline">main.rs</strong></span><span class="No-Break"> file:</span><pre class="source-code">
. . .</pre><pre class="source-code">
mod counter;</pre><pre class="source-code">
. . .</pre><pre class="source-code">
#[actix_web::main]</pre><pre class="source-code">
async fn main() -&gt; std::io::Result&lt;()&gt; {</pre><pre class="source-code">
    . . .</pre><pre class="source-code">
    let site_counter = counter::Counter{count: 0};</pre><pre class="source-code">
    site_counter.save();</pre><pre class="source-code">
    HttpServer::new(|| {</pre><pre class="source-code">
        . . .</pre><pre class="source-code">
        let app = App::new()</pre><pre class="source-code">
            .wrap_fn(|req, srv|{</pre><pre class="source-code">
                let passed: bool;</pre><pre class="source-code">
                let mut site_counter = counter::</pre><pre class="source-code">
                                       Counter::load()</pre><pre class="source-code">
                                       .unwrap();</pre><pre class="source-code">
                site_counter.count += 1;</pre><pre class="source-code">
                println!("{:?}", &amp;site_counter);</pre><pre class="source-code">
                site_counter.save();</pre><pre class="source-code">
                . . .</pre></li>
				<li>Here, we can see that we define the <strong class="source-inline">counter</strong> module. Before we spin up the server, we need to create the new <strong class="source-inline">Counter</strong> struct and insert it into Redis. We then get the <strong class="source-inline">Counter</strong> from Redis, increase the count, then save it for <span class="No-Break">every request.</span></li>
			</ol>
			<p>Now when we run our server, we can see that our counter is increasing every time we hit our server with a request. Our printout should look like <span class="No-Break">the following:</span></p>
			<pre class="console">
Counter { count: 1 }
Counter { count: 2 }
Counter { count: 3 }
Counter { count: 4 }</pre>
			<p>Now that we have <a id="_idIndexMarker842"/>integrated another storage option, our app essentially functions the way we want it to. If we wanted to ship our application now, there is nothing really stopping us from configuring the build with Docker and deploying it on a server with a database <a id="_idIndexMarker843"/><span class="No-Break">and </span><span class="No-Break"><strong class="bold">NGINX</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Concurrency issues</p>
			<p class="callout">If two servers request the counter at the same time, there is a risk of missing a request. The counter example was explored to demonstrate how to store serialized structs in Redis. If you need to implement a simple counter in a Redis database and concurrency is a concern, it is suggested that you use the <strong class="source-inline">INCR</strong> command. The <strong class="source-inline">INCR</strong> command increases the number under the key you select by one in the Redis database, returning the new increased number as a result. Seeing as the counter is increased in the Redis database, we have reduced the risk of <span class="No-Break">concurrency issues.</span></p>
			<p>However, there are always things we can add. In the next section, we’ll investigate <span class="No-Break">logging requests.</span></p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor175"/>Logging our server traffic</h1>
			<p>So far, our application does not log <a id="_idIndexMarker844"/>anything. This does not directly affect the running of the app. However, there are some advantages to logging. Logging enables us to debug <span class="No-Break">our applications.</span></p>
			<p>Right now, as we are developing locally, it may not seem like logging is really needed. However, in a production environment, there are many reasons why an application can fail, including Docker container orchestration issues. Logs that note what processes have happened can help us to pinpoint an error. We can also use logging to see when edge cases and errors arise for us to monitor the general health of our application. When it comes to logging, there are four types of logs that we <span class="No-Break">can build:</span></p>
			<ul>
				<li><strong class="bold">Informational (info)</strong>: This is<a id="_idIndexMarker845"/> general logging. If we want to track a general process and how it is progressing, we use this type of log. Examples of using this are starting and stopping the server and logging certain checkpoints that we want to monitor, such as <span class="No-Break">HTTP requests.</span></li>
				<li><strong class="bold">Verbose</strong>: This <a id="_idIndexMarker846"/>is information such as the type defined in the previous point. However, it is more granular to inform us of a more detailed flow of a process. This type of log is mainly used for debugging purposes and should generally be avoided when it comes to <span class="No-Break">production settings.</span></li>
				<li><strong class="bold">Warning</strong>: We use this type <a id="_idIndexMarker847"/>when we are logging a process that is failing and should not be ignored. However, we can use this instead of raising an error because we do not want the service to be interrupted or the user to be aware of the specific error. The logs themselves are for us to be alerted of the problem to allow us to then act. Problems such as calls to another server failing are appropriate for <span class="No-Break">this category.</span></li>
				<li><strong class="bold">Error</strong>: This is where the <a id="_idIndexMarker848"/>process is interrupted due to an error and we need to sort it out as quickly as possible. We also need to inform the user that the transaction did not go through. A good example of this is a failure to connect or insert data into a database. If this happens, there is no record of the transaction happening and it cannot be solved retroactively. However, it should be noted that the process can <span class="No-Break">continue running.</span></li>
			</ul>
			<p>If a warning comes up about<a id="_idIndexMarker849"/> the server failing to send an email, connect to another server to dispatch a product for shipping, and so on. Once we have sorted out the problem, we can retroactively make a database call to transactions in this timeframe and make the calls to the server with the <span class="No-Break">right information.</span></p>
			<p>In the worst case, there will be a delay. With the error type, we will not be able to make the database call as the server was interrupted by an error before the order was even entered in the database. Considering this, it is clear why error logging is highly critical, as the user needs to be informed that there is a problem and their transaction did not go through, prompting them to try <span class="No-Break">again later.</span></p>
			<p>We could consider the option of including enough information in the error logs to retroactively go back and update the database and complete the rest of the process when the issue is resolved, removing the need to inform the user. While this is tempting, we must consider two things. Log data is <span class="No-Break">generally unstructured.</span></p>
			<p>There is no quality control for what goes into a log. Therefore, once we have finally managed to manipulate the log data into the right format, there is still a chance that corrupt data could find its way into <span class="No-Break">the database.</span></p>
			<p>The second issue is that logs are not considered secure. They get copied and sent to other developers in a crisis and they can be plugged into other pipelines and websites, such as <em class="italic">Bugsnag</em>, to<a id="_idIndexMarker850"/> monitor logs. Considering the nature of logs, it is not good practice to have any identifiable information in <span class="No-Break">a log.</span></p>
			<p>Now that we have understood the uses of logging, we can start configuring our own logger. When it comes to logging, we are going to use the Actix-web logger. This gives us flexibility on what we log while having the underlying mechanics of logging configured and working well with our Actix server. To build our logger, we must define a new crate in our <strong class="source-inline">Cargo.toml</strong> file with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
[dependencies]
. . .
env_logger = "0.9.0"</pre>
			<p>This enables us to configure <a id="_idIndexMarker851"/>a logger using environment variables. We can now focus on <strong class="source-inline">main.rs</strong> as this is where our logger is configured and used. First, we will import our logger with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
use actix_web::{. . ., middleware::Logger};</pre>
			<p>With this import, we can define our logger in the <strong class="source-inline">main</strong> function with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
. . .
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    . . .
    env_logger::init_from_env(env_logger::Env::new()
                              .default_filter_or("info"));
. . .</pre>
			<p>Here, we are stating that our logger is going to log information to the info stream. With the logger configured, we can then wrap our server with the logger using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
. . .
        async move {
            let result = end_result.await?;
            Ok(result)
        }
}).configure(views::views_factory).wrap(cors)
    .wrap(Logger::new("%a %{User-Agent}i %r %s %D"));
return app
. . .</pre>
			<p>We can see in our logger<a id="_idIndexMarker852"/> that we passed in the <strong class="source-inline">"&amp;a %{User-Agent}I %r %s %D"</strong> string. This string is interpreted by the logger, tell them what to log. The Actix logger can take the <span class="No-Break">following inputs:</span></p>
			<ul>
				<li><strong class="source-inline">%%</strong>: The <span class="No-Break">percent sign</span></li>
				<li><strong class="source-inline">%a</strong>: The remote IP address (IP address of proxy if using <span class="No-Break">reverse proxy)</span></li>
				<li><strong class="source-inline">%t</strong>: The time when the request was started <span class="No-Break">to process</span></li>
				<li><strong class="source-inline">%P</strong>: The process ID of the child that serviced <span class="No-Break">the request</span></li>
				<li><strong class="source-inline">%r</strong>: The first line <span class="No-Break">of request</span></li>
				<li><strong class="source-inline">%s</strong>: The response <span class="No-Break">status code</span></li>
				<li><strong class="source-inline">%b</strong>: The size of the response in bytes, including <span class="No-Break">HTTP headers</span></li>
				<li><strong class="source-inline">%T</strong>: The time taken to serve the request, in seconds with floating fraction in .<span class="No-Break">06f format</span></li>
				<li><strong class="source-inline">%D</strong>: The time taken to serve the request, <span class="No-Break">in milliseconds</span></li>
				<li><strong class="source-inline">%{</strong><span class="No-Break"><strong class="source-inline">FOO}i</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">request.headers['FOO']</strong></span></li>
				<li><strong class="source-inline">%{</strong><span class="No-Break"><strong class="source-inline">FOO}o</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">response.headers['FOO']</strong></span></li>
				<li><strong class="source-inline">%{</strong><span class="No-Break"><strong class="source-inline">FOO}e</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">os.environ['FOO']</strong></span></li>
			</ul>
			<p>With these inputs, we can work out that we are going to log the remote IP address, user-agent, endpoint of the request, and the time taken to process the request. We will do this for every request that our Rust server has. Starting our Rust server with the logging gives us the <span class="No-Break">following output:</span></p>
			<pre class="console">
[2022-05-25T17:22:32Z INFO  actix_server::builder] Starting 8 workers
[2022-05-25T17:22:32Z INFO  actix_server::server] Actix runtime found; starting in Actix runtime</pre>
			<p>Here, we can see that the time of the starting of the server with the number of workers is automatically logged. Then, if we start our frontend, we should be prompted to log in as the token should have expired by now. A full standard request log should look like the <span class="No-Break">following output:</span></p>
			<pre class="console">
[2022-05-25T17:14:56Z INFO  actix_web::middleware::logger]
127.0.0.1 Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)
AppleWebKit/537.36 (KHTML, like Gecko)
Chrome/101.0.4951.64 Safari/537.36
GET /v1/item/get HTTP/1.1 401 9.466000</pre>
			<p>We can see the time, the fact that it is an <strong class="source-inline">INFO</strong>-level log, and what logger is logging it. We can also see my IP address, which is local as I am running my application locally, my computer/browser details, and the API call with a <strong class="source-inline">401</strong> response code. If we trim everything from the request log apart from the method, API endpoint, response code, and response time, our login<a id="_idIndexMarker853"/> prompt will look like <span class="No-Break">the following:</span></p>
			<pre class="console">
GET /v1/item/get HTTP/1.1 401 9.466000
OPTIONS /v1/auth/login HTTP/1.1 200 0.254000
POST /v1/auth/login HTTP/1.1 200 1405.585000
OPTIONS /v1/item/get HTTP/1.1 200 0.082000
GET /v1/item/get HTTP/1.1 200 15.470000</pre>
			<p>We can see that I fail in getting items, with an unauthorized response. I then log in and get an <strong class="source-inline">OK</strong> response from the login. Here, we can see an <strong class="source-inline">OPTIONS</strong> and <strong class="source-inline">POST</strong> method. The <strong class="source-inline">OPTIONS</strong> method is for our CORS, which is why the <strong class="source-inline">OPTIONS</strong> calls processing time is a fraction of other API requests. We can see that we then get our items that are then rendered to the page. However, we can see what happens in the following logs when we refresh <span class="No-Break">the page:</span></p>
			<pre class="console">
OPTIONS /v1/item/get HTTP/1.1 200 0.251000
OPTIONS /v1/item/get HTTP/1.1 200 0.367000
GET /v1/item/get HTTP/1.1 200 88.317000
GET /v1/item/get HTTP/1.1 200 9.523000</pre>
			<p>We can see that there are <a id="_idIndexMarker854"/>two <strong class="source-inline">GET</strong> requests for the items. However, we have not altered the to-do items in the database. This is not an error, but it is wasteful. To optimize this, we can utilize the REST constraint of caching in the <span class="No-Break">following section.</span></p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor176"/>Caching</h1>
			<p>Caching<a id="_idIndexMarker855"/> is where we store data in the frontend to be reused. This enables us to reduce the number of API calls to the backend and reduce latency. Because the benefits are so clear, it can be tempting to cache everything. However, there are some things <span class="No-Break">to consider.</span></p>
			<p>Concurrency<a id="_idIndexMarker856"/> is a clear issue. The data could be outdated, leading to confusion and data corruption when sending the wrong information to the backend. There are also security concerns. If one user logs out and another user logs in on the same computer, there is a risk that the second user will be able to access the first user’s items. With this, there must be a couple of checks in place. The correct user needs to be logged in and the data needs to be timestamped so that if the cached data is accessed past a certain period, a <strong class="source-inline">GET</strong> request is made to refresh <span class="No-Break">the data.</span></p>
			<p>Our application is fairly locked down. We cannot access anything unless we are logged in. The main process that we could cache in our application is the <strong class="source-inline">GET</strong> items call. All other calls that edit the state of the item list in the backend return the updated items. Considering this, our caching mechanism looks like <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/Figure_8.7_B18722.jpg" alt="Figure 8.7 – Our caching approach"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Our caching approach</p>
			<p>The loop in the diagram<a id="_idIndexMarker857"/> can be executed as many times as we want when refreshing the page. However, this might not be a good idea. If a user logs onto our application on their phone when they are in the kitchen to update the list, then the user will have a problem when they go back to their computer to do some work, refreshing the page on the computer and updating the list. This caching system would expose the user to out-of-date data that will be sent to the backend. We can reduce the risk of this happening by referencing the time stamp. When the timestamp is older than the cutoff, we will then make another API call to refresh the data when the user refreshes <span class="No-Break">the page.</span></p>
			<p>When it comes to our caching logic, it will all be implemented in the <strong class="source-inline">front_end/src/App.js</strong> file under the <strong class="source-inline">getItems</strong> function. Our <strong class="source-inline">getItems</strong> function will take the <span class="No-Break">following layout:</span></p>
			<pre class="source-code">
getItems() {
  . . .
  if (difference &lt;= 120) {
      . . .
  }
  else {
      axios.get("http://127.0.0.1:8000/v1/item/get",
          {headers: {"token": localStorage
                      .getItem("token")}}).then(response =&gt; {
              . . .
              })
          }).catch(error =&gt; {
          if (error.response.status === 401) {
              this.logout();
          }
      });
  }
}</pre>
			<p>Here, we state that the<a id="_idIndexMarker858"/> time difference between the last cached items and the current time must be less than 120 seconds, which is 2 minutes. If the time difference is below 2 minutes, we will get our data from the cache. However, if the time difference is above 2 minutes, then we make a request to our API backend. If we get an unauthorized response, we will then log out. First, in this <strong class="source-inline">getItems</strong> function, we get the date that the items were cached and calculate the difference between then and now, using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
let cachedData = Date.parse(localStorage
                            .getItem("item-cache-date"));
let now = new Date();
let difference = Math.round((now - cachedData) / (1000));</pre>
			<p>If our time difference is 2 minutes, we will get our data from the local storage and update our state with that data with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
let pendingItems =
    JSON.parse(localStorage.getItem("item-cache-data-pending"));
let doneItems =
    JSON.parse(localStorage.getItem("item-cache-data-
                                     done"));
let pendingItemsCount = pendingItems.length;
let doneItemsCount = doneItems.length;
this.setState({
  "pending_items": this.processItemValues(pendingItems),
  "done_items": this.processItemValues(doneItems),
  "pending_items_count": pendingItemsCount,
  "done_items_count": doneItemsCount
})</pre>
			<p>Here, we must parse the data from the local storage because the local storage only handles string data. Because local storage only handles strings, we must stringify the data that we are <a id="_idIndexMarker859"/>inserting into the local storage when we make the API request with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
let pending_items = response.data["pending_items"]
let done_items = response.data["done_items"]
localStorage.setItem("item-cache-date", new Date());
localStorage.setItem("item-cache-data-pending",
                      JSON.stringify(pending_items));
localStorage.setItem("item-cache-data-done",
                      JSON.stringify(done_items));
this.setState({
  "pending_items": this.processItemValues(pending_items),
  "done_items": this.processItemValues(done_items),
  "pending_items_count":
      response.data["pending_item_count"],
  "done_items_count": response.data["done_item_count"]
})</pre>
			<p>If we run our application, we only make one API call. If we refresh our application before the 2 minutes are up, we can see that there are no new API calls despite our frontend rendering all the items from the cache. However, if we create, edit, or delete an item, and then refresh the page before the 2 minutes is up, we will see that our view will revert to the previous out-of-date state. This is because the created, edited, and deleted items also return to their previous state but they are not stored in the local storage. This can be handled by <a id="_idIndexMarker860"/>updating our <strong class="source-inline">handleReturnedState</strong> function with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
handleReturnedState = (response) =&gt; {
  let pending_items = response.data["pending_items"]
  let done_items = response.data["done_items"]
  localStorage.setItem("item-cache-date", new Date());
  localStorage.setItem("item-cache-data-pending",
                        JSON.stringify(pending_items));
  localStorage.setItem("item-cache-data-done",
                        JSON.stringify(done_items));
  this.setState({
     "pending_items":this.processItemValues(pending_items),
     "done_items": this.processItemValues(done_items),
     "pending_items_count":response
         .data["pending_item_count"],
      "done_items_count": response.data["done_item_count"]
  })
}</pre>
			<p>Here, we have it. We have managed to cache our data and reuse it to prevent our backend API from being hit excessively. This can be applied to other frontend processes too. For instance, a customer basket could be cached and used when the user <span class="No-Break">checks out.</span></p>
			<p>This takes our<a id="_idIndexMarker861"/> simple website one step closer to being a web app. However, we must acknowledge that as we use caching more, the complexity of the frontend increases. For our application, this is where the caching stops. Right now, there are no more alterations needed on our applications for the rest of the hour. However, there is one more concept that we should briefly cover, which is code <span class="No-Break">on demand.</span></p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor177"/>Code on demand</h1>
			<p>Code on demand <a id="_idIndexMarker862"/>is where the backend server directly executes code on the frontend. This constraint is optional and not widely used. However, it can be useful as it gives the backend server the right to decide as and when code is executed on the frontend. We have already been doing this; in our logout view, we directly execute JavaScript on the frontend by simply returning it in a string. This is done in the <strong class="source-inline">src/views/auth/logout.rs</strong> file. We must remember that we have now added to-do items to our local storage. If we do not remove these items from our local storage when logging out, somebody else would be able to access our to-do items if they manage to log in to their own account on the same computer within 2 minutes. While this is highly unlikely, we might as well be safe. Remember that our logout view in the <strong class="source-inline">src/views/auth/logout.rs</strong> file takes the <span class="No-Break">following form:</span></p>
			<pre class="source-code">
pub async fn logout() -&gt; HttpResponse {
    HttpResponse::Ok()
        .content_type("text/html; charset=utf-8")
        .body(. . .)
}</pre>
			<p>Inside the body of our<a id="_idIndexMarker863"/> response, we have the <span class="No-Break">following content:</span></p>
			<pre class="source-code">
"&lt;html&gt;\
&lt;script&gt;\
    localStorage.removeItem('user-token'); \
    localStorage.removeItem('item-cache-date'); \
    localStorage.removeItem('item-cache-data-pending'); \
    localStorage.removeItem('item-cache-data-done'); \
    window.location.replace(
        document.location.origin);\
&lt;/script&gt;\
&lt;/html&gt;"</pre>
			<p>With this, we have not only removed the user token, but we have also removed all items and dates. With this, our data is safe once we have <span class="No-Break">logged out.</span></p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor178"/>Summary</h1>
			<p>In this chapter, we have gone through the different aspects of RESTful design and implemented them into our application. We have assessed the layers of our application, enabling us to refactor the middleware to enable two different futures to be processed depending on the outcome. This doesn’t just stop at authorizing requests. Based on the parameters of the request, we could implement middleware to redirect requests to other servers, or directly respond with a code-on-demand response that makes some changes to the frontend and then makes another API call. This approach gives us another tool, custom logic with multiple future outcomes in the middleware before the view <span class="No-Break">is hit.</span></p>
			<p>We then refactored our path struct to make the interface uniform, preventing clashes between frontend and <span class="No-Break">backend views.</span></p>
			<p>We then explored the different levels of logging and logged all our requests to highlight silent yet undesirable behavior. After refactoring our frontend to rectify this, we then used our logging to assess whether our caching mechanism was working correctly when caching to-do items into the frontend to prevent excessive API calls. Now, our application is passable. We can always make improvements; however, we are not at the stage where if we were to deploy our application onto a server, we would be able to monitor it, check the logs when something is going wrong, manage multiple users with their own to-do lists, and reject unauthorized requests before they even hit the view. We also have caching, and our application is stateless, accessing and writing data on a PostgreSQL and <span class="No-Break">Redis database.</span></p>
			<p>In the next chapter, we will be writing unit tests for our Rust structs and functional tests for our API endpoints, as well as cleaning the code up ready <span class="No-Break">for deployment.</span></p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor179"/>Questions</h1>
			<ol>
				<li value="1">Why can we not simply code multiple futures into the middleware and merely call and return the one that is right considering request parameters and authorization outcomes, but must wrap them in an <span class="No-Break">enum instead?</span></li>
				<li>How do we add a new version of views but still support the old views in case our API is serving mobile apps and third parties that might not <span class="No-Break">update instantly?</span></li>
				<li>Why is the stateless constraint becoming more important in the era of elastic <span class="No-Break">cloud computing?</span></li>
				<li>How could we enable another service to be incorporated utilizing the properties of <span class="No-Break">the JWT?</span></li>
				<li>A warning log message hides the fact that an error has happened from the user but still alerts us to fix it. Why do we ever bother telling the user that an error has occurred and to try again with an <span class="No-Break">error log?</span></li>
				<li>What are the advantages of logging <span class="No-Break">all requests?</span></li>
				<li>Why do we sometimes have to use <span class="No-Break"><strong class="source-inline">async move</strong></span><span class="No-Break">?</span></li>
			</ol>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor180"/>Answers</h1>
			<ol>
				<li value="1">Rust’s strong typing system will complain. This is because <strong class="source-inline">async</strong> blocks behave like closures meaning that every <strong class="source-inline">async</strong> block is its own type. Pointing to multiple futures is like pointing to multiple types, and thus it will look like we are returning multiple <span class="No-Break">different types.</span></li>
				<li>We add a new module in the views directory with the new views. These have the same endpoints and views with new parameters that are needed. We can then add a version parameter in the factory function. These new views will have the same endpoints with <strong class="source-inline">v2</strong> in them. This enables users to use the new and old API endpoints. We then notify users when the old version will no longer be supported, giving them time to update. At a specific time, we will move our version in the build to <strong class="source-inline">v2</strong>, cutting all requests that make <strong class="source-inline">v1</strong> calls and responding with a helpful message that <strong class="source-inline">v1</strong> is no longer supported. For this transition to work, we will have to update the allowed versions in the <strong class="source-inline">build</strong> config to a list of <span class="No-Break">supported versions.</span></li>
				<li>With orchestration tools, microservices, and elastic computing instances on demand, spinning up and shutting down elastic computing instances due to demand is becoming more common practice. If we store data on the instance itself, when the user makes another API call, there is no guarantee that the user will hit the same instance, getting inconsistent data read <span class="No-Break">and writes.</span></li>
				<li>The JWT token enables us to store the user ID. If the second service has the same secret key, we can merely pass requests to the other service with the JWT in the header. The other service does not have to have the login views or access to the user database and can <span class="No-Break">still function.</span></li>
				<li>When an error happens that prevents us from retroactively going back and sorting out the issue, then we must raise an error instead of a warning. A classic example of an error is not being able to write to a database. A good example of a warning is another service not responding. When the other service is up and running, we can do a database call and call the service to finish off <span class="No-Break">the process.</span></li>
				<li>In production, it is needed to assess the state of a server when troubleshooting. For instance, if a user is not experiencing an update, we can quickly check the logs to see if the server is in fact receiving the request or if there is an error with the caching in the frontend. We can also use it to see if our app is behaving the way we expect <span class="No-Break">it to.</span></li>
				<li>There could be a possibility that the lifetime of the variable that we are referencing in the <strong class="source-inline">async</strong> block might not live long enough to see the end of the <strong class="source-inline">async</strong> block. To resolve this, we can shift the ownership of the variable to the block with an <strong class="source-inline">async </strong><span class="No-Break"><strong class="source-inline">move</strong></span><span class="No-Break"> block.</span></li>
			</ol>
		</div>
	

		<div id="_idContainer091" class="Content">
			<h1 id="_idParaDest-180"><a id="_idTextAnchor181"/>Part 4:Testing and Deployment</h1>
			<p>When an application is built, we need to deploy it on a server so others can use it. We also need to test it to ensure it works to our expectations before we deploy it. In this part, we will cover unit and end-to-end testing using tools such as Postman. We’ll build our own build and testing pipelines to automate the testing, building, and deployment processes. We’ll cover how HTTP requests route to servers and what the HTTPS protocol is so we can implement it on AWS. We will also route traffic to our frontend and backend with NGINX, balance traffic between two individual servers on AWS, and lock down traffic to these servers and load balancer with AWS security groups. We will automate the AWS infrastructure using Terraform. </p>
			<p>This part includes the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B18722_09.xhtml#_idTextAnchor182"><em class="italic">Chapter 9</em></a>, <em class="italic">Testing Our Application Endpoints and Components</em></li>
				<li><a href="B18722_10.xhtml#_idTextAnchor200"><em class="italic">Chapter 10</em></a>, <em class="italic">Deploying Our Application on AWS</em></li>
				<li><a href="B18722_11.xhtml#_idTextAnchor222"><em class="italic">Chapter 11</em></a>, <em class="italic">Configuring HTTPS with NGINX on AWS</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer092">
			</div>
		</div>
		<div>
			<div id="_idContainer093">
			</div>
		</div>
	</body></html>